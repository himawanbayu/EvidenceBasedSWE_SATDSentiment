project,issue_number,issue_type,text,classification,indicator,zz_created,zz_updated,zz_resolved,zz_duration,zz_text,zz_wink_a,zz_wink_b,roberta
camel,10153,description,"Camel 2.17.x upgraded spring version to 4.x in most of the components. but for camel-cxf component, it still has to use spring-dm and spring version 3.x, the spring version range in the Import-Package should keep [3.2,4), not [4.1,5). Now the ERROR will happen when install camel-cxf feature into karaf container (in case of both Spring 4.x and Spring 3.x are installed in the container) To fix it, make change to the pom.xml",architecture_debt,using_obsolete_technology,"Mon, 18 Jul 2016 06:14:14 +0000","Sat, 20 Aug 2016 13:44:18 +0000","Sat, 20 Aug 2016 13:44:17 +0000",2878203,"Camel 2.17.x upgraded spring version to 4.x in most of the components. but for camel-cxf component, it still has to use spring-dm and spring version 3.x, the spring version range in the Import-Package should keep [3.2,4), not [4.1,5). Now the ERROR will happen when install camel-cxf feature into karaf container (in case of both Spring 4.x and Spring 3.x are installed in the container) To fix it, make change to the pom.xml",-0.01322222222,-0.01322222222,neutral
camel,11734,description,It'd be nice if we could upgrade camel-grpc to use the latest grpc-java library as there are some improvements to how it does class loading:,architecture_debt,using_obsolete_technology,"Fri, 1 Sep 2017 06:17:00 +0000","Fri, 8 Sep 2017 08:49:52 +0000","Fri, 8 Sep 2017 08:49:52 +0000",613972,It'd be nice if we could upgrade camel-grpc to use the latest grpc-java library as there are some improvements to how it does class loading: https://github.com/grpc/grpc-java/releases/tag/v1.6.1,0.675,0.675,positive
camel,11868,description,The current java transport client is due EOL in near future and it will be a good idea we switch the the new high level rest client instead. The new high level rest client is only released from version 5.6.0 and towards so an general upgrade of the dependencies is required for the ELK component. This also add the support for basic authentication without the need of x-pack. This is usefull when running ELK behind a reverse proxy.,architecture_debt,using_obsolete_technology,"Sat, 30 Sep 2017 05:29:19 +0000","Wed, 18 Oct 2017 06:54:13 +0000","Wed, 18 Oct 2017 06:54:13 +0000",1560294,The current java transport client is due EOL in near future and it will be a good idea we switch the the new high level rest client instead. The new high level rest client is only released from version 5.6.0 and towards so an general upgrade of the dependencies is required for the ELK component. https://www.elastic.co/guide/en/elasticsearch/client/java-rest/current/java-rest-high.html This also add the support for basic authentication without the need of x-pack. This is usefull when running ELK behind a reverse proxy. https://www.elastic.co/guide/en/elasticsearch/client/java-rest/current/_basic_authentication.html,0.30025,0.2402,neutral
camel,2535,description,"As we don't use the CxfSoap component any more, it's time to clean it up.",architecture_debt,using_obsolete_technology,"Wed, 10 Mar 2010 10:06:54 +0000","Sun, 24 Apr 2011 10:01:27 +0000","Thu, 11 Mar 2010 09:13:31 +0000",83197,"As we don't use the CxfSoap component any more, it's time to clean it up.",0.4,0.4,neutral
camel,2670,comment_1,"@ Charles I just checked out the example, I think you can put all the camel routes configuration files into a single module and use Profile to start the services one by one.",architecture_debt,violation_of_modularity,"Fri, 23 Apr 2010 08:17:35 +0000","Mon, 26 Apr 2010 07:54:46 +0000","Fri, 23 Apr 2010 08:20:09 +0000",154,"@ Charles I just checked out the example, I think you can put all the camel routes configuration files into a single module and use Profile to start the services one by one.",0,0,neutral
camel,2901,description,HawtDB 1.1 has been released. Change log at: We should upgrade to pick up the listed bug fixes:,architecture_debt,using_obsolete_technology,"Fri, 2 Jul 2010 13:12:52 +0000","Sun, 24 Apr 2011 10:01:39 +0000","Fri, 2 Jul 2010 13:13:18 +0000",26,"HawtDB 1.1 has been released. Change log at: http://github.com/chirino/hawtdb/blob/master/changelog.md We should upgrade to pick up the listed bug fixes: Fixing BTree node next linking.. It was possible that a next link would not properly get set in some conditions during a node removal. You can add get callbacks when a commit gets flushed to disk. Changed the way the journal was handling callback based write completed notifications. They are now delivered in batch form to a single JournalListener. This reduces thread contention and increases throughput. Moved the built in predicate implementations into a Predicates class. Added close method to the Transaction interface. Implementation now asserts it is no longer used after a close. Making the appender's max write batch size configurable. Revamped how Update and DefferedUpdates track shadow pages. A little easier to follow now. - changed the interface to PagedAccessor so that instead of removing the linked pages, it just needs to report what the linked pages are. Got rid of the WriteKey wrapper class, updated logging. Better looking printStrucuture BTree method Added a few Logging classes to reduce the number of places we need to update if in case we decided to switch logging APIs. Fixing free page allocation bug when using deferred updates. Javadoc improvements Expose a config property to control the read cache size. Reworked how snapshot tracking was being done. Fixes errors that occurred during heavy concurrent access. Added a non-blocking flush method to the TxPageFile Read cache was not getting updated when a update batch was performed. Cached entries that were updated and flushed to disk continued returning stale data. Fixed an recovery edge cases Don't start the thread from the thread factory. that causes illegal state exceptions Fixed journal bug where getting next location could return a the current location Renamed EncoderDecoder to PagedAccessor The util.buffer package has moved into it's own project at http://github.com/chirino/hawtbuf Fixes #4 : Errors occur when you re-open an empty data file. Extracted a SortedIndex interface from the Index interface to non sorted indexes having to deal with that leaky abstraction. added a free() method to the Paged for symmetry with the alloc() method. Improved website documentation",0.2625,0.05144230769,neutral
camel,3315,comment_2,Tracy I think the SMX bundle is *wrong* as Jasypt 1.7 got rid of commons-lang and commons-codec (the latter has bugs). So we need an updated SMX bundle which *do not* important/use any of those commons stuff.,architecture_debt,using_obsolete_technology,"Fri, 5 Nov 2010 14:18:01 +0000","Tue, 25 Oct 2011 11:36:22 +0000","Fri, 25 Feb 2011 06:58:03 +0000",9650402,Tracy I think the SMX bundle is wrong as Jasypt 1.7 got rid of commons-lang and commons-codec (the latter has bugs). http://www.jasypt.org/whatsnew17.html So we need an updated SMX bundle which do not important/use any of those commons stuff.,-0.325,-0.325,negative
camel,3315,comment_5,I would like to upgrade to Jasypt 1.7 as we can lose the JDK1.5 stuff and other dependencies which is no longer needed.,architecture_debt,using_obsolete_technology,"Fri, 5 Nov 2010 14:18:01 +0000","Tue, 25 Oct 2011 11:36:22 +0000","Fri, 25 Feb 2011 06:58:03 +0000",9650402,I would like to upgrade to Jasypt 1.7 as we can lose the JDK1.5 stuff and other dependencies which is no longer needed.,0.2625,0.2625,neutral
camel,4132,description,The camel-atom component is using an ancient incubator version of abdera which will make it hard to work with camel-cxf. Updating to 1.1.2 which is what CXF uses would help.,architecture_debt,using_obsolete_technology,"Tue, 21 Jun 2011 19:03:36 +0000","Wed, 22 Jun 2011 14:05:54 +0000","Wed, 22 Jun 2011 14:05:54 +0000",68538,The camel-atom component is using an ancient incubator version of abdera which will make it hard to work with camel-cxf. Updating to 1.1.2 which is what CXF uses would help.,0.3,0.3,negative
camel,4357,description,I am currently looking into the dependencies betwen packages in camel-core. The packages org.apache.camel and form the camel api. So I am trying to make them not depend on other packages from camel-core. One problem there is the starter class Main. It needs access to impl packages as it needs to start camel. So it should not live in org.apache.camel. I propose to move it to To not break anything right now I will create a deprecated class Main in org.apache.camel that extends the moved Main. We can remove the deprecated version in camel 3.0,architecture_debt,violation_of_modularity,"Fri, 19 Aug 2011 13:38:41 +0000","Mon, 22 Aug 2011 17:08:15 +0000","Mon, 22 Aug 2011 13:34:55 +0000",258974,I am currently looking into the dependencies betwen packages in camel-core. The packages org.apache.camel and org.apache.camel.spi form the camel api. So I am trying to make them not depend on other packages from camel-core. One problem there is the starter class Main. It needs access to impl packages as it needs to start camel. So it should not live in org.apache.camel. I propose to move it to org.apache.camel.main. To not break anything right now I will create a deprecated class Main in org.apache.camel that extends the moved Main. We can remove the deprecated version in camel 3.0,-0.08764285714,-0.05842857143,neutral
camel,4543,description,"Camel Blueprint support is limited/hardcoded to Aries. This makes it impossible to use in JBoss 7 or with another blueprint implementation like Gemini. The following classes use various things from Aries. * * * * * Now obviously the last three are related to the custom namespace handler for Aries. It would be good if these were moved into their own module, something like or the like. That people can choose to include if they would like to use the custom blueprint namespace handler in Aries. Otherwise the camel-blueprint module should be implementation agnostic and work on all blueprint containers. Not just Aries.",architecture_debt,violation_of_modularity,"Thu, 13 Oct 2011 16:19:13 +0000","Sun, 1 May 2016 10:43:46 +0000","Sun, 1 May 2016 10:43:46 +0000",143576673,"Camel Blueprint support is limited/hardcoded to Aries. This makes it impossible to use in JBoss 7 or with another blueprint implementation like Gemini. The following classes use various things from Aries. BlueprintContainerRegistry BlueprintPropertiesParser CamelContextFactoryBean CamelProxyFactoryBean CamelNamespaceHandler Now obviously the last three are related to the custom namespace handler for Aries. It would be good if these were moved into their own module, something like camel-aries-namespace or the like. That people can choose to include if they would like to use the custom blueprint namespace handler in Aries. Otherwise the camel-blueprint module should be implementation agnostic and work on all blueprint containers. Not just Aries.",0.1845,0.1845,neutral
camel,4593,description,"There are bunch fixes of ABDERA-281, ABDERA-290 can help camel-atom work smoothly with OSGi platform. We need to upgrade the the abdera to 1.1.3 once it is out.",architecture_debt,using_obsolete_technology,"Fri, 28 Oct 2011 08:59:47 +0000","Sun, 26 May 2013 09:54:16 +0000","Sun, 26 May 2013 09:54:16 +0000",49769669,"There are bunch fixes of ABDERA-281, ABDERA-290 can help camel-atom work smoothly with OSGi platform. We need to upgrade the the abdera to 1.1.3 once it is out.",0.5125,0.5125,positive
camel,7127,description,CXF 2.x uses deprecated class removed in Spring 4. Because of that camel-cxf doesn't work with Spring 4. This issue has been discussed on this (2) forum thread already. (1) (2),architecture_debt,using_obsolete_technology,"Sat, 11 Jan 2014 17:44:01 +0000","Tue, 27 May 2014 07:22:44 +0000","Tue, 27 May 2014 07:22:44 +0000",11713123,CXF 2.x uses deprecated org.springframework.jms.connection.SingleConnectionFactory102 class removed in Spring 4. Because of that camel-cxf doesn't work with Spring 4. This issue has been discussed on this (2) forum thread already. (1) https://builds.apache.org/view/A-D/view/Camel/job/Camel.trunk.fulltest.spring4/lastBuild/org.apache.camel$camel-cxf/testReport/org.apache.camel.component.cxf.jms/CxfEndpointJMSConsumerTest/testInvocation (2) http://camel.465427.n5.nabble.com/Spring-4-update-td5745746.html,-0.12,-0.06666666667,negative
camel,789,description,When downloading artifacts we have a many repos. Several of them are specific for a single/few components. We should move these repo settings to these targeted components and let the uber pom.xml have only the major public maven repos. Maybe some of the repos is out-of-date and not used anymore.,architecture_debt,violation_of_modularity,"Thu, 7 Aug 2008 04:15:15 +0000","Thu, 23 Oct 2008 04:37:12 +0000","Thu, 7 Aug 2008 18:24:53 +0000",50978,When downloading artifacts we have a many repos. Several of them are specific for a single/few components. We should move these repo settings to these targeted components and let the uber pom.xml have only the major public maven repos. Maybe some of the repos is out-of-date and not used anymore.,0,0,neutral
camel,9338,description,"Should be updated to latest release as we are far behind on this one. It may no longer work in OSGi and that is fine, just remove the feature",architecture_debt,using_obsolete_technology,"Thu, 19 Nov 2015 07:20:38 +0000","Thu, 10 Dec 2015 09:32:15 +0000","Thu, 10 Dec 2015 08:12:59 +0000",1817541,"Should be updated to latest release as we are far behind on this one. It may no longer work in OSGi and that is fine, just remove the feature",-0.5,-0.5,negative
camel,9403,summary,camel-examples should not be in BOM,architecture_debt,violation_of_modularity,"Wed, 9 Dec 2015 16:05:59 +0000","Fri, 11 Dec 2015 07:59:44 +0000","Fri, 11 Dec 2015 07:59:29 +0000",143610,camel-examples should not be in BOM,0,0,negative
camel,946,comment_0,There is somekind of routebuilder ref you can add to ref to a single class. However we could consider restrucutre the DSL a bit so the package / routebuilderef or what we call it is in a configuration element. We could move all the jmx and other stuff in that one too. so you have a <configuration> element to start with where we can add all the weird camel stuff. And then have a more clean root element with fewer high level tags to select among.,architecture_debt,violation_of_modularity,"Mon, 29 Sep 2008 12:17:19 +0000","Sat, 21 Nov 2009 11:57:54 +0000","Tue, 14 Apr 2009 09:40:38 +0000",17011399,There is somekind of routebuilder ref you can add to ref to a single class. However we could consider restrucutre the DSL a bit so the package / routebuilderef or what we call it is in a configuration element. We could move all the jmx and other stuff in that one too. so you have a <configuration> element to start with where we can add all the weird camel stuff. And then have a more clean root element with fewer high level tags to select among.,0.0288,0.0288,neutral
camel,3315,description,Jasypt 1.7 has trimmed down on the 3rd party dependencies which means we can get rid of some libraries,build_debt,over-declared_dependencies,"Fri, 5 Nov 2010 14:18:01 +0000","Tue, 25 Oct 2011 11:36:22 +0000","Fri, 25 Feb 2011 06:58:03 +0000",9650402,Jasypt 1.7 has trimmed down on the 3rd party dependencies which means we can get rid of some libraries http://www.jasypt.org/whatsnew17.html,-0.292,-0.292,positive
camel,4331,summary,"Avoid the redundant direct dependency on log4j by the components (of the scope 'test'), as it's transitively given for free through the slf4j-log4j12 dependency with the RIGHT / COMPLIANT version",build_debt,over-declared_dependencies,"Sat, 13 Aug 2011 17:08:06 +0000","Mon, 19 Sep 2011 20:33:04 +0000","Sun, 14 Aug 2011 06:54:34 +0000",49588,"Avoid the redundant direct dependency on log4j by the components (of the scope 'test'), as it's transitively given for free through the slf4j-log4j12 dependency with the RIGHT / COMPLIANT version",0.31925,0.31925,neutral
camel,5342,description,"Package is included/shaded inside the camel-core jar. It is not very nice if is already on the path. It is a deal breaker, if their versions are different. For example cassandra-1.1.1 requires which is missing from the version included in camel. It would be nice if was included as a normal dependency. Comment in the pom.xml says ""Shade the googlecode stuff for OSGi"". Well, if that is strictly required, maybe it could be better included in camel-core-osgi package. In any case, if it must be shaded at all, it would be safer to use relocation property of the maven-shade-plugin. In this case, camel could stay with the version it wants, without conflicting with explicit dependencies.",build_debt,build_others,"Thu, 7 Jun 2012 01:33:39 +0000","Thu, 14 Jun 2012 13:25:01 +0000","Thu, 14 Jun 2012 13:25:01 +0000",647482,"Package com.googlecode.concurrentlinkedhashmap:concurrentlinkedhashmap-lru is included/shaded inside the camel-core jar. It is not very nice if concurrentlinkedhashmap-lru.jar is already on the path. It is a deal breaker, if their versions are different. For example cassandra-1.1.1 requires ConcurrentLinkedHashMap$Builder.maximumWeightedCapacity(), which is missing from the version included in camel. It would be nice if concurrentlinkedhashmap-lru was included as a normal dependency. Comment in the pom.xml says ""Shade the googlecode stuff for OSGi"". Well, if that is strictly required, maybe it could be better included in camel-core-osgi package. In any case, if it must be shaded at all, it would be safer to use relocation property of the maven-shade-plugin. In this case, camel could stay with the version it wants, without conflicting with explicit dependencies.",0.047125,0.03534375,neutral
camel,789,comment_2,"This patch pushes down... well... all of the repositories that were in the root pom lower in the build tree. This should speed up builds quite a bit! :) Note that if you build from a clean repo, the build will fail trying to find the mina ftpserver. This patch does not introduce this failure however. See for more details about this failure.",build_debt,build_others,"Thu, 7 Aug 2008 04:15:15 +0000","Thu, 23 Oct 2008 04:37:12 +0000","Thu, 7 Aug 2008 18:24:53 +0000",50978,"This patch pushes down... well... all of the repositories that were in the root pom lower in the build tree. This should speed up builds quite a bit! Note that if you build from a clean repo, the build will fail trying to find the mina ftpserver. This patch does not introduce this failure however. See http://www.nabble.com/-HEADS-UP--Missing-ftpserver-dependency-td18870393s22882.html for more details about this failure.",-0.01573333333,-0.04406666667,positive
camel,789,summary,pom.xml - maven repositories - clean up,build_debt,build_others,"Thu, 7 Aug 2008 04:15:15 +0000","Thu, 23 Oct 2008 04:37:12 +0000","Thu, 7 Aug 2008 18:24:53 +0000",50978,pom.xml - maven repositories - clean up,0.2,0.2,neutral
camel,9709,description,"Without proper dependency definition, clients may see a variety of versions",build_debt,build_others,"Tue, 15 Mar 2016 12:03:24 +0000","Tue, 15 Mar 2016 13:47:05 +0000","Tue, 15 Mar 2016 13:47:05 +0000",6221,"Without proper dependency definition, clients may see a variety of versions",-0.531,-0.531,negative
camel,9721,comment_1,"Hi Claus I've read your email, it sounds like the new version would not present the issue, anyway, if camel-spring-batch depends only on org.springbatch, that should be the dependency and not camel-spring. I think the pom.xml and features.xml should be aligned. As this project has many, many dependencies, the shorter the dependency chain the better. Does it make sense?",build_debt,under-declared_dependencies,"Thu, 17 Mar 2016 12:19:12 +0000","Thu, 17 Mar 2016 13:21:25 +0000","Thu, 17 Mar 2016 12:46:03 +0000",1611,"Hi Claus I've read your email, it sounds like the new version would not present the issue, anyway, if camel-spring-batch depends only on org.springbatch, that should be the dependency and not camel-spring. I think the pom.xml and features.xml should be aligned. As this project has many, many dependencies, the shorter the dependency chain the better. Does it make sense?",0.07142857143,0.07142857143,neutral
camel,9721,description,"Hi all the camel-spring-batch component does only depend on spring-batch (by pom.xml) but in the karaf features.xml it is said that it depends on camel-spring (which is not correct but in test). So this has a very downside, that drags the deprecated sprin-dm and impossible to run anythign on spring higher than 3.2. And the spring-batch version used by camel (2.16.2 needs spring-batch 3.0.4, that depends on spring 4, which is blocked by adding spring-camel), result, jar hell, and unable to read the XML Namespace errors. Good this is: removing the dependency of camel-spring and adding spring directly, solves the issue and can run spring-batch in any upper version correctly. thanks!",build_debt,build_others,"Thu, 17 Mar 2016 12:19:12 +0000","Thu, 17 Mar 2016 13:21:25 +0000","Thu, 17 Mar 2016 12:46:03 +0000",1611,"Hi all the camel-spring-batch component does only depend on spring-batch (by pom.xml) but in the karaf features.xml it is said that it depends on camel-spring (which is not correct but in test). So this has a very downside, that drags the deprecated sprin-dm and impossible to run anythign on spring higher than 3.2. And the spring-batch version used by camel (2.16.2 needs spring-batch 3.0.4, that depends on spring 4, which is blocked by adding spring-camel), result, jar hell, and unable to read the XML Namespace errors. Good this is: removing the dependency of camel-spring and adding spring directly, solves the issue and can run spring-batch in any upper version correctly. thanks!",-0.1096071429,-0.1096071429,negative
camel,10048,summary,Memory leak in RoutingSlip,code_debt,low_quality_code,"Sat, 11 Jun 2016 21:41:44 +0000","Thu, 4 Jan 2018 09:11:44 +0000","Sun, 12 Jun 2016 09:44:54 +0000",43390,Memory leak in RoutingSlip,-0.2,-0.2,negative
camel,10507,description,"Addressing the issue raised in the review, Jackson TypeReferences should be declared constant.",code_debt,low_quality_code,"Tue, 22 Nov 2016 11:07:21 +0000","Tue, 22 Nov 2016 12:28:26 +0000","Tue, 22 Nov 2016 11:24:34 +0000",1033,"Addressing the issue raised in the PR1265 review, Jackson TypeReferences should be declared constant.",0,0,neutral
camel,10517,summary,Remove unnecessary SuppressWarnings,code_debt,low_quality_code,"Wed, 23 Nov 2016 14:34:56 +0000","Wed, 23 Nov 2016 15:53:21 +0000","Wed, 23 Nov 2016 15:53:19 +0000",4703,Remove unnecessary SuppressWarnings,0,0,negative
camel,10563,comment_0,Need to add better handling for hz instance cleanup,code_debt,low_quality_code,"Tue, 6 Dec 2016 16:23:30 +0000","Tue, 13 Dec 2016 13:20:24 +0000","Tue, 13 Dec 2016 13:20:22 +0000",593812,Need to add better handling for hz instance cleanup,0.5,0.5,neutral
camel,1107,description,"When using the options to configure httpClient using URI option, they should be removed from the uri that is left over to the HTTPProducer. should remove the httpClient.xxx so it's",code_debt,dead_code,"Fri, 21 Nov 2008 10:23:50 +0000","Fri, 31 Jul 2009 06:33:43 +0000","Fri, 21 Nov 2008 18:43:37 +0000",29987,"When using the options to configure httpClient using URI option, they should be removed from the uri that is left over to the HTTPProducer. should remove the httpClient.xxx so it's",0,0,neutral
camel,11104,description,"We have implemented a camel route where we are having camel sftp producer to transfer a files to remote SFTP location but on performance testing on client environment and on our local environment we have observed degradation in the time for transferring files to remote SFTP location. Please find the detailed analysis below. The we tried the various test in our local environment. In each test we put around 22 files on camel file consumer and each file took below time to write the file. PFB details When target directory having 20,000 files. Camel sftp producer took around 1 minute 43 second to a transfer file DEBUG 07:00:38 (Camel thread #6 - INFO 07:00:38 (Camel thread #6 - INFO 07:00:38 (Camel thread #6 - DEBUG 07:00:38 (Camel thread #6 - DEBUG 07:02:19 (Camel thread #6 - DEBUG 07:02:19 (Camel thread #6 - DEBUG 07:02:19 (Camel thread #6 - DEBUG 07:02:20 (Camel thread #6 - DEBUG 07:02:20 (Camel thread #6 - INFO 07:02:20 (Camel thread #6 - When target directory having 40,000 files. Camel sftp producer took around 3 minute 17 second to transfer file DEBUG 07:47:23 (Camel thread #6 - using exchange: INFO 07:47:23 (Camel thread #6 - from IOP FTP directory INFO 07:47:23 (Camel thread #6 - : to ICOMS FTP directory DEBUG 07:47:23 (Camel thread #6 - DEBUG 07:50:40 (Camel thread #6 - DEBUG 07:50:40 (Camel thread #6 - DEBUG 07:50:40 (Camel thread #6 - DEBUG 07:50:41 (Camel thread #6 - DEBUG 07:50:41 (Camel thread #6 - INFO 07:50:41 (Camel thread #6 - Similarly when we achieved the files from target directory. It took around 6 sec.It seems like there is a performance issue with camel sftp component. Does it list the files in target directory which is taking time. PFB the producer route which we set up",code_debt,slow_algorithm,"Tue, 4 Apr 2017 07:32:26 +0000","Tue, 4 Apr 2017 07:49:13 +0000","Tue, 4 Apr 2017 07:35:55 +0000",209,"We have implemented a camel route where we are having camel sftp producer to transfer a files to remote SFTP location but on performance testing on client environment and on our local environment we have observed degradation in the time for transferring files to remote SFTP location. Please find the detailed analysis below. The we tried the various test in our local environment. In each test we put around 22 files on camel file consumer and each file took below time to write the file. PFB details ======================================================================================== When target directory having 20,000 files. Camel sftp producer took around 1 minute 43 second to a transfer file ======================================================================================== DEBUG 07:00:38 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.component.file.FileConsumer> About to process file: GenericFile[/data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1/ST30PERFORPMAJAR17020726.txt] using exchange: Exchange[ST30PERFORPMAJAR17020726.txt] INFO 07:00:38 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) com.sigma.samp.imp.virginmedia.bss.voice.camelroutes.iopRoute.IOPResponseFtpRouteBuilder> Picked IOP response file : ST30PERFORPMAJAR17020726.txt from IOP FTP directory INFO 07:00:38 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) com.sigma.samp.imp.virginmedia.bss.voice.camelroutes.iopRoute.IOPResponseFtpRouteBuilder> Sending IOP response file : ST30PERFORPMAJAR17020726.txt to ICOMS FTP directory DEBUG 07:00:38 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.processor.SendProcessor> >>>> Endpoint[sftp://10.100.150.190/icoms/1?download=false&maxMessagesPerPoll=10&password=xxxxxx&tempPrefix=Q&username=sigmauser] Exchange[ST30PERFORPMAJAR17020726.txt] DEBUG 07:02:19 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.component.file.GenericFileConverter> Read file /data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1/ST30PERFORPMAJAR17020726.txt (no charset) DEBUG 07:02:19 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.component.file.remote.SftpOperations> About to store file: QST30PERFORPMAJAR17020726.txt using stream: java.io.BufferedInputStream@54a89ff5 DEBUG 07:02:19 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.component.file.remote.SftpOperations> Took 0.658 seconds (658 millis) to store file: QST30PERFORPMAJAR17020726.txt and FTP client returned: true DEBUG 07:02:20 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.component.file.remote.RemoteFileProducer> Wrote [icoms/1/QST30PERFORPMAJAR17020726.txt] to [Endpoint[sftp://10.100.150.190/icoms/1?download=false&maxMessagesPerPoll=10&password=xxxxxx&tempPrefix=Q&username=sigmauser]] DEBUG 07:02:20 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.component.file.remote.SftpOperations> Renaming file: icoms/1/QST30PERFORPMAJAR17020726.txt to: icoms/1/ST30PERFORPMAJAR17020726.txt INFO 07:02:20 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) com.sigma.samp.imp.virginmedia.bss.voice.camelroutes.iopRoute.IOPResponseFtpRouteBuilder> IOP response file : ST30PERFORPMAJAR17020726.txt successfully sent to ICOMS FTP directory ======================================================================================= When target directory having 40,000 files. Camel sftp producer took around 3 minute 17 second to transfer file ======================================================================================= DEBUG 07:47:23 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.component.file.FileConsumer> About to process file: GenericFile[/data/users/slvm02/smp53/domains/v mb/Icoms/iop_responses/1/SX30RPMAJAR1702483756.txt] using exchange: Exchange[SX30RPMAJAR1702483756.txt] INFO 07:47:23 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) com.sigma.samp.imp.virginmedia.bss.voice.camelroutes.iopRoute.IOPResponseFtpRouteBuilder> Picked IOP response file : SX30RPMAJAR1702483756.txt from IOP FTP directory INFO 07:47:23 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) com.sigma.samp.imp.virginmedia.bss.voice.camelroutes.iopRoute.IOPResponseFtpRouteBuilder> Sending IOP response file : SX30RPMAJAR1702483756.txt to ICOMS FTP directory DEBUG 07:47:23 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.processor.SendProcessor> >>>> Endpoint[sftp://10.100.150.190/icoms/1?download=false&maxMessagesPer Poll=10&password=xxxxxx&tempPrefix=Q&username=sigmauser] Exchange[SX30RPMAJAR1702483756.txt] DEBUG 07:50:40 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.component.file.GenericFileConverter> Read file /data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1/SX30RPMAJAR1702483756.txt (no charset) DEBUG 07:50:40 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.component.file.remote.SftpOperations> About to store file: QSX30RPMAJAR1702483756.txt using stream: java.io.BufferedInputStream@7954cfa8 DEBUG 07:50:40 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.component.file.remote.SftpOperations> Took 0.659 seconds (659 millis) to store file: QSX30RPMAJAR1702483756.txt and FTP client returned: true DEBUG 07:50:41 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.component.file.remote.RemoteFileProducer> Wrote [icoms/1/QSX30RPMAJAR1702483756.txt] to [Endpoint[sftp://10.100.150.190/icoms/1?download=false&maxMessagesPerPoll=10&password=xxxxxx&tempPrefix=Q&username=sigmauser]] DEBUG 07:50:41 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.component.file.remote.SftpOperations> Renaming file: icoms/1/QSX30RPMAJAR1702483756.txt to: icoms/1/SX30RPMAJAR1702483756.txt INFO 07:50:41 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) com.sigma.samp.imp.virginmedia.bss.voice.camelroutes.iopRoute.IOPResponseFtpRouteBuilder> IOP response file : SX30RPMAJAR1702483756.txt successfully sent to ICOMS FTP directory ==================================================================================================== Similarly when we achieved the files from target directory. It took around 6 sec.It seems like there is a performance issue with camel sftp component. Does it list the files in target directory which is taking time. =================================================================================================== PFB the producer route which we set up sftp://10.100.150.190/icoms/1?download=false&maxMessagesPerPoll=10&password=xxxxxx&tempPrefix=Q&username=sigmauser",0.1284090909,-0.03657758621,neutral
camel,1138,description,"Using a memory profiler, we've identified what appears to be a substantial memory leak in FileConsumer in Camel 1.5.0. It appears that the noopMap is constantly having items added to it, but nothing performs a remove on it when the file is consumed. This causes a very large amount of string data to be accumulated in the heap. In our application, this was a leak of several hundred megabytes and is a showstopper. Considering the apparent severity of this issue, it would really be nice if a fix could be incorporated into a 1.5.1 version.",code_debt,low_quality_code,"Tue, 2 Dec 2008 16:36:14 +0000","Mon, 23 Mar 2009 08:40:34 +0000","Wed, 3 Dec 2008 17:15:46 +0000",88772,"Using a memory profiler, we've identified what appears to be a substantial memory leak in FileConsumer in Camel 1.5.0. It appears that the noopMap is constantly having items added to it, but nothing performs a remove on it when the file is consumed. This causes a very large amount of string data to be accumulated in the heap. In our application, this was a leak of several hundred megabytes and is a showstopper. Considering the apparent severity of this issue, it would really be nice if a fix could be incorporated into a 1.5.1 version. http://www.nabble.com/Memory-leak-in-FileConsumer-in-Camel-1.5.0-td20794405s22882.html#a20794405",0.125,0.1041666667,negative
camel,1138,summary,Memory leak in FileConsumer,code_debt,low_quality_code,"Tue, 2 Dec 2008 16:36:14 +0000","Mon, 23 Mar 2009 08:40:34 +0000","Wed, 3 Dec 2008 17:15:46 +0000",88772,Memory leak in FileConsumer,-0.2,-0.2,negative
camel,11524,comment_0,"No there is not, your workaround is to not use $ in the file name, which also is a bad habit to do so. The source code needs to be patched where you need to quote the file name in the GenericFileEndpoint method via You are welcome to work on a github PR to fix this",code_debt,low_quality_code,"Sun, 9 Jul 2017 08:02:48 +0000","Mon, 17 Jul 2017 05:39:44 +0000","Mon, 17 Jul 2017 05:37:19 +0000",682471,"No there is not, your workaround is to not use $ in the file name, which also is a bad habit to do so. The source code needs to be patched where you need to quote the file name in the GenericFileEndpoint method via java.util.regex.Matcher#quoteReplacement. You are welcome to work on a github PR to fix this",0.0375,0.0125,negative
camel,11524,comment_1,"Hi Claus Thanks for the response. The source of the '$' is Ola Hallengren's SQL maintenance script and that in turn is escaping a '\' in a database engine name. It seems like a better idea to fix the Camel code and make it more robust. I will look into providing a patch for this. Regards, Saycat",code_debt,low_quality_code,"Sun, 9 Jul 2017 08:02:48 +0000","Mon, 17 Jul 2017 05:39:44 +0000","Mon, 17 Jul 2017 05:37:19 +0000",682471,"Hi Claus Thanks for the response. The source of the '$' is Ola Hallengren's SQL maintenance script and that in turn is escaping a '\' in a database engine name. It seems like a better idea to fix the Camel code and make it more robust. I will look into providing a patch for this. Regards, Saycat",0.13,0.13,neutral
camel,1173,comment_1,Put in an improved scatter-gather example in revision 740056. The problem with the first example was that it was not dynamic enough. Also added wiki docs here,code_debt,low_quality_code,"Tue, 9 Dec 2008 19:01:43 +0000","Mon, 23 Mar 2009 08:40:31 +0000","Mon, 2 Feb 2009 19:39:35 +0000",4754272,Put in an improved scatter-gather example in revision 740056. The problem with the first example was that it was not dynamic enough. Also added wiki docs here http://cwiki.apache.org/confluence/display/CAMEL/Scatter-Gather,-0.01666666667,-0.01666666667,negative
camel,12166,comment_3,"It end up calling some rollback code, that performs the disconnect. Its a bit more complicated as ftp extends file component. But as said you use an old version of Camel we dont support anymore.",code_debt,complex_code,"Fri, 19 Jan 2018 13:41:03 +0000","Mon, 29 Jan 2018 18:09:43 +0000","Fri, 19 Jan 2018 13:46:32 +0000",329,"It end up calling some rollback code, that performs the disconnect. Its a bit more complicated as ftp extends file component. But as said you use an old version of Camel we dont support anymore.",-0.06666666667,-0.06666666667,negative
camel,1228,summary,Clean up the OSGI bundle profiles,code_debt,low_quality_code,"Tue, 6 Jan 2009 06:30:43 +0000","Fri, 31 Jul 2009 06:33:56 +0000","Wed, 7 Jan 2009 06:38:41 +0000",86878,Clean up the OSGI bundle profiles,0.4,0.4,neutral
camel,12414,comment_2,"Okay it works fine with an unit test that does not use pax-exam. Its some weird osgi stuff, it may not start/run properly via pax-exam inside the osgi/karaf container with the unit test.",code_debt,low_quality_code,"Wed, 28 Mar 2018 18:36:34 +0000","Wed, 22 Aug 2018 12:04:17 +0000","Wed, 22 Aug 2018 11:52:14 +0000",12676540,"Okay it works fine with an unit test that does not use pax-exam. Its some weird osgi stuff, it may not start/run properly via pax-exam inside the osgi/karaf container with the unit test.",0.102,0.102,neutral
camel,1256,comment_0,"A fix has been submitted. Here is some highlights. * The fix is for 2.0 only since it involves some APIs changes that is an overkill to keep it backward compatible. Will update wiki. * It depends on CXF 2.2-SNAPSHOT now. Will upgrade CXF once the next release of CXF is available. * CxfBinding, Bus [CAMEL-1239], can be looked up from registry by the ""#"" notation. * Decouple CXF Message from Camel Message. That is, users are no longer required to cast Camel's message body to CXF Message in order to access SOAP headers and body in PAYLOAD. In PAYLOAD mode, Camel message body now returns a new type CxfPayload which contains SOAP headers and body. With CxfPayload being the body in PAYLOAD mode, It makes using Camel converter to convert to other types pretty easy and transparent. * Cleaning up the old CxfBinding and ""invoking context"". Both tried to do Camel/CXF message binding. CxfBinding is now an interface that an custom impl can be set on each endpoint. impl can also be set on each endpoint. * [CAMEL-1254] support serviceClass=#bean * some major refactoring to make code cleaner.",code_debt,low_quality_code,"Wed, 14 Jan 2009 15:13:12 +0000","Fri, 31 Jul 2009 06:33:58 +0000","Wed, 28 Jan 2009 03:48:19 +0000",1168507,"A fix has been submitted. Here is some highlights. The fix is for 2.0 only since it involves some APIs changes that is an overkill to keep it backward compatible. Will update wiki. It depends on CXF 2.2-SNAPSHOT now. Will upgrade CXF once the next release of CXF is available. CxfBinding, Bus CAMEL-1239, HeaderFilterStrategy can be looked up from registry by the ""#"" notation. Decouple CXF Message from Camel Message. That is, users are no longer required to cast Camel's message body to CXF Message in order to access SOAP headers and body in PAYLOAD. In PAYLOAD mode, Camel message body now returns a new type CxfPayload which contains SOAP headers and body. With CxfPayload being the body in PAYLOAD mode, It makes using Camel converter to convert to other types pretty easy and transparent. Cleaning up the old CxfBinding and ""invoking context"". Both tried to do Camel/CXF message binding. CxfBinding is now an interface that an custom impl can be set on each endpoint. HeaderFilterStrategy impl can also be set on each endpoint. CAMEL-1254 support serviceClass=#bean some major refactoring to make code cleaner.",0.081,0.081,neutral
camel,1256,description,"The camel-cxf component is dued for some code cleanup and refactoring. We can clean some of of the interfaces and redundant code, etc.",code_debt,dead_code,"Wed, 14 Jan 2009 15:13:12 +0000","Fri, 31 Jul 2009 06:33:58 +0000","Wed, 28 Jan 2009 03:48:19 +0000",1168507,"The camel-cxf component is dued for some code cleanup and refactoring. We can clean some of of the interfaces and redundant code, etc.",0.2,0.2,neutral
camel,1256,summary,Clean up camel-cxf,code_debt,dead_code,"Wed, 14 Jan 2009 15:13:12 +0000","Fri, 31 Jul 2009 06:33:58 +0000","Wed, 28 Jan 2009 03:48:19 +0000",1168507,Clean up camel-cxf,0.4,0.4,neutral
camel,1270,description,The countdown latch in MainSupport is not completed when Main is stopping. Then we have a hanging thread. Can bee seen using ctrl + \,code_debt,multi-thread_correctness,"Sun, 18 Jan 2009 09:56:18 +0000","Fri, 31 Jul 2009 06:33:59 +0000","Sun, 18 Jan 2009 10:06:27 +0000",609,The countdown latch in MainSupport is not completed when Main is stopping. Then we have a hanging thread. Can bee seen using ctrl + \,0.06666666667,0.06666666667,negative
camel,1304,comment_8,"Hi, Actually I toyed with this a while ago and it's not hard to implement. It would help in a few ways, one being distributing our own tests on multiple nodes and reduce the ridiculously long building time (but hey, we have some 4000 unit tests, something to brag about). The issue is that gridgain repackaged and distribute their api under ASL2.0 at our own's jstrachan request quite a while ago. The issue is that the nodes are still GPL so I have no idea how we could test such a component unless they would make available a mock or something as ASL2.0 as well. I am willing to bet that they won't relicense the whole thing. I spoke to Nikita some 6 months ago about this and it looks like the GG crowd is not quite happy about the prospect. One alternative would be to implement and host it at the fuse forge and camel extra and depending on the community interest see if we could work something out with GG and move the component later at apache.",code_debt,slow_algorithm,"Thu, 29 Jan 2009 08:49:55 +0000","Sun, 24 Apr 2011 09:58:24 +0000","Sun, 26 Sep 2010 16:28:51 +0000",52299536,"Hi, Actually I toyed with this a while ago and it's not hard to implement. It would help in a few ways, one being distributing our own tests on multiple nodes and reduce the ridiculously long building time (but hey, we have some 4000 unit tests, something to brag about). The issue is that gridgain repackaged and distribute their api under ASL2.0 at our own's jstrachan request quite a while ago. The issue is that the nodes are still GPL so I have no idea how we could test such a component unless they would make available a mock or something as ASL2.0 as well. I am willing to bet that they won't relicense the whole thing. I spoke to Nikita some 6 months ago about this and it looks like the GG crowd is not quite happy about the prospect. One alternative would be to implement and host it at the fuse forge and camel extra and depending on the community interest see if we could work something out with GG and move the component later at apache.",0.1482222222,0.1482222222,positive
camel,1447,description,"See article, listening 2 We should consider renaming *handle* to *catchBlock* so its using the same name as try .. catch. And we could add a finallyBlock so end user got them all.",code_debt,low_quality_code,"Wed, 11 Mar 2009 06:02:04 +0000","Sat, 21 Nov 2009 11:57:55 +0000","Fri, 17 Apr 2009 15:38:43 +0000",3231399,"See article, listening 2 http://architects.dzone.com/articles/fuse-esb-4-osgi-based?page=0,2 We should consider renaming handle to catchBlock so its using the same name as try .. catch. And we could add a finallyBlock so end user got them all.",0,0,neutral
camel,1507,comment_0,might need some cleanup - but I have tested it and it works for me,code_debt,low_quality_code,"Wed, 1 Apr 2009 00:50:08 +0000","Sat, 27 Nov 2010 06:42:58 +0000","Thu, 2 Apr 2009 07:42:27 +0000",111139,might need some cleanup - but I have tested it and it works for me,0.5815,0.5815,positive
camel,1507,comment_4,I renamed the constant so its according to the Camel 2.0 syntax: rev 761182 on trunk,code_debt,low_quality_code,"Wed, 1 Apr 2009 00:50:08 +0000","Sat, 27 Nov 2010 06:42:58 +0000","Thu, 2 Apr 2009 07:42:27 +0000",111139,I renamed the constant so its according to the Camel 2.0 syntax: rev 761182 on trunk,0.5,0.5,neutral
camel,1507,description,"To send a email you have to follow a pretty specific course. This adds a property (which is poorly named in this patch) to the MailConfiguration that names the header that contains the plaintext version of the email, and adds a property where you can embed images inline. If an attachment has a filename starting with ""cid:"" then this will add the ""Content-ID"" header to that multipart body - which will allow the email client to put the image in the appropriate place when it is viewed. (i.e. the html email has something like <image src=""cid:0001"" /",code_debt,low_quality_code,"Wed, 1 Apr 2009 00:50:08 +0000","Sat, 27 Nov 2010 06:42:58 +0000","Thu, 2 Apr 2009 07:42:27 +0000",111139,"To send a multipart/alternative email you have to follow a pretty specific course. This adds a property (which is poorly named in this patch) to the MailConfiguration that names the header that contains the plaintext version of the email, and adds a property where you can embed images inline. If an attachment has a filename starting with ""cid:"" then this will add the ""Content-ID"" header to that multipart body - which will allow the email client to put the image in the appropriate place when it is viewed. (i.e. the html email has something like <image src=""cid:0001"" /> and the attachment is named 'cid:0001' - when it sees an inline attachment with ""Content-ID: 0001"" it will put it in the right spot)",0.03795833333,0.1663333333,neutral
camel,1944,description,Currently only the camel-irc producer installs an event listener on the IRC connection to log incoming messages from the IRC server. This patch revamps the logging so that the component installs an event adapter to do logging before the connection is even established so it's easier to debug various issues that could be occurring with the connection to the IRC server.,code_debt,low_quality_code,"Wed, 26 Aug 2009 15:47:30 +0000","Thu, 3 Jun 2010 07:23:59 +0000","Thu, 27 Aug 2009 02:51:53 +0000",39863,Currently only the camel-irc producer installs an event listener on the IRC connection to log incoming messages from the IRC server. This patch revamps the logging so that the component installs an event adapter to do logging before the connection is even established so it's easier to debug various issues that could be occurring with the connection to the IRC server.,0,0,neutral
camel,251,description,"displays the file instead of directory (a boolean) automagically attempt to reconnect if from their destinations. This should probably be shared with Consumers, and could maybe done more cleanly using camel's try/catch features, but there you go. Smarter directory/file building/handling. This lets you handle URI's like similarly, avoid putting files in '/' on servers that expose the full filesystem, etc. More verbose logging. Stuff like ""what file went where,"" reconnection attempts, etc.",code_debt,low_quality_code,"Thu, 29 Nov 2007 19:16:53 +0000","Mon, 12 May 2008 07:56:35 +0000","Fri, 7 Dec 2007 03:52:39 +0000",635746,"RemoteFileConfiguration.toString() displays the file instead of directory (a boolean) FtpProducer/SftpProducer automagically attempt to reconnect if disconnected/timed-out from their destinations. This should probably be shared with Consumers, and could maybe done more cleanly using camel's try/catch features, but there you go. Smarter directory/file building/handling. This lets you handle URI's like ftp://somehost/somedir, ftp://somehost/somedir/ similarly, avoid putting files in '/' on servers that expose the full filesystem, etc. More verbose logging. Stuff like ""what file went where,"" reconnection attempts, etc.",0.09166666667,0.06428571429,neutral
camel,2535,summary,Get ride of the cxfsoap component,code_debt,dead_code,"Wed, 10 Mar 2010 10:06:54 +0000","Sun, 24 Apr 2011 10:01:27 +0000","Thu, 11 Mar 2010 09:13:31 +0000",83197,Get ride of the cxfsoap component,0,0,neutral
camel,2756,comment_0,Willem I think the xxxConverter name is misleading as you would think its a Camel TypeConverter. I wonder if there is a better naming for this? or the likes.,code_debt,low_quality_code,"Thu, 27 May 2010 02:09:50 +0000","Sun, 24 Apr 2011 10:00:59 +0000","Thu, 27 May 2010 03:10:03 +0000",3613,Willem I think the xxxConverter name is misleading as you would think its a Camel TypeConverter. I wonder if there is a better naming for this? AuthenticationAdapter AuthenticationStrategy AuthenticationService or the likes.,0.1055,0.1055,negative
camel,2892,summary,remove ugly warnings running tests since upgrading to jetty 7,code_debt,dead_code,"Thu, 1 Jul 2010 15:47:59 +0000","Sun, 24 Apr 2011 10:01:01 +0000","Thu, 1 Jul 2010 15:49:41 +0000",102,remove ugly warnings running tests since upgrading to jetty 7,-0.8,-0.8,negative
camel,302,comment_1,"Could you look at the solution I made in CAMEL-254 and say if it is OK for you? We could have also your solution implemented (as by default in CAMEL-254 I skip all headers that start with 'org.apache.camel') but I think it could be too much ;) If it works for you, then lets close the issue.",code_debt,low_quality_code,"Mon, 21 Jan 2008 14:45:50 +0000","Mon, 16 Feb 2009 05:53:17 +0000","Thu, 11 Sep 2008 16:12:35 +0000",20222805,"Could you look at the solution I made in CAMEL-254 and say if it is OK for you? We could have also your solution implemented (as by default in CAMEL-254 I skip all headers that start with 'org.apache.camel') but I think it could be too much If it works for you, then lets close the issue.",0.1229166667,0.09375,neutral
camel,302,comment_2,In CAMEL-254 you add all headers except in the HTTP Header. In this way it is more difficulty to exclude some headers. It is better perhaps to use a special map for these headings. Does thing think of it?,code_debt,low_quality_code,"Mon, 21 Jan 2008 14:45:50 +0000","Mon, 16 Feb 2009 05:53:17 +0000","Thu, 11 Sep 2008 16:12:35 +0000",20222805,"In CAMEL-254 you add all headers except ""org.apache.camel.component.http.query"" in the HTTP Header. In this way it is more difficulty to exclude some headers. It is better perhaps to use a special map for these headings. Does thing think of it?",0.010125,0.0045,neutral
camel,3100,summary,${file:length} should return 0 instead of null if the file length is 0,code_debt,low_quality_code,"Thu, 2 Sep 2010 03:48:33 +0000","Sun, 24 Apr 2011 09:57:14 +0000","Thu, 2 Sep 2010 05:13:53 +0000",5120,${file:length} should return 0 instead of null if the file length is 0,0,0,neutral
camel,3287,summary,remove http feature from Camel features as its duplicated in Karaf,code_debt,duplicated_code,"Thu, 28 Oct 2010 06:55:51 +0000","Sun, 24 Apr 2011 09:58:01 +0000","Thu, 28 Oct 2010 07:03:47 +0000",476,remove http feature from Camel features as its duplicated in Karaf,0,0,negative
camel,3349,comment_0,"Made the getBinding() method synchronized to overcome this issue. The penalty for this is very minimal since, the need for creating a binding is only on the first set of invocations. The binding is then held until the endpoint is in operation.",code_debt,multi-thread_correctness,"Fri, 19 Nov 2010 22:32:35 +0000","Sun, 24 Apr 2011 09:57:38 +0000","Fri, 19 Nov 2010 22:57:34 +0000",1499,"Made the getBinding() method synchronized to overcome this issue. The penalty for this is very minimal since, the need for creating a binding is only on the first set of invocations. The binding is then held until the endpoint is in operation.",-0.1333333333,-0.1333333333,neutral
camel,3349,comment_2,Nice catch. I wonder if the initialization of the binding can be done in doStart. This is much better as it would avoid the synchronized block on the getter. Which I assume is invoked lazy at runtime on processing a new Exchange. Generally initialization should be done in doStart because starting it is single threaded and we don't care _so much_ about performance at startup.,code_debt,multi-thread_correctness,"Fri, 19 Nov 2010 22:32:35 +0000","Sun, 24 Apr 2011 09:57:38 +0000","Fri, 19 Nov 2010 22:57:34 +0000",1499,Nice catch. I wonder if the initialization of the binding can be done in doStart. This is much better as it would avoid the synchronized block on the getter. Which I assume is invoked lazy at runtime on processing a new Exchange. Generally initialization should be done in doStart because starting it is single threaded and we don't care so much about performance at startup.,0.2182666667,0.2182666667,positive
camel,3349,summary,Race condition found in CxfRsEndpoint while getting the endpoint binding under load and performing sync and async invocation,code_debt,multi-thread_correctness,"Fri, 19 Nov 2010 22:32:35 +0000","Sun, 24 Apr 2011 09:57:38 +0000","Fri, 19 Nov 2010 22:57:34 +0000",1499,Race condition found in CxfRsEndpoint while getting the endpoint binding under load and performing sync and async invocation,0.5,0.5,neutral
camel,3351,comment_2,I got carried away and made several changes and fixed a minor bug. Here are my notes: IrcComponent: - Removed IrcConfiguration member variable. Didn't make sense. Removed constructor with IrcConfiguration as the param. - ircLogger moved to method. IrcProducer: - Changed doStart to call instead of doing it in the start method. Ditto for IrcConsumer removing dupe code. - Check to see if we're still connected before sending a message in process. If disconnected throw a - Removed unused imports - Add a listener so we can get error messages - Change listener type from to IRCEventAdapter and added getter/setter for easier testing - doStop didn't remove the listener fixed. Added. This would've caused an NPE if a user was stopping individual routes. IrcConsumer: - Changed doStart to call instead of doing it in the start method. - Removed unused imports - Added check in onKick to see if we got kicked and rejoin if so - Change listener type from to IRCEventAdapter and added getter/setter for easier testing IrcEndpoint: - Extracted method getExchange. Same 2 lines of code in 9 methods. Slightly cleaner this way. - Added handleIrcError to handle any IRC errors that the producer or consumer hit - Added handleNickInUse to handle nick in use errors. On endpoint startup this would cause a failed connection. For the Consumer this would just mean we'd never consume anything. For the Producer sends would throw an NPE (now there's a check for a valid connection and is thrown instead) - Added method joinChannels - Added method joinChannel IrcConfiguration - Add autoRejoin setting - Changed key storage to a Dictionary. Added several new tests using mockito,code_debt,duplicated_code,"Sun, 21 Nov 2010 13:54:04 +0000","Sun, 24 Apr 2011 09:57:55 +0000","Wed, 8 Dec 2010 06:54:35 +0000",1443631,I got carried away and made several changes and fixed a minor bug. Here are my notes: IrcComponent: Removed IrcConfiguration member variable. Didn't make sense. Removed constructor with IrcConfiguration as the param. ircLogger moved to method. IrcProducer: Changed doStart to call component.joinChannels instead of doing it in the start method. Ditto for IrcConsumer removing dupe code. Check to see if we're still connected before sending a message in process. If disconnected throw a RuntimeCamelException. Removed unused imports Add a listener so we can get error messages Change listener type from FilteredIRCEventAdapter to IRCEventAdapter and added getter/setter for easier testing doStop didn't remove the listener fixed. Added. This would've caused an NPE if a user was stopping individual routes. IrcConsumer: Changed doStart to call component.joinChannels instead of doing it in the start method. Removed unused imports Added check in onKick to see if we got kicked and rejoin if so Change listener type from FilteredIRCEventAdapter to IRCEventAdapter and added getter/setter for easier testing IrcEndpoint: Extracted method getExchange. Same 2 lines of code in 9 methods. Slightly cleaner this way. Added handleIrcError to handle any IRC errors that the producer or consumer hit Added handleNickInUse to handle nick in use errors. On endpoint startup this would cause a failed connection. For the Consumer this would just mean we'd never consume anything. For the Producer sends would throw an NPE (now there's a check for a valid connection and RuntimeCamelException is thrown instead) Added method joinChannels Added method joinChannel IrcConfiguration Add autoRejoin setting Changed key storage to a Dictionary. Added several new tests using mockito,-0.05033333333,-0.03669565217,negative
camel,3769,comment_0,I suggest to change this section of the code to always use getInstance instead of getDefaultInstance.,code_debt,low_quality_code,"Tue, 8 Mar 2011 16:08:49 +0000","Tue, 25 Oct 2011 11:35:59 +0000","Wed, 9 Mar 2011 09:22:07 +0000",61998,I suggest to change this section of the code to always use getInstance instead of getDefaultInstance.,0,0,neutral
camel,383,comment_1,A patch with the following changes: - copyright headers for missing files - removed an unused import - new feature to transfer exchange objects using TCP protocol see how to use it The new feature only works for the TCP protocol and for the default codec (= object). So do not use textline=true as option.,code_debt,low_quality_code,"Wed, 12 Mar 2008 16:18:36 +0000","Mon, 12 May 2008 12:45:33 +0000","Mon, 24 Mar 2008 11:47:55 +0000",1020559,A patch with the following changes: copyright headers for missing files removed an unused import new feature to transfer exchange objects using TCP protocol (transferExchange=true) see MinaTransferExchangeOptionTest how to use it The new feature only works for the TCP protocol and for the default codec (= object). So do not use textline=true as option.,-0.39425,-0.3154,neutral
camel,383,comment_2,I think this ticket should be changed from bug to new feature. Also anyone got a better name for the option (transferExchange)? I considered the option bodyOnly also but then I needed to make sure the defaults would be bodyOnly=true even when the parameter is not specified.,code_debt,low_quality_code,"Wed, 12 Mar 2008 16:18:36 +0000","Mon, 12 May 2008 12:45:33 +0000","Mon, 24 Mar 2008 11:47:55 +0000",1020559,I think this ticket should be changed from bug to new feature. Also anyone got a better name for the option (transferExchange)? I considered the option bodyOnly also but then I needed to make sure the defaults would be bodyOnly=true even when the parameter is not specified.,0.3455,0.3455,neutral
camel,3888,comment_1,"Dan as said before just because a method is deprecated do *not* mean we should not unit test it. And hence why you would see unit test that uses the deprecated methods. The handled(true|false) on try .. catch, error handler, was a mistake and there are *no* alternative on those. Instead you should use for example onException (exception clause). Or in case of a doCatch you can rethrow the exception if you want that.",code_debt,low_quality_code,"Thu, 21 Apr 2011 12:12:23 +0000","Mon, 27 Jun 2011 08:05:27 +0000","Mon, 27 Jun 2011 08:05:27 +0000",5773984,"Dan as said before just because a method is deprecated do not mean we should not unit test it. And hence why you would see unit test that uses the deprecated methods. The handled(true|false) on try .. catch, error handler, was a mistake and there are no alternative on those. Instead you should use for example onException (exception clause). Or in case of a doCatch you can rethrow the exception if you want that.",0.01865,0.01865,negative
camel,4007,description,"If attachment file names contain unicode characters, they do not appear correctly encoded in the message.",code_debt,low_quality_code,"Mon, 23 May 2011 20:45:18 +0000","Tue, 25 Oct 2011 11:35:26 +0000","Sat, 11 Jun 2011 17:56:38 +0000",1631480,"If attachment file names contain unicode characters, they do not appear correctly encoded in the message.",-0.5,-0.5,negative
camel,4139,description,"Currently, spring parses into a CxfEndpointBean whereas blueprint parses into a CxfEndpoint directly. This mismatch then requires extra casting in the CxfComponent. Also, it means there are features supported by Spring that are not supported yet by blueprint (like configuring interceptors). I'm attaching a patch that removes the spring specific CxfEndpointBean stuff and allows both spring and blueprint to parse directly into the CxfEndpoint (well, subclasses of) so the two approaches can be maintained together. It also reduces the usage of ""Strings"" for properties in CxfEndpoint and uses QNames where appropriate (added a Converter for that) and Class<?",code_debt,complex_code,"Wed, 22 Jun 2011 15:17:58 +0000","Thu, 23 Jun 2011 00:57:02 +0000","Thu, 23 Jun 2011 00:30:12 +0000",33134,"Currently, spring parses into a CxfEndpointBean whereas blueprint parses into a CxfEndpoint directly. This mismatch then requires extra casting in the CxfComponent. Also, it means there are features supported by Spring that are not supported yet by blueprint (like configuring interceptors). I'm attaching a patch that removes the spring specific CxfEndpointBean stuff and allows both spring and blueprint to parse directly into the CxfEndpoint (well, subclasses of) so the two approaches can be maintained together. It also reduces the usage of ""Strings"" for properties in CxfEndpoint and uses QNames where appropriate (added a Converter for that) and Class<?> for the service class. That will allow OSGi to provide the Class directly instead of having to Class.forName the thing. (still need to do that, but this is a start)",0.2694,0.2031111111,neutral
camel,4202,comment_0,"Okay I think there is a tradeoff if we lower the spring receiveTimeout to lower than 1 sec. For example if we use 1 millis, then it will overload and send pull packets to the AMQ broker to receive messages. The reason why temporary queues is faster is because they dont have any JMSMessageListener as the persistent does. So with temporary queues it can pull every 1sec and pickup messages as fast as possible, as the received message is always for the reply manager.",code_debt,slow_algorithm,"Sat, 9 Jul 2011 14:05:41 +0000","Mon, 19 Sep 2011 20:32:51 +0000","Sun, 14 Aug 2011 10:07:44 +0000",3096123,"Okay I think there is a tradeoff if we lower the spring receiveTimeout to lower than 1 sec. For example if we use 1 millis, then it will overload and send pull packets to the AMQ broker to receive messages. The reason why temporary queues is faster is because they dont have any JMSMessageListener as the persistent does. So with temporary queues it can pull every 1sec and pickup messages as fast as possible, as the received message is always for the reply manager.",-0.06566666667,-0.06566666667,neutral
camel,4202,comment_2,"If there is an use case for using persistent queues that are *exclusive* for the Camel consumer, then we can avoid using JMSMessageSelector and thus be as fast as temporary queues.",code_debt,slow_algorithm,"Sat, 9 Jul 2011 14:05:41 +0000","Mon, 19 Sep 2011 20:32:51 +0000","Sun, 14 Aug 2011 10:07:44 +0000",3096123,"If there is an use case for using persistent queues that are exclusive for the Camel consumer, then we can avoid using JMSMessageSelector and thus be as fast as temporary queues.",0.1,0.1,neutral
camel,4202,description,"See nabble When using persistent replyTo queues for request/reply over JMS, then the ReplyManager should be faster to pickup replies. The default spring-jms timeout is 1 sec and it impacts the performance. Likewise the receiveTimeout should not be set on the reply managers as that does not apply here.",code_debt,slow_algorithm,"Sat, 9 Jul 2011 14:05:41 +0000","Mon, 19 Sep 2011 20:32:51 +0000","Sun, 14 Aug 2011 10:07:44 +0000",3096123,"See nabble http://camel.465427.n5.nabble.com/slow-reply-for-jms-component-when-url-contains-replyTo-tp4563075p4563075.html When using persistent replyTo queues for request/reply over JMS, then the ReplyManager should be faster to pickup replies. The default spring-jms timeout is 1 sec and it impacts the performance. Likewise the receiveTimeout should not be set on the reply managers as that does not apply here.",-0.1666666667,-0.1666666667,neutral
camel,4202,summary,camel-jms - Using request/reply over persistent queues should be faster,code_debt,slow_algorithm,"Sat, 9 Jul 2011 14:05:41 +0000","Mon, 19 Sep 2011 20:32:51 +0000","Sun, 14 Aug 2011 10:07:44 +0000",3096123,camel-jms - Using request/reply over persistent queues should be faster,0,0,neutral
camel,4230,description,"See nabble If for some reason the method cannot be invoked, you may get a caused exception We should catch this and provide a wrapped exception with a more descriptive error message, about the bean and method attempted to invoke.",code_debt,low_quality_code,"Fri, 15 Jul 2011 05:17:52 +0000","Mon, 2 Apr 2012 17:57:42 +0000","Wed, 7 Mar 2012 22:30:40 +0000",20452368,"See nabble http://camel.465427.n5.nabble.com/How-to-impl-bean-side-of-proxy-w-Future-return-tp4581104p4581104.html If for some reason the method cannot be invoked, you may get a caused exception We should catch this and provide a wrapped exception with a more descriptive error message, about the bean and method attempted to invoke.",-0.4,-0.4,negative
camel,4230,summary,BeanProcessor - Improved exception message if failed to invoke method,code_debt,low_quality_code,"Fri, 15 Jul 2011 05:17:52 +0000","Mon, 2 Apr 2012 17:57:42 +0000","Wed, 7 Mar 2012 22:30:40 +0000",20452368,BeanProcessor - Improved exception message if failed to invoke method,0,0,neutral
camel,4273,summary,Favor static member classes over nonstatic ones.,code_debt,low_quality_code,"Tue, 26 Jul 2011 14:59:28 +0000","Wed, 27 Jul 2011 07:12:49 +0000","Wed, 27 Jul 2011 06:38:43 +0000",56355,Favor static member classes over nonstatic ones.,-0.275,-0.275,neutral
camel,4317,description,"After upgrading to CXF 2.4.x, we don't need to import the resources file like any more, so it's time to clean up these imports.",code_debt,dead_code,"Tue, 9 Aug 2011 03:20:04 +0000","Tue, 9 Aug 2011 14:24:05 +0000","Tue, 9 Aug 2011 14:24:05 +0000",39841,"After upgrading to CXF 2.4.x, we don't need to import the resources file like /META-INF/cxf/cxf-extension-xxx.xml any more, so it's time to clean up these imports.",0.2,0.1333333333,neutral
camel,4317,summary,Clean up the camel context file of the example after upgrading to CXF 2.4.x,code_debt,low_quality_code,"Tue, 9 Aug 2011 03:20:04 +0000","Tue, 9 Aug 2011 14:24:05 +0000","Tue, 9 Aug 2011 14:24:05 +0000",39841,Clean up the camel context file of the example after upgrading to CXF 2.4.x,0.2,0.2,neutral
camel,4357,comment_2,"There is a CS error as the license header is missing in one file. When adding a new package in camel-core, a package.html file should be included, which briefly summarizes the package. See some of the other for examples to copy. And in the camel-core/pom.xml file there is some javadoc grouping of packages. I guess the new main package is to be added as well.",code_debt,low_quality_code,"Fri, 19 Aug 2011 13:38:41 +0000","Mon, 22 Aug 2011 17:08:15 +0000","Mon, 22 Aug 2011 13:34:55 +0000",258974,"There is a CS error as the license header is missing in one file. When adding a new package in camel-core, a package.html file should be included, which briefly summarizes the package. See some of the other for examples to copy. And in the camel-core/pom.xml file there is some javadoc grouping of packages. I guess the new main package is to be added as well.",0.033,0.033,neutral
camel,4417,comment_3,"@Christian, all the changes you make in 2.9 should be backwards compatible. So if you make any changes, please make sure leave existing classes in place (even as extensions of refactored classes) and change as few tests as possible, ideally none. That ensures two things: one that we didn't break anything and existing code still works, second that users have a migration path that could take at any time. We can remove the old classes later in 3.0. Changes that break backward compatibility I'd leave for later.",code_debt,low_quality_code,"Mon, 5 Sep 2011 13:46:59 +0000","Tue, 22 Oct 2013 07:34:27 +0000","Tue, 22 Oct 2013 07:34:27 +0000",67196848,"@Christian, all the changes you make in 2.9 should be backwards compatible. So if you make any changes, please make sure leave existing classes in place (even as extensions of refactored classes) and change as few tests as possible, ideally none. That ensures two things: one that we didn't break anything and existing code still works, second that users have a migration path that could take at any time. We can remove the old classes later in 3.0. Changes that break backward compatibility I'd leave for later.",0.1731,0.1731,neutral
camel,4958,comment_0,"Is now less verbose, and Spring no longer complains about no error handler configured. Added options to tweak logging levels and verbosity.",code_debt,low_quality_code,"Tue, 31 Jan 2012 06:28:34 +0000","Tue, 31 Jan 2012 08:22:28 +0000","Tue, 31 Jan 2012 08:22:28 +0000",6834,"Is now less verbose, and Spring no longer complains about no error handler configured. Added options to tweak logging levels and verbosity.",0.2,0.2,neutral
camel,4958,description,"When using a JMS route, and an exception occurs, then you get a WARN logging from Spring JMS that there is no error handler configured. And you get the stacktrace etc. However Camel itself will also by default log the stacktrace, so we have it logged twice. We should make this less verbose. And allow end users to more easily customize the logging level and/or whether stracktraces should be included.",code_debt,low_quality_code,"Tue, 31 Jan 2012 06:28:34 +0000","Tue, 31 Jan 2012 08:22:28 +0000","Tue, 31 Jan 2012 08:22:28 +0000",6834,"When using a JMS route, and an exception occurs, then you get a WARN logging from Spring JMS that there is no error handler configured. And you get the stacktrace etc. However Camel itself will also by default log the stacktrace, so we have it logged twice. We should make this less verbose. And allow end users to more easily customize the logging level and/or whether stracktraces should be included.",-0.06,-0.06,neutral
camel,4958,summary,camel-jms - JmsConsumer make it less verbose logging in case of an exception or TX rollback,code_debt,low_quality_code,"Tue, 31 Jan 2012 06:28:34 +0000","Tue, 31 Jan 2012 08:22:28 +0000","Tue, 31 Jan 2012 08:22:28 +0000",6834,camel-jms - JmsConsumer make it less verbose logging in case of an exception or TX rollback,0,0,neutral
camel,4959,comment_5,"Yeah I have corrected the coverters to deal with NaN. And yes we need to lookup fast in type converter registry as its used a lot in Camel, and it should be as fast as possible, as it can become a bottleneck.",code_debt,slow_algorithm,"Tue, 31 Jan 2012 07:06:19 +0000","Tue, 28 Feb 2012 11:43:59 +0000","Thu, 23 Feb 2012 12:47:32 +0000",2007673,"Yeah I have corrected the coverters to deal with NaN. And yes we need to lookup fast in type converter registry as its used a lot in Camel, and it should be as fast as possible, as it can become a bottleneck.",0.2,0.2,neutral
camel,4993,comment_1,"Applied the patch into trunk with thanks to Joshua, I also fixed some Checkstyle errors.",code_debt,low_quality_code,"Wed, 8 Feb 2012 10:05:03 +0000","Sat, 2 Jun 2012 12:16:50 +0000","Sat, 2 Jun 2012 12:16:50 +0000",9943907,"Applied the patch into trunk with thanks to Joshua, I also fixed some Checkstyle errors.",0,0,positive
camel,5012,comment_0,Now Camel is less verbose by default.,code_debt,low_quality_code,"Thu, 16 Feb 2012 17:07:49 +0000","Thu, 16 Feb 2012 17:18:42 +0000","Thu, 16 Feb 2012 17:18:42 +0000",653,Now Camel is less verbose by default.,-0.5,-0.5,neutral
camel,5012,description,"When starting and shutting down Camel, it reports a bit stuff at INFO level. We should make it less verbose. For example the type converter logs 3-4 lines, we should just log 1 line instead.",code_debt,low_quality_code,"Thu, 16 Feb 2012 17:07:49 +0000","Thu, 16 Feb 2012 17:18:42 +0000","Thu, 16 Feb 2012 17:18:42 +0000",653,"When starting and shutting down Camel, it reports a bit stuff at INFO level. We should make it less verbose. For example the type converter logs 3-4 lines, we should just log 1 line instead.",-0.09733333333,-0.09733333333,neutral
camel,5012,summary,Starting and stopping Camel should be less verbose,code_debt,low_quality_code,"Thu, 16 Feb 2012 17:07:49 +0000","Thu, 16 Feb 2012 17:18:42 +0000","Thu, 16 Feb 2012 17:18:42 +0000",653,Starting and stopping Camel should be less verbose,-0.2,-0.2,neutral
camel,5527,comment_1,I removed the surefire plugin as its no longer needed,code_debt,dead_code,"Tue, 21 Aug 2012 10:13:47 +0000","Tue, 21 Aug 2012 10:34:23 +0000","Tue, 21 Aug 2012 10:34:05 +0000",1218,I removed the surefire plugin as its no longer needed,0,0,neutral
camel,5586,comment_2,I polished the camel-elasticsearch codebase a bit:,code_debt,low_quality_code,"Sun, 9 Sep 2012 20:39:34 +0000","Wed, 12 Sep 2012 05:46:08 +0000","Wed, 12 Sep 2012 05:46:08 +0000",205594,I polished the camel-elasticsearch codebase a bit: http://svn.apache.org/viewvc?rev=1382732&view=rev http://svn.apache.org/viewvc?rev=1382802&view=rev http://svn.apache.org/viewvc?rev=1382807&view=rev,0.4,0.4,neutral
camel,6043,comment_3,"This further improved performance for bean and simple language when using OGNL like expressions, that is evaluated at runtime. Now the cache ensures we do not introspect the same class types over and over again.",code_debt,slow_algorithm,"Wed, 6 Feb 2013 21:53:19 +0000","Thu, 28 Feb 2013 05:30:55 +0000","Thu, 28 Feb 2013 05:30:55 +0000",1841856,"This further improved performance for bean and simple language when using OGNL like expressions, that is evaluated at runtime. Now the cache ensures we do not introspect the same class types over and over again.",0.31875,0.31875,positive
camel,6043,summary,Improve the BeanInfo performance,code_debt,slow_algorithm,"Wed, 6 Feb 2013 21:53:19 +0000","Thu, 28 Feb 2013 05:30:55 +0000","Thu, 28 Feb 2013 05:30:55 +0000",1841856,Improve the BeanInfo performance,0.4,0.4,neutral
camel,612,comment_2,-1 [edit: retracting my +1] In other words if there is no explicit otherwise() to go the errorHandler (no redelivery of course). Thinking more about this I think the behavior of choice should be the way it already is. If no clause matches and there is no otherwise it should silently succeed with a noop. Users are familiar with this from switch() statements and is i think a reasonable expectation. I think it's perfectly reasonable to add the otherwise (as one would add a default: ) to make it explicit that the exchange should fail if there is no match: What I would do though is adding a log (below INFO) in the if otherwise == null that would say that exchange is not processed.,code_debt,low_quality_code,"Mon, 16 Jun 2008 06:38:46 +0000","Fri, 11 Jul 2008 04:21:45 +0000","Mon, 23 Jun 2008 12:41:58 +0000",626592,-1 [edit: retracting my +1] In other words if there is no explicit otherwise() to go the errorHandler (no redelivery of course). Thinking more about this I think the behavior of choice should be the way it already is. If no clause matches and there is no otherwise it should silently succeed with a noop. Users are familiar with this from switch() statements and is i think a reasonable expectation. I think it's perfectly reasonable to add the otherwise (as one would add a default: ) to make it explicit that the exchange should fail if there is no match: What I would do though is adding a log (below INFO) in the ChoiceProcessor.process() if otherwise == null that would say that exchange is not processed.,0.11408,0.09506666667,neutral
camel,6320,comment_1,I think your test file data.ics is missing. Could you please add it? Adding a bit of logging would help too. Thanks for your contribution.,code_debt,low_quality_code,"Fri, 26 Apr 2013 17:23:36 +0000","Fri, 5 Jul 2013 21:44:07 +0000","Fri, 5 Jul 2013 21:44:07 +0000",6063631,I think your test file data.ics is missing. Could you please add it? Adding a bit of logging would help too. Thanks for your contribution.,0.12,0.12,neutral
camel,7133,comment_3,"Can you remove the <cxf:rsServer id=""rsServer"" I don't think you can put the url options into rsServer address attribut.",code_debt,low_quality_code,"Tue, 14 Jan 2014 14:54:55 +0000","Thu, 16 Jan 2014 07:44:18 +0000","Thu, 16 Jan 2014 07:44:18 +0000",146963,"Can you remove the <cxf:rsServer id=""rsServer"" > part ? I don't think you can put the url options into rsServer address attribut.",0,0,negative
camel,7139,comment_1,I reproduced the problem with itest please see the attach. To Fix the problem a possible solution is remove TCCL to MVEL until is not resolved Please see the attach for the patch;I hope it's ok for you. If it's ok for you please clean it a bit Any feedback is welcome.,code_debt,low_quality_code,"Fri, 17 Jan 2014 16:47:11 +0000","Mon, 3 Feb 2014 14:17:23 +0000","Mon, 3 Feb 2014 14:17:23 +0000",1459812,I reproduced the problem with itest please see the attach. To Fix the problem a possible solution is remove TCCL to MVEL until http://jira.codehaus.org/browse/MVEL-250 is not resolved Please see the attach for the patch;I hope it's ok for you. If it's ok for you please clean it a bit Any feedback is welcome.,0.1527777778,0.1527777778,neutral
camel,7139,comment_2,Yeah you are welcome to cleanup the patch.,code_debt,low_quality_code,"Fri, 17 Jan 2014 16:47:11 +0000","Mon, 3 Feb 2014 14:17:23 +0000","Mon, 3 Feb 2014 14:17:23 +0000",1459812,Yeah you are welcome to cleanup the patch.,0.475,0.475,positive
camel,721,description,"Currently log component only logs the payload. We have a nice ExchangeFormatter that can format an exchange with all kind of options. The options could be enabled on the log component so you can customize your logging. Also there should be a *multiline* option to the exchange formatter so it can log all the stuff on multi lines if for instance there are many options, they get very long.",code_debt,low_quality_code,"Tue, 15 Jul 2008 06:56:17 +0000","Thu, 17 Jul 2008 12:01:14 +0000","Tue, 15 Jul 2008 09:33:28 +0000",9431,"Currently log component only logs the payload. We have a nice ExchangeFormatter that can format an exchange with all kind of options. The options could be enabled on the log component so you can customize your logging. Also there should be a multiline option to the exchange formatter so it can log all the stuff on multi lines if for instance there are many options, they get very long.",0.20625,0.20625,neutral
camel,721,summary,Log component should use ExchangeFormatter for formatting log output,code_debt,low_quality_code,"Tue, 15 Jul 2008 06:56:17 +0000","Thu, 17 Jul 2008 12:01:14 +0000","Tue, 15 Jul 2008 09:33:28 +0000",9431,Log component should use ExchangeFormatter for formatting log output,0,0,neutral
camel,7300,comment_0,Pushed to master. Not this patch makes use of a deprecated API call. In time we should move the entire camel-hl7 component to the new HAPI API: CAMEL-7301,code_debt,low_quality_code,"Mon, 17 Mar 2014 09:01:31 +0000","Tue, 25 Mar 2014 08:59:16 +0000","Mon, 17 Mar 2014 09:32:58 +0000",1887,Pushed to master. Not this patch makes use of a deprecated API call. In time we should move the entire camel-hl7 component to the new HAPI API: CAMEL-7301,0.1666666667,0.1666666667,negative
camel,7319,description,"The test in is ""dead"" since checkin git-svn-id: if you enable xalan on testing classpath.",code_debt,dead_code,"Fri, 21 Mar 2014 18:51:05 +0000","Fri, 28 Mar 2014 03:09:29 +0000","Fri, 28 Mar 2014 03:09:29 +0000",548304,"The test testUsingJavaExtensions in camel-core/src/test/java/org/apache/camel/builder/xml/XPathTest.java is ""dead"" since checkin git-svn-id: https://svn.apache.org/repos/asf/camel/trunk@824639 13f79535-47bb-0310-9956-ffa450edef68 if you enable xalan on testing classpath.",-0.05,-0.025,neutral
camel,7319,summary,Dead not working JUnittest,code_debt,dead_code,"Fri, 21 Mar 2014 18:51:05 +0000","Fri, 28 Mar 2014 03:09:29 +0000","Fri, 28 Mar 2014 03:09:29 +0000",548304,Dead not working JUnittest testUsingJavaExtensions,-0.6,-0.6,negative
camel,7644,description,"Since the camel DSL is invoked prior to being invoked there is no camel context set on the delegate java RouteBuilder which causes it to create a new context when the first dsl method is invoked. With the implementation of CAMEL-7327 introduced in 2.13.1 which stores created camel contexts in a set in this causes instances of DefaultCamelContext to be leaked, they are never removed from the static set. This is especially aparrent during unit testing. The following test shows that an additional context is registered for the scala route builder as opposed to java. Verification of the leak can be requires profiler and capturing of heap after termination of the test case (in ParentRunner.java).",code_debt,low_quality_code,"Mon, 28 Jul 2014 16:39:53 +0000","Tue, 3 Mar 2015 09:12:32 +0000","Tue, 3 Mar 2015 09:12:32 +0000",18808359,"Since the camel DSL is invoked prior to `.addRoutesToCamelContext(CamelContext)` being invoked there is no camel context set on the delegate java RouteBuilder which causes it to create a new context when the first dsl method is invoked. With the implementation of CAMEL-7327 introduced in 2.13.1 which stores created camel contexts in a set in `Container.Instance#CONTEXT`; this causes instances of DefaultCamelContext to be leaked, they are never removed from the static set. This is especially aparrent during unit testing. The following test shows that an additional context is registered for the scala route builder as opposed to java. Verification of the leak can be requires profiler and capturing of heap after termination of the test case (in ParentRunner.java).",0.02291666667,0.0171875,neutral
camel,7715,description,"SjmsConsumer and SjmsProducer always register a new ThreadPool on Camel context every time a new instance is created for an endpoint. If consumer or producer is stopped or removed or even component is removed, thread pool still exists.",code_debt,low_quality_code,"Tue, 19 Aug 2014 00:37:45 +0000","Fri, 5 Sep 2014 07:09:36 +0000","Wed, 20 Aug 2014 05:51:21 +0000",105216,"SjmsConsumer and SjmsProducer always register a new ThreadPool on Camel context ExecutorServiceManager every time a new instance is created for an endpoint. If consumer or producer is stopped or removed or even component is removed, thread pool still exists.",0.02025,0.02025,neutral
camel,7715,summary,SjmsConsumer and SjmsProducer do not remove thread pool when stop,code_debt,multi-thread_correctness,"Tue, 19 Aug 2014 00:37:45 +0000","Fri, 5 Sep 2014 07:09:36 +0000","Wed, 20 Aug 2014 05:51:21 +0000",105216,SjmsConsumer and SjmsProducer do not remove thread pool when stop,0.2,0.2,negative
camel,8174,comment_4,"Hi Claus, It's good to see that we have better API to track the long processing exchange, I will remove the from the repo shortly.",code_debt,dead_code,"Tue, 23 Dec 2014 14:10:36 +0000","Sun, 15 Feb 2015 08:23:13 +0000","Sat, 31 Jan 2015 08:53:41 +0000",3350585,"Hi Claus, It's good to see that we have better API to track the long processing exchange, I will remove the TimeoutInflightRepository from the repo shortly.",0.638,0.638,positive
camel,8328,comment_3,I've changed the name of the test package as OSX seems to confuse test package name with class Can you give it a shot now?,code_debt,low_quality_code,"Tue, 10 Feb 2015 05:58:48 +0000","Mon, 16 Feb 2015 14:18:45 +0000","Sun, 15 Feb 2015 21:01:35 +0000",486167,I've changed the name of the test package as OSX seems to confuse test package name org.apache.camel.spring.boot.fatjarrouter with class org.apache.camel.spring.boot.FatJarRouter. Can you give it a shot now?,-0.4,-0.03333333333,neutral
camel,872,comment_0,"This is the third time I attempt this and the change set becomes pretty large, making it quite hard to deal with failures. I will make the fixes in a few stages and this will impact other developers with potential need for merge. It would be great if you could avoid commits for the next 24-48 hours or so (if it's not too much to ask).",code_debt,low_quality_code,"Mon, 1 Sep 2008 09:23:29 +0000","Fri, 31 Jul 2009 06:33:35 +0000","Tue, 11 Nov 2008 22:37:48 +0000",6182059,"This is the third time I attempt this and the change set becomes pretty large, making it quite hard to deal with failures. I will make the fixes in a few stages and this will impact other developers with potential need for merge. It would be great if you could avoid commits for the next 24-48 hours or so (if it's not too much to ask).",0.04722222222,0.04722222222,negative
camel,8734,comment_0,"Hi, Please consider that URLs should be case sensitive according to W3. [quote]URLs in general are case-sensitive (with the exception of machine names). There may be URLs, or parts of URLs, where case doesn't matter, but identifying these may not be easy. Users should always consider that URLs are The header CamelHttpPath should provide me what the user actually entered.",code_debt,low_quality_code,"Sun, 3 May 2015 09:47:45 +0000","Fri, 11 Sep 2015 14:19:08 +0000","Fri, 17 Jul 2015 09:07:17 +0000",6477572,"Hi, Please consider that URLs should be case sensitive according to W3. [quote]URLs in general are case-sensitive (with the exception of machine names). There may be URLs, or parts of URLs, where case doesn't matter, but identifying these may not be easy. Users should always consider that URLs are case-sensitive.[/quote] http://www.w3.org/TR/WD-html40-970708/htmlweb.html The header CamelHttpPath should provide me what the user actually entered.",-0.026,-0.0208,neutral
camel,910,comment_0,#9 its not easily understood how the client can send using the template. Maybe divide the server and client into each section. The client just need much less code than the server. #10 webservice example. It is not clear that its the *true* parameter that turns the multicast into parallel mode. This is not documented to well in the code.,code_debt,low_quality_code,"Tue, 16 Sep 2008 17:47:21 +0000","Thu, 23 Oct 2008 04:39:20 +0000","Mon, 22 Sep 2008 11:55:24 +0000",497283,#9 its not easily understood how the client can send using the template. Maybe divide the server and client into each section. The client just need much less code than the server. #10 webservice example. It is not clear that its the true parameter that turns the multicast into parallel mode. This is not documented to well in the code.,-0.2006666667,-0.2301666667,negative
camel,9499,summary,JMS - destination type for temp should use dash instead of colon,code_debt,low_quality_code,"Mon, 11 Jan 2016 12:04:01 +0000","Tue, 19 Jan 2016 18:15:04 +0000","Tue, 19 Jan 2016 18:15:04 +0000",713463,JMS - destination type for temp should use dash instead of colon,0,0,neutral
camel,9752,description,"When any file:// or ftp:// consumer has a quartz2 schedule it can start throwing exceptions because too many worker-threads are busy at the same time Both workers can find the same files during filling the maxMessagesPerPoll, or during processing of those files. This happens when the route can not process all files before the next trigger happens. example (every minute): Attached you can find a stacktrace that would happen very often if worker1 processes and moves files that worker2 would also like to start processing. This does not happen when using scheduler=spring or when using delay=1m. The only way I have found to make sure a file or ftp component does not use multiple threads while consuming very large batches is to annotate the class with I am not familiar enough with the Camelcode to say what side effects it has and if this would prevent any quartz job in camel to now be single threaded, even if the user does not want it to be. But to me it looks like an oversight when moving from quartz to quartz2. A file or ftp consumer should be single threaded while retrieving.",code_debt,multi-thread_correctness,"Thu, 24 Mar 2016 08:51:36 +0000","Thu, 24 Mar 2016 10:30:53 +0000","Thu, 24 Mar 2016 10:30:53 +0000",5957,"When any file:// or ftp:// consumer has a quartz2 schedule it can start throwing exceptions because too many worker-threads are busy at the same time (FileNotFoundException). Both workers can find the same files during filling the maxMessagesPerPoll, or during processing of those files. This happens when the route can not process all files before the next trigger happens. example (every minute): from(file:///mnt/sl-nl/bij/outbox/?sortBy=ignoreCase:file:name&filter=#fileFilter&recursive=false&move=processed&moveFailed=failed&scheduler.cron=0+0/1+0-23+?+*+1,2,3,4,5,6,7&scheduler=quartz2&scheduler.triggerId=nl_bij-export-to-archive-276) to(file:///data/work/sl/work-archive/work/276/) Attached you can find a stacktrace that would happen very often if worker1 processes and moves files that worker2 would also like to start processing. This does not happen when using scheduler=spring or when using delay=1m. The only way I have found to make sure a file or ftp component does not use multiple threads while consuming very large batches is to annotate the QuartzScheduledPollConsumerJob class with @DisallowConcurrentExecution. I am not familiar enough with the Camelcode to say what side effects it has and if this would prevent any quartz job in camel to now be single threaded, even if the user does not want it to be. But to me it looks like an oversight when moving from quartz to quartz2. A file or ftp consumer should be single threaded while retrieving.",-0.1298611111,-0.090625,neutral
camel,9812,description,"After shutting down a camel context, there are still threads running kafka consumers. In the logs after the shutdown I can see: So in theory the context is stopped, but I can see threads running with the polling of the sockets of kafka consumers (see attached immage). This deployed in an application server (wilfly in my case), causes a lot of issues, because apps get deployed and undeployed without stopping the JVM, but threads from previous deployments are left there. Please also bear in mind that kafka (9.0.1) throws warning messages due to the fact that un expected config items are thrown to the kafka consumer properties. Thanks!",code_debt,multi-thread_correctness,"Mon, 4 Apr 2016 13:00:56 +0000","Tue, 5 Apr 2016 11:19:48 +0000","Tue, 5 Apr 2016 09:24:07 +0000",73391,"After shutting down a camel context, there are still threads running kafka consumers. In the logs after the shutdown I can see: So in theory the context is stopped, but I can see threads running with the polling of the sockets of kafka consumers (see attached immage). This deployed in an application server (wilfly in my case), causes a lot of issues, because apps get deployed and undeployed without stopping the JVM, but threads from previous deployments are left there. Please also bear in mind that kafka (9.0.1) throws warning messages due to the fact that un expected config items are thrown to the kafka consumer properties. Thanks!",0.0389,0.0389,neutral
camel,6735,comment_1,This works in 2.12.x onwards. Hunting this down on 2.11.x is low priority. End users is encourage to upgrade if they really need this.,defect_debt,uncorrected_known_defects,"Wed, 11 Sep 2013 20:25:23 +0000","Fri, 14 Feb 2014 16:02:02 +0000","Fri, 14 Feb 2014 16:02:02 +0000",13462599,This works in 2.12.x onwards. Hunting this down on 2.11.x is low priority. End users is encourage to upgrade if they really need this.,0.1599,0.1599,neutral
camel,8312,comment_2,"Hi Claus, are you sure that you want to delay this till 2.15.0? I think this issue is serious. Best regards Stephan",defect_debt,uncorrected_known_defects,"Wed, 4 Feb 2015 07:25:25 +0000","Tue, 3 Mar 2015 21:12:58 +0000","Mon, 2 Mar 2015 12:01:31 +0000",2262966,"Hi Claus, are you sure that you want to delay this till 2.15.0? I think this issue is serious. Best regards Stephan",0.2916666667,0.2916666667,negative
camel,10048,comment_1,"How to fix? I think, the best way is to remove this dangerous caching at all. There might be a temptation to implement equals() and hashCode() methods in the helper class in a way to delegate both these calls to the processor wrapped by this class. However, the root cause of the problem is the incorrect usage of a hash map. Key must implement equals() and hashCode(). We cannot require all implementations of Processor and RouteContext to implement these methods - it would be an unmotivated bloating of their contracts with irrelevant functionality. Error handlers in RoutingSlip are short-living objects, they shouldn't get into Old Gen, so GC will clean them without significant performance overhead.",design_debt,non-optimal_design,"Sat, 11 Jun 2016 21:41:44 +0000","Thu, 4 Jan 2018 09:11:44 +0000","Sun, 12 Jun 2016 09:44:54 +0000",43390,"How to fix? I think, the best way is to remove this dangerous caching at all. There might be a temptation to implement equals() and hashCode() methods in the helper class AsyncProcessorConverterHelper.ProcessorToAsyncProcessorBridge in a way to delegate both these calls to the processor wrapped by this class. However, the root cause of the problem is the incorrect usage of a hash map. Key must implement equals() and hashCode(). We cannot require all implementations of Processor and RouteContext to implement these methods - it would be an unmotivated bloating of their contracts with irrelevant functionality. Error handlers in RoutingSlip are short-living objects, they shouldn't get into Old Gen, so GC will clean them without significant performance overhead.",0.07083333333,0.06197916667,negative
camel,10048,description,"RoutingSlip has a cache of error handlers implemented as a ConcurrentHashMap. This map stores error handlers as values, and uses some synthetic objects as keys. For some kind of destinations provided in routing slip, map lookup operation does not work. Hence, new error handlers are always added to the map and existing error handlers never reused. Finally, the program runs out of memory. The synthetic keys are actually instances of class Such key is based on two objects: RouteContext and destination Processor. Neither RouteContext nor Processor do not require their implementations to provide equals() and hashCode() methods. Strictly speaking, caching implementation in RoutingSlip is incorrect, because it uses hash map in the discouraged way. However, for some cases it works. The problem occurs when routing slip contains a 'sync' destination, in other words - destination is a Processor that does not implement AsyncProcessor interface. RoutingSlip determines destination producer via and the latter uses method. This method creates new instance of Processor for every processor that is not an instance of AsyncProcessor. This is where the problem hides: new object has different hash code (defined by Object.hashCode()) and new object isn't equal to the object used as a key in the hash map (well, Object.equals()). Finally, new key for the hash map is calculated, lookup operation cannot find this key in the hash map, new key-value pair is put into the hash map.",design_debt,non-optimal_design,"Sat, 11 Jun 2016 21:41:44 +0000","Thu, 4 Jan 2018 09:11:44 +0000","Sun, 12 Jun 2016 09:44:54 +0000",43390,"RoutingSlip has a cache of error handlers implemented as a ConcurrentHashMap. This map stores error handlers as values, and uses some synthetic objects as keys. For some kind of destinations provided in routing slip, map lookup operation does not work. Hence, new error handlers are always added to the map and existing error handlers never reused. Finally, the program runs out of memory. The synthetic keys are actually instances of class RoutingSlip.PreparedErrorHandler. Such key is based on two objects: RouteContext and destination Processor. Neither RouteContext nor Processor do not require their implementations to provide equals() and hashCode() methods. Strictly speaking, caching implementation in RoutingSlip is incorrect, because it uses hash map in the discouraged way. However, for some cases it works. The problem occurs when routing slip contains a 'sync' destination, in other words - destination is a Processor that does not implement AsyncProcessor interface. RoutingSlip determines destination producer via ProducerCache.doInAsyncProducer(), and the latter uses AsyncProcessorConverterHelper.convert() method. This method creates new instance of Processor for every processor that is not an instance of AsyncProcessor. This is where the problem hides: new object has different hash code (defined by Object.hashCode()) and new object isn't equal to the object used as a key in the hash map (well, Object.equals()). Finally, new key for the hash map is calculated, lookup operation cannot find this key in the hash map, new key-value pair is put into the hash map.",-0.02797916667,-0.02238333333,neutral
camel,10476,comment_1,Let me know if I can provide any more information or if something isn't clear. I also was able to successfully test the workaround and I can run locally(in an IDE) using java:exec with the same properties file that the unit tests load by overriding and reading that config file. The only thing I am not confident in with the workaround is the difference between running in a DefaultCamelContext vs a in the IDE.,design_debt,non-optimal_design,"Mon, 14 Nov 2016 16:09:36 +0000","Fri, 16 Dec 2016 08:10:55 +0000","Fri, 18 Nov 2016 17:09:14 +0000",349178,Let me know if I can provide any more information or if something isn't clear. I also was able to successfully test the workaround and I can run locally(in an IDE) using java:exec with the same properties file that the unit tests load by overriding loadConfigAdminConfigurationFile and reading that config file. The only thing I am not confident in with the workaround is the difference between running in a DefaultCamelContext vs a BlueprintCamelContext in the IDE.,0.04866666667,0.04866666667,neutral
camel,10476,description,"Problem: When running with a Camel Blueprint project a configAdminFile is not used to populate propertyplacehoders in when exectued with So a user can't run camel locally in a similar way to running in Karaf with file based property placeholder values. Workaround: I think, but haven't tested yet, that you can work around this locally using the methods described here: and/or how this solution appears to use exec:java locally and loads the properties via To reproduce the problem: Create a new project using (You need to change the log4j config to make it run.) To reduce the time, I created a project that runs here: Instead of using a default in the blueprint XML for the I setup the POM to include the following: In Camel 2.15.2 or earlier, this file would be loaded when mvn camel:run was invoked and the properties would be available via the PID at run time. After the changes made in CAMEL-9313, it appears that the method is only called in when the createTestBundle pathway is taken in java.lang.String, boolean, java.lang.String, java.lang.String, java.lang.String, So it appears test using get this functionality (as shown by the tests) but things executed from camel:run do not. Here you can see in Camel 2.14 that call to is made after the bundelContext is created. In the master branch version, that call is no longer made from main after the context is returned. I made a change locally to add a similar call to in Camel 2.18: Here is the output of the log statement from the example before this change: Here is the output of the log statement from the example after this change: As you can see before the change, the ${greeting} property is not poplulated via propertyplacehoder. After the change it is replaced. Given all the discussion of timing related issues in CAMEL-9313, I'm hesitant to say this is a good enough solution or that it aligns with the intention of the changes made in that fix. Given that configAdminFileName and configAdminPid are passed into perhaps the call to should happen inside createBundleContext or one of it sub-methods. Overall, I ""think"" a user should be able to use the configAdminPid and configAdminFileName settings to load properties via camel:run rather than work aound it, but I could be persumptious there.",design_debt,non-optimal_design,"Mon, 14 Nov 2016 16:09:36 +0000","Fri, 16 Dec 2016 08:10:55 +0000","Fri, 18 Nov 2016 17:09:14 +0000",349178,"Problem: When running with a Camel Blueprint project a configAdminFile is not used to populate propertyplacehoders in camel-test-blueprint when exectued with camel-maven-plugin(camel:run). So a user can't run camel locally in a similar way to running in Karaf with file based property placeholder values. Workaround: I think, but haven't tested yet, that you can work around this locally using the methods described here: http://ggrzybek.blogspot.com/2015/12/camel-blueprint-test-support.html and/or how this solution https://github.com/cschneider/Karaf-Tutorial/tree/master/camel/order/src appears to use exec:java locally and loads the properties via PropertiesComponent. To reproduce the problem: Create a new project using camel-archetype-blueprint. (You need to change the log4j config to make it run.) To reduce the time, I created a project that runs here: https://github.com/ryanco/propertyconfig. Instead of using a default in the blueprint XML for the propertyplaceholder, I setup the POM to include the following: In Camel 2.15.2 or earlier, this file would be loaded when mvn camel:run was invoked and the properties would be available via the PID at run time. After the changes made in CAMEL-9313, it appears that the method org.apache.camel.test.blueprint.CamelBlueprintHelper#setPersistentFileForConfigAdmin is only called in when the createTestBundle pathway is taken in org.apache.camel.test.blueprint.CamelBlueprintHelper#createBundleContext(java.lang.String, java.lang.String, boolean, java.lang.String, java.lang.String, java.lang.String, java.lang.String[]...). So it appears test using CamelBlueprintTestSupport get this functionality (as shown by the tests) but things executed from camel:run do not. Here you can see in Camel 2.14 that call to org.apache.camel.test.blueprint.CamelBlueprintHelper#setPersistentFileForConfigAdmin is made after the bundelContext is created. https://github.com/apache/camel/blob/camel-2.14.x/components/camel-test-blueprint/src/main/java/org/apache/camel/test/blueprint/Main.java#L103 In the master branch version, that call is no longer made from main after the context is returned. https://github.com/apache/camel/blob/master/components/camel-test-blueprint/src/main/java/org/apache/camel/test/blueprint/Main.java#L106 I made a change locally to add a similar call to org.apache.camel.test.blueprint.CamelBlueprintHelper#setPersistentFileForConfigAdmin in Camel 2.18: Here is the output of the log statement from the example before this change: Here is the output of the log statement from the example after this change: As you can see before the change, the ${greeting} property is not poplulated via propertyplacehoder. After the change it is replaced. Given all the discussion of timing related issues in CAMEL-9313, I'm hesitant to say this is a good enough solution or that it aligns with the intention of the changes made in that fix. Given that configAdminFileName and configAdminPid are passed into createBundleContext, perhaps the call to org.apache.camel.test.blueprint.CamelBlueprintHelper#setPersistentFileForConfigAdmin should happen inside createBundleContext or one of it sub-methods. Overall, I ""think"" a user should be able to use the configAdminPid and configAdminFileName settings to load properties via camel:run rather than work aound it, but I could be persumptious there.",0.002364035088,-0.004440251572,neutral
camel,10678,description,We should use each individual fields instead of a string field eg use fields - from - to - name - model As individual fields. And use name as the index field if its intended to be unique. This allows users to better query/browse the registry from JMX as well to find which transformers there is. Also remember to adjust the code in the jolokia command.,design_debt,non-optimal_design,"Fri, 6 Jan 2017 08:23:41 +0000","Fri, 6 Jan 2017 13:45:21 +0000","Fri, 6 Jan 2017 13:45:21 +0000",19300,We should use each individual fields instead of a string field https://github.com/apache/camel/blob/master/camel-core/src/main/java/org/apache/camel/management/mbean/ManagedTransformerRegistry.java#L86 eg use fields from to name model As individual fields. And use name as the index field if its intended to be unique. This allows users to better query/browse the registry from JMX as well to find which transformers there is. Also remember to adjust the code in the jolokia command. https://github.com/apache/camel/blob/master/platforms/commands/commands-jolokia/src/main/java/org/apache/camel/commands/jolokia/DefaultJolokiaCamelController.java#L763,0.266375,0.2131,neutral
camel,10950,description,"One of the drawbacks with the docker-java library used in the camel-docker component is that it depends on Jersey for the JAX-RS client. This can be problematic if you already have another JAX-RS implementation on the classpath. Therefore, it'd be nice if you could specify the that you want to work with. By default it'd be but you could choose to use the if you wanted to avoid Jersey. Similarly, users could implement their own and have camel-docker load this when it comes to build the Docker client.",design_debt,non-optimal_design,"Mon, 6 Mar 2017 13:53:42 +0000","Tue, 7 Mar 2017 12:50:27 +0000","Tue, 7 Mar 2017 12:50:27 +0000",82605,"One of the drawbacks with the docker-java library used in the camel-docker component is that it depends on Jersey for the JAX-RS client. This can be problematic if you already have another JAX-RS implementation on the classpath. Therefore, it'd be nice if you could specify the DockerCmdExecFactory that you want to work with. By default it'd be JerseyDockerCmdExecFactory, but you could choose to use the NettyDockerCmdExecFactory if you wanted to avoid Jersey. Similarly, users could implement their own DockerCmdExecFactory and have camel-docker load this when it comes to build the Docker client.",0.03833333333,0.03833333333,neutral
camel,1112,description,"when reading files from file,ftp/sftp component, it is often neccesarry to get the files in a predifined order (e.g. created date or filename). to solve this, an extra uri parameter could be used, to set the order of files beeing processed. ordering can be done after reading the file-list. i know this can also be solved by applying a resequencer pattern in the route, but i think for large files it would be hard to apply this. also a processing in an created date order would be more natural.",design_debt,non-optimal_design,"Mon, 24 Nov 2008 15:48:05 +0000","Fri, 31 Jul 2009 06:33:43 +0000","Tue, 2 Dec 2008 07:41:32 +0000",662007,"when reading files from file,ftp/sftp component, it is often neccesarry to get the files in a predifined order (e.g. created date or filename). to solve this, an extra uri parameter could be used, to set the order of files beeing processed. ordering can be done after reading the file-list. i know this can also be solved by applying a resequencer pattern in the route, but i think for large files it would be hard to apply this. also a processing in an created date order would be more natural.",0.08,0.08,neutral
camel,11196,comment_0,"We should also make it easy to reuse an existing connector and have separate configuration, eg so the same user can use a twitter-connector and then configure it for user A and user B in the same application without configuration clashes. Today the component level would reuse the same component / configuration. So we may want to find a way to clone a component or whatever we can think of. A little bit of problem is the generated spring boot auto configuration which is hard-coded to one prefix.",design_debt,non-optimal_design,"Mon, 24 Apr 2017 14:22:57 +0000","Mon, 15 May 2017 15:18:48 +0000","Mon, 15 May 2017 15:18:48 +0000",1817751,"We should also make it easy to reuse an existing connector and have separate configuration, eg so the same user can use a twitter-connector and then configure it for user A and user B in the same application without configuration clashes. Today the component level would reuse the same component / configuration. So we may want to find a way to clone a component or whatever we can think of. A little bit of problem is the generated spring boot auto configuration which is hard-coded to one prefix.",0.048,0.048,neutral
camel,11196,description,"A Camel connector can be configured on two levels - component - endpoint Just like a regular Camel component. But we should allow users to configure a connector in one place, and not worry about if its component or endpoint level. And then let camel-connector when it prepares the connector figure out all of this for you. This may require supporting loading configuration from external resource files such as .properties files etc. Or something else. But from end users we can make this easier.",design_debt,non-optimal_design,"Mon, 24 Apr 2017 14:22:57 +0000","Mon, 15 May 2017 15:18:48 +0000","Mon, 15 May 2017 15:18:48 +0000",1817751,"A Camel connector can be configured on two levels component endpoint Just like a regular Camel component. But we should allow users to configure a connector in one place, and not worry about if its component or endpoint level. And then let camel-connector when it prepares the connector figure out all of this for you. This may require supporting loading configuration from external resource files such as .properties files etc. Or something else. But from end users we can make this easier.",0.06521428571,0.06521428571,neutral
camel,11282,description,We should extend the plain DefaultComponent (the is deprecated) ensure there is a plain default no-arg constructor so it makes using and configuring components easier. See eg SO If it was just a plain no-arg constructor then the bean style would have worked.,design_debt,non-optimal_design,"Tue, 16 May 2017 08:07:42 +0000","Thu, 25 Jul 2019 13:39:20 +0000","Thu, 25 Jul 2019 13:39:20 +0000",69139898,We should extend the plain DefaultComponent (the UriEndpointComponent is deprecated) ensure there is a plain default no-arg constructor so it makes using and configuring components easier. See eg SO http://stackoverflow.com/questions/43918252/how-to-increase-or-configure-maxthreads-in-apache-came-restlet?noredirect=1#comment74982775_43918252 If it was just a plain no-arg constructor then the bean style would have worked.,-0.535,-0.535,neutral
camel,1138,comment_1,These maps is part of some code logic to determine if the file has been changed using timestamp and filesize checks. All this code has been @deprecated and removed in 2.0. It leads to unexpected behavior and is hard to test. And shouldn't generally be used. If you use file connectivity then you should either delete or move files after they are processed and not keep then around. As a fix for this in 1.5.1 I have added the LRUCache so the maps will contain at most 1000 elements.,design_debt,non-optimal_design,"Tue, 2 Dec 2008 16:36:14 +0000","Mon, 23 Mar 2009 08:40:34 +0000","Wed, 3 Dec 2008 17:15:46 +0000",88772,These maps is part of some code logic to determine if the file has been changed using timestamp and filesize checks. All this code has been @deprecated and removed in 2.0. It leads to unexpected behavior and is hard to test. And shouldn't generally be used. If you use file connectivity then you should either delete or move files after they are processed and not keep then around. As a fix for this in 1.5.1 I have added the LRUCache so the maps will contain at most 1000 elements.,-0.1375,-0.1375,negative
camel,1138,comment_4,"I think so. It actually solves the leak problem. The side effect is that if a file is not moved/renamed after 1000 other entries/files it will disappear from the lru cache and look new again and get reprocessed, which may also lead to an infinite loop (that would make another one of those 1000+ files new/out of cache, and so on). However I don't think this scenario is a valid one as it has other flaws, such as the fact that the state is lost after a camel restart, so files would get processed again in that case too (a comment in the code mentions that, actually). I would add a warn in the wiki that not renaming/moving files is a dangerous scenario and should not be used.",design_debt,non-optimal_design,"Tue, 2 Dec 2008 16:36:14 +0000","Mon, 23 Mar 2009 08:40:34 +0000","Wed, 3 Dec 2008 17:15:46 +0000",88772,"I think so. It actually solves the leak problem. The side effect is that if a file is not moved/renamed after 1000 other entries/files it will disappear from the lru cache and look new again and get reprocessed, which may also lead to an infinite loop (that would make another one of those 1000+ files new/out of cache, and so on). However I don't think this scenario is a valid one as it has other flaws, such as the fact that the state is lost after a camel restart, so files would get processed again in that case too (a comment in the code mentions that, actually). I would add a warn in the wiki that not renaming/moving files is a dangerous scenario and should not be used.",-0.1216666667,-0.1216666667,neutral
camel,11655,comment_0,"IMHO this's a breaking change for the upcoming {{2.20.0}} version as the query parameter has been simply removed and replaced with a new {{encryption}} query parameter, which _would_ break user's code making use of this option. AFAIK a minor release should not include any breaking changes, right? Shouldn't we better somehow mark as deprecated and encourage users to make use of the new {{encryption}} parameter instead? Other than that some feedback on your made code changes: What would be wrong to do: Instead of the following if / else if: As because: Also maybe mark the enum itself as deprecated so we don't forget to remove it in Camel 3.",design_debt,non-optimal_design,"Wed, 9 Aug 2017 12:27:03 +0000","Tue, 22 Aug 2017 15:38:03 +0000","Tue, 22 Aug 2017 06:30:11 +0000",1101788,"ancosen IMHO this's a breaking change for the upcoming 2.20.0 version as the encryptionMethod query parameter has been simply removed and replaced with a new encryption query parameter, which would break user's code making use of this option. AFAIK a minor release should not include any breaking changes, right? Shouldn't we better somehow mark encryptionMethod as deprecated and encourage users to make use of the new encryption parameter instead? Other than that some feedback on your made code changes: What would be wrong to do: Instead of the following if / else if: As because: Also maybe mark the NagiosEncryptionMethod enum itself as deprecated so we don't forget to remove it in Camel 3.",0.12025,0.12025,negative
camel,11655,comment_4,"LOL it's not critical for me but for the community :-) I am not convineced by your answers and kindly ask  &  to share their thoughts about this ticket as well as it's corresponding code changes. To summurize: - The made changes are breaking for the upcoming 2.20.0 _minor_ release if a Camel based application already makes use of the query parameter today in production. - IMHO Camel should be transparent and not restrict an underlying library API just because it seems to be buggy or non-working. Bugs _should_ be resolved by the underlying library itself and not _artificially_ through Camel by hiding/restricting a given API. See my comments above regarding this point. - In general, shouldn't we mark a Camel API as deprecated and encourage users to make use of the right/new API. E.g. as it's done when some {{StringHelper}} new utiliy methods were extracted out of {{ObjectHelper}} and the corresponding {{ObjectHelper}} methods were marked as deprecated, see CAMEL-10389. This would make it much much easier both for Camel code base itself as well as user's code to avoid using the legacy API. I guess that's exactly what the {{@Deprecated}} annotation good for. I don't seem to understand what it would be wrong with such an approach, that's, marking query parameter as deprecaterd and encourage useres to make use of the new {{encryption}} query parameter and then support both of them. Supporting both would not break a pre-existing application on the production and then we could find and remove the deprecated API by Camel 3 much easier. WDYT?",design_debt,non-optimal_design,"Wed, 9 Aug 2017 12:27:03 +0000","Tue, 22 Aug 2017 15:38:03 +0000","Tue, 22 Aug 2017 06:30:11 +0000",1101788,"If this is something critical for you, you can revert commit and do what you think is best for this component. It was just an attempt to avoid the death of camel-nagios. ancosen LOL it's not critical for me but for the community I am not convineced by your answers and kindly ask davsclaus & lb to share their thoughts about this ticket as well as it's corresponding code changes. To summurize: The made changes are breaking for the upcoming 2.20.0 minor release if a Camel based application already makes use of the encryptionMethod query parameter today in production. IMHO Camel should be transparent and not restrict an underlying library API just because it seems to be buggy or non-working. Bugs should be resolved by the underlying library itself and not artificially through Camel by hiding/restricting a given API. See my comments above regarding this point. In general, shouldn't we mark a not-recommended-to-be-used Camel API as deprecated and encourage users to make use of the right/new API. E.g. as it's done when some StringHelper new utiliy methods were extracted out of ObjectHelper and the corresponding ObjectHelper methods were marked as deprecated, see CAMEL-10389. This would make it much much easier both for Camel code base itself as well as user's code to avoid using the legacy API. I guess that's exactly what the @Deprecated annotation good for. I don't seem to understand what it would be wrong with such an approach, that's, marking encryptionMethod query parameter as deprecaterd and encourage useres to make use of the new encryption query parameter and then support both of them. Supporting both would not break a pre-existing application on the production and then we could find and remove the deprecated API by Camel 3 much easier. WDYT?",0.2647866667,0.1805833333,neutral
camel,11868,comment_1,"It might be a good idea, since not all features is supported in the high level rest client yet I already got it to work with the rest client, but then I would suggest we create a super class for and a and In the we decide which one we should instantiate based on the type client on the class path. This of course mean the client dependency is not included in the the component. Regarding the it's not possible to use the Hight level REST client since it only support ELK from 5.6.0 and forward. One think to be aware of with the REST API that it is slightly different, which mean various request and response object is not called the same with the REST client. So either we have to document this in the manual or we have to come up with a common request / response object for both clients",design_debt,non-optimal_design,"Sat, 30 Sep 2017 05:29:19 +0000","Wed, 18 Oct 2017 06:54:13 +0000","Wed, 18 Oct 2017 06:54:13 +0000",1560294,"It might be a good idea, since not all features is supported in the high level rest client yet I already got it to work with the rest client, but then I would suggest we create a super class for ElasticsearchProducer and a ElasticsearchTransportProducer and ElasticsearchRestProducer. In the ElasticsearchEndpoint we decide which one we should instantiate based on the type client on the class path. This of course mean the client dependency is not included in the the camel-elasticsearch5 component. Regarding the camel-elasticsearch, it's not possible to use the Hight level REST client since it only support ELK from 5.6.0 and forward. One think to be aware of with the REST API that it is slightly different, which mean various request and response object is not called the same with the REST client. So either we have to document this in the manual or we have to come up with a common request / response object for both clients",-0.1164,-0.097,neutral
camel,11868,comment_2,"Can we not create a new component that use this new rest client only. And then the two older components can stay for older users, and eventually we can start marking them as deprecated because the java client is EOL from Elastic and the rest client is the future/way to go.",design_debt,non-optimal_design,"Sat, 30 Sep 2017 05:29:19 +0000","Wed, 18 Oct 2017 06:54:13 +0000","Wed, 18 Oct 2017 06:54:13 +0000",1560294,"Can we not create a new camel-elasticsearch5-rest component that use this new rest client only. And then the two older components can stay for older users, and eventually we can start marking them as deprecated because the java client is EOL from Elastic and the rest client is the future/way to go.",0.3125,0.3125,neutral
camel,12104,comment_5,I think we can extend cxf Continuation interface a bit to add an isTimeout method so that we know the timeout happen and can handle this situation accordingly outside CXF,design_debt,non-optimal_design,"Wed, 27 Dec 2017 18:31:14 +0000","Mon, 26 Mar 2018 15:56:24 +0000","Tue, 20 Mar 2018 01:27:00 +0000",7109746,I think we can extend cxf Continuation interface a bit to add an isTimeout method so that we know the timeout happen and can handle this situation accordingly outside CXF,0.2,0.2,neutral
camel,12104,description,"There is very strange behavior in Camel cxf and cxfrs timeouts which could lead to sensitive data being released. Below is a code sample which illustrates the unexpected behavior. I think any developer would expect the test API to return ""Valid Response"" or some kind exception, but in fact it returns ""SENSITIVE DATA"" due to the default continuationTimeout of 30 seconds. This issue seems to have been introduced by",design_debt,non-optimal_design,"Wed, 27 Dec 2017 18:31:14 +0000","Mon, 26 Mar 2018 15:56:24 +0000","Tue, 20 Mar 2018 01:27:00 +0000",7109746,"There is very strange behavior in Camel cxf and cxfrs timeouts which could lead to sensitive data being released. Below is a code sample which illustrates the unexpected behavior. I think any developer would expect the test API to return ""Valid Response"" or some kind exception, but in fact it returns ""SENSITIVE DATA"" due to the default continuationTimeout of 30 seconds. This issue seems to have been introduced by https://issues.apache.org/jira/browse/CAMEL-7401",-0.175,-0.175,negative
camel,12104,summary,Unintuitive default cxf timeout behavior,design_debt,non-optimal_design,"Wed, 27 Dec 2017 18:31:14 +0000","Mon, 26 Mar 2018 15:56:24 +0000","Tue, 20 Mar 2018 01:27:00 +0000",7109746,Unintuitive default cxf timeout behavior,-0.5,-0.5,neutral
camel,12624,description,"Currently we are running Camel AMQP component against Active MQ 5 (Amazon MQ) we want to move to an Artemis solution but this hasn't worked seamlessly. In CAMEL-9204 I believe a hardcoded topic prefix of ""topic://"" was introduced I think for a workaround of an Active MQ 5 bug. In Artemis this means Camel connects to a topic named instead of and therefore receives no events. The only possible workaround is to manually create the connection factory which means then the topic prefix is not set. My believe is that the hardcoded topic prefix is a bug and should be removed or an option to override at the AMQP component level should be introduced. Bug location: AMQPComponent.java line 59 Workaround:",design_debt,non-optimal_design,"Wed, 4 Jul 2018 21:44:12 +0000","Wed, 18 Jul 2018 03:44:00 +0000","Fri, 13 Jul 2018 15:29:16 +0000",755104,"Currently we are running Camel AMQP component against Active MQ 5 (Amazon MQ) we want to move to an Artemis solution but this hasn't worked seamlessly. In CAMEL-9204I believe a hardcoded topic prefix of ""topic://"" was introduced I think for a workaround of an Active MQ 5 bug. In Artemis this means Camel connects to a topic named ""topic://example.topic.event"" instead of ""example.topic.event"" and therefore receives no events. The only possible workaround is to manually create the connection factory which means then the topic prefix is not set. My believe is that the hardcoded topic prefix is a bug and should be removed or an option to override at the AMQP component level should be introduced. Bug location: AMQPComponent.java line 59 Workaround:",-0.007142857143,-0.005555555556,negative
camel,12646,description,"If you have complex types like and wants to allow to configure this via spring boot autoconfiguration in - then the generated spring boot classes with all the options will use getter/setter of types That seems correct, but the spring-boot tooling itself (that generates additional json file) will skip those as it only support primitives and string types. So we may need to fool, and generate the getter/setter as String type as you use it for configuring it as a bean reference by id anyway, eg = #myDataSource We can add in the javadoc that the type is",design_debt,non-optimal_design,"Thu, 12 Jul 2018 12:44:45 +0000","Sun, 15 Jul 2018 07:00:07 +0000","Sun, 15 Jul 2018 07:00:07 +0000",238522,"If you have complex types like javax.sql.DataSource and wants to allow to configure this via spring boot autoconfiguration in application.properties - then the generated spring boot classes with all the options will use getter/setter of types javax.sql.DataSource. That seems correct, but the spring-boot tooling itself (that generates additional json file) will skip those as it only support primitives and string types. So we may need to fool, and generate the getter/setter as String type as you use it for configuring it as a bean reference by id anyway, eg camel.component.jdbc.data-source = #myDataSource We can add in the javadoc that the type is javax.sql.DataSource.",0.04583333333,0.03365384615,neutral
camel,12646,summary,camel-spring-boot - Auto configuration of complex types should be more tooling friendly,design_debt,non-optimal_design,"Thu, 12 Jul 2018 12:44:45 +0000","Sun, 15 Jul 2018 07:00:07 +0000","Sun, 15 Jul 2018 07:00:07 +0000",238522,camel-spring-boot - Auto configuration of complex types should be more tooling friendly,0.344,0.344,neutral
camel,1320,comment_1,"It is very similar to zip data format - it even uses the same deflate algorithm. The point is, that zip data format uses deflate algorithm directly, while gzip would use gzip file format, that is (I believe more popular). The problem with zip data format is that is is not a zip really - zip is a format that compresses set of files, while our zip data format doesn't - it just compresses data using compression algorithm of zip. On the other hand gzip is a format that compresses data (not files), uses the same algorithm, but has its own headers, that has to be interpreted. I even believe that our zip data format should be renamed to 'deflate' data format as it is the name that describes what it really is.",design_debt,non-optimal_design,"Fri, 6 Feb 2009 14:55:23 +0000","Sat, 21 Nov 2009 11:57:55 +0000","Thu, 19 Mar 2009 15:18:16 +0000",3543773,"It is very similar to zip data format - it even uses the same deflate algorithm. The point is, that zip data format uses deflate algorithm directly, while gzip would use gzip file format, that is (I believe more popular). http://java.sun.com/j2se/1.5.0/docs/api/java/util/zip/GZIPInputStream.html http://java.sun.com/j2se/1.5.0/docs/api/java/util/zip/InflaterInputStream.html The problem with zip data format is that is is not a zip really - zip is a format that compresses set of files, while our zip data format doesn't - it just compresses data using compression algorithm of zip. On the other hand gzip is a format that compresses data (not files), uses the same algorithm, but has its own headers, that has to be interpreted. I even believe that our zip data format should be renamed to 'deflate' data format as it is the name that describes what it really is.",0.04,0.04,neutral
camel,13214,comment_0,"It should not be moved, but be there both places. Then endpoint override component level, this is how its done in other components.",design_debt,non-optimal_design,"Mon, 18 Feb 2019 09:17:11 +0000","Thu, 7 Mar 2019 03:49:48 +0000","Thu, 7 Mar 2019 03:49:39 +0000",1449148,"It should not be moved, but be there both places. Then endpoint override component level, this is how its done in other components.",0,0,neutral
camel,13681,description,Any of the options you can configure via such as: camel.main.name And so on should be configurable via ENV variables which will override any existing configuration. This is good practice in containers and also how SB can do etc.,design_debt,non-optimal_design,"Tue, 25 Jun 2019 09:40:24 +0000","Wed, 26 Jun 2019 12:46:38 +0000","Wed, 26 Jun 2019 12:46:38 +0000",97574,Any of the options you can configure via application.properties such as: camel.main.name camel.component.xxx=yyy And so on should be configurable via ENV variables which will override any existing configuration. This is good practice in containers and also how SB can do etc.,0.1595,0.09114285714,neutral
camel,149,description,Brian McCallister had this great idea.. add a thread() method to the DSL instead of using the seda component to do async processing. We should be able to: or ThreadPoolExecutor pool = ...,design_debt,non-optimal_design,"Mon, 17 Sep 2007 23:45:09 +0000","Mon, 12 May 2008 07:56:31 +0000","Mon, 8 Oct 2007 15:14:25 +0000",1783756,"Brian McCallister had this great idea.. add a thread() method to the DSL instead of using the seda component to do async processing. We should be able to: from(""file:foo"").thread(10).maxSize(20).to(""wombat:nugget"") or ThreadPoolExecutor pool = ... from(""file:foo"").thread(pool).to(""wombat:nugget"")",0.644,0.184,positive
camel,1608,comment_0,"Hello Hadrian In Maven, when you put the: <project declaration on multiple lines, Maven will flatten it into one line and throw out the preceding comments -in this case the copyright. I guess it also messes up all the spacing. On the OW2 projects, we solved the issue by putting the declaration on one line. It doesn't look very good that way, but it works :) Cheers S. Ali Tokmen",design_debt,non-optimal_design,"Wed, 13 May 2009 03:03:57 +0000","Sat, 21 Nov 2009 11:58:00 +0000","Tue, 16 Jun 2009 02:16:40 +0000",2934763,"Hello Hadrian In Maven, when you put the: <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd""> declaration on multiple lines, Maven will flatten it into one line and throw out the preceding comments -in this case the copyright. I guess it also messes up all the spacing. On the OW2 projects, we solved the issue by putting the declaration on one line. It doesn't look very good that way, but it works Cheers S. Ali Tokmen http://ali.tokmen.com/",0.089,0.06866666667,neutral
camel,1608,description,"When profiles are explicitly activated in maven using -P, the <activeByDefault However there is one more blocker I will have to take care before 1.6.1 can be released. During the release:prepare phase the poms are regenerated and the <?xml> decl, the apache license notice is lost and there are issues with white spaces. This seems related to the fact that the <project> element is not on one line as per the new maven release guide. Fixing this may take a day or two as we have so many poms but I'll do it as the highest prio. Once I'll get it to work and release 1.6.1, I'll tackle 2.0-M2. The good news is that once we have this fixed, the release process will be much simplified.",design_debt,non-optimal_design,"Wed, 13 May 2009 03:03:57 +0000","Sat, 21 Nov 2009 11:58:00 +0000","Tue, 16 Jun 2009 02:16:40 +0000",2934763,"When profiles are explicitly activated in maven using -P, the <activeByDefault> value specified in a profile activation configuration is not considered. This is the case with the maven-release-plugin configuration where the profiles to be activated are specified as <arguments>. As the enable-schemagen profile (and a couple others) where not specified, camel-schema.xsd was not generated and pretty much all tests using the xml dsl were failing. Many thanks to Dan Kulp from the Apache Maven team for helping with this. However there is one more blocker I will have to take care before 1.6.1 can be released. During the release:prepare phase the poms are regenerated and the <?xml> decl, the apache license notice is lost and there are issues with white spaces. This seems related to the fact that the <project> element is not on one line as per the new maven release guide. Fixing this may take a day or two as we have so many poms but I'll do it as the highest prio. Once I'll get it to work and release 1.6.1, I'll tackle 2.0-M2. The good news is that once we have this fixed, the release process will be much simplified.",0.2429285714,0.183375,neutral
camel,2094,description,We now have option {{autoStartup}} on routes. Lets also have this on the camel context itself so it replaces the option that is a bit confusing. For example the code above will not start Camel on startup. You can then later start camel manually by And then all runs. This also means that we will remove the option so if you use this option you must migrate to the autoStartup instead.,design_debt,non-optimal_design,"Thu, 22 Oct 2009 06:01:59 +0000","Thu, 3 Jun 2010 07:24:59 +0000","Thu, 22 Oct 2009 07:02:18 +0000",3619,We now have option autoStartup on routes. Lets also have this on the camel context itself so it replaces the shouldStartContext option that is a bit confusing. For example the code above will not start Camel on startup. You can then later start camel manually by And then all runs. This also means that we will remove the option shouldStartContext! so if you use this option you must migrate to the autoStartup instead.,0.0126,0.0105,neutral
camel,2163,comment_3,"Stan, yeah I can see the pain now. The original code is a producer which doesnt easily allow to spawn exchanges",design_debt,non-optimal_design,"Thu, 12 Nov 2009 08:32:16 +0000","Thu, 3 Jun 2010 07:25:25 +0000","Thu, 19 Nov 2009 11:57:46 +0000",617130,"Stan, yeah I can see the pain now. The original code is a producer which doesnt easily allow to spawn exchanges",-0.2375,-0.2375,negative
camel,2446,comment_0,Good work Chris. Is it possible to remove the as its not commonly use and the XML files should help new users to understand what happens. So its best to be a bit more verbose than playing tricks with auto wiring.,design_debt,non-optimal_design,"Thu, 4 Feb 2010 02:20:01 +0000","Sun, 24 Apr 2011 09:57:31 +0000","Thu, 26 Aug 2010 06:56:20 +0000",17555779,Good work Chris. Is it possible to remove the default-autowire as its not commonly use and the XML files should help new users to understand what happens. So its best to be a bit more verbose than playing tricks with auto wiring.,0.3876666667,0.371,positive
camel,2481,comment_2,@Gert Yes we have removed that as a default aggregator as its just too confusing for new users. We want people to force to provide their own aggregation strategy so they learn how to use them and merge the data themselves.,design_debt,non-optimal_design,"Wed, 17 Feb 2010 15:18:34 +0000","Sun, 24 Apr 2011 10:01:32 +0000","Wed, 17 Feb 2010 18:05:58 +0000",10044,@Gert Yes we have removed that UseLastAggregationStrategy as a default aggregator as its just too confusing for new users. We want people to force to provide their own aggregation strategy so they learn how to use them and merge the data themselves.,-0.02283333333,-0.02283333333,neutral
camel,2617,description,Currently CamelContext have a {{Properties}} for misc options. We should introduce a so we have type safe options in Java DSL and Spring XML. Then we also have one place to *advert* and document the options we currently support.,design_debt,non-optimal_design,"Tue, 6 Apr 2010 12:02:47 +0000","Sun, 24 Apr 2011 10:01:07 +0000","Wed, 16 Jun 2010 08:19:01 +0000",6120974,Currently CamelContext have a Properties for misc options. We should introduce a CamelContextPropertiesDefinition so we have type safe options in Java DSL and Spring XML. Then we also have one place to advert and document the options we currently support.,0.2166666667,0.2166666667,neutral
camel,2650,description,"This only applies for prototype scoped POJOs which uses @Produce or @Consume to inject a Camel Producer/Consumer. As the prototype scoped POJO is short lived and there could potentially be created many of those POJOs. Then @Produce / @Consume will inject new instances of Producer / Consumer as well. And since there is no standard way of knowing when the POJO is no longer in need, which is where we can to stop the Producer/Consumer. For singleton scoped this is not a problem as CamelContext will keep track on the created Producer/Consumer in its internal _servicesToClose_. Which is then stopped when CamelContext stops. For prototype we need a different strategy such as - proxy it to use pooled producers/consumers which CamelContext manage the lifecycle - use a shared ProducerTemplate / ConsumerTemplate instead which CamelContext manages the lifecycle - other - maybe some thread local tricks",design_debt,non-optimal_design,"Thu, 15 Apr 2010 11:26:28 +0000","Wed, 25 Mar 2015 08:24:56 +0000","Wed, 25 Mar 2015 08:24:56 +0000",155941108,"This only applies for prototype scoped POJOs which uses @Produce or @Consume to inject a Camel Producer/Consumer. As the prototype scoped POJO is short lived and there could potentially be created many of those POJOs. Then @Produce / @Consume will inject new instances of Producer / Consumer as well. And since there is no standard way of knowing when the POJO is no longer in need, which is where we can to stop the Producer/Consumer. For singleton scoped this is not a problem as CamelContext will keep track on the created Producer/Consumer in its internal servicesToClose. Which is then stopped when CamelContext stops. For prototype we need a different strategy such as proxy it to use pooled producers/consumers which CamelContext manage the lifecycle use a shared ProducerTemplate / ConsumerTemplate instead which CamelContext manages the lifecycle other maybe some thread local tricks",0.1022142857,0.1022142857,neutral
camel,2682,description,This allows end users to continue routing the original message. Instead of now the just get the last splitted message which is confusing.,design_debt,non-optimal_design,"Thu, 29 Apr 2010 16:05:25 +0000","Sun, 24 Apr 2011 10:00:53 +0000","Fri, 30 Apr 2010 09:24:13 +0000",62328,This allows end users to continue routing the original message. Instead of now the just get the last splitted message which is confusing.,-0.2185,-0.2185,neutral
camel,2879,description,"The file strategies was not designed to be as flexible as we may want them today, when introducing more and more options to dictate what should happens with the files when success or failure. As we got - noop - delete - move - moveFailed etc. you may combine then in ways in which we current doesn't support. You should be able to say that if OK then just delete the file, but if ERROR then move it to this error folder instead.",design_debt,non-optimal_design,"Wed, 30 Jun 2010 04:17:15 +0000","Sun, 24 Apr 2011 10:00:44 +0000","Wed, 30 Jun 2010 07:01:45 +0000",9870,"The file strategies was not designed to be as flexible as we may want them today, when introducing more and more options to dictate what should happens with the files when success or failure. As we got noop delete move moveFailed etc. you may combine then in ways in which we current doesn't support. You should be able to say that if OK then just delete the file, but if ERROR then move it to this error folder instead.",-0.05794444444,-0.05794444444,negative
camel,2979,comment_3,"Hi I've tested this and it works. When logging in with incorrect user/pass it only polls once and skips, which is nice. However could it be possible for the component to throw an exception as I would like to catch this exception and make sure that no further polls are done to the ftp (ie shut down the route/application).",design_debt,non-optimal_design,"Wed, 21 Jul 2010 09:07:59 +0000","Sun, 24 Apr 2011 09:57:22 +0000","Thu, 22 Jul 2010 15:00:59 +0000",107580,"Hi I've tested this and it works. When logging in with incorrect user/pass it only polls once and skips, which is nice. However could it be possible for the component to throw an exception as I would like to catch this exception and make sure that no further polls are done to the ftp (ie shut down the route/application).",0.566,0.566,positive
camel,302,comment_4,Roman we might need a for end users to easier adding their own headers to ignore instead of clearing the already default ignored ones.,design_debt,non-optimal_design,"Mon, 21 Jan 2008 14:45:50 +0000","Mon, 16 Feb 2009 05:53:17 +0000","Thu, 11 Sep 2008 16:12:35 +0000",20222805,Roman we might need a addIgnoreHeader(key) for end users to easier adding their own headers to ignore instead of clearing the already default ignored ones.,-0.3666666667,-0.15,neutral
camel,3048,description,The current sample is based on Direct component which is a bad sample as its a very special component. We should do a HelloWorldComponent - the producer will just print the message body to system out - the consumer is scheduled and triggers every 5th second with a dummy exchange,design_debt,non-optimal_design,"Thu, 12 Aug 2010 08:26:16 +0000","Sun, 24 Apr 2011 09:57:16 +0000","Mon, 23 Aug 2010 01:22:49 +0000",924993,The current sample is based on Direct component which is a bad sample as its a very special component. We should do a HelloWorldComponent the producer will just print the message body to system out the consumer is scheduled and triggers every 5th second with a dummy exchange,-0.3,-0.3,negative
camel,3048,summary,Archetype for creating component should have better sample component,design_debt,non-optimal_design,"Thu, 12 Aug 2010 08:26:16 +0000","Sun, 24 Apr 2011 09:57:16 +0000","Mon, 23 Aug 2010 01:22:49 +0000",924993,Archetype for creating component should have better sample component,0.5,0.5,neutral
camel,3050,description,"Spring 3 changed internally how dependency resolution works. Its now worse as we have to play tricks in the Camel namespace handler to tell Camel the various pieces you can ref, should depend on Camel. Otherwise the dependency resolution in Spring is not working properly. This used to work like a charm in Spring 2.5.6.",design_debt,non-optimal_design,"Thu, 12 Aug 2010 13:30:15 +0000","Sun, 24 Apr 2011 09:57:37 +0000","Thu, 12 Aug 2010 13:51:59 +0000",1304,"Spring 3 changed internally how dependency resolution works. Its now worse as we have to play tricks in the Camel namespace handler to tell Camel the various pieces you can ref, should depend on Camel. Otherwise the dependency resolution in Spring is not working properly. This used to work like a charm in Spring 2.5.6.",0.096875,0.096875,negative
camel,3077,description,"EhCache often has a bit of lag time when invalidating expired cache elements, first setting the Element value to null and then removing the key. If you are hitting a cache rapidly one often will run across a key that is present in the cache that still has a null element entry. The logic for successfully key retrieval just needs to be slightly tweaked to check for null values.",design_debt,non-optimal_design,"Wed, 25 Aug 2010 03:22:51 +0000","Sun, 24 Apr 2011 09:57:39 +0000","Wed, 25 Aug 2010 11:52:31 +0000",30580,"EhCache often has a bit of lag time when invalidating expired cache elements, first setting the Element value to null and then removing the key. If you are hitting a cache rapidly one often will run across a key that is present in the cache that still has a null element entry. The logic for successfully key retrieval just needs to be slightly tweaked to check for null values.",0.2815555556,0.2815555556,neutral
camel,3077,summary,Cache Component needs to check for null values during GET operations,design_debt,non-optimal_design,"Wed, 25 Aug 2010 03:22:51 +0000","Sun, 24 Apr 2011 09:57:39 +0000","Wed, 25 Aug 2010 11:52:31 +0000",30580,Cache Component needs to check for null values during GET operations,0,0,neutral
camel,3112,description,To make it easier to read/write files using that given encoding. Currently you have to set that as a property on the Exchange using setProperty in the DSL. But when using @Produce or @Consume you only got the endpoint uri. So having this in the component makes it easier.,design_debt,non-optimal_design,"Thu, 9 Sep 2010 10:37:37 +0000","Sun, 24 Apr 2011 09:58:25 +0000","Fri, 10 Sep 2010 11:34:15 +0000",89798,To make it easier to read/write files using that given encoding. Currently you have to set that as a property on the Exchange using setProperty in the DSL. But when using @Produce or @Consume you only got the endpoint uri. So having this in the component makes it easier.,0,0,neutral
camel,3125,description,"The problem is with the default implementation of When an error occurs rollback() is called, and this method just logs the error and returns false, which means the polling will not be retried for that execution of the run() method. This means it will be retried after the delay. I created an implementation that suspends the Consumer, which seems like acceptable default behavior. It also allows for extension and adding some hooks for custom stuff. In order for that to be useful it should be made easier to set your own implementation.",design_debt,non-optimal_design,"Tue, 14 Sep 2010 14:09:51 +0000","Sun, 24 Apr 2011 09:57:58 +0000","Fri, 17 Sep 2010 10:52:20 +0000",247349,"The problem is with the default implementation of org.apache.camel.spi.PollingConsumerPollStrategy: org.apache.camel.impl.DefaultPollingConsumerPollStrategy When an error occurs rollback() is called, and this method just logs the error and returns false, which means the polling will not be retried for that execution of the run() method. This means it will be retried after the delay. I created an implementation that suspends the Consumer, which seems like acceptable default behavior. It also allows for extension and adding some hooks for custom stuff. In order for that to be useful it should be made easier to set your own PollingConsumerPollStrategy implementation.",-0.0004,-0.03117948718,negative
camel,3125,summary,"When the fetching of a feed causes, polling continues endlessly, flooding the logs and creating unwanted network load.",design_debt,non-optimal_design,"Tue, 14 Sep 2010 14:09:51 +0000","Sun, 24 Apr 2011 09:57:58 +0000","Fri, 17 Sep 2010 10:52:20 +0000",247349,"When the fetching of a feed causes, polling continues endlessly, flooding the logs and creating unwanted network load.",0.3,0.3,negative
camel,3139,description,The Java UUID is too slow. Switching back to AMQ based will improve performance. We should just have to switch to use the JDK UUID for the camel-gae component as it cannot use the AMQ based.,design_debt,non-optimal_design,"Tue, 21 Sep 2010 06:32:37 +0000","Sun, 24 Apr 2011 09:58:20 +0000","Tue, 21 Sep 2010 09:13:33 +0000",9656,The Java UUID is too slow. Switching back to AMQ based will improve performance. We should just have to switch to use the JDK UUID for the camel-gae component as it cannot use the AMQ based.,0.1333333333,0.1333333333,negative
camel,3139,summary,Use as the default uuid generator as its faster than the JDK UUID generator,design_debt,non-optimal_design,"Tue, 21 Sep 2010 06:32:37 +0000","Sun, 24 Apr 2011 09:58:20 +0000","Tue, 21 Sep 2010 09:13:33 +0000",9656,Use ActiveMQUuidGenerator as the default uuid generator as its faster than the JDK UUID generator,-0.5,-0.5,neutral
camel,3351,comment_1,All irc events are just logged (I missed it in the logs). In this case the collision is logged as well as the server initiated disconnect. However the component code is unaware of the state. In general there are a lot of useful things that could be done with the various IRC events. Specifically though the code needs to be aware of disconnects.,design_debt,non-optimal_design,"Sun, 21 Nov 2010 13:54:04 +0000","Sun, 24 Apr 2011 09:57:55 +0000","Wed, 8 Dec 2010 06:54:35 +0000",1443631,All irc events are just logged (I missed it in the logs). In this case the collision is logged as well as the server initiated disconnect. However the component code is unaware of the state. In general there are a lot of useful things that could be done with the various IRC events. Specifically though the code needs to be aware of disconnects.,-0.0006,-0.0006,neutral
camel,3524,description,"By default Jetty uses 30 sec timeout for continuations. We should allow end users to configure a value of choice, in case their system may take longer time to process an exchange.",design_debt,non-optimal_design,"Mon, 10 Jan 2011 14:34:35 +0000","Tue, 25 Oct 2011 11:35:20 +0000","Mon, 10 Jan 2011 15:27:37 +0000",3182,"By default Jetty uses 30 sec timeout for continuations. We should allow end users to configure a value of choice, in case their system may take longer time to process an exchange.",-0.15,-0.15,neutral
camel,3576,comment_1,"trunk: 1067682. Don't use thread pool for single threaded reply manager as Spring DMLC is a bit pants as it will keep using new tasks every second when idle, and that just confuses people, as task count will grow very large.",design_debt,non-optimal_design,"Sat, 22 Jan 2011 08:23:42 +0000","Tue, 25 Oct 2011 11:36:12 +0000","Sun, 6 Feb 2011 13:25:12 +0000",1314090,"trunk: 1067682. Don't use thread pool for single threaded reply manager as Spring DMLC is a bit pants as it will keep using new tasks every second when idle, and that just confuses people, as task count will grow very large.",-0.25,-0.25,negative
camel,3576,description,When using camel-jms the consumers dont provide a default task executor. That means spring just creates a thread manually and dont reuse the thread. We should provided a task executor from camel using the This allows us to use human readable thread names which can provide details about the consumer. The thread pool is also managed and even a 3rd party provider can manage thread creation. This only works when using Spring 3 as the task executor API is now aligned with the executor service api from the JDK. For Spring 2.x users we should not do this. We will then have to detect the spring version in use.,design_debt,non-optimal_design,"Sat, 22 Jan 2011 08:23:42 +0000","Tue, 25 Oct 2011 11:36:12 +0000","Sun, 6 Feb 2011 13:25:12 +0000",1314090,When using camel-jms the consumers dont provide a default task executor. That means spring just creates a thread manually and dont reuse the thread. We should provided a task executor from camel using the ExecutorServiceStrategy. This allows us to use human readable thread names which can provide details about the consumer. The thread pool is also managed and even a 3rd party provider can manage thread creation. This only works when using Spring 3 as the task executor API is now aligned with the executor service api from the JDK. For Spring 2.x users we should not do this. We will then have to detect the spring version in use.,0.075,0.06666666667,neutral
camel,3677,description,"When splitting inside another split, the custom aggregationStrategy is not used. For example in the route: (where the does nothing more than to concat the bodies with a space inbetween.) The expected results would be: and But that is not what happens. The actual results are two times the same: The reason is, that the strategy is not used. In the class in the method {{protected AggregationStrategy exchange)}}, the first step is to find an aggregationStrategy in the Exchange. This is set to and because it is not null, this aggregation strategy will be used, not the one declared for the splitter.  A workaround would be to remove the AggregationStrategy of the Exchange, before it is aggregated, by using a processor with the following process method: After integrating this in my route, I got the desired results.",design_debt,non-optimal_design,"Thu, 17 Feb 2011 12:01:28 +0000","Thu, 17 Feb 2011 12:47:51 +0000","Thu, 17 Feb 2011 12:47:51 +0000",2783,"When splitting inside another split, the custom aggregationStrategy is not used. For example in the route: (where the concatWithSpaceStrategy does nothing more than to concat the bodies with a space inbetween.) The expected results would be: and But that is not what happens. The actual results are two times the same: The reason is, that the strategy is not used. In the class org.apache.camel.processor.MulticastProcessor, in the method protected AggregationStrategy getAggregationStrategy(Exchange exchange), the first step is to find an aggregationStrategy in the Exchange. This is set to UseOriginalAggregationStrategy, and because it is not null, this aggregation strategy will be used, not the one declared for the splitter.  A workaround would be to remove the AggregationStrategy of the Exchange, before it is aggregated, by using a processor with the following process method: After integrating this in my route, I got the desired results.",0.09285714286,0.065,neutral
camel,3764,description,to replace the use of the hacked and make it consistent across all components.,design_debt,non-optimal_design,"Tue, 8 Mar 2011 01:37:25 +0000","Tue, 25 Oct 2011 11:35:20 +0000","Tue, 8 Mar 2011 01:38:43 +0000",78,to replace the use of the hacked maven-jaxb-schemagen-plugin and make it consistent across all components.,-0.2,-0.2,neutral
camel,3777,comment_0,"We should possible have a that uses pax exam as its base line. Willem have upgraded to pax exam 2, so it may be easier to do now.",design_debt,non-optimal_design,"Thu, 10 Mar 2011 17:42:37 +0000","Sat, 11 Jul 2015 22:19:13 +0000","Sat, 11 Jul 2015 22:19:13 +0000",136874196,"We should possible have a camel-test-pax-exam, that uses pax exam as its base line. Willem have upgraded to pax exam 2, so it may be easier to do now.",0,0,neutral
camel,3777,comment_1,There is some ticket about an osgi test component. However as there is pax-exam this is less needed as ppl can use plain pax-exam,design_debt,non-optimal_design,"Thu, 10 Mar 2011 17:42:37 +0000","Sat, 11 Jul 2015 22:19:13 +0000","Sat, 11 Jul 2015 22:19:13 +0000",136874196,There is some ticket about an osgi test component. However as there is pax-exam this is less needed as ppl can use plain pax-exam,-0.375,-0.375,neutral
camel,3777,description,Put and in it's own component (camel-osgi-test) to allow users to reuse it in its own OSGI integration tests It's the same as with CamelTestSupport and,design_debt,non-optimal_design,"Thu, 10 Mar 2011 17:42:37 +0000","Sat, 11 Jul 2015 22:19:13 +0000","Sat, 11 Jul 2015 22:19:13 +0000",136874196,Put OSGiIntegrationSpringTestSupport and OSGiIntegrationTestSupport in it's own component (camel-osgi-test) to allow users to reuse it in its own OSGI integration tests It's the same as with CamelTestSupport and CamelSpringTestSupport,0.2,0.2,neutral
camel,3777,summary,Put and in it's own component to allow users to reuse it in its own OSGI integration tests,design_debt,non-optimal_design,"Thu, 10 Mar 2011 17:42:37 +0000","Sat, 11 Jul 2015 22:19:13 +0000","Sat, 11 Jul 2015 22:19:13 +0000",136874196,Put OSGiIntegrationSpringTestSupport and OSGiIntegrationTestSupport in it's own component to allow users to reuse it in its own OSGI integration tests,0.2,0.2,neutral
camel,4202,comment_1,A workaround is for the end user to use to eg pull messages 4 times per sec and thus be 4x faster.,design_debt,non-optimal_design,"Sat, 9 Jul 2011 14:05:41 +0000","Mon, 19 Sep 2011 20:32:51 +0000","Sun, 14 Aug 2011 10:07:44 +0000",3096123,"A workaround is for the end user to use ?receiveTimeout=250, to eg pull messages 4 times per sec and thus be 4x faster.",0,0,neutral
camel,4226,comment_0,"Frankly sometimes its best to post on @dev before opening a ticket. Anyway end users should use the EIPs supplied. And the recipient list is the dynamic EIP. There is a FAQ as well We should also be careful to not overload the DSL with new options etc. as it just confuses end users as they get a huge list in the IDE when accessing code completion. Likewise we should keep the DSL in sync between Java, XML and Scala (later is a bit more hard). But the Java and XML is in sync due the JAXB models. And you cannot with JAXB define that an expression should be *optional* so you can keep doing <to uri=""xxx""/ This is not possible and you get a validation exception as it would expect an expression, so you would have to do <to <constant</to Which is ugly/verbose, and breaks every Camel applications that are using XML DSL. I dont think its a good idea to add this only to Java DSL, as we have end users who migrate between Java and XML. Likewise its best they are 1:1.",design_debt,non-optimal_design,"Wed, 13 Jul 2011 18:13:16 +0000","Sun, 31 Jul 2011 10:54:19 +0000","Thu, 14 Jul 2011 17:18:13 +0000",83097,"Frankly sometimes its best to post on @dev before opening a ticket. Anyway end users should use the EIPs supplied. And the recipient list is the dynamic EIP. There is a FAQ as well http://camel.apache.org/how-do-i-use-dynamic-uri-in-to.html We should also be careful to not overload the DSL with new options etc. as it just confuses end users as they get a huge list in the IDE when accessing code completion. Likewise we should keep the DSL in sync between Java, XML and Scala (later is a bit more hard). But the Java and XML is in sync due the JAXB models. And you cannot with JAXB define that an expression should be optional so you can keep doing <to uri=""xxx""/> in XML. This is not possible and you get a validation exception as it would expect an expression, so you would have to do <to> <constant>xxx</constant> </to> Which is ugly/verbose, and breaks every Camel applications that are using XML DSL. I dont think its a good idea to add this only to Java DSL, as we have end users who migrate between Java and XML. Likewise its best they are 1:1.",0.1174166667,0.105675,neutral
camel,4398,comment_0,"Sorry the issue is the tools JAR is added as dep on the SNAPSHOTs, but not on the releases etc. So its a bit weird. Track down to be possible in parent/pom.xml with some antrun tasks that does some osgi versioning regex replacement. And since this dependency is added to camel-core it gets inheirted by all the other artifcats. But only for SNAPSHOTs, and not releases. So I guess the release profile makes something different. The tools JAR is thus not needed at all as a dependency to use Camel, so what we want is to get rid of tools JAR in the camel-core/pom.xml file.",design_debt,non-optimal_design,"Wed, 31 Aug 2011 09:19:39 +0000","Mon, 19 Sep 2011 13:47:19 +0000","Wed, 31 Aug 2011 15:05:52 +0000",20773,"Sorry the issue is the tools JAR is added as dep on the SNAPSHOTs, but not on the releases etc. So its a bit weird. Track down to be possible in parent/pom.xml with some antrun tasks that does some osgi versioning regex replacement. And since this dependency is added to camel-core it gets inheirted by all the other artifcats. But only for SNAPSHOTs, and not releases. So I guess the release profile makes something different. The tools JAR is thus not needed at all as a dependency to use Camel, so what we want is to get rid of tools JAR in the camel-core/pom.xml file.",-0.1102222222,-0.1102222222,neutral
camel,4417,comment_2,"Patch moving all support classes from impl to support. I moved the following classes and placed compatibility classes in their impl location: DefaultComponent, DefaultConsumer, DefaultEndpoint, DefaultExchange, DefaultMessage, DefaultProducer, DefaultUnitOfWork, ExpressionSupport, ProcessorEndpoint, ProducerCache, I moved the following classes without compat stubs as they were not needed outside camel-core: DefaultRouteNode, ExpressionAdapter, MDCUnitOfWork, MessageSupport, SimpleUuidGenerator I moved from processor to util as it is needed from support. The only problematic class I moved was DefaultConsumer as it needed So the above including the inner class had to move to util. The problem here was that the Bridge had to extend DelegateProcessor which of course is in processor. As util should not depend on processor I had to introduce an interface DelegateProcessor in camel that could be used to abstract from DelegateProcessor and This is a good thing anyway and I will open a jira to do this first. I also had to move PipelineHelper and to util as they were used from support classes. This is a fairly large patch. So I am not sure if it is good for 2.9. On the other hand if we wait with this till 3.0 we are either really incompatible or we can not remove the deprecated classes",design_debt,non-optimal_design,"Mon, 5 Sep 2011 13:46:59 +0000","Tue, 22 Oct 2013 07:34:27 +0000","Tue, 22 Oct 2013 07:34:27 +0000",67196848,"Patch moving all support classes from impl to support. I moved the following classes and placed compatibility classes in their impl location: DefaultAsyncProducer, DefaultComponent, DefaultConsumer, DefaultEndpoint, DefaultExchange, DefaultMessage, DefaultPollingEndpoint, DefaultProducer, DefaultUnitOfWork, ExpressionSupport, HeaderFilterStrategyComponent, InterceptSendToMockEndpointStrategy, LoggingExceptionHandler, PollingConsumerSupport, ProcessorEndpoint, ProducerCache, ScheduledPollConsumer, ScheduledPollEndpoint, I moved the following classes without compat stubs as they were not needed outside camel-core: DefaultPollingConsumerPollStrategy, DefaultRouteNode, DefaultScheduledPollConsumer, DefaultSubUnitOfWork, DefaultTracedRouteNodes, EventDrivenPollingConsumer, ExpressionAdapter, InterceptSendToEndpoint, MDCUnitOfWork, MessageSupport, ProcessorPollingConsumer, SimpleUuidGenerator I moved AsyncProcessorConverterHelper from processor to util as it is needed from support. The only problematic class I moved was DefaultConsumer as it needed AsyncProcessorConverterHelper. So the above including the inner class ProcessorToAsyncProcessorBridge had to move to util. The problem here was that the Bridge had to extend DelegateProcessor which of course is in processor. As util should not depend on processor I had to introduce an interface DelegateProcessor in camel that could be used to abstract from ProcessorToAsyncProcessorBridge, DelegateProcessor and DelegateAsyncProcessor. This is a good thing anyway and I will open a jira to do this first. I also had to move PipelineHelper and SimpleUuidGeneratopr to util as they were used from support classes. This is a fairly large patch. So I am not sure if it is good for 2.9. On the other hand if we wait with this till 3.0 we are either really incompatible or we can not remove the deprecated classes",-0.05022222222,0.1,neutral
camel,4417,comment_5,This change is probably too destructive even for 3.0,design_debt,non-optimal_design,"Mon, 5 Sep 2011 13:46:59 +0000","Tue, 22 Oct 2013 07:34:27 +0000","Tue, 22 Oct 2013 07:34:27 +0000",67196848,This change is probably too destructive even for 3.0,-0.75,-0.75,negative
camel,4417,description,"Several classes in impl are used or extended by components. We should avoid this. The base classes should be moved to support. Examples are DefaultComponent, DefaultEndpoint, DefaultProducer. Another case is the The typeconverter is well placed in impl but the class also has a public static convert method that is used from many components. So this functionality should be moved to processor so it is available to components.",design_debt,non-optimal_design,"Mon, 5 Sep 2011 13:46:59 +0000","Tue, 22 Oct 2013 07:34:27 +0000","Tue, 22 Oct 2013 07:34:27 +0000",67196848,"Several classes in impl are used or extended by components. We should avoid this. The base classes should be moved to support. Examples are DefaultComponent, DefaultEndpoint, DefaultProducer. Another case is the AsyncProcessorTypeConverter. The typeconverter is well placed in impl but the class also has a public static convert method that is used from many components. So this functionality should be moved to processor so it is available to components.",0.013,0.01114285714,neutral
camel,4430,description,"As the proxy lifecycle cleanup work are done in CXF 2.4.3, we are facing some test failed in camel-cxf. After digging the code, I found the proxy instance which is created by the CxfProxyFactoryBean will be GC and the CXF client which is used in CxfProducer will be affect. The endpoint which is set on the conduit will gone, and CxfProducer will complain it with a NPE exception. We can use the to create client instead of CxfProxyFactoryBean to avoid the GC and NPE exception. I checked the difference between using CxfProxyFactoryBean and they are same in most case. We just need to take care of handler setting part which is used in JAXWS frontend.",design_debt,non-optimal_design,"Fri, 9 Sep 2011 10:02:59 +0000","Sat, 10 Sep 2011 05:59:05 +0000","Sat, 10 Sep 2011 05:59:05 +0000",71766,"As the proxy lifecycle cleanup work are done in CXF 2.4.3, we are facing some test failed in camel-cxf. After digging the code, I found the proxy instance which is created by the CxfProxyFactoryBean will be GC and the CXF client which is used in CxfProducer will be affect. The endpoint which is set on the conduit will gone, and CxfProducer will complain it with a NPE exception. We can use the CxfClientFactoryBean to create client instead of CxfProxyFactoryBean to avoid the GC and NPE exception. I checked the difference between using CxfProxyFactoryBean and CxfClientFactoryBean, they are same in most case. We just need to take care of handler setting part which is used in JAXWS frontend.",-0.01666666667,-0.01666666667,neutral
camel,4657,description,"See nabble This bug is in ActiveMQ, but creating a ticket to get it resolved as the leak is apparent when using Spring DMLC with CACHE_SESSION, which Camel by default does when doing request/reply over JMS with fixed replyTo queues. Then the consumer is not cached, and therefore created on each poll, but the ActiveMQSessionPool keeps growing in its internal list of created consumers, as the session is cached. Most likely a patch is needed to fix this in the AMQ side",design_debt,non-optimal_design,"Thu, 10 Nov 2011 11:05:43 +0000","Fri, 11 Nov 2011 08:34:44 +0000","Fri, 11 Nov 2011 08:34:44 +0000",77341,"See nabble http://camel.465427.n5.nabble.com/Possible-memory-leak-in-org-apache-activemq-pool-PooledSession-tp4964951p4964951.html This bug is in ActiveMQ, but creating a ticket to get it resolved as the leak is apparent when using Spring DMLC with CACHE_SESSION, which Camel by default does when doing request/reply over JMS with fixed replyTo queues. Then the consumer is not cached, and therefore created on each poll, but the ActiveMQSessionPool keeps growing in its internal list of created consumers, as the session is cached. Most likely a patch is needed to fix this in the AMQ side",0.03333333333,0.03333333333,negative
camel,4657,summary,"camel-jms - Request/Reply - Leak in ActiveMQSessionPool causing it to eat up memory, when using fixed replyTo queue names",design_debt,non-optimal_design,"Thu, 10 Nov 2011 11:05:43 +0000","Fri, 11 Nov 2011 08:34:44 +0000","Fri, 11 Nov 2011 08:34:44 +0000",77341,"camel-jms - Request/Reply - Leak in ActiveMQSessionPool causing it to eat up memory, when using fixed replyTo queue names",-0.35,-0.35,negative
camel,4676,description,"split into camel-script-groovy camel-script-jruby so that we can control granularity much better, and version upgrades will be more manageable. In most cases, if the user does scripting, he'll probably standardise on only one particular language for all his routes.",design_debt,non-optimal_design,"Mon, 14 Nov 2011 06:50:13 +0000","Mon, 14 Nov 2011 07:11:47 +0000","Mon, 14 Nov 2011 07:11:47 +0000",1294,"split camel-script-optional into camel-script-javascript camel-script-groovy camel-script-jruby so that we can control granularity much better, and version upgrades will be more manageable. In most cases, if the user does scripting, he'll probably standardise on only one particular language for all his routes.",0.2813333333,0.2813333333,neutral
camel,4741,description,"This is a trivial issue, yet it is sometimes a bit annoying. People that are familiar with jms/activemq components expect that the default behavior of the a queue producer is to add the message to the queue, without having to specify a special operation. Hazelcast Queue Producer instead will through an exception if no operation is defined. It would be good if it was consistent with the rest of the queue producers.",design_debt,non-optimal_design,"Mon, 5 Dec 2011 09:03:15 +0000","Mon, 5 Dec 2011 09:33:39 +0000","Mon, 5 Dec 2011 09:33:39 +0000",1824,"This is a trivial issue, yet it is sometimes a bit annoying. People that are familiar with jms/activemq components expect that the default behavior of the a queue producer is to add the message to the queue, without having to specify a special operation. Hazelcast Queue Producer instead will through an exception if no operation is defined. It would be good if it was consistent with the rest of the queue producers.",-0.306,-0.306,negative
camel,4793,comment_1,"Hi Christian, if you need any help with this let me know. BTW is there any agreed naming convention for components URI options and message headers? Looking at the existing components seems like URI options can have component specific names whereas the headers use format? But in this case, if you want to override an option from the URI using the message header, you have to specify a different name, which may be lead to confusion. Any thoughts?",design_debt,non-optimal_design,"Sun, 18 Dec 2011 13:09:51 +0000","Tue, 27 Dec 2011 22:43:26 +0000","Mon, 19 Dec 2011 14:26:25 +0000",90994,"Hi Christian, if you need any help with this let me know. BTW is there any agreed naming convention for components URI options and message headers? Looking at the existing components seems like URI options can have component specific names whereas the headers use CamelComponnetNameXXX format? But in this case, if you want to override an option from the URI using the message header, you have to specify a different name, which may be lead to confusion. Any thoughts?",0.0825,0.0825,neutral
camel,4793,description,I would like to improve the aws-sdb component in the following places: - Using the [Amazon operation instead of CamelAwsSdbXXX -- Add OSGI integration tests I would like to resolve this issue for Camel 2.9.0 because renaming of the operation names are not backwards compatible. I will resolve this issue today.,design_debt,non-optimal_design,"Sun, 18 Dec 2011 13:09:51 +0000","Tue, 27 Dec 2011 22:43:26 +0000","Mon, 19 Dec 2011 14:26:25 +0000",90994,I would like to improve the aws-sdb component in the following places: Using the Amazon operation names instead of CamelAwsSdbXXX -> With this we can easily support more operations (in the future) and the users are more familiar with this names Add OSGI integration tests I would like to resolve this issue for Camel 2.9.0 because renaming of the operation names are not backwards compatible. I will resolve this issue today.,0.159375,0.1339285714,neutral
camel,4928,description,It would be great to have timer component support asynchronous API. Such a feature can be useful when timer component generates events which must be processed by multiple threads. Current implementation of the timer component makes a blocking call so the usage of thread pools hardly possible to process multiple timer event simultaneously.,design_debt,non-optimal_design,"Sun, 22 Jan 2012 03:07:05 +0000","Wed, 7 Aug 2013 16:49:11 +0000","Wed, 7 Aug 2013 16:49:11 +0000",48692526,It would be great to have timer component support asynchronous API. Such a feature can be useful when timer component generates events which must be processed by multiple threads. Current implementation of the timer component makes a blocking call so the usage of thread pools hardly possible to process multiple timer event simultaneously.,0.2666666667,0.2666666667,positive
camel,4959,comment_4,"IMHO the changes introduced by of this ticket causes regression failure. To avoid null values being misinterpreted as cache misses we do now convert e.g. Float.NaN = which was not the case before. Why not just simply *not* mark the conversion as miss *if* the conversion result is (Float.NaN / Double.NaN == And I simply don't get why the misses cache is a ConcurrentMap: To my understanding would be just fine! Why do we need a Map to memorize the cache misses? Where we then do things like On it which is not really intuitive while reading the code, is this maybe because ideally we want to do lookup by the misses cache in O(1) instead of O(N)? See also:",design_debt,non-optimal_design,"Tue, 31 Jan 2012 07:06:19 +0000","Tue, 28 Feb 2012 11:43:59 +0000","Thu, 23 Feb 2012 12:47:32 +0000",2007673,"IMHO the changes introduced by of this ticket causes regression failure. To avoid null values being misinterpreted as cache misses we do now convert e.g. Float.NaN => (Byte) 0 which was not the case before. Why not just simply not mark the conversion as miss if the conversion result is (Float.NaN / Double.NaN ==> null). That's do not add any entry for such conversion results into the misses cache and revert back ObjectConverter to what it was before. Does this make sense to you? And I simply don't get why the misses cache is a ConcurrentMap: To my understanding would be just fine! Why do we need a Map to memorize the cache misses? Where we then do things like On it which is not really intuitive while reading the code, is this maybe because ideally we want to do lookup by the misses cache (misses.containsKey()) in O(1) instead of O(N)? See also: http://camel.465427.n5.nabble.com/ObjectConverter-problem-td5517376.html",-0.1444444444,-0.1,negative
camel,5008,description,"When working with streams, stream caching must be activated in order to use log:set trace, otherwise the streams will be consumed, as stated here (""#Using Streaming Bodies""). When the stream caching now gets activated, the streams will be reseted after each step (as far as I Understand). This makes it impossible to work with InputStreams in a pipe manner (e.g. Read the first char, then in the next step work with the next chars), as the stream is after this every time in the beginning. I would except that the stream caching provides a mechanism for the ""user"" to be able to read it more than once. Also its the right procedure to reset the streams after they are traced with the tracing mechanism, BUT the should be reseted to the state they were before and not to the very first beginning. I didn't dig into the code that deep but it seems that exactly this happens from user perspective. So to summarize there are several problems: - Working in stream in camel is impossible when log:set debug trace get enabled. (Thus enable Stream caching) - When Stream caching is enabled it becomes impossible to work with ""stream pointers"" as camel reset the streams to the very beginning. I illustrated the problem in the attached jUnit test.",design_debt,non-optimal_design,"Wed, 15 Feb 2012 11:51:24 +0000","Thu, 16 Feb 2012 08:02:43 +0000","Thu, 16 Feb 2012 05:42:50 +0000",64286,"When working with streams, stream caching must be activated in order to use log:set trace, otherwise the streams will be consumed, as stated here http://camel.apache.org/jbi.html (""#Using Streaming Bodies""). When the stream caching now gets activated, the streams will be reseted after each step (as far as I Understand). This makes it impossible to work with InputStreams in a pipe manner (e.g. Read the first char, then in the next step work with the next chars), as the stream is after this every time in the beginning. I would except that the stream caching provides a mechanism for the ""user"" to be able to read it more than once. Also its the right procedure to reset the streams after they are traced with the tracing mechanism, BUT the should be reseted to the state they were before and not to the very first beginning. I didn't dig into the code that deep but it seems that exactly this happens from user perspective. So to summarize there are several problems: Working in stream in camel is impossible when log:set debug trace get enabled. (Thus enable Stream caching) When Stream caching is enabled it becomes impossible to work with ""stream pointers"" as camel reset the streams to the very beginning. I illustrated the problem in the attached jUnit test.",0.1553703704,0.1553703704,neutral
camel,5008,summary,Stream handling inconsistent.,design_debt,non-optimal_design,"Wed, 15 Feb 2012 11:51:24 +0000","Thu, 16 Feb 2012 08:02:43 +0000","Thu, 16 Feb 2012 05:42:50 +0000",64286,Stream handling inconsistent.,0,0,negative
camel,5045,comment_0,"Also CAMEL-4500 introduced a leak as well, in terms of ManagedTracer being kept around in a separate Map. We need to remove not needed tracer from that map as well.",design_debt,non-optimal_design,"Sun, 26 Feb 2012 14:42:40 +0000","Mon, 27 Feb 2012 07:12:49 +0000","Mon, 27 Feb 2012 07:10:37 +0000",59277,"Also CAMEL-4500 introduced a leak as well, in terms of ManagedTracer being kept around in a separate Map. We need to remove not needed tracer from that map as well.",0.42325,0.42325,negative
camel,5045,comment_1,I created CAMEL-5046 to track the leak from CAMEL-4500 with ManagedTracer as it only affects 2.9 onwards.,design_debt,non-optimal_design,"Sun, 26 Feb 2012 14:42:40 +0000","Mon, 27 Feb 2012 07:12:49 +0000","Mon, 27 Feb 2012 07:10:37 +0000",59277,I created CAMEL-5046 to track the leak from CAMEL-4500 with ManagedTracer as it only affects 2.9 onwards.,-0.2,-0.2,neutral
camel,5045,description,"If you add and remove a lot of routes to CamelContext and have JMX enabled, then the will accumulate a map of provisional JMX performance counters for the route mbeans. The map should be cleared after usage, as the map is no longer needed. Memory will accumulate as the map has reference to old objects which cannot be GC.",design_debt,non-optimal_design,"Sun, 26 Feb 2012 14:42:40 +0000","Mon, 27 Feb 2012 07:12:49 +0000","Mon, 27 Feb 2012 07:10:37 +0000",59277,"If you add and remove a lot of routes to CamelContext and have JMX enabled, then the DefaultManagementLifecycleStrategy will accumulate a map of provisional JMX performance counters for the route mbeans. The map should be cleared after usage, as the map is no longer needed. Memory will accumulate as the map has reference to old objects which cannot be GC.",0,0,neutral
camel,5045,summary,Memory leak when adding/removing a lot of routes with JMX enabled,design_debt,non-optimal_design,"Sun, 26 Feb 2012 14:42:40 +0000","Mon, 27 Feb 2012 07:12:49 +0000","Mon, 27 Feb 2012 07:10:37 +0000",59277,Memory leak when adding/removing a lot of routes with JMX enabled,-0.2,-0.2,neutral
camel,612,comment_0,Gert I do think that the choice() should *always* have an otherwise() so there always is a match. If the otherwise() is missing on the choice() then Camel should thrown an exception. Maybe checked during the route creation stuff.,design_debt,non-optimal_design,"Mon, 16 Jun 2008 06:38:46 +0000","Fri, 11 Jul 2008 04:21:45 +0000","Mon, 23 Jun 2008 12:41:58 +0000",626592,Gert I do think that the choice() should always have an otherwise() so there always is a match. If the otherwise() is missing on the choice() then Camel should thrown an exception. Maybe checked during the route creation stuff.,-0.1333333333,-0.1333333333,neutral
camel,612,description,"When you define a route with a choice() and no matching when() clause is found, the Exchange just ends successfully without doing anything. In my mind, it should fail by default in this case (or we should at least have an easy way to get this behavior).",design_debt,non-optimal_design,"Mon, 16 Jun 2008 06:38:46 +0000","Fri, 11 Jul 2008 04:21:45 +0000","Mon, 23 Jun 2008 12:41:58 +0000",626592,"When you define a route with a choice() and no matching when() clause is found, the Exchange just ends successfully without doing anything. In my mind, it should fail by default in this case (or we should at least have an easy way to get this behavior).",0.2111666667,0.2111666667,negative
camel,6188,description,"The CXF consumer copies the content-type http header to the camel exchange. This header may indicate the character set used in the request (for instance and if so this should be made available in the normal place for Camel (i.e. a property in the exchange called This may (of course) be done in each route by a separate processor, but it simplifies life if this is done by default. seems the logical place) Sample processor that performs this job.",design_debt,non-optimal_design,"Wed, 20 Mar 2013 13:20:03 +0000","Thu, 28 Mar 2013 02:25:58 +0000","Thu, 28 Mar 2013 02:25:58 +0000",651955,"The CXF consumer copies the content-type http header to the camel exchange. This header may indicate the character set used in the request (for instance ""text/xml;charset=UTF-8""), and if so this should be made available in the normal place for Camel (i.e. a property in the exchange called 'CamelCharsetName'). This may (of course) be done in each route by a separate processor, but it simplifies life if this is done by default. (org.apache.camel.component.cxf.DefaultCxfBinding.populateExchangeFromCxfRequest() seems the logical place) Sample processor that performs this job.",0.2083333333,0.0625,neutral
camel,6403,description,"When using a {{camel-jetty}} endpoint, the {{UnitOfWork}} is not being managed by the servlet handling the request but by the Camel route that's being invoked. This means that some resources have already been removed/cleaned up when the servlet is writing the response, e.g. files for cached streams have already removed before the servlet gets a chance to read from them. It would be nice to have an option available to configure the servlet itself to handle the unit of work and mark it {{done}} after the HTTP response has been written. That way, the unit of work can be matched up with the actual HTTP request.",design_debt,non-optimal_design,"Tue, 28 May 2013 07:25:26 +0000","Mon, 12 Aug 2013 17:45:21 +0000","Fri, 9 Aug 2013 07:14:44 +0000",6306558,"When using a camel-jetty endpoint, the UnitOfWork is not being managed by the servlet handling the request but by the Camel route that's being invoked. This means that some resources have already been removed/cleaned up when the servlet is writing the response, e.g. files for cached streams have already removed before the servlet gets a chance to read from them. It would be nice to have an option available to configure the servlet itself to handle the unit of work and mark it done after the HTTP response has been written. That way, the unit of work can be matched up with the actual HTTP request.",0.415625,0.415625,neutral
camel,6635,description,"Due the recent SPI which allows to plugin a different scheduler we can improved this and use a non scheduled thread pool, which avoids the suspend/resume and run for at least one poll ""hack"" we have today in the codebase. Instead we can use a regular thread pool as the scheduler, and then submit the task on demand when receive() is called on the PollingConsumer API.",design_debt,non-optimal_design,"Wed, 14 Aug 2013 08:13:10 +0000","Wed, 14 Aug 2013 14:55:18 +0000","Wed, 14 Aug 2013 14:55:18 +0000",24128,"Due the recent SPI which allows to plugin a different scheduler we can improved this and use a non scheduled thread pool, which avoids the suspend/resume and run for at least one poll ""hack"" we have today in the codebase. Instead we can use a regular thread pool as the scheduler, and then submit the task on demand when receive() is called on the PollingConsumer API.",-0.1,-0.1,neutral
camel,7015,comment_1,What is the idea with the Will it not confuse end users that there is 2 timeouts now?,design_debt,non-optimal_design,"Tue, 26 Nov 2013 22:24:15 +0000","Fri, 5 Sep 2014 07:48:10 +0000","Fri, 5 Sep 2014 07:48:10 +0000",24398635,What is the idea with the maxCompletionTimeout? Will it not confuse end users that there is 2 timeouts now?,0.4,0.2,neutral
camel,7015,comment_2,"Willem hold on any merges, as at first glance I think its confusing with that max timeout option.",design_debt,non-optimal_design,"Tue, 26 Nov 2013 22:24:15 +0000","Fri, 5 Sep 2014 07:48:10 +0000","Fri, 5 Sep 2014 07:48:10 +0000",24398635,"Willem hold on any merges, as at first glance I think its confusing with that max timeout option.",-0.437,-0.437,negative
camel,7015,comment_3,"At first I though of adding an option telling whether or not the timeout is for the first message received in this correlation group or for the last message. But then I though, why not provide both timeouts ? You might want to 5 seconds after the last exchange received but not more than 60 seconds after the first message. That way, if you have an exchange every 4 seconds, you'll aggregate after one minute an not any longer. Thus I named this parameter because I didn't wanted to rename the parameter. I'm not saying that it's the right way, or maybe it's not properly named. Don't hesitate to ask if you want more insights about this patch.",design_debt,non-optimal_design,"Tue, 26 Nov 2013 22:24:15 +0000","Fri, 5 Sep 2014 07:48:10 +0000","Fri, 5 Sep 2014 07:48:10 +0000",24398635,"At first I though of adding an option telling whether or not the timeout is for the first message received in this correlation group or for the last message. But then I though, why not provide both timeouts ? You might want to 5 seconds after the last exchange received but not more than 60 seconds after the first message. That way, if you have an exchange every 4 seconds, you'll aggregate after one minute an not any longer. Thus I named this parameter maxCompletionTimeout because I didn't wanted to rename the completionTimeout parameter. I'm not saying that it's the right way, or maybe it's not properly named. Don't hesitate to ask if you want more insights about this patch.",-0.03242857143,-0.03242857143,neutral
camel,7015,description,"It would be nice to have a possibility of a timeout starting from the first Exchange for messages having the same when using Aggregation. This scenario is currently not supported because ""Its a bit tougher to implement as it would require an improvement to TimeoutMap to support that as well."" The full scenario is described in this thread:",design_debt,non-optimal_design,"Tue, 26 Nov 2013 22:24:15 +0000","Fri, 5 Sep 2014 07:48:10 +0000","Fri, 5 Sep 2014 07:48:10 +0000",24398635,"It would be nice to have a possibility of a timeout starting from the first Exchange for messages having the same correlationExpression when using Aggregation. This scenario is currently not supported because ""Its a bit tougher to implement as it would require an improvement to TimeoutMap to support that as well."" The full scenario is described in this thread: http://camel.465427.n5.nabble.com/A-simple-Aggregator-use-case-td5743633.html#a5743634",0.37275,0.37275,neutral
camel,7071,comment_1,This is strange way to solve that. Should not we (application developers) use exception wrapping? Please note this method is used By,design_debt,non-optimal_design,"Mon, 16 Dec 2013 14:03:19 +0000","Wed, 18 Dec 2013 12:35:13 +0000","Tue, 17 Dec 2013 03:02:23 +0000",46744,This is strange way to solve that. Should not we (application developers) use exception wrapping? Please note this method is used By ExchangeExceptionannotation.,0.06666666667,0.06666666667,negative
camel,7201,description,"In the current PGPDataFormat implementation, you provide the public and secret keyring as file or byte array and the keyrings are parsed into object representation during each call. This is fine if you want dynamically exchange the keyrings. However, this has a performance impact. In the provided patch I added the possibility that the PGP keys can be cached so that the performance can be improved. I did this by adding the two interfaces PGPPublicKeyAccess and PGPSecretKeyAccess. So user can now implement his own key access or use the provided default implementations and",design_debt,non-optimal_design,"Thu, 13 Feb 2014 09:42:56 +0000","Wed, 19 Feb 2014 13:28:43 +0000","Wed, 19 Feb 2014 13:28:43 +0000",531947,"In the current PGPDataFormat implementation, you provide the public and secret keyring as file or byte array and the keyrings are parsed into object representation during each call. This is fine if you want dynamically exchange the keyrings. However, this has a performance impact. In the provided patch I added the possibility that the PGP keys can be cached so that the performance can be improved. I did this by adding the two interfaces PGPPublicKeyAccess and PGPSecretKeyAccess. So user can now implement his own key access or use the provided default implementations PGPPublicKeyAccessDefault and PGPSecretKeyAccessDefault.",0.023,0.023,neutral
camel,7461,description,"See for a background. Basically there is inconsistency between a idempotent consumer and the repository. The repository is capable of holding any type, while the consumer is non-parameterized and uses String as it's message type. It would be very handy to have the messageid as a domain type for any application, and thus the consumer should allow for a parameterized type T. This will also probably mean that should allow for any persistent type. If doing this camel would be generic on the type, and allow for supporting application domain types w/o customizations.",design_debt,non-optimal_design,"Fri, 23 May 2014 07:49:45 +0000","Thu, 29 Mar 2018 11:30:12 +0000","Thu, 29 Mar 2018 11:30:12 +0000",121491627,"See http://camel.465427.n5.nabble.com/Idempotent-inconsistencies-td5751484.html for a background. Basically there is inconsistency between a idempotent consumer and the repository. The repository is capable of holding any type, while the consumer is non-parameterized and uses String as it's message type. It would be very handy to have the messageid as a domain type for any application, and thus the consumer should allow for a parameterized type T. This will also probably mean that JpaMessageIdRepository should allow for any persistent type. If doing this camel would be generic on the type, and allow for supporting application domain types w/o customizations.",0.2466666667,0.2466666667,neutral
camel,7587,description,"The MessageHistory feature currently keeps passwords in plain text in case they are part of the URI. does some sanitizing, but only for the from node - other nodes/processors are currently not sanitized. In order to prevent handling sensitive information in the message history in general, I would suggest to sanitize the URI already when storing a MessageHistory item.",design_debt,non-optimal_design,"Wed, 9 Jul 2014 14:08:49 +0000","Thu, 10 Jul 2014 05:57:23 +0000","Thu, 10 Jul 2014 05:57:23 +0000",56914,"The MessageHistory feature currently keeps passwords in plain text in case they are part of the URI. MessageHelper.doDumpMessageHistoryStacktrace() does some sanitizing, but only for the from node - other nodes/processors are currently not sanitized. In order to prevent handling sensitive information in the message history in general, I would suggest to sanitize the URI already when storing a MessageHistory item.",-0.2,-0.15,neutral
camel,7813,description,"When a JMS queue is used as a camel consumer for a route it may well be one of possibly many intermediate stops in a chain of processing. If the previous processing step itself used Camel to route the message, then both the JMS replyTo and the camel-header JMSReplyTo will both be populated with the same value. This will cause an infinite loop. Of course, this is in some sense a developer error, but it is a pain to constantly add code to clear the camel JMSReplyTo header if it equals the destination. This should probably be internal to the camel-jms component itself.",design_debt,non-optimal_design,"Sun, 14 Sep 2014 02:28:11 +0000","Tue, 26 Jan 2016 16:42:37 +0000","Tue, 8 Sep 2015 09:11:40 +0000",31041809,"When a JMS queue is used as a camel consumer for a route it may well be one of possibly many intermediate stops in a chain of processing. If the previous processing step itself used Camel to route the message, then both the JMS replyTo and the camel-header JMSReplyTo will both be populated with the same value. This will cause an infinite loop. Of course, this is in some sense a developer error, but it is a pain to constantly add code to clear the camel JMSReplyTo header if it equals the destination. This should probably be internal to the camel-jms component itself.",-0.0719,-0.0719,neutral
camel,7813,summary,Make camel-jms more robust for replyTo,design_debt,non-optimal_design,"Sun, 14 Sep 2014 02:28:11 +0000","Tue, 26 Jan 2016 16:42:37 +0000","Tue, 8 Sep 2015 09:11:40 +0000",31041809,Make camel-jms more robust for replyTo,0.4,0.4,neutral
camel,7954,description,Currently camel-olingo2 uses SSLContext directly to create connections; it should use,design_debt,non-optimal_design,"Fri, 24 Oct 2014 20:18:18 +0000","Tue, 6 Aug 2019 05:14:42 +0000","Tue, 6 Aug 2019 05:14:42 +0000",150886584,Currently camel-olingo2 uses SSLContext directly to create connections; it should use SSLContextParameters.,0,0,neutral
camel,7956,description,Currently camel-olingo2 uses SSLContext directly to create connections; it should use,design_debt,non-optimal_design,"Fri, 24 Oct 2014 20:19:09 +0000","Tue, 6 Aug 2019 05:14:50 +0000","Tue, 6 Aug 2019 05:14:50 +0000",150886541,Currently camel-olingo2 uses SSLContext directly to create connections; it should use SSLContextParameters.,0,0,neutral
camel,8091,description,"The does not consider the context property that is supposed to limit the size of the logged payload. It is possible to set a maxChars on the but that has a different semantics (limits the length of the formatted exchange, not of the message payload) and is complicated to set in some cases (e.g. in the case of the default error handler) The attached extension also honors the context property when formatting the exchange.",design_debt,non-optimal_design,"Fri, 28 Nov 2014 09:20:59 +0000","Tue, 9 Dec 2014 19:26:23 +0000","Tue, 9 Dec 2014 19:26:23 +0000",986724,"The DefaultExchangeFormatter does not consider the Exchange.LOG_DEBUG_BODY_MAX_CHARS context property that is supposed to limit the size of the logged payload. It is possible to set a maxChars on the DefaultExchangeFormatter, but that has a different semantics (limits the length of the formatted exchange, not of the message payload) and is complicated to set in some cases (e.g. in the case of the default error handler) The attached extension also honors the Exchange.LOG_DEBUG_BODY_MAX_CHARS context property when formatting the exchange.",-0.1833333333,-0.09166666667,neutral
camel,8174,comment_3,"Willem, we have a better solution out of the box with the default inflight repoistory. It has a browse API now, that returns details about how long time the exchange has been inflight. And its also available from JMX so end users can use that to track long processing exchanges. And for Karaf we have a Camel command also. So I would like to remove the as its no longer needed, and also its code is not complete. Any thoughts?",design_debt,non-optimal_design,"Tue, 23 Dec 2014 14:10:36 +0000","Sun, 15 Feb 2015 08:23:13 +0000","Sat, 31 Jan 2015 08:53:41 +0000",3350585,"Willem, we have a better solution out of the box with the default inflight repoistory. It has a browse API now, that returns details about how long time the exchange has been inflight. And its also available from JMX so end users can use that to track long processing exchanges. And for Karaf we have a Camel command also. So I would like to remove the TimeoutInflightRepository as its no longer needed, and also its code is not complete. Any thoughts?",0.01111111111,0.01111111111,neutral
camel,8312,comment_0,"One thing I overlooked: the javadoc of the method says: ... For example you can set it to InputSource to use SAX streams. By default Camel uses Document as the type. ... Using InputSource as documentType is a particularily bad idea. If the XPath implementation supports it (Saxon does, the JDK implementation doesn't), SAXSource can be a more memory efficient choice for the docuementType. We should probably also change the Javadoc here...",design_debt,non-optimal_design,"Wed, 4 Feb 2015 07:25:25 +0000","Tue, 3 Mar 2015 21:12:58 +0000","Mon, 2 Mar 2015 12:01:31 +0000",2262966,"One thing I overlooked: the javadoc of the XPathBuilder.documentType() method says: ... For example you can set it to InputSource to use SAX streams. By default Camel uses Document as the type. ... Using InputSource as documentType is a particularily bad idea. If the XPath implementation supports it (Saxon does, the JDK implementation doesn't), SAXSource can be a more memory efficient choice for the docuementType. We should probably also change the Javadoc here...",-0.18,-0.15,negative
camel,8321,comment_3,"Why are Box headers being used/fed in other These headers are only supposed to be fed to the Box component, then should be cleaned up n the next step. I wasn't aware of the Camel 'no dots in header' convention. I'll have to take a look at the API framework to see if that's affected. At the very minimum code generation maven plugin is affected since it uses that convention for all generated components, and also would make the new components backward incompatible.",design_debt,non-optimal_design,"Fri, 6 Feb 2015 11:28:52 +0000","Wed, 18 Feb 2015 01:45:56 +0000","Wed, 18 Feb 2015 01:45:56 +0000",1001824,"Why are Box headers being used/fed in other components/processors? These headers are only supposed to be fed to the Box component, then should be cleaned up n the next step. I wasn't aware of the Camel 'no dots in header' convention. I'll have to take a look at the API framework to see if that's affected. At the very minimum code generation maven plugin is affected since it uses that convention for all generated components, and also would make the new components backward incompatible.",-0.24075,-0.1926,neutral
camel,8844,description,"Currently there are issues when directory in (s)ftp(s) endpoint is absolute (or contains arbitrary number of leading slashes). Sometimes the path is normalized, sometimes don't and camel-ftp code can't find correct prefix in _absolute_ path when retrieving remote file. [Documentation I suggest normalizing the behavior and treating absolute FTP endpoint URI directories as relative. When URI contains absolute path, WARNing will be printed:",design_debt,non-optimal_design,"Mon, 8 Jun 2015 09:35:14 +0000","Tue, 9 Jun 2015 10:23:10 +0000","Mon, 8 Jun 2015 10:11:38 +0000",2184,"Currently there are issues when directory in (s)ftp(s) endpoint is absolute (or contains arbitrary number of leading slashes). Sometimes the path is normalized, sometimes don't and camel-ftp code can't find correct prefix in absolute path when retrieving remote file. Documentation says: Absolute path is not supported. I suggest normalizing the behavior and treating absolute FTP endpoint URI directories as relative. When URI contains absolute path, WARNing will be printed:",-0.3270833333,-0.3416666667,negative
camel,903,description,"The camel-ftp component uses stored as last poll time. The remote file timestamp is used for comparing against last poll time if its new and thus a candidate for download. As timestamps over FTP is not reliable we should not use this feature by default, but turn it off. It should only be there for test or experimental usage. Most FTP servers only sent file timestamp as HH:mm (no seconds). And as a bonus we avoid timezone issues as well. Instead end-users should use a different strategy for ""marking files done"" such as: - delete consumed files - rename consumed files See nabble:",design_debt,non-optimal_design,"Mon, 15 Sep 2008 09:26:57 +0000","Thu, 23 Oct 2008 04:37:13 +0000","Tue, 16 Sep 2008 06:18:43 +0000",75106,"The camel-ftp component uses System.currentTimeMillis stored as last poll time. The remote file timestamp is used for comparing against last poll time if its new and thus a candidate for download. As timestamps over FTP is not reliable we should not use this feature by default, but turn it off. It should only be there for test or experimental usage. Most FTP servers only sent file timestamp as HH:mm (no seconds). And as a bonus we avoid timezone issues as well. Instead end-users should use a different strategy for ""marking files done"" such as: delete consumed files rename consumed files See nabble: http://www.nabble.com/Ftp-consumer-td19489526s22882.html",-0.02673809524,-0.02339583333,neutral
camel,9181,description,"Previous ScrHelper could break when there are certain XML libraries in the classpath (e.g. XOM or Saxon). Also, it doesn't work with component description files generated by the latest version 1.21.0 (format has changed). This is a simpler, less picky implementation using StAX.",design_debt,non-optimal_design,"Mon, 28 Sep 2015 14:32:41 +0000","Mon, 28 Sep 2015 15:09:04 +0000","Mon, 28 Sep 2015 15:09:04 +0000",2183,"Previous ScrHelper could break when there are certain XML libraries in the classpath (e.g. XOM or Saxon). Also, it doesn't work with component description files generated by the latest org.apache.felix/maven-scr-plugin version 1.21.0 (format has changed). This is a simpler, less picky implementation using StAX.",-0.3,-0.18,negative
camel,9181,summary,"Simpler, less picky ScrHelper",design_debt,non-optimal_design,"Mon, 28 Sep 2015 14:32:41 +0000","Mon, 28 Sep 2015 15:09:04 +0000","Mon, 28 Sep 2015 15:09:04 +0000",2183,"Simpler, less picky ScrHelper",-0.5,-0.5,positive
camel,946,description,We should improve the <package Also when doing unit testing it would be great to be sure only YOUR route is loaded and not all in the same package.,design_debt,non-optimal_design,"Mon, 29 Sep 2008 12:17:19 +0000","Sat, 21 Nov 2009 11:57:54 +0000","Tue, 14 Apr 2009 09:40:38 +0000",17011399,"We should improve the <package> or to allow setting 1..N classes instead or ref to spring ids. The <package> is sometimes a bit to ""magic"" for new users. Also when doing unit testing it would be great to be sure only YOUR route is loaded and not all in the same package.",0.5,0.3,neutral
camel,9499,description,We use colon today which makes the uri ambigious and confusing. As colon is a separator for other options. So we should use dash instead temp:queue -temp:topic -,design_debt,non-optimal_design,"Mon, 11 Jan 2016 12:04:01 +0000","Tue, 19 Jan 2016 18:15:04 +0000","Tue, 19 Jan 2016 18:15:04 +0000",713463,We use colon today which makes the uri ambigious and confusing. As colon is a separator for other options. So we should use dash instead temp:queue -> temp-queue temp:topic -> temp-topic,-0.1456666667,-0.1456666667,neutral
camel,9616,description,"For the moment, a custom {{MetricRegistry}} bean can be provided but must be named. It would be easier to relax that constraint in case only one {{MetricRegistry}} bean exist and do the lookup by type only.",design_debt,non-optimal_design,"Thu, 18 Feb 2016 14:52:17 +0000","Thu, 18 Feb 2016 16:44:48 +0000","Thu, 18 Feb 2016 16:22:30 +0000",5413,"For the moment, a custom MetricRegistry bean can be provided but must be named. It would be easier to relax that constraint in case only one MetricRegistry bean exist and do the lookup by type only.",0,0,neutral
camel,9690,description,"Hello Im using camel 2.16.2 and Im finding the bean parameter binding doesnt seem to work very well on overloaded methods. See below for an example Here are the routes And here are the tests I dont understand why test2Param_string and test2Param_classB throw ambiguous call exceptions. Heres a sample stack trace. From looking at the code in BeanInfo, I *think* it just tries to match the type on the body and if it sees multiple possible methods then it throws the exception. I believe it should go further and try to match the type on the other parameters as well? To get around this issue temporarily, Ive had to write an adapter class that wraps around ClassA but its not an ideal solution.",design_debt,non-optimal_design,"Wed, 9 Mar 2016 23:33:29 +0000","Tue, 22 Mar 2016 12:02:20 +0000","Tue, 22 Mar 2016 12:02:20 +0000",1081731,"Hello Im using camel 2.16.2 and Im finding the bean parameter binding doesnt seem to work very well on overloaded methods. See below for an example Here are the routes And here are the tests I dont understand why test2Param_string and test2Param_classB throw ambiguous call exceptions. Heres a sample stack trace. From looking at the code in BeanInfo, I think it just tries to match the type on the body and if it sees multiple possible methods then it throws the exception. I believe it should go further and try to match the type on the other parameters as well? To get around this issue temporarily, Ive had to write an adapter class that wraps around ClassA but its not an ideal solution.",0.1806944444,-0.02447222222,negative
camel,9812,comment_0,"Thanks for reporting. Sound like the consumer need to call some stop/shutdown on kafka somewhere if its not already doing that, or missing something.",design_debt,non-optimal_design,"Mon, 4 Apr 2016 13:00:56 +0000","Tue, 5 Apr 2016 11:19:48 +0000","Tue, 5 Apr 2016 09:24:07 +0000",73391,"Thanks for reporting. Sound like the consumer need to call some stop/shutdown on kafka somewhere if its not already doing that, or missing something.",0.05,0.05,neutral
camel,990,comment_3,BTW - its not really a big deal to use annotations from any particular framework in your code as annotations are a soft dependency. e.g. adding Camel annotations to your code doesn't add any direct dependency on camel. You can then use your code totally fine without any camel jars on the classpath. So there's not a huge need to 'hide' dependencies on Camel annotations - as you only need them to compile your source code. But using indirect annotations can be useful; e.g. you can have a foo.jar which defines a @Foo annotation which builds itself with camel - then folks can just add foo.jar to their classpath to get camel goodness without adding camel jars to the build. So you could introduce a kinda macro annotation that includes various framework annotations on it (say spring and camel and guice annotations :),design_debt,non-optimal_design,"Thu, 16 Oct 2008 11:35:25 +0000","Mon, 16 Feb 2009 05:51:55 +0000","Thu, 23 Oct 2008 06:06:58 +0000",585093,BTW - its not really a big deal to use annotations from any particular framework in your code as annotations are a soft dependency. e.g. adding Camel annotations to your code doesn't add any direct dependency on camel. You can then use your code totally fine without any camel jars on the classpath. So there's not a huge need to 'hide' dependencies on Camel annotations - as you only need them to compile your source code. But using indirect annotations can be useful; e.g. you can have a foo.jar which defines a @Foo annotation which builds itself with camel - then folks can just add foo.jar to their classpath to get camel goodness without adding camel jars to the build. So you could introduce a kinda macro annotation that includes various framework annotations on it (say spring and camel and guice annotations,0.24225,0.2485,neutral
camel,990,comment_4,"The original need is the following: Let's say I build a Routing System based on Camel. Of course my System will depend on Camel. But I don't want the end user of my System (who will be responsible for adding routes, converters...) to depend on any camel jar. Your solution is better (and also more time consuming than my simple metaannotation patch :)), will think about it.",design_debt,non-optimal_design,"Thu, 16 Oct 2008 11:35:25 +0000","Mon, 16 Feb 2009 05:51:55 +0000","Thu, 23 Oct 2008 06:06:58 +0000",585093,"The original need is the following: Let's say I build a Routing System based on Camel. Of course my System will depend on Camel. But I don't want the end user of my System (who will be responsible for adding routes, converters...) to depend on any camel jar. Your solution is better (and also more time consuming than my simple metaannotation patch ), will think about it.",0.04875,0.045,neutral
camel,10048,comment_2,"Hello Claus, Is it ok to use Processor as a key in a hash map? Should every implementer keep this in mind and provide equals() and hashCode()? Javadoc in Processor interface tells nothing about it.",documentation_debt,low_quality_documentation,"Sat, 11 Jun 2016 21:41:44 +0000","Thu, 4 Jan 2018 09:11:44 +0000","Sun, 12 Jun 2016 09:44:54 +0000",43390,"Hello Claus, Is it ok to use Processor as a key in a hash map? Should every implementer keep this in mind and provide equals() and hashCode()? Javadoc in Processor interface tells nothing about it.",0.2083333333,0.2083333333,neutral
camel,1112,comment_1,"Wiki documentation needed for file component: - idempotent options *DONE* - headers with current index and total *DONE* - important camel 2.0 changes *DONE* - file filter options *DONE* - file sorter options *DONE* - out of box file sorters for: by name, by timestamp, etc. *DONE* - idempotent sample *DONE* - file filter sample *DONE* - file sorter sample *DONE* - file sortBy sample *DONE* - sort by: ignore case option *DONE*",documentation_debt,outdated_documentation,"Mon, 24 Nov 2008 15:48:05 +0000","Fri, 31 Jul 2009 06:33:43 +0000","Tue, 2 Dec 2008 07:41:32 +0000",662007,"Wiki documentation needed for file component: idempotent options DONE headers with current index and total DONE important camel 2.0 changes DONE file filter options DONE file sorter options DONE out of box file sorters for: by name, by timestamp, etc. DONE idempotent sample DONE file filter sample DONE file sorter sample DONE file sortBy sample DONE sort by: ignore case option DONE",0.1,0.1,neutral
camel,11408,comment_0,"The component docs are in adoc files with the source code - the wiki is dead so don't update there. Make sure to fix/update in adoc, and if you want you can do wiki too. But wiki only changes will be lost in the future when wiki is discarded completely",documentation_debt,outdated_documentation,"Wed, 14 Jun 2017 07:16:37 +0000","Thu, 1 Feb 2018 15:34:14 +0000","Thu, 1 Feb 2018 15:34:14 +0000",20074657,"The component docs are in adoc files with the source code - the wiki is dead so don't update there. Make sure to fix/update in adoc, and if you want you can do wiki too. But wiki only changes will be lost in the future when wiki is discarded completely",-0.2166666667,-0.2166666667,negative
camel,11408,description,"On page the section ""Sample when using OSGi"" is missing the code snippets. Can these examples be put back in and, if possible, be verified? Thanks much!",documentation_debt,low_quality_documentation,"Wed, 14 Jun 2017 07:16:37 +0000","Thu, 1 Feb 2018 15:34:14 +0000","Thu, 1 Feb 2018 15:34:14 +0000",20074657,"On page http://camel.apache.org/servlet.html the section ""Sample when using OSGi"" is missing the code snippets. Can these examples be put back in and, if possible, be verified? Thanks much!",0.1666666667,0.1666666667,neutral
camel,11504,description,"We should be accessible and there should be no broken pages in the website, to do that we should incorporate",documentation_debt,low_quality_documentation,"Mon, 3 Jul 2017 08:41:20 +0000","Mon, 22 Apr 2019 16:21:52 +0000","Mon, 22 Apr 2019 16:21:44 +0000",56878824,"We should be accessible and there should be no broken pages in the website, to do that we should incorporate check-pages.",0.3815,0.3815,neutral
camel,1165,comment_0,I've put up a manual with simple cover page at let me know what you think. I still need to put in something that will dynamically insert the version number. That is for tomorrow though :),documentation_debt,low_quality_documentation,"Tue, 9 Dec 2008 09:46:04 +0000","Fri, 31 Jul 2009 06:33:50 +0000","Wed, 10 Dec 2008 14:54:13 +0000",104889,"I've put up a manual with simple cover page at http://people.apache.org/~janstey/temp/camel-manual-2.0-SNAPSHOT.pdf, let me know what you think. I still need to put in something that will dynamically insert the version number. That is for tomorrow though",0.227,0.09366666667,neutral
camel,1165,summary,"can we add a front page for the manual specifying Camel User Guide, Version XX?",documentation_debt,low_quality_documentation,"Tue, 9 Dec 2008 09:46:04 +0000","Fri, 31 Jul 2009 06:33:50 +0000","Wed, 10 Dec 2008 14:54:13 +0000",104889,"can we add a front page for the manual specifying Camel User Guide, Version XX?",0,0,neutral
camel,11734,comment_2,great :-) Maybe we can document very well how everything works and then setup an example in OSGi (we currently have a spring-boot example and a kubernetes one).,documentation_debt,low_quality_documentation,"Fri, 1 Sep 2017 06:17:00 +0000","Fri, 8 Sep 2017 08:49:52 +0000","Fri, 8 Sep 2017 08:49:52 +0000",613972,dmvolod great Maybe we can document very well how everything works and then setup an example in OSGi (we currently have a spring-boot example and a kubernetes one).,0.55775,0.6103333333,positive
camel,1173,comment_0,Adding Transmitting file data . Committed revision 724833. This is my first stab at the pattern. Not completely happy with it at the moment... maybe we need to put better support into Camel somewhere for this kinda thing.,documentation_debt,low_quality_documentation,"Tue, 9 Dec 2008 19:01:43 +0000","Mon, 23 Mar 2009 08:40:31 +0000","Mon, 2 Feb 2009 19:39:35 +0000",4754272,Adding camel-core/src/test/java/org/apache/camel/processor/ScatterGatherTest.java Transmitting file data . Committed revision 724833. This is my first stab at the pattern. Not completely happy with it at the moment... maybe we need to put better support into Camel somewhere for this kinda thing.,-0.1125,-0.09,negative
camel,1173,description,See We should be able to do this in Camel already... just need to cook up an example/doco for it.,documentation_debt,low_quality_documentation,"Tue, 9 Dec 2008 19:01:43 +0000","Mon, 23 Mar 2009 08:40:31 +0000","Mon, 2 Feb 2009 19:39:35 +0000",4754272,See http://www.eaipatterns.com/BroadcastAggregate.html We should be able to do this in Camel already... just need to cook up an example/doco for it.,0.688,0.688,positive
camel,1173,summary,Need example of scatter-gather pattern,documentation_debt,low_quality_documentation,"Tue, 9 Dec 2008 19:01:43 +0000","Mon, 23 Mar 2009 08:40:31 +0000","Mon, 2 Feb 2009 19:39:35 +0000",4754272,Need example of scatter-gather pattern,0,0,neutral
camel,12042,comment_0,"The old wiki system is deprecated, a new website and documentation is in the works. You can help with the new docs which are the adoc files in the source code you can find in the src/main/docs folder of the various Camel components.",documentation_debt,outdated_documentation,"Mon, 27 Nov 2017 11:45:17 +0000","Mon, 27 Nov 2017 12:21:12 +0000","Mon, 27 Nov 2017 12:09:36 +0000",1459,"The old wiki system is deprecated, a new website and documentation is in the works. You can help with the new docs which are the adoc files in the source code you can find in the src/main/docs folder of the various Camel components.",0.5,0.5,neutral
camel,12042,description,"The rendering of code snippets on the camel homepage is broken. For example see I expect the following should be java syntax highlighting? {{ javaEndpoint endpoint = PollingConsumer consumer = Exchange exchange = Also further down the page I get: {{The example below illustrates I'm not sure if this is the right place to report this, but since the homepage is part of the github repository it seems to make sense using the projects bug tracker. I checked the Gitter history and had a look at the ML and nobody seems to mention this. I've also tried to search on JIRA but always end up with Issues for Zookeeper or other projects.",documentation_debt,low_quality_documentation,"Mon, 27 Nov 2017 11:45:17 +0000","Mon, 27 Nov 2017 12:21:12 +0000","Mon, 27 Nov 2017 12:09:36 +0000",1459,"The rendering of code snippets on the camel homepage is broken. For example see http://camel.apache.org/polling-consumer.html I expect the following should be java syntax highlighting? {{ javaEndpoint endpoint = context.getEndpoint(""activemq:my.queue""); PollingConsumer consumer = endpoint.createPollingConsumer(); Exchange exchange = consumer.receive();}} Also further down the page I get: {{The example below illustrates this: {snippet:id=e1|lang=xml|url=camel/components/camel-spring/src/test/resources/org/apache/camel/spring/SpringConsumerTemplateTest-context.xml} }} I'm not sure if this is the right place to report this, but since the homepage is part of the github repository it seems to make sense using the projects bug tracker. I checked the Gitter history and had a look at the ML and nobody seems to mention this. I've also tried to search on JIRA but always end up with Issues for Zookeeper or other projects.",-0.03626666667,-0.04355,negative
camel,12042,summary,Rendering of code/snippets inside documentation is broken,documentation_debt,low_quality_documentation,"Mon, 27 Nov 2017 11:45:17 +0000","Mon, 27 Nov 2017 12:21:12 +0000","Mon, 27 Nov 2017 12:09:36 +0000",1459,Rendering of code/snippets inside documentation is broken,-0.2,-0.2,negative
camel,13111,comment_5,": No sir. I think I had a misspelling on the PR, so the bot didn't pick anything up. This ticket is done. Sorry for the confusion.",documentation_debt,low_quality_documentation,"Wed, 23 Jan 2019 10:18:52 +0000","Fri, 1 Mar 2019 13:25:05 +0000","Thu, 7 Feb 2019 17:45:36 +0000",1322804,"ancosen : No sir. I think I had a misspelling on the PR, so the bot didn't pick anything up. This ticket is done. Sorry for the confusion.",-0.25,-0.25,negative
camel,13111,description,"I noticed that we dont have in some component docs, the spring boot START END markers for the SB auto configuration docs we see WARNs when building SB starter JARs",documentation_debt,low_quality_documentation,"Wed, 23 Jan 2019 10:18:52 +0000","Fri, 1 Mar 2019 13:25:05 +0000","Thu, 7 Feb 2019 17:45:36 +0000",1322804,"I noticed that we dont have in some component docs, the spring boot START END markers for the SB auto configuration docs we see WARNs when building SB starter JARs",-0.4,-0.4,neutral
camel,13111,summary,Add missing spring boot auto configuration doc markers in the adoc files,documentation_debt,low_quality_documentation,"Wed, 23 Jan 2019 10:18:52 +0000","Fri, 1 Mar 2019 13:25:05 +0000","Thu, 7 Feb 2019 17:45:36 +0000",1322804,Add missing spring boot auto configuration doc markers in the adoc files,-0.4,-0.4,negative
camel,1317,description,We can definitely do this in Camel already... just need an example/docs for it.,documentation_debt,outdated_documentation,"Thu, 5 Feb 2009 14:40:46 +0000","Fri, 31 Jul 2009 06:34:03 +0000","Thu, 5 Feb 2009 21:07:47 +0000",23221,We can definitely do this in Camel already... just need an example/docs for it.,0,0,positive
camel,1320,comment_4,"And we need to add it to the wiki page as well, under data formats.",documentation_debt,outdated_documentation,"Fri, 6 Feb 2009 14:55:23 +0000","Sat, 21 Nov 2009 11:57:55 +0000","Thu, 19 Mar 2009 15:18:16 +0000",3543773,"And we need to add it to the wiki page as well, under data formats.",0.631,0.631,neutral
camel,1507,comment_5,"Ryan/William We also need to update the mail wiki page with this feature. At least document its possible and give some hints how to do it. And it would be great with a sample how to do it, and you can use the SNIPPETS in the unit test to put it on the wiki pages.",documentation_debt,outdated_documentation,"Wed, 1 Apr 2009 00:50:08 +0000","Sat, 27 Nov 2010 06:42:58 +0000","Thu, 2 Apr 2009 07:42:27 +0000",111139,"Ryan/William We also need to update the mail wiki page with this feature. At least document its possible and give some hints how to do it. And it would be great with a sample how to do it, and you can use the SNIPPETS in the unit test to put it on the wiki pages.",0.2,0.2,positive
camel,168,description,camel-ognl has a typo in its packaging. This causes the classes to be not included in the snapshot,documentation_debt,low_quality_documentation,"Tue, 9 Oct 2007 15:24:36 +0000","Mon, 12 May 2008 07:56:32 +0000","Tue, 9 Oct 2007 15:39:02 +0000",866,camel-ognl has a typo in its packaging. This causes the classes to be not included in the snapshot,0,0,negative
camel,1846,comment_2,"ugh, I didn't notice those links all went to the same place. Can those be fixed to point at the pdf, or at least removed to avoid confusing anyone else?",documentation_debt,low_quality_documentation,"Wed, 22 Jul 2009 20:15:26 +0000","Sun, 7 Feb 2010 09:58:27 +0000","Sun, 13 Sep 2009 18:55:42 +0000",4574416,"ugh, I didn't notice those links all went to the same place. Can those be fixed to point at the pdf, or at least removed to avoid confusing anyone else?",-0.35925,-0.35925,negative
camel,1846,comment_3,"Well, the pdf manual doesn't have the details about the xml elements, so I guess the only option at the moment is to try to read the xsd directly, or figure out how to generate the xsddoc pages. :(",documentation_debt,outdated_documentation,"Wed, 22 Jul 2009 20:15:26 +0000","Sun, 7 Feb 2010 09:58:27 +0000","Sun, 13 Sep 2009 18:55:42 +0000",4574416,"Well, the pdf manual doesn't have the details about the xml elements, so I guess the only option at the moment is to try to read the xsd directly, or figure out how to generate the xsddoc pages.",0.114,0.628,negative
camel,1846,comment_4,Fixed the links. Sadly we cannot maintain easily a v1.x and 2.0 branch of the online docs as part of build process etc. The xsddoc do not bring much value so I am looking into removing it as well.,documentation_debt,outdated_documentation,"Wed, 22 Jul 2009 20:15:26 +0000","Sun, 7 Feb 2010 09:58:27 +0000","Sun, 13 Sep 2009 18:55:42 +0000",4574416,Fixed the links. Sadly we cannot maintain easily a v1.x and 2.0 branch of the online docs as part of build process etc. The xsddoc do not bring much value so I am looking into removing it as well.,0.064,0.064,negative
camel,1846,comment_6,The maven reports is just getting to old and intermixed with 1.x and trunk releases. We should just remove them as they dont bring much value anyway. The xsddoc has been removed from the build files. So its now a matter of removing from the apache http site (people with credentials is needed),documentation_debt,outdated_documentation,"Wed, 22 Jul 2009 20:15:26 +0000","Sun, 7 Feb 2010 09:58:27 +0000","Sun, 13 Sep 2009 18:55:42 +0000",4574416,The maven reports is just getting to old and intermixed with 1.x and trunk releases. We should just remove them as they dont bring much value anyway. The xsddoc has been removed from the build files. So its now a matter of removing from the apache http site (people with credentials is needed),0.04,0.04,negative
camel,1846,description,"This affects at least version 1.5.0 of the docs. If I look through the xsddocs on the website (starting from the Camel Xml Reference page) for the throwFault element, it's not there even though it _is_ present in the actual xsd. I guess something went wrong with the generation of those pages and it makes it rather difficult to figure out what's valid and what isn't.",documentation_debt,low_quality_documentation,"Wed, 22 Jul 2009 20:15:26 +0000","Sun, 7 Feb 2010 09:58:27 +0000","Sun, 13 Sep 2009 18:55:42 +0000",4574416,"This affects at least version 1.5.0 of the docs. If I look through the xsddocs on the website (starting from the Camel Xml Reference page) for the throwFault element, it's not there even though it is present in the actual xsd. I guess something went wrong with the generation of those pages and it makes it rather difficult to figure out what's valid and what isn't.",-0.075,-0.075,negative
camel,201,comment_1,Johathan. The patch looks great. Only two issues with the ident of the code. One @override was not idented properly. And one method parameter was on a new line instead of singleline. Just nitpicking. Would love to have it documented on the wiki that we got this new transform DSL now. And we should remember to add it to the release notes that setOutBody() is depreacted and replaced with transform() And since setOutBody() is to be replaced with transform. Could we have a unit test that verifies a setOutBody() test that is done by transform render the same OUT body?,documentation_debt,outdated_documentation,"Fri, 2 Nov 2007 17:37:12 +0000","Fri, 11 Jul 2008 04:25:15 +0000","Tue, 6 May 2008 16:22:20 +0000",16065908,Johathan. The patch looks great. Only two issues with the ident of the code. One @override was not idented properly. And one method parameter was on a new line instead of singleline. Just nitpicking. Would love to have it documented on the wiki that we got this new transform DSL now. And we should remember to add it to the release notes that setOutBody() is depreacted and replaced with transform() And since setOutBody() is to be replaced with transform. Could we have a unit test that verifies a setOutBody() test that is done by transform render the same OUT body?,0.1077777778,0.1077777778,positive
camel,201,comment_2,"Hey Claus, Nitpick away :) I fixed up the indentations and added a test that verifies the deprecated setOutBody method still behaves. Yeah, the wiki will need to be updated once this feature gets in.",documentation_debt,outdated_documentation,"Fri, 2 Nov 2007 17:37:12 +0000","Fri, 11 Jul 2008 04:25:15 +0000","Tue, 6 May 2008 16:22:20 +0000",16065908,"Hey Claus, Nitpick away I fixed up the indentations and added a test that verifies the deprecated setOutBody method still behaves. Yeah, the wiki will need to be updated once this feature gets in.",0.27025,0.2405,positive
camel,201,comment_3,"Patch applied. Excellent contribution, thanks Jon. I will leave this issue open until we update the documentation.",documentation_debt,outdated_documentation,"Fri, 2 Nov 2007 17:37:12 +0000","Fri, 11 Jul 2008 04:25:15 +0000","Tue, 6 May 2008 16:22:20 +0000",16065908,"Patch applied. Excellent contribution, thanks Jon. I will leave this issue open until we update the documentation.",0.125,0.125,positive
camel,201,comment_4,"Updated the docs for this at: Unfortunately, it doesn't look as nice as it should because confluence is barfing all over the place with the following message: ""An error occurred: Connection refused. The system administrator has been notified.""",documentation_debt,low_quality_documentation,"Fri, 2 Nov 2007 17:37:12 +0000","Fri, 11 Jul 2008 04:25:15 +0000","Tue, 6 May 2008 16:22:20 +0000",16065908,"Updated the docs for this at: http://cwiki.apache.org/confluence/display/CAMEL/Message+Translator Unfortunately, it doesn't look as nice as it should because confluence is barfing all over the place with the following message: ""An error occurred: Connection refused. The system administrator has been notified.""",-0.090625,-0.090625,negative
camel,2163,comment_4,Stan lets add an example to camel-jdbc and camel-sql wiki pages how to use the split(body()) to process each row one by one.,documentation_debt,outdated_documentation,"Thu, 12 Nov 2009 08:32:16 +0000","Thu, 3 Jun 2010 07:25:25 +0000","Thu, 19 Nov 2009 11:57:46 +0000",617130,Stan lets add an example to camel-jdbc and camel-sql wiki pages how to use the split(body()) to process each row one by one.,0,0,neutral
camel,231,description,"I tried to figure out how i could edit the wiki page myself - i created an account but do not have edit rights, or I could not find the edit button. Type converts has a broken link: In the chapter ""Discovering Type Converts"" the link for @Converter is missing the h in http so the link is broken.",documentation_debt,low_quality_documentation,"Sun, 18 Nov 2007 14:58:28 +0000","Mon, 12 May 2008 07:48:06 +0000","Mon, 19 Nov 2007 11:42:46 +0000",74658,"I tried to figure out how i could edit the wiki page myself - i created an account but do not have edit rights, or I could not find the edit button. Type converts has a broken link: http://cwiki.apache.org/confluence/display/CAMEL/Type+Converter In the chapter ""Discovering Type Converts"" the link for @Converter is missing the h in http so the link is broken. ""ttp://activemq.apache.org/camel/maven/camel-core/apidocs/org/apache/camel/Converter""",-0.1333333333,-0.08888888889,negative
camel,231,summary,Broken link on wiki page for Type Converter,documentation_debt,low_quality_documentation,"Sun, 18 Nov 2007 14:58:28 +0000","Mon, 12 May 2008 07:48:06 +0000","Mon, 19 Nov 2007 11:42:46 +0000",74658,Broken link on wiki page for Type Converter,-0.2,-0.2,negative
camel,3157,comment_1,"Good work! If I may add something, then maybe a comment that the getOut/getIn methods does not map to the InOut/InOnly patterns.",documentation_debt,low_quality_documentation,"Sat, 25 Sep 2010 10:38:18 +0000","Sun, 24 Apr 2011 09:58:17 +0000","Mon, 27 Sep 2010 14:16:36 +0000",185898,"Good work! If I may add something, then maybe a comment that the getOut/getIn methods does not map to the InOut/InOnly patterns.",0.344,0.344,neutral
camel,3157,comment_2,Thanks Tarjei I have added more notes to Exchange java doc.,documentation_debt,outdated_documentation,"Sat, 25 Sep 2010 10:38:18 +0000","Sun, 24 Apr 2011 09:58:17 +0000","Mon, 27 Sep 2010 14:16:36 +0000",185898,Thanks Tarjei I have added more notes to Exchange java doc.,0.4,0.4,positive
camel,3157,description,The Exchange java doc should be updated to help end users more about the API of the Exchange,documentation_debt,low_quality_documentation,"Sat, 25 Sep 2010 10:38:18 +0000","Sun, 24 Apr 2011 09:58:17 +0000","Mon, 27 Sep 2010 14:16:36 +0000",185898,The Exchange java doc should be updated to help end users more about the API of the Exchange,0.4,0.4,neutral
camel,3157,summary,"Update Exchange javadoc to better explain properties, in, out, mep etc.",documentation_debt,low_quality_documentation,"Sat, 25 Sep 2010 10:38:18 +0000","Sun, 24 Apr 2011 09:58:17 +0000","Mon, 27 Sep 2010 14:16:36 +0000",185898,"Update Exchange javadoc to better explain properties, in, out, mep etc.",0.5,0.5,neutral
camel,3168,description,We need a REST based example to show how to do that. It should accept XML/JSON as input.,documentation_debt,low_quality_documentation,"Tue, 28 Sep 2010 04:25:29 +0000","Wed, 25 Mar 2015 08:27:23 +0000","Wed, 25 Mar 2015 08:27:23 +0000",141624114,We need a REST based example to show how to do that. It should accept XML/JSON as input.,0.1,0.1,neutral
camel,3440,description,"After the jira migration from activemq to the main ASF jira, the project id changed from 11020 to 12311211. As a result all links to Release Notes are broken and need to be updated.",documentation_debt,outdated_documentation,"Sat, 18 Dec 2010 03:51:01 +0000","Fri, 28 Jan 2011 00:46:44 +0000","Fri, 28 Jan 2011 00:46:36 +0000",3531335,"After the jira migration from activemq to the main ASF jira, the project id changed from 11020 to 12311211. As a result all links to Release Notes are broken and need to be updated.",-0.1,-0.1,negative
camel,3469,comment_0,I will document this procedure by working on,documentation_debt,low_quality_documentation,"Tue, 28 Dec 2010 18:19:17 +0000","Sun, 24 Apr 2011 09:58:05 +0000","Tue, 28 Dec 2010 23:11:56 +0000",17559,I will document this procedure by working on CAMEL-3471,0.6,0.6,neutral
camel,3469,description,"Upgrading to newer versions of the dependencies is indeed normally a simple procedure. If we add a wiki page with the procedure (which pom should be updated and checked, do we need an OSGI bundle from the ServiceMix guys, run the full test suite, ...), it could be one of the ""low hanging fruits"" for new to work on Camel.",documentation_debt,low_quality_documentation,"Tue, 28 Dec 2010 18:19:17 +0000","Sun, 24 Apr 2011 09:58:05 +0000","Tue, 28 Dec 2010 23:11:56 +0000",17559,"Upgrading to newer versions of the dependencies is indeed normally a simple procedure. If we add a wiki page with the procedure (which pom should be updated and checked, do we need an OSGI bundle from the ServiceMix guys, run the full test suite, ...), it could be one of the ""low hanging fruits"" for new contributors/committers to work on Camel.",0.3,0.3,neutral
camel,3637,description,"The Camel Karaf feature camel-eventAdmin is not correct. The features is described as follow: <feature version=""2.6.0"" <feature version=""2.6.0"" <bundle</feature but the camel-eventAdmin artifact correct name is camel-eventadmin: This typo mistake provides: Downloading: [INFO] Unable to find resource in repository central [INFO] [ERROR] BUILD FAILURE [INFO] [INFO] Can't resolve bundle [INFO] I'm gonna submit a patch to fix that.",documentation_debt,low_quality_documentation,"Mon, 7 Feb 2011 07:47:05 +0000","Tue, 25 Oct 2011 11:35:31 +0000","Mon, 7 Feb 2011 09:41:20 +0000",6855,"The Camel Karaf feature camel-eventAdmin is not correct. The features is described as follow: <feature name=""camel-eventAdmin"" version=""2.6.0""> <feature version=""2.6.0"">camel-core</feature> <bundle>mvn:org.apache.camel/camel-eventAdmin/2.6.0</bundle> </feature> but the camel-eventAdmin artifact correct name is camel-eventadmin: http://repo2.maven.org/maven2/org/apache/camel/camel-eventadmin/2.6.0/camel-eventadmin-2.6.0.jar This typo mistake provides: Downloading: http://repo1.maven.org/maven2/org/apache/camel/camel-eventAdmin/2.6.0/camel-eventAdmin-2.6.0.jar [INFO] Unable to find resource 'org.apache.camel:camel-eventAdmin:jar:2.6.0' in repository central (http://repo1.maven.org/maven2) [INFO] ------------------------------------------------------------------------ [ERROR] BUILD FAILURE [INFO] ------------------------------------------------------------------------ [INFO] Can't resolve bundle org.apache.camel:camel-eventAdmin:jar:2.6.0 [INFO] ------------------------------------------------------------------------ I'm gonna submit a patch to fix that.",-0.53125,-0.15625,negative
camel,383,comment_3,TODO: Remember to update wiki component documentation with the new option if patch is accepted.,documentation_debt,outdated_documentation,"Wed, 12 Mar 2008 16:18:36 +0000","Mon, 12 May 2008 12:45:33 +0000","Mon, 24 Mar 2008 11:47:55 +0000",1020559,TODO: Remember to update wiki component documentation with the new option if patch is accepted.,0.2,0.2,neutral
camel,383,comment_5,"Hi Willem I am on vacation at my parents at the time being and there I have not access to the net. So its a bit challenge to work on Camel using maven in offline mode. I decided in last minute to remove the transfer of exchangeid, but I forgot to remove it in the unit tests that still could pass if the generated ids are the same. Today I have access to the net for a few hours so I am trying to catch up on the mail and the camel commits to merge with a few changes in the mina component I have improved during the last 3 days. I will update the wiki documents on sunday when I am back home. I am also working on providing a camel-mina-example sample for the distribution.",documentation_debt,outdated_documentation,"Wed, 12 Mar 2008 16:18:36 +0000","Mon, 12 May 2008 12:45:33 +0000","Mon, 24 Mar 2008 11:47:55 +0000",1020559,"Hi Willem I am on vacation at my parents at the time being and there I have not access to the net. So its a bit challenge to work on Camel using maven in offline mode. I decided in last minute to remove the transfer of exchangeid, but I forgot to remove it in the unit tests that still could pass if the generated ids are the same. Today I have access to the net for a few hours so I am trying to catch up on the mail and the camel commits to merge with a few changes in the mina component I have improved during the last 3 days. I will update the wiki documents on sunday when I am back home. I am also working on providing a camel-mina-example sample for the distribution.",0.1822777778,0.1822777778,neutral
camel,3864,description,The file ehcache.xml which is the default configuration file for ehcache initialized by camel-cache component contains deprecated description about jms replication.,documentation_debt,outdated_documentation,"Thu, 14 Apr 2011 17:11:50 +0000","Sat, 16 Apr 2011 12:19:08 +0000","Sat, 16 Apr 2011 12:19:08 +0000",155238,The file ehcache.xml which is the default configuration file for ehcache initialized by camel-cache component contains deprecated description about jms replication.,-0.25,-0.25,neutral
camel,3888,comment_0,"This is definitely not resolved. There are a lot of tests that are using which is marked deprecated. So far, I haven't found any documentation about an alternative to use. THere are a bunch (22) of tests that use it and I haven't been able to figure out any type of alternative for it. Thus, IMO, if there isn't an alternative and is important enough to be required by a bunch of tests, I have to wonder why it's deprecated. But no details on that either. Similar situation for Deprecated, no alternative. For TryDefinition, I could see a ""rethrow()"" or similar method added to define that behavior (which would match the Java users expectation), but that certainly isn't there right now.",documentation_debt,low_quality_documentation,"Thu, 21 Apr 2011 12:12:23 +0000","Mon, 27 Jun 2011 08:05:27 +0000","Mon, 27 Jun 2011 08:05:27 +0000",5773984,"This is definitely not resolved. There are a lot of tests that are using DefaultErrorHandlerBuilder.handled which is marked deprecated. So far, I haven't found any documentation about an alternative to use. THere are a bunch (22) of tests that use it and I haven't been able to figure out any type of alternative for it. Thus, IMO, if there isn't an alternative and is important enough to be required by a bunch of tests, I have to wonder why it's deprecated. But no details on that either. Similar situation for TryDefinition.handled. Deprecated, no alternative. For TryDefinition, I could see a ""rethrow()"" or similar method added to define that behavior (which would match the Java users expectation), but that certainly isn't there right now.",-0.2046666667,-0.1488484848,negative
camel,3888,comment_2,"@Claus, if you read more carefully, the reason for reopening was that we need to document what to use instead of the deprecated feature. Has this been done? How would a user know what to do? Why did you mark the issue as resolved?",documentation_debt,outdated_documentation,"Thu, 21 Apr 2011 12:12:23 +0000","Mon, 27 Jun 2011 08:05:27 +0000","Mon, 27 Jun 2011 08:05:27 +0000",5773984,"@Claus, if you read more carefully, the reason for reopening was that we need to document what to use instead of the deprecated feature. Has this been done? How would a user know what to do? Why did you mark the issue as resolved?",0.2,0.2,neutral
camel,3888,comment_5,"Dan/Hadrian you are welcome to look at the added documentation to @deprecated javadoc. The easiest is probably to see this commit rev: I spotted a minor mistake and corrected it in this: I checked all the i could find in (eg in main, and *not* test). You are of course welcome to improve the documentation if you find anything needed to be added.",documentation_debt,low_quality_documentation,"Thu, 21 Apr 2011 12:12:23 +0000","Mon, 27 Jun 2011 08:05:27 +0000","Mon, 27 Jun 2011 08:05:27 +0000",5773984,"Dan/Hadrian you are welcome to look at the added documentation to @deprecated javadoc. The easiest is probably to see this commit rev: http://svn.apache.org/viewvc?rev=1095673&view=rev I spotted a minor mistake and corrected it in this: http://svn.apache.org/viewvc?rev=1095735&view=rev I checked all the @Deprecated/@deprecated i could find in camel-core/src/main/java (eg in main, and not test). You are of course welcome to improve the documentation if you find anything needed to be added.",0.4083333333,0.4083333333,neutral
camel,3888,comment_6,I believe most of the users are looking at the wiki/manual we need to document there. I'll take care of it.,documentation_debt,outdated_documentation,"Thu, 21 Apr 2011 12:12:23 +0000","Mon, 27 Jun 2011 08:05:27 +0000","Mon, 27 Jun 2011 08:05:27 +0000",5773984,I believe most of the users are looking at the wiki/manual we need to document there. I'll take care of it.,0.2,0.2,neutral
camel,3888,comment_7,Information about @deprecation in the API is standard documented in the java doc. And this is where users would expect the information and go look. We have never had any special wiki page or the likes where @deprecated API is being further detailed. IMHO this ticket can be resolved.,documentation_debt,outdated_documentation,"Thu, 21 Apr 2011 12:12:23 +0000","Mon, 27 Jun 2011 08:05:27 +0000","Mon, 27 Jun 2011 08:05:27 +0000",5773984,Information about @deprecation in the API is standard documented in the java doc. And this is where users would expect the information and go look. We have never had any special wiki page or the likes where @deprecated API is being further detailed. IMHO this ticket can be resolved.,0.06666666667,0.06666666667,neutral
camel,3888,description,We should ensure all deprecated classes/methods in camel-core is documented what alternatives to use. Also if possible give a hint when it could be removed.,documentation_debt,low_quality_documentation,"Thu, 21 Apr 2011 12:12:23 +0000","Mon, 27 Jun 2011 08:05:27 +0000","Mon, 27 Jun 2011 08:05:27 +0000",5773984,We should ensure all deprecated classes/methods in camel-core is documented what alternatives to use. Also if possible give a hint when it could be removed.,0.1,0.1,neutral
camel,4202,comment_3,"The reason why the persistent reply to example at the nabble discussion, is because Camel will have to use a Dummy value for JMSMessageSelector when there is no expected replies to receive. And it thus takes 1 sec. for it to timeout, before it can update the JMSMessageSelector with the CorrelationsIDs to receive. We may try to suspend/resume the message listener container, but we could potential end up with some synchronized issue where we would suspend the listener, where as in the mean time on another thread a new message is being send. And thus we may miss resume the listener. Therefore we end up not pulling the message from the AMQ broker. We should add some documentation / FAQ as the drawbacks of using persistent queues.",documentation_debt,low_quality_documentation,"Sat, 9 Jul 2011 14:05:41 +0000","Mon, 19 Sep 2011 20:32:51 +0000","Sun, 14 Aug 2011 10:07:44 +0000",3096123,"The reason why the persistent reply to example at the nabble discussion, is because Camel will have to use a Dummy value for JMSMessageSelector when there is no expected replies to receive. And it thus takes 1 sec. for it to timeout, before it can update the JMSMessageSelector with the CorrelationsIDs to receive. We may try to suspend/resume the message listener container, but we could potential end up with some synchronized issue where we would suspend the listener, where as in the mean time on another thread a new message is being send. And thus we may miss resume the listener. Therefore we end up not pulling the message from the AMQ broker. We should add some documentation / FAQ as the drawbacks of using persistent queues.",-0.08571428571,-0.08571428571,neutral
camel,4202,comment_4,"A new option is to be introduced: replyToType and its an enum with the following options: Temporary, Shared, Exclusive So if you set the then Camel assumes the reply queue is exclusive for Camel and will not use any JMS message selectors, as it pickup all the messages arriving on that queue. This means its as fast as temporary queues. In fact this will also help highlight the fact the current fixed replyTo queues are consider Shared by default. I will add better documentation at the jms page to help highlight this.",documentation_debt,low_quality_documentation,"Sat, 9 Jul 2011 14:05:41 +0000","Mon, 19 Sep 2011 20:32:51 +0000","Sun, 14 Aug 2011 10:07:44 +0000",3096123,"A new option is to be introduced: replyToType and its an enum with the following options: Temporary, Shared, Exclusive So if you set the replyToType=Exclusive then Camel assumes the reply queue is exclusive for Camel and will not use any JMS message selectors, as it pickup all the messages arriving on that queue. This means its as fast as temporary queues. In fact this will also help highlight the fact the current fixed replyTo queues are consider Shared by default. I will add better documentation at the jms page to help highlight this.",0.2229166667,0.2270833333,neutral
camel,478,comment_0,"The patch is in the trunk now, so we need to add some document to show how to use the CamelSourceAdpater and CamelTargetAdpater.",documentation_debt,low_quality_documentation,"Wed, 23 Apr 2008 09:24:18 +0000","Wed, 18 Jun 2008 05:04:27 +0000","Fri, 16 May 2008 13:11:54 +0000",2000856,"The patch is in the trunk now, so we need to add some document to show how to use the CamelSourceAdpater and CamelTargetAdpater.",0,0,neutral
camel,4793,comment_2,Will make sure the documentation is updated in the next few hours,documentation_debt,outdated_documentation,"Sun, 18 Dec 2011 13:09:51 +0000","Tue, 27 Dec 2011 22:43:26 +0000","Mon, 19 Dec 2011 14:26:25 +0000",90994,Will make sure the documentation is updated in the next few hours,0,0,neutral
camel,4793,comment_3,"There is still no documentation for sdb component, I will add updated docs today",documentation_debt,outdated_documentation,"Sun, 18 Dec 2011 13:09:51 +0000","Tue, 27 Dec 2011 22:43:26 +0000","Mon, 19 Dec 2011 14:26:25 +0000",90994,"There is still no documentation for sdb component, I will add updated docs today",0.281,0.281,negative
camel,4905,summary,Trivial typos in tests,documentation_debt,low_quality_documentation,"Tue, 17 Jan 2012 00:36:22 +0000","Tue, 17 Jan 2012 06:30:10 +0000","Tue, 17 Jan 2012 06:30:10 +0000",21228,Trivial typos in tests,0,0,negative
camel,493,description,The camel mail component registers nntp as a supported protocol but there is no javacode or documentation how to use it. It is currently not support out-of-the-box in Camel. This feature has been requested by an end-user by private mail correspondence.,documentation_debt,outdated_documentation,"Sun, 4 May 2008 07:31:50 +0000","Sun, 7 Feb 2010 09:54:14 +0000","Thu, 12 Nov 2009 19:56:47 +0000",48169497,The camel mail component registers nntp as a supported protocol but there is no javacode or documentation how to use it. It is currently not support out-of-the-box in Camel. This feature has been requested by an end-user by private mail correspondence.,0,0,negative
camel,52,description,"sometimes we get some strange stuff inserted into the generated docbook. e.g. the first part of contains some bogus stuff... Everything between <section> and <para> looks bogus (or at least generates strangeness in the PDF). I wonder if might help if we keep the HTML that is downloaded from the site so we can see, is this a wiki issue or XSL issue etc. As right now I've no idea! :)",documentation_debt,low_quality_documentation,"Mon, 25 Jun 2007 10:46:04 +0000","Mon, 12 May 2008 08:01:38 +0000","Tue, 26 Jun 2007 04:11:56 +0000",62752,"sometimes we get some strange stuff inserted into the generated docbook. e.g. the first part of book-architecture.xml contains some bogus stuff... Everything between <section> and <para> looks bogus (or at least generates strangeness in the PDF). I wonder if might help if we keep the HTML that is downloaded from the site so we can see, is this a wiki issue or XSL issue etc. As right now I've no idea!",0.0687,-0.0113,negative
camel,612,comment_3,"Hadrian good comments. How can we archive this, the DeadLetterChannel is probably already ""chaining"" the routing? Would be good to have it documented somehow. I do think the wiki needs a section where each DSL types is documented one-by-one. That is a however a extensive work to do.",documentation_debt,outdated_documentation,"Mon, 16 Jun 2008 06:38:46 +0000","Fri, 11 Jul 2008 04:21:45 +0000","Mon, 23 Jun 2008 12:41:58 +0000",626592,"Hadrian good comments. >In other words if there is no explicit otherwise() to go the errorHandler (no redelivery of course). How can we archive this, the DeadLetterChannel is probably already ""chaining"" the routing? Would be good to have it documented somehow. I do think the wiki needs a section where each DSL types is documented one-by-one. That is a however a extensive work to do.",0.4304,0.3586666667,neutral
camel,612,comment_4,"OK, I wasn't aware of the {{throwFault()}} method that allows a user to easily make the message exchange fail instead of having to use the {{Processor}} trick. Reducing the priority of the issue, because I guess I only need to document the default, messsage filtering behavior of the choice() on the site to make people more aware of it.",documentation_debt,outdated_documentation,"Mon, 16 Jun 2008 06:38:46 +0000","Fri, 11 Jul 2008 04:21:45 +0000","Mon, 23 Jun 2008 12:41:58 +0000",626592,"OK, I wasn't aware of the throwFault() method that allows a user to easily make the message exchange fail instead of having to use the Processor trick. Reducing the priority of the issue, because I guess I only need to document the default, messsage filtering behavior of the choice() on the site to make people more aware of it.",-0.089,-0.089,neutral
camel,6178,comment_5,I added this to the Camel 2.11 release notes and updated the WIKI pages with the correct version which with this option is available.,documentation_debt,low_quality_documentation,"Mon, 18 Mar 2013 21:54:13 +0000","Wed, 27 Mar 2013 15:59:07 +0000","Wed, 27 Mar 2013 10:13:41 +0000",735568,I added this to the Camel 2.11 release notes and updated the WIKI pages with the correct version which with this option is available.,0.875,0.875,neutral
camel,6291,comment_0,"says: ""Reusable routes The routes defined in <routeContext/So, this is a bug in documented feature. If you think, it's an enhancement, the page should be corrected.",documentation_debt,low_quality_documentation,"Tue, 16 Apr 2013 14:45:35 +0000","Mon, 26 Aug 2013 09:50:28 +0000","Mon, 26 Aug 2013 09:50:28 +0000",11387093,"http://camel.apache.org/configuring-camel.html says: ""Reusable routes The routes defined in <routeContext/> can be reused by multiple <camelContext/>."" So, this is a bug in documented feature. If you think, it's an enhancement, the page should be corrected.",0.53125,0.3541666667,negative
camel,6320,comment_2,"Lukasz, thanks for the contribution. I did commit it to the trunk for now. Will create a placeholder for doc in the wiki, we'd appreciate some documentation. Since it's a new component and 2.12 will not be out anytime soon, I will port it back to the maintenance branch and then close this issue.",documentation_debt,outdated_documentation,"Fri, 26 Apr 2013 17:23:36 +0000","Fri, 5 Jul 2013 21:44:07 +0000","Fri, 5 Jul 2013 21:44:07 +0000",6063631,"Lukasz, thanks for the contribution. I did commit it to the trunk for now. Will create a placeholder for doc in the wiki, we'd appreciate some documentation. Since it's a new component and 2.12 will not be out anytime soon, I will port it back to the maintenance branch and then close this issue.",0.25,0.25,positive
camel,6320,comment_3,Can we get this new component documented?,documentation_debt,outdated_documentation,"Fri, 26 Apr 2013 17:23:36 +0000","Fri, 5 Jul 2013 21:44:07 +0000","Fri, 5 Jul 2013 21:44:07 +0000",6063631,Can we get this new component documented?,0,0,neutral
camel,6446,comment_0,Thanks for the patch. Feel free to help with adding some docs to:,documentation_debt,outdated_documentation,"Mon, 10 Jun 2013 16:32:56 +0000","Tue, 25 Jun 2013 09:10:24 +0000","Tue, 25 Jun 2013 09:10:24 +0000",1269448,Thanks for the patch. Feel free to help with adding some docs to: http://camel.apache.org/json,0.35,0.35,positive
camel,6578,comment_1,Don't forget to update the Camel 2.12.0 release page at,documentation_debt,outdated_documentation,"Fri, 26 Jul 2013 11:28:11 +0000","Wed, 31 Jul 2013 07:24:53 +0000","Mon, 29 Jul 2013 08:02:40 +0000",246869,Don't forget to update the Camel 2.12.0 release page at https://cwiki.apache.org/confluence/display/CAMEL/Camel+2.12.0+Release,0.2,0.2,neutral
camel,6620,comment_0,"I just checked the code AuthMethod is just used in camel-http component, not the camel-http4 component. So it is not strange that there is no mention of AuthMethod in the http4 document. I also checked the camel-http wiki page, there is entry of the options about the AuthMethod.",documentation_debt,outdated_documentation,"Thu, 8 Aug 2013 18:52:34 +0000","Fri, 9 Aug 2013 01:03:08 +0000","Fri, 9 Aug 2013 01:03:08 +0000",22234,"I just checked the code AuthMethod is just used in camel-http component, not the camel-http4 component. So it is not strange that there is no mention of AuthMethod in the http4 document. I also checked the camel-http wiki page, there is entry of the options about the AuthMethod.",0.06666666667,0.06666666667,neutral
camel,6620,description,"I got an exception saying that an authMethod value is required. So, I went to the docs and there is no mention of authMethod or the acceptable values. I had to search the code find the AuthMethod enum to know what value is acceptable and to find the authMethod parameter name.",documentation_debt,outdated_documentation,"Thu, 8 Aug 2013 18:52:34 +0000","Fri, 9 Aug 2013 01:03:08 +0000","Fri, 9 Aug 2013 01:03:08 +0000",22234,"I got an exception saying that an authMethod value is required. So, I went to the docs and there is no mention of authMethod or the acceptable values. I had to search the code find the AuthMethod enum to know what value is acceptable and to find the authMethod parameter name.",0,0,neutral
camel,6620,summary,No mention of authMethod in http4 documentation,documentation_debt,outdated_documentation,"Thu, 8 Aug 2013 18:52:34 +0000","Fri, 9 Aug 2013 01:03:08 +0000","Fri, 9 Aug 2013 01:03:08 +0000",22234,No mention of authMethod in http4 documentation,0,0,neutral
camel,6896,description,"I've been writing a URI Builder for NetBeans using the Property Sheet api. Everything works great with the API that was added in 2.12, but it would be really nice to be able to grab parameter documentation through this api as well. For instance, with the property sheet, I can set a Short Description of what each endpoint does, and the user would be able to go from there.",documentation_debt,outdated_documentation,"Thu, 24 Oct 2013 21:01:37 +0000","Sat, 29 Nov 2014 09:21:35 +0000","Sat, 29 Nov 2014 09:21:35 +0000",34604398,"I've been writing a URI Builder for NetBeans using the Property Sheet api. Everything works great with the ComponentConfiguration API that was added in 2.12, but it would be really nice to be able to grab parameter documentation through this api as well. For instance, with the property sheet, I can set a Short Description of what each endpoint does, and the user would be able to go from there.",0.4209333333,0.4209333333,positive
camel,6973,comment_3,"because 2.12.x was being cut for a 2.12.2 release because of that a few things were not done (integrating into 2.12.x, updating the documentation).",documentation_debt,outdated_documentation,"Mon, 18 Nov 2013 15:14:18 +0000","Mon, 25 Nov 2013 13:20:59 +0000","Mon, 25 Nov 2013 13:20:59 +0000",598001,"because 2.12.x was being cut for a 2.12.2 release because of that a few things were not done (integrating into 2.12.x, updating the documentation).",-0.1166666667,-0.1166666667,negative
camel,706,comment_3,"code committed, need to update wiki",documentation_debt,outdated_documentation,"Sat, 12 Jul 2008 05:45:52 +0000","Thu, 23 Oct 2008 04:37:12 +0000","Wed, 10 Sep 2008 19:52:34 +0000",5234802,"code committed, need to update wiki",0.5,0.5,neutral
camel,7342,comment_1,"Having looked at this issue afresh I wonder if my description of implementing the flag is a cause of confusion and reluctance to add this feature. On re-reading, I realise that this description of what is required is misleading - of course you always want the consumer to attempt to connect upon start of the route, however if the bind to the SMSC fails for whatever reason, I need it to enter into a reconnection attempt cycle until the SMSC is available rather than stopping the route (and any others in the same camel context). Anyway, this is a requirement for me, and so I have implemented the functionality change for myself as a branch from 2.15.1. It is configured using a new option If this is set to true, then a reconnect thread is started instead of trying once only to create the session. The major change here is the removal of the wait for the thread completion in the reconnect method (to allow doStart to complete), and movement of the reconnectLock mutex.",documentation_debt,low_quality_documentation,"Thu, 3 Apr 2014 09:15:34 +0000","Fri, 10 Apr 2015 15:35:20 +0000","Mon, 6 Apr 2015 13:27:27 +0000",31810313,"Having looked at this issue afresh I wonder if my description of implementing the ""lazySessionCreation"" flag is a cause of confusion and reluctance to add this feature. On re-reading, I realise that this description of what is required is misleading - of course you always want the consumer to attempt to connect upon start of the route, however if the bind to the SMSC fails for whatever reason, I need it to enter into a reconnection attempt cycle until the SMSC is available rather than stopping the route (and any others in the same camel context). Anyway, this is a requirement for me, and so I have implemented the functionality change for myself as a branch from 2.15.1. It is configured using a new option ""startConsumerIfDown"". If this is set to true, then a reconnect thread is started instead of trying once only to create the session. The major change here is the removal of the wait for the thread completion in the reconnect method (to allow doStart to complete), and movement of the reconnectLock mutex. https://github.com/RaySlater/camel/tree/CAMEL-7342",0.1564,0.1117142857,neutral
camel,7412,description,The docs at state This statement however seems to be wrong. I have a demo at that uses the camel-jdbc component in an XA transaction scenario without errors. Can someone please confirm the docs is wrong and I can correct it in the docs? This statement was introduced in,documentation_debt,low_quality_documentation,"Mon, 5 May 2014 14:17:35 +0000","Tue, 6 May 2014 15:12:34 +0000","Tue, 6 May 2014 15:12:34 +0000",89699,"The docs at http://camel.apache.org/jdbc.html state This component can not be used as a Transactional Client. If you need transaction support in your route, you should use the SQL component instead. This statement however seems to be wrong. I have a demo at https://github.com/tmielke/fuse-demos/tree/master/Camel/Camel-JMS-JDBC-XA-TX that uses the camel-jdbc component in an XA transaction scenario without errors. Can someone please confirm the docs is wrong and I can correct it in the docs? This statement was introduced in https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27822683",0.10625,0.1375,negative
camel,7412,summary,[docs] The docs state camel-jdbc does not support transactions.,documentation_debt,low_quality_documentation,"Mon, 5 May 2014 14:17:35 +0000","Tue, 6 May 2014 15:12:34 +0000","Tue, 6 May 2014 15:12:34 +0000",89699,[docs] The docs state camel-jdbc does not support transactions.,-0.4,-0.4,negative
camel,7413,comment_0,"I fixed this by making all of the parameters configurable, and updating the documentation. Here are the new docs. ## Usage ## The plugin configuration has the following properties. * - Salesforce url to use to generate the dtos. Defaults to but is useful for development environments. * - Maximum time to hold the connection open to Salesforce when generating the dtos. Defaults to 60 seconds, which is usually fine unless you have a big Salesforce schema or slow connection. * - Salesforce client Id for Remote API access * - Salesforce client secret for Remote API access * - Salesforce account user name * - Salesforce account password (including secret token) * - Salesforce Rest API version, defaults to 27.0 NOTE: Currently no version higher than 27.0 will work. * - Directory where to place generated DTOs, defaults to * - List of SObject types to include * - List of SObject types to exclude * - Java RegEx for SObject types to include * - Java RegEx for SObject types to exclude * - Java package name for generated DTOs, defaults to For obvious security reasons it is recommended that the clientId, clientSecret, userName and password fields be not set in the pom.xml. The plugin should be configured for the rest of the properties, and can be executed using the following command: mvn The generated DTOs use Jackson and XStream annotations. All Salesforce field types are supported. Date and time fields are mapped to Joda DateTime, and picklist fields are mapped to generated Java Enumerations.",documentation_debt,outdated_documentation,"Mon, 5 May 2014 15:12:16 +0000","Sun, 18 Mar 2018 20:30:17 +0000","Sun, 18 Mar 2018 20:30:01 +0000",122102265,"I fixed this by making all of the parameters configurable, and updating the documentation. Here are the new docs. Usage ## The plugin configuration has the following properties. camelSalesforce.loginUrl - Salesforce url to use to generate the dtos. Defaults to https://login.salesforce.com, but https://test.salesforce.com is useful for development environments. camelSalesforce.timeOutInMilliseconds - Maximum time to hold the connection open to Salesforce when generating the dtos. Defaults to 60 seconds, which is usually fine unless you have a big Salesforce schema or slow connection. camelSalesforce.clientId - Salesforce client Id for Remote API access camelSalesforce.clientSecret - Salesforce client secret for Remote API access camelSalesforce.userName - Salesforce account user name camelSalesforce.password - Salesforce account password (including secret token) camelSalesforce.version - Salesforce Rest API version, defaults to 27.0 NOTE: Currently no version higher than 27.0 will work. camelSalesforce.outputDirectory - Directory where to place generated DTOs, defaults to ${project.build.directory}/generated-sources/camel-salesforce camelSalesforce.includes - List of SObject types to include camelSalesforce.excludes - List of SObject types to exclude camelSalesforce.includePattern - Java RegEx for SObject types to include camelSalesforce.excludePattern - Java RegEx for SObject types to exclude camelSalesforce.packageName - Java package name for generated DTOs, defaults to org.apache.camel.salesforce.dto. For obvious security reasons it is recommended that the clientId, clientSecret, userName and password fields be not set in the pom.xml. The plugin should be configured for the rest of the properties, and can be executed using the following command: mvn camel-salesforce:generate -DcamelSalesforce.clientId=<clientid> -DcamelSalesforce.clientSecret=<clientsecret> -DcamelSalesforce.userName=<username> -DcamelSalesforce.password=<password> The generated DTOs use Jackson and XStream annotations. All Salesforce field types are supported. Date and time fields are mapped to Joda DateTime, and picklist fields are mapped to generated Java Enumerations.",0.05,0.002702702703,neutral
camel,7681,comment_2,I was going to ask you the best way to update the documentation but you were faster. Thanks !,documentation_debt,low_quality_documentation,"Mon, 11 Aug 2014 20:43:28 +0000","Wed, 13 Aug 2014 08:44:23 +0000","Wed, 13 Aug 2014 08:38:45 +0000",129317,I was going to ask you the best way to update the documentation but you were faster. Thanks !,0.6375,0.6375,positive
camel,7690,comment_0,A plain JAR example. As a WAB is not simple to create.,documentation_debt,low_quality_documentation,"Wed, 13 Aug 2014 07:51:39 +0000","Wed, 13 Aug 2014 09:57:47 +0000","Wed, 13 Aug 2014 09:57:47 +0000",7568,A plain JAR example. As a WAB is not simple to create.,-0.375,-0.375,neutral
camel,7690,description,We have a WAR example for Apache Tomcat etc. We should have a osgi example as well.,documentation_debt,outdated_documentation,"Wed, 13 Aug 2014 07:51:39 +0000","Wed, 13 Aug 2014 09:57:47 +0000","Wed, 13 Aug 2014 09:57:47 +0000",7568,We have a WAR example for Apache Tomcat etc. We should have a osgi example as well.,0.1155,0.1155,neutral
camel,7954,comment_0,"Component updated, need to update documentation",documentation_debt,outdated_documentation,"Fri, 24 Oct 2014 20:18:18 +0000","Tue, 6 Aug 2019 05:14:42 +0000","Tue, 6 Aug 2019 05:14:42 +0000",150886584,"Component updated, need to update documentation",0,0,neutral
camel,7956,comment_0,"Component updated, need to update documentation",documentation_debt,outdated_documentation,"Fri, 24 Oct 2014 20:19:09 +0000","Tue, 6 Aug 2019 05:14:50 +0000","Tue, 6 Aug 2019 05:14:50 +0000",150886541,"Component updated, need to update documentation",0,0,neutral
camel,8029,comment_0,I suggest to add some defails as <description Then you can tell the user how to install that bundle needed.,documentation_debt,low_quality_documentation,"Tue, 11 Nov 2014 00:24:02 +0000","Sun, 30 Nov 2014 20:58:10 +0000","Sun, 30 Nov 2014 15:24:44 +0000",1695642,"I suggest to add some defails as <description> or <note> or what that xml tag is in the features.xml file you can do. Then people can see it using features:info camel-xxx. We have this for a few other features, you can find as examples. Then you can tell the user how to install that bundle needed.",0,0.1,neutral
camel,808,description,Just a ticket for a reminder for myself to improve the wiki documentation a bit for these two components.,documentation_debt,low_quality_documentation,"Mon, 11 Aug 2008 08:38:51 +0000","Thu, 23 Oct 2008 04:37:12 +0000","Tue, 12 Aug 2008 12:55:43 +0000",101812,Just a ticket for a reminder for myself to improve the wiki documentation a bit for these two components.,0.4,0.4,neutral
camel,808,summary,http and jetty - improve documentation based on parent fix (dynamic options in URI) and some more samples,documentation_debt,low_quality_documentation,"Mon, 11 Aug 2008 08:38:51 +0000","Thu, 23 Oct 2008 04:37:12 +0000","Tue, 12 Aug 2008 12:55:43 +0000",101812,http and jetty - improve documentation based on parent fix (dynamic options in URI) and some more samples,0.4,0.4,neutral
camel,8101,comment_3,We need to update the documentation with the new command,documentation_debt,outdated_documentation,"Mon, 1 Dec 2014 20:11:20 +0000","Mon, 9 Feb 2015 05:33:20 +0000","Mon, 9 Feb 2015 05:33:20 +0000",5995320,We need to update the documentation with the new command http://camel.apache.org/mongodb,0,0,neutral
camel,8101,comment_4,Yes Totaly agree. I also want to add a aggregate example. I just sign the comiter agreement yesterday. Will update wiki soon. Best regards,documentation_debt,outdated_documentation,"Mon, 1 Dec 2014 20:11:20 +0000","Mon, 9 Feb 2015 05:33:20 +0000","Mon, 9 Feb 2015 05:33:20 +0000",5995320,Yes Totaly agree. I also want to add a aggregate example. I just sign the comiter agreement yesterday. Will update wiki soon. Best regards,0.295,0.295,positive
camel,8101,comment_5,Hi Pierre Did you have any chance to update the documentation?,documentation_debt,outdated_documentation,"Mon, 1 Dec 2014 20:11:20 +0000","Mon, 9 Feb 2015 05:33:20 +0000","Mon, 9 Feb 2015 05:33:20 +0000",5995320,Hi Pierre Did you have any chance to update the documentation?,0.4,0.4,neutral
camel,8101,description,Add runCommand to MongoDB Camel component operations list Javadoc of MongoDB driver is there I should update wiki right after the PR is merge.,documentation_debt,outdated_documentation,"Mon, 1 Dec 2014 20:11:20 +0000","Mon, 9 Feb 2015 05:33:20 +0000","Mon, 9 Feb 2015 05:33:20 +0000",5995320,Add runCommand to MongoDB Camel component operations list Javadoc of MongoDB driver is there https://api.mongodb.org/java/2.12/com/mongodb/DB.html#command(com.mongodb.DBObject) I should update wiki right after the PR is merge.,0.527,0.527,neutral
camel,8370,description,"I sent the following to the list earlier today: After digging around in the code, it looks like the HTTP status code is set via the message header line 308). However, there is no mention of this message header in either the camel-netty-http or camel-netty4-http documentation. It would be helpful to add this header to the list of applicable message headers, and also to include an example that demonstrates how to set the response status code and body: Finally, the existing ""Access to Netty types"" example should be modified to be clear that only the request can be accessed in this way.",documentation_debt,low_quality_documentation,"Tue, 17 Feb 2015 22:40:54 +0000","Wed, 18 Feb 2015 06:43:23 +0000","Wed, 18 Feb 2015 06:43:23 +0000",28949,"I sent the following to the users@camel.apache.org list earlier today: Im using the camel-netty4-http (latest 2.14.2-SNAPSHOT) component to create an endpoint that receives an HTTP POST from a device, translates the message from binary to JSON, then sends it along to a kafka topic for further processing. When there are errors in validating or translating the incoming message, I need to be able to return a HTTP response code and response body. The camel-netty4-http documentation has an Access to Netty types section, which says that I should be able to do the following to access the instance of io.netty.handler.codec.http.HttpResponse: HttpResponse response = exchange.getIn(NettyHttpMessage.class).getHttpResponse(); Regardless of where I access the exchange within the route, getHttpResponse() is always returning null. On the ""happy path I can return 200 by calling exchange.getOut().setBody(myResponse), but I have been unable to figure out how to return another response code. Is this a bug? Or is there another way to accomplish what Im trying to do? After digging around in the code, it looks like the HTTP status code is set via the CamelHttpResponseCode message header (org.apache.camel.component.netty4.http.DefaultNettyHttpBinding, line 308). However, there is no mention of this message header in either the camel-netty-http or camel-netty4-http documentation. It would be helpful to add this header to the list of applicable message headers, and also to include an example that demonstrates how to set the response status code and body: Finally, the existing ""Access to Netty types"" example should be modified to be clear that only the request can be accessed in this way.",0.1875,0.11086,neutral
camel,8370,summary,missing from documentation,documentation_debt,outdated_documentation,"Tue, 17 Feb 2015 22:40:54 +0000","Wed, 18 Feb 2015 06:43:23 +0000","Wed, 18 Feb 2015 06:43:23 +0000",28949,CamelHttpResponseCode missing from documentation,-0.4,-0.4,neutral
camel,856,summary,Typo in system property name to enable tracing in DefaultCamelContext,documentation_debt,low_quality_documentation,"Wed, 27 Aug 2008 03:55:11 +0000","Mon, 16 Feb 2009 05:51:51 +0000","Wed, 27 Aug 2008 04:01:14 +0000",363,Typo in system property name to enable tracing in DefaultCamelContext,0.5,0.5,neutral
camel,903,comment_3,TODO: Need to document it in wiki,documentation_debt,outdated_documentation,"Mon, 15 Sep 2008 09:26:57 +0000","Thu, 23 Oct 2008 04:37:13 +0000","Tue, 16 Sep 2008 06:18:43 +0000",75106,TODO: Need to document it in wiki,0,0,neutral
camel,910,description,"First of all I think we need to promote this great example some more. Maybe it should be easier to find on our wiki site. I will post findings in this ticket: #1 Link to EIP book loan broker sample doesnt work #2 I think the 2 parts in the introduction should be listed as bullets (one for JMS, one for webservice) #3 spelling (comman in the sentence below) credit agency , and banks) #4 Maybe the exchange pattern InOnly, InOut is not easily understood by new users and non JBI/ServiceMix end-users. Maybe use terms such as sync/async instead (and list the MEP in parathes) #5 Could ""test-jms"" component name be renamed to jms or activemq or something with non test? #6 multicast().to() ah clever if its really using a multicast ;) I didn't know that we have the .to on multicase. Are you sure its working as expected? and it should not be multicase(""bank1"", ""bank2"", ...) without the to? #7 Use the getHeader with the expected type as 2nd param, to avoid plain java type cast String ssn = #8 The aggregator. I am wondering if we have or should have a counter build in Camel so you can use a build in header instead of ""remebering"" to code this yourself old + 1); will continue...",documentation_debt,low_quality_documentation,"Tue, 16 Sep 2008 17:47:21 +0000","Thu, 23 Oct 2008 04:39:20 +0000","Mon, 22 Sep 2008 11:55:24 +0000",497283,"First of all I think we need to promote this great example some more. Maybe it should be easier to find on our wiki site. I will post findings in this ticket: #1 Link to EIP book loan broker sample doesnt work #2 I think the 2 parts in the introduction should be listed as bullets (one for JMS, one for webservice) #3 spelling (comman in the sentence below) credit agency , and banks) #4 Maybe the exchange pattern InOnly, InOut is not easily understood by new users and non JBI/ServiceMix end-users. Maybe use terms such as sync/async instead (and list the MEP in parathes) #5 Could ""test-jms"" component name be renamed to jms or activemq or something with non test? #6 multicast().to() ah clever if its really using a multicast I didn't know that we have the .to on multicase. Are you sure its working as expected? and it should not be multicase(""bank1"", ""bank2"", ...) without the to? #7 Use the getHeader with the expected type as 2nd param, to avoid plain java type cast String ssn = (String)exchange.getIn().getHeader(Constants.PROPERTY_SSN); #8 The aggregator. I am wondering if we have or should have a counter build in Camel so you can use a build in header instead of ""remebering"" to code this yourself result.setProperty(""aggregated"", old + 1); will continue...",0.1371363636,0.1094666667,neutral
camel,9403,comment_0,"We also only had some of the examples in the bom, not all of them.",documentation_debt,outdated_documentation,"Wed, 9 Dec 2015 16:05:59 +0000","Fri, 11 Dec 2015 07:59:44 +0000","Fri, 11 Dec 2015 07:59:29 +0000",143610,"We also only had some of the examples in the bom, not all of them.",0,0,neutral
camel,9412,description,fix some typo correct url,documentation_debt,low_quality_documentation,"Thu, 10 Dec 2015 18:17:34 +0000","Mon, 14 Dec 2015 16:55:22 +0000","Fri, 11 Dec 2015 08:00:13 +0000",49359,fix some typo correct url,0.875,0.875,negative
camel,9412,summary,Correct documentation,documentation_debt,low_quality_documentation,"Thu, 10 Dec 2015 18:17:34 +0000","Mon, 14 Dec 2015 16:55:22 +0000","Fri, 11 Dec 2015 08:00:13 +0000",49359,Correct camel-example-swagger-cdi documentation,0.875,0.875,neutral
camel,948,comment_0,1) Added cookbook and tutorials into the book page. 2) Fixed the missing images error 3) Added the Languages Appendix page,documentation_debt,low_quality_documentation,"Tue, 30 Sep 2008 06:16:24 +0000","Mon, 16 Feb 2009 05:51:54 +0000","Mon, 20 Oct 2008 13:47:22 +0000",1755058,1) Added cookbook and tutorials into the book page. 2) Fixed the missing images error 3) Added the Languages Appendix page,-0.2,-0.2,neutral
camel,948,description,"e.g. we should include the tutorials, maybe some of the cook book and am sure there's other useful documentation to include - like the recent changes to XPath / XQuery and other languages. Maybe some reformatting is needed too - missing images and so forth. The various sections of the book are here in the wiki here is the entire book...",documentation_debt,low_quality_documentation,"Tue, 30 Sep 2008 06:16:24 +0000","Mon, 16 Feb 2009 05:51:54 +0000","Mon, 20 Oct 2008 13:47:22 +0000",1755058,"e.g. we should include the tutorials, maybe some of the cook book and am sure there's other useful documentation to include - like the recent changes to XPath / XQuery and other languages. Maybe some reformatting is needed too - missing images and so forth. The various sections of the book are here in the wiki http://activemq.apache.org/camel/book.html here is the entire book... http://activemq.apache.org/camel/book-in-one-page.html",0.03333333333,0.03333333333,neutral
camel,9594,comment_0,We can now use /swagger.json or /swagger.yaml with camel-swagger-java so we can update the examples to use the url.,documentation_debt,low_quality_documentation,"Fri, 12 Feb 2016 09:52:20 +0000","Mon, 21 Mar 2016 13:48:34 +0000","Mon, 21 Mar 2016 13:48:34 +0000",3297374,We can now use /swagger.json or /swagger.yaml with camel-swagger-java so we can update the examples to use the url.,0,0,neutral
camel,9594,description,We should use swagger.json which is the convention name used by swagger api-docs was the old for 1.x spec.,documentation_debt,low_quality_documentation,"Fri, 12 Feb 2016 09:52:20 +0000","Mon, 21 Mar 2016 13:48:34 +0000","Mon, 21 Mar 2016 13:48:34 +0000",3297374,We should use swagger.json which is the convention name used by swagger http://swagger.io/specification/ api-docs was the old for 1.x spec.,0,0,neutral
camel,9643,comment_4,"Hi Claus, This issue seems to have been fixed by 2.16.3 (I will test it more thoroughly later). But I still cannot test it under 2.17.0 or 2.17.1 because I have to use Spring for DSL, transactions and jdbc templates but I have trouble to even load my bundle in Karaf with camel-spring (or spring-dm). camel-sql also cannot be loaded into Karaf (installing it would cause Karaf to hang). If the issue is indeed fully fixed in 2.16.3, then I can start to move my application off 2.15.x. I hope more tutorials are provided and/or updated for 2.17.x, especially in the area of deploying camel/cxf with spring in karaf. Thank you for your help!",documentation_debt,outdated_documentation,"Thu, 25 Feb 2016 14:51:24 +0000","Sun, 22 May 2016 06:36:30 +0000","Sun, 22 May 2016 06:36:30 +0000",7487106,"Hi Claus, This issue seems to have been fixed by 2.16.3 (I will test it more thoroughly later). But I still cannot test it under 2.17.0 or 2.17.1 because I have to use Spring for DSL, transactions and jdbc templates but I have trouble to even load my bundle in Karaf with camel-spring (or spring-dm). camel-sql also cannot be loaded into Karaf (installing it would cause Karaf to hang). If the issue is indeed fully fixed in 2.16.3, then I can start to move my application off 2.15.x. I hope more tutorials are provided and/or updated for 2.17.x, especially in the area of deploying camel/cxf with spring in karaf. Thank you for your help!",0.07278571429,0.07278571429,neutral
camel,9789,comment_5,"That's good to know, i was a bit miss lead by the documentation. I think a sample for ""good pratice"" with spring boot, java config and camel would help.... Much to do at the moment, but i will try to zip a basic sample project and upload it to this JIRA",documentation_debt,low_quality_documentation,"Fri, 1 Apr 2016 09:20:05 +0000","Thu, 7 Apr 2016 08:17:53 +0000","Thu, 7 Apr 2016 08:17:53 +0000",514668,"That's good to know, i was a bit miss lead by the documentation. I think a sample for ""good pratice"" with spring boot, java config and camel would help.... Much to do at the moment, but i will try to zip a basic sample project and upload it to this JIRA",0.388,0.388,positive
camel,990,comment_1,We must remember to update the wiki with this new feature (if patch is committed) Do you mind writing a snippet what such documentation should be? As accepting a patch is only really useable if we also updates and improves the related documentation as well. However I feel your idea is great. Maybe there should be a that just contains the few annotations needed and with your strategy end-users can use their own annotations and thus imports.,documentation_debt,outdated_documentation,"Thu, 16 Oct 2008 11:35:25 +0000","Mon, 16 Feb 2009 05:51:55 +0000","Thu, 23 Oct 2008 06:06:58 +0000",585093,We must remember to update the wiki with this new feature (if patch is committed) http://activemq.apache.org/camel/type-converter.html Do you mind writing a snippet what such documentation should be? As accepting a patch is only really useable if we also updates and improves the related documentation as well. However I feel your idea is great. Maybe there should be a camel-annotation.jar that just contains the few annotations needed and with your strategy end-users can use their own annotations and thus imports.,0.3775833333,0.3020666667,neutral
camel,11655,comment_1,"That code portion is needed because currently jsendnsca works only with Encryption NONE, XOR and Triple des, the others aren't working. I guess that we can accept a change like this one is just a minor change to align with the new library. So IMO it's not a problem for the end users.",requirement_debt,requirement_partially_implemented,"Wed, 9 Aug 2017 12:27:03 +0000","Tue, 22 Aug 2017 15:38:03 +0000","Tue, 22 Aug 2017 06:30:11 +0000",1101788,"That code portion is needed because currently jsendnsca works only with Encryption NONE, XOR and Triple des, the others aren't working. I guess that we can accept a change like this one is just a minor change to align with the new library. So IMO it's not a problem for the end users.",0.2,0.2,neutral
camel,11655,comment_3,"It's stated here in this thread: for Blowfish, yes, in case it will be supported we can add another else if there. It doesn't make sense to allow all the encryption if most of them don't work in nsca. Marking the as deprecated in my opinion it's not so important, it's just an option. If we start to deprecate all the options when we upgrade something we will end with a lot of dead code until Camel 3.0 (that actually doesn't have an ETA by the way). If this is something critical for you, you can revert commit and do what you think is best for this component. It was just an attempt to avoid the death of camel-nagios.",requirement_debt,requirement_partially_implemented,"Wed, 9 Aug 2017 12:27:03 +0000","Tue, 22 Aug 2017 15:38:03 +0000","Tue, 22 Aug 2017 06:30:11 +0000",1101788,"It's stated here in this thread: https://groups.google.com/forum/#!msg/jsend-nsca/QI4OUTqzv6w/jNhOC-8z4AwJ for Blowfish, yes, in case it will be supported we can add another else if there. It doesn't make sense to allow all the encryption if most of them don't work in nsca. Marking the NagiosEncryptionMethod as deprecated in my opinion it's not so important, it's just an option. If we start to deprecate all the options when we upgrade something we will end with a lot of dead code until Camel 3.0 (that actually doesn't have an ETA by the way). If this is something critical for you, you can revert commit and do what you think is best for this component. It was just an attempt to avoid the death of camel-nagios.",0.05763888889,0.05763888889,neutral
camel,1641,summary,camel-ftp is not thread safe,requirement_debt,non-functional_requirements_not_fully_satisfied,"Sat, 23 May 2009 08:14:55 +0000","Sat, 21 Nov 2009 11:58:01 +0000","Mon, 25 May 2009 13:26:34 +0000",191499,camel-ftp is not thread safe,-0.25,-0.25,negative
camel,3349,description,"The CxfRsEndpoint's getBinding method is not thread safe. At a customer site, I ran into an issue at startup if 2 threads raced to perform sync and async invocation, the code for getBinding (given below) would react in the following way. - Thread 1 would proceed to create a binding object - Thread 2 would mean while still find the binding to be null and proceed to create a new binding - Meanwhile thread one would have its binding and set the Atomic boolean for binding initialized and proceed to set the HeaderStrategy. - Thread 2 meanwhile would overwrite the original binding object and find that Atomic boolean already set and would have no way to associate a object since the flag is up. - In the absence of a copying of ProtocolHeaders etc will throw exceptions on every following request/invocation. public CxfRsBinding getBinding() { if (binding == null) { binding = new if { LOG.debug(""Create default CXF Binding "" + binding); } } if && binding instanceof { } return binding; }",requirement_debt,non-functional_requirements_not_fully_satisfied,"Fri, 19 Nov 2010 22:32:35 +0000","Sun, 24 Apr 2011 09:57:38 +0000","Fri, 19 Nov 2010 22:57:34 +0000",1499,"The CxfRsEndpoint's getBinding method is not thread safe. At a customer site, I ran into an issue at startup if 2 threads raced to perform sync and async invocation, the code for getBinding (given below) would react in the following way. Thread 1 would proceed to create a binding object Thread 2 would mean while still find the binding to be null and proceed to create a new binding Meanwhile thread one would have its binding and set the Atomic boolean for binding initialized and proceed to set the HeaderStrategy. Thread 2 meanwhile would overwrite the original binding object and find that Atomic boolean already set and would have no way to associate a HeaderFilterStrategy object since the flag is up. In the absence of a HeaderFilterStrategy, copying of ProtocolHeaders etc will throw exceptions on every following request/invocation. -------------------------------------------------- public CxfRsBinding getBinding() { if (binding == null) { binding = new DefaultCxfRsBinding(); if (LOG.isDebugEnabled()) { LOG.debug(""Create default CXF Binding "" + binding); } } if (!bindingInitialized.getAndSet(true) && binding instanceof HeaderFilterStrategyAware) { ((HeaderFilterStrategyAware)binding).setHeaderFilterStrategy(getHeaderFilterStrategy()); } return binding; } ------------------------------------------------",-0.17655,0.01659090909,neutral
camel,3709,comment_0,You found the easter egg. There is a todo in the source code to support endpoint via ref's // TODO: Support lookup endpoint by ref (requires a bit more work),requirement_debt,requirement_partially_implemented,"Wed, 23 Feb 2011 12:12:35 +0000","Tue, 25 Oct 2011 11:35:47 +0000","Thu, 24 Feb 2011 05:21:03 +0000",61708,You found the easter egg. There is a todo in the source code to support endpoint via ref's // TODO: Support lookup endpoint by ref (requires a bit more work),0.2333333333,0.2333333333,positive
camel,5950,summary,Cache producer is not thread safe,requirement_debt,non-functional_requirements_not_fully_satisfied,"Thu, 10 Jan 2013 15:13:47 +0000","Fri, 11 Jan 2013 07:17:56 +0000","Fri, 11 Jan 2013 07:13:31 +0000",57584,Cache producer is not thread safe,-0.25,-0.25,negative
camel,6403,comment_1,"Ideally the consumer should write its response in the async callback done method, then there UoW is not done yet. Though some of these http related components is a bit different and needed this support. Well spotted Gert.",requirement_debt,requirement_partially_implemented,"Tue, 28 May 2013 07:25:26 +0000","Mon, 12 Aug 2013 17:45:21 +0000","Fri, 9 Aug 2013 07:14:44 +0000",6306558,"Ideally the consumer should write its response in the async callback done method, then there UoW is not done yet. Though some of these http related components is a bit different and needed this support. Well spotted Gert.",0.3436666667,0.3436666667,neutral
camel,6973,comment_1,"Jan, actually, the test code that you referred to was added with this patch (see the source list) and that was not there before. so, you are right that it's already implemented but it was not implemented before. regards, aki",requirement_debt,requirement_partially_implemented,"Mon, 18 Nov 2013 15:14:18 +0000","Mon, 25 Nov 2013 13:20:59 +0000","Mon, 25 Nov 2013 13:20:59 +0000",598001,"Jan, actually, the test code that you referred to was added with this patch (see the source list) and that was not there before. so, you are right that it's already implemented but it was not implemented before. regards, aki",0.1756666667,0.1756666667,neutral
camel,11171,description,"component has an issue with the usage of {{RAW()}} function in child endpoint configuration. will mishandle the the content of {{RAW()}} , when at some point the escaped ampersand symbol is unescaped, and a wrong set of parameters is used. The attached PR fixed the issues and adds a unit test to verify the behavior before and after the fix.",test_debt,low_coverage,"Wed, 19 Apr 2017 15:55:11 +0000","Thu, 20 Apr 2017 08:02:09 +0000","Thu, 20 Apr 2017 07:49:47 +0000",57276,"camel-zookeeper-master component has an issue with the usage of RAW() function in child endpoint configuration. zookeeper-master://name:sftp://myhost/inbox?password=RAW(BEFORE_AMPERSAND&AFTER_AMPERSAND)&username=jdoe will mishandle the the content of RAW() , when at some point the escaped ampersand symbol is unescaped, and a wrong set of parameters is used. The attached PR fixed the issues and adds a unit test to verify the behavior before and after the fix.",0.04166666667,0.13125,negative
camel,11504,comment_3,That could be a good start and an easy way to fix broken links we have currently. I was hoping that we might have a build-time tool/integration with a tool to check for links. I think that would be useful for PR jobs (when those get fixed).,test_debt,lack_of_tests,"Mon, 3 Jul 2017 08:41:20 +0000","Mon, 22 Apr 2019 16:21:52 +0000","Mon, 22 Apr 2019 16:21:44 +0000",56878824,That could be a good start and an easy way to fix broken links we have currently. I was hoping that we might have a build-time tool/integration with a tool to check for links. I think that would be useful for PR jobs (when those get fixed).,0.3964444444,0.3964444444,positive
camel,1196,summary,MockEndpoint - sleep for empty test doesnt work,test_debt,lack_of_tests,"Thu, 18 Dec 2008 05:59:04 +0000","Fri, 31 Jul 2009 06:33:53 +0000","Thu, 18 Dec 2008 13:09:59 +0000",25855,MockEndpoint - sleep for empty test doesnt work,-0.4,-0.4,negative
camel,12104,comment_0,Would you be able to build an unit test of this sample code so we can take that and add to the tests of camel-cxf and work on a fix.,test_debt,lack_of_tests,"Wed, 27 Dec 2017 18:31:14 +0000","Mon, 26 Mar 2018 15:56:24 +0000","Tue, 20 Mar 2018 01:27:00 +0000",7109746,Would you be able to build an unit test of this sample code so we can take that and add to the tests of camel-cxf and work on a fix.,0.644,0.644,neutral
camel,13533,comment_0,Xml related tests are now ignored for composite batch api,test_debt,low_coverage,"Thu, 16 May 2019 07:16:23 +0000","Thu, 16 May 2019 09:19:05 +0000","Thu, 16 May 2019 09:19:05 +0000",7362,Xml related tests are now ignored for composite batch api,-0.4,-0.4,negative
camel,1641,comment_0,trunk: 778354. When/if CAMEL-1644 is implemented then this bug is already fixed. However remember to add the unit test in java. There is a TODO.,test_debt,lack_of_tests,"Sat, 23 May 2009 08:14:55 +0000","Sat, 21 Nov 2009 11:58:01 +0000","Mon, 25 May 2009 13:26:34 +0000",191499,trunk: 778354. When/if CAMEL-1644 is implemented then this bug is already fixed. However remember to add the unit test in FtpProducerConcurrentTest java. There is a TODO.,0,0,neutral
camel,1734,comment_0,"Hi Fernando, Thanks for the contribution. I just have quick review of your code, there is no unit test, can you add them ? Since Progress OpenEdge is a commercial product , I don't know if we could download the jar from public mvn repository and use it freely, can you clarify it ? If not , we need to find a way to accept your code. BTW, the codes have no Apache License declaration, you can take the other camel component module as an example. Willem",test_debt,lack_of_tests,"Sat, 20 Jun 2009 20:41:11 +0000","Sun, 7 Feb 2010 09:58:26 +0000","Thu, 2 Jul 2009 11:53:59 +0000",1005168,"Hi Fernando, Thanks for the contribution. I just have quick review of your code, there is no unit test, can you add them ? Since Progress OpenEdge is a commercial product , I don't know if we could download the jar from public mvn repository and use it freely, can you clarify it ? If not , we need to find a way to accept your code. BTW, the codes have no Apache License declaration, you can take the other camel component module as an example. Willem",0.1666666667,0.1666666667,neutral
camel,1734,comment_1,"Regarding the dependency on Progress OpenEdge, it is only available to their customers, which will need instructions for wrapping and deploying it in SMX. I will work on the unit tests and Apache License issues right away.",test_debt,lack_of_tests,"Sat, 20 Jun 2009 20:41:11 +0000","Sun, 7 Feb 2010 09:58:26 +0000","Thu, 2 Jul 2009 11:53:59 +0000",1005168,"Regarding the dependency on Progress OpenEdge, it is only available to their customers, which will need instructions for wrapping and deploying it in SMX. I will work on the unit tests and Apache License issues right away.",0.48175,0.48175,neutral
camel,1734,comment_3,"I don't have any problem with that, let me know how to proceed. Updated the submission with the Apache License (are we going to stick to it?), no unit tests yet.",test_debt,lack_of_tests,"Sat, 20 Jun 2009 20:41:11 +0000","Sun, 7 Feb 2010 09:58:26 +0000","Thu, 2 Jul 2009 11:53:59 +0000",1005168,"I don't have any problem with that, let me know how to proceed. Updated the submission with the Apache License (are we going to stick to it?), no unit tests yet.",0.1333333333,0.1333333333,neutral
camel,1842,description,"camel-cxf and camel-mail are the popular components in camel, so we need add some OSGi integration test for them.",test_debt,lack_of_tests,"Mon, 20 Jul 2009 12:45:31 +0000","Sun, 7 Feb 2010 09:56:20 +0000","Wed, 29 Jul 2009 09:52:45 +0000",767234,"camel-cxf and camel-mail are the popular components in camel, so we need add some OSGi integration test for them.",0.6,0.6,neutral
camel,1863,comment_1,trunk: 798902. TODO: HL7 segment that fails validation and add it to the validate unit test,test_debt,low_coverage,"Wed, 29 Jul 2009 12:36:06 +0000","Sun, 7 Feb 2010 09:56:20 +0000","Thu, 30 Jul 2009 06:59:36 +0000",66210,trunk: 798902. TODO: HL7 segment that fails validation and add it to the validate unit test,-0.05,-0.05,negative
camel,1881,comment_1,"@Stan, maybe you want to add a unit test? Many thanks for the patch!",test_debt,lack_of_tests,"Wed, 5 Aug 2009 17:53:32 +0000","Sun, 7 Feb 2010 09:56:22 +0000","Sat, 8 Aug 2009 13:53:15 +0000",244783,"@Stan, maybe you want to add a unit test? Many thanks for the patch!",0.3,0.3,positive
camel,201,comment_0,"I finally got around to doing this one up. There are no XQuery specific tests (mainly because of not knowing where to put them ;)), but it is generic enough to work for any expression language. Let me know if you have any questions!",test_debt,lack_of_tests,"Fri, 2 Nov 2007 17:37:12 +0000","Fri, 11 Jul 2008 04:25:15 +0000","Tue, 6 May 2008 16:22:20 +0000",16065908,"I finally got around to doing this one up. There are no XQuery specific tests (mainly because of not knowing where to put them ), but it is generic enough to work for any expression language. Let me know if you have any questions!",0.09741666667,0.06322222222,positive
camel,201,comment_6,"IIRC one of my issues for putting a spring transform test in camel-saxon was that the public camel xsd did not contain the transform element *yet* and so it failed to resolve. The camel-spring module has some magic in there to use a locally built xsd, so the transform element could be tested there fine. Since putting an XQuery test in camel-spring is a bad idea (circular dependency!), I opted for a test case using the ""simple"" expression language. Make sense?",test_debt,low_coverage,"Fri, 2 Nov 2007 17:37:12 +0000","Fri, 11 Jul 2008 04:25:15 +0000","Tue, 6 May 2008 16:22:20 +0000",16065908,"IIRC one of my issues for putting a spring transform test in camel-saxon was that the public camel xsd did not contain the transform element yet and so it failed to resolve. The camel-spring module has some magic in there to use a locally built xsd, so the transform element could be tested there fine. Since putting an XQuery test in camel-spring is a bad idea (circular dependency!), I opted for a test case using the ""simple"" expression language. Make sense?",-0.1024666667,-0.1024666667,negative
camel,2446,description,"I have attached patches of various changes that I have made to context files. The changes fall into following categories: * using p: for properties rather than full xml * using u:list for lists rather than full xml * using in order to reduce amount of xml required * updating files to use camel:camelContext schema rather than <camelContext Please let me know if I need to tweek any of the changes. I have smoke tested the examples, but is there a general unit test that will validate that the context file?",test_debt,lack_of_tests,"Thu, 4 Feb 2010 02:20:01 +0000","Sun, 24 Apr 2011 09:57:31 +0000","Thu, 26 Aug 2010 06:56:20 +0000",17555779,"I have attached patches of various changes that I have made to context files. The changes fall into following categories: using p: for properties rather than full xml using u:list for lists rather than full xml using default-autowire=""byName"" in order to reduce amount of xml required updating files to use camel:camelContext schema rather than <camelContext xmlns=""http://camel.apache.org/schema/spring""> Please let me know if I need to tweek any of the changes. I have smoke tested the examples, but is there a general unit test that will validate that the context file?",0.2438333333,0.1493888889,neutral
camel,2559,comment_3,"I think I got it covered now, will do more testing. But I have reworked camel-http and camel-jetty a little big to - use endpoint configured options over component configured - not mess with the component configured - using a single shared as that is whats meant - properly registering http/https with correct port number on SchemeRegistry on",test_debt,low_coverage,"Thu, 18 Mar 2010 15:04:22 +0000","Sun, 24 Apr 2011 10:01:31 +0000","Sun, 21 Mar 2010 12:29:22 +0000",249900,"I think I got it covered now, will do more testing. But I have reworked camel-http and camel-jetty a little big to use endpoint configured options over component configured not mess with the component configured using a single shared clientConnectionManager as that is whats meant properly registering http/https with correct port number on SchemeRegistry on clientConnectionManager",0.159375,0.159375,neutral
camel,3709,description,"When using together with from(Endpoint), the below Exception occurs during the routes building process. Looking at reveals, that the FromDefintion just created has no URI. That causes the comparison to all the interceptFroms' URIs to fail. As far as I can tell, the way to fix this would be to add in the constructor endpoint)}}. Below the stack trace, there is a unit test that demonstrates the issue. Until it if fixed, it can be easily circumvented by adding the commented-out line, and then change to",test_debt,low_coverage,"Wed, 23 Feb 2011 12:12:35 +0000","Tue, 25 Oct 2011 11:35:47 +0000","Thu, 24 Feb 2011 05:21:03 +0000",61708,"When using interceptFrom(String) together with from(Endpoint), the below Exception occurs during the routes building process. Looking at RoutesDefinition.java:217 reveals, that the FromDefintion just created has no URI. That causes the comparison to all the interceptFroms' URIs to fail. As far as I can tell, the way to fix this would be to add setUri(myEndpoint.getEndpointUri()) in the constructor FromDefinition(Endpoint endpoint). Below the stack trace, there is a unit test that demonstrates the issue. Until it if fixed, it can be easily circumvented by adding the commented-out line, and then change to from(""myEndpoint"").",-0.06666666667,-0.05,negative
camel,383,comment_4,"Hi Claus, I just committed your patch. Here is one issue for the since we do not mashal or unmashal the exchange's exchangeId, so I comment out the process assertion for the exchangeId to pass the test . BTW, the document for the transferExchange properties is highly demanded, and I like to use the transferExchange as the parameter name since it is more clear for the user :) Regards, Willem.",test_debt,lack_of_tests,"Wed, 12 Mar 2008 16:18:36 +0000","Mon, 12 May 2008 12:45:33 +0000","Mon, 24 Mar 2008 11:47:55 +0000",1020559,"Hi Claus, I just committed your patch. Here is one issue for the MinaTransferExchangeOptionTest, since we do not mashal or unmashal the exchange's exchangeId, so I comment out the process assertion for the exchangeId to pass the test . BTW, the document for the transferExchange properties is highly demanded, and I like to use the transferExchange as the parameter name since it is more clear for the user Regards, Willem.",0.2333333333,0.2,neutral
camel,4543,comment_1,"Patch that creates a new module called and moves all aries specific stuff into that. camel-blueprint no longer depends on any Aries classes. Tested camel-blueprint on JBoss 7.0.2 and works wonderfully. Haven't been able to test the new jar yet, as I haven't setup ServiceMix or the like..",test_debt,lack_of_tests,"Thu, 13 Oct 2011 16:19:13 +0000","Sun, 1 May 2016 10:43:46 +0000","Sun, 1 May 2016 10:43:46 +0000",143576673,"Patch that creates a new module called camel-aries-blueprint-support and moves all aries specific stuff into that. camel-blueprint no longer depends on any Aries classes. Tested camel-blueprint on JBoss 7.0.2 and works wonderfully. Haven't been able to test the new camel-aries-blueprint-support jar yet, as I haven't setup ServiceMix or the like..",-0.008416666667,0.1275833333,positive
camel,4602,description,"See nabble By invoking the filter for directories, it allows end users to skip entire directories, which would make the polling faster. And no need to walk down unwanted directories. The logic need to apply for both file/ftp consumers, as well having unit tests to ensure it works.",test_debt,lack_of_tests,"Tue, 1 Nov 2011 10:48:33 +0000","Sun, 4 Mar 2012 12:30:28 +0000","Sun, 4 Mar 2012 10:44:52 +0000",10713379,"See nabble http://camel.465427.n5.nabble.com/Copying-files-from-an-FTP-server-using-search-criteria-tp4882889p4882889.html By invoking the filter for directories, it allows end users to skip entire directories, which would make the polling faster. And no need to walk down unwanted directories. The logic need to apply for both file/ftp consumers, as well having unit tests to ensure it works.",0.2743333333,0.2743333333,neutral
camel,4736,description,"There's been a mail in the mailing list about camel-xstream not being blueprint friendly. Even though in the current trunk, it does work its a nice chance to add an integration test for it.",test_debt,lack_of_tests,"Fri, 2 Dec 2011 16:41:57 +0000","Fri, 2 Dec 2011 17:19:00 +0000","Fri, 2 Dec 2011 17:19:00 +0000",2223,"There's been a mail in the mailing list about camel-xstream not being blueprint friendly. Even though in the current trunk, it does work its a nice chance to add an integration test for it.",0.1321666667,0.1321666667,neutral
camel,4959,comment_0,That is correct. Andrey can you provide a patch with unit test that test this fix?,test_debt,low_coverage,"Tue, 31 Jan 2012 07:06:19 +0000","Tue, 28 Feb 2012 11:43:59 +0000","Thu, 23 Feb 2012 12:47:32 +0000",2007673,That is correct. Andrey can you provide a patch with unit test that test this fix?,0.4375,0.4375,neutral
camel,4959,comment_2,"Yeah lets return 0 for the primitive types. Patches is welcome, with unit tests.",test_debt,low_coverage,"Tue, 31 Jan 2012 07:06:19 +0000","Tue, 28 Feb 2012 11:43:59 +0000","Thu, 23 Feb 2012 12:47:32 +0000",2007673,"Yeah lets return 0 for the primitive types. Patches is welcome, with unit tests.",0.475,0.475,neutral
camel,4993,comment_3,I found an issue with the previous patch. Fixes and more tests attached in second patch.,test_debt,lack_of_tests,"Wed, 8 Feb 2012 10:05:03 +0000","Sat, 2 Jun 2012 12:16:50 +0000","Sat, 2 Jun 2012 12:16:50 +0000",9943907,I found an issue with the previous patch. Fixes and more tests attached in second patch.,0,0,neutral
camel,5008,comment_0,Works if streaming gets out commented in test. Sorry if the test is a bit incomplete.,test_debt,low_coverage,"Wed, 15 Feb 2012 11:51:24 +0000","Thu, 16 Feb 2012 08:02:43 +0000","Thu, 16 Feb 2012 05:42:50 +0000",64286,Works if streaming gets out commented in test. Sorry if the test is a bit incomplete.,0.125,0.125,neutral
camel,5184,description,"When shutting down seda endpoints they take a lille while to shutdown properly. However during testing we dont need to do that, so we could shortcut this and shutdown faster. For example the camel-test kit could tweak that. As well in camel-core. I suspect we can cut down minutes of testing times.",test_debt,expensive_tests,"Tue, 17 Apr 2012 05:55:01 +0000","Sun, 22 Apr 2012 08:46:17 +0000","Wed, 18 Apr 2012 09:59:33 +0000",101072,"When shutting down seda endpoints they take a lille while to shutdown properly. However during testing we dont need to do that, so we could shortcut this and shutdown faster. For example the camel-test kit could tweak that. As well in camel-core. I suspect we can cut down minutes of testing times.",0.02166666667,0.02166666667,neutral
camel,5184,summary,Faster testing with seda endpoints by shutting down seda endpoints faster,test_debt,expensive_tests,"Tue, 17 Apr 2012 05:55:01 +0000","Sun, 22 Apr 2012 08:46:17 +0000","Wed, 18 Apr 2012 09:59:33 +0000",101072,Faster testing with seda endpoints by shutting down seda endpoints faster,-0.292,-0.292,neutral
camel,5586,comment_3,O.K. it's fixed now. The tests also run now much faster than before (through overriding the On my box now running all the tests take about 3 seconds compared to around 30 seconds before!,test_debt,expensive_tests,"Sun, 9 Sep 2012 20:39:34 +0000","Wed, 12 Sep 2012 05:46:08 +0000","Wed, 12 Sep 2012 05:46:08 +0000",205594,O.K. it's fixed now. The tests also run now much faster than before (through overriding the hook). On my box now running all the tests take about 3 seconds compared to around 30 seconds before!,0.6875,0.4583333333,positive
camel,5983,description,"We've got bunch of (negative) tests on the current codebase expecting a thrown {{XYZException}} however they don't realize if the expected exception is *not* thrown, the typical pattern for this is: Which correctly should be:",test_debt,expensive_tests,"Sun, 20 Jan 2013 10:09:23 +0000","Mon, 21 Jan 2013 20:51:45 +0000","Mon, 21 Jan 2013 20:51:45 +0000",124942,"We've got bunch of (negative) tests on the current codebase expecting a thrown XYZException however they don't realize if the expected exception is not thrown, the typical pattern for this is: Which correctly should be:",0.05,0.05,negative
camel,6296,description,"parameters are not supported for HttpClient3 on Camel-Http component at least for version of camel 2.9.4. Here is a patch that provides support for parameters support. There is no test case provided, but a patch.",test_debt,lack_of_tests,"Thu, 18 Apr 2013 09:13:04 +0000","Tue, 17 Sep 2013 07:28:58 +0000","Wed, 22 May 2013 03:18:25 +0000",2916321,"httpConnectionManager.* parameters are not supported for HttpClient3 on Camel-Http component at least for version of camel 2.9.4. Here is a patch that provides support for httpConnectionManager.* parameters support. There is no test case provided, but a patch.",0,0.08,negative
camel,6563,description,"When using a route to listen to UDP multicast address , no messages seem to get consumed. No exceptions are observed. Multicast address is defined as addresses in the range of 224.0.0.0 through 239.255.255.255 Input was simple string (e.g. ""Test String"") Example Route: <route <from Found an old topic in the user discussion forum that seems related. Did not find any unit tests in the Camel source code exercising this behavior.",test_debt,lack_of_tests,"Thu, 18 Jul 2013 18:29:49 +0000","Wed, 28 May 2014 20:17:12 +0000","Mon, 22 Jul 2013 09:13:39 +0000",312230,"When using a route to listen to UDP multicast address , no messages seem to get consumed. No exceptions are observed. Multicast address is defined as addresses in the range of 224.0.0.0 through 239.255.255.255 (http://en.wikipedia.org/wiki/Multicast_address) Input was simple string (e.g. ""Test String"") Example Route: <route> <from uri=""netty:udp://225.1.1.1:8001?allowDefaultCodec=false&sync=false&broadcast=true""/> </route> Found an old topic in the user discussion forum that seems related. Did not find any unit tests in the Camel source code exercising this behavior. (http://camel.465427.n5.nabble.com/camel-netty-and-multicast-tt4638622.html)",0.125,0.1038333333,neutral
camel,6735,comment_0,attached a polished test from Preben which shows the issue.,test_debt,lack_of_tests,"Wed, 11 Sep 2013 20:25:23 +0000","Fri, 14 Feb 2014 16:02:02 +0000","Fri, 14 Feb 2014 16:02:02 +0000",13462599,attached a polished test from Preben which shows the issue.,0.4,0.4,neutral
camel,6826,comment_0,"I have been able to successfully speed up the producer tests. However, the consumer tests are a bit tricky. They use a callback API to do their work. I don't know if it's worth mocking all of that out or not, but I'll play with it. I'm committing what I have, though.",test_debt,expensive_tests,"Fri, 4 Oct 2013 13:51:25 +0000","Sat, 11 Jul 2015 18:27:31 +0000","Sat, 11 Jul 2015 18:27:21 +0000",55744556,"I have been able to successfully speed up the producer tests. However, the consumer tests are a bit tricky. They use a callback API to do their work. I don't know if it's worth mocking all of that out or not, but I'll play with it. I'm committing what I have, though.",0.2913,0.2913,neutral
camel,6826,comment_1,"I have implemented mocks for the consumer tests. The spring-based tests still need to be mocked out as well as the seda tests. It is much faster now, though.",test_debt,expensive_tests,"Fri, 4 Oct 2013 13:51:25 +0000","Sat, 11 Jul 2015 18:27:31 +0000","Sat, 11 Jul 2015 18:27:21 +0000",55744556,"I have implemented mocks for the consumer tests. The spring-based tests still need to be mocked out as well as the seda tests. It is much faster now, though.",-0.07644444444,-0.07644444444,neutral
camel,6826,description,"The unit tests for the camel-hazelcast component use real HazelcastInstance objects, which is very slow. We should use mock objects instead to speed up testing. Testing the integration can be done with a few, select tests, but the majority of the logic can be tested with mocks.",test_debt,expensive_tests,"Fri, 4 Oct 2013 13:51:25 +0000","Sat, 11 Jul 2015 18:27:31 +0000","Sat, 11 Jul 2015 18:27:21 +0000",55744556,"The unit tests for the camel-hazelcast component use real HazelcastInstance objects, which is very slow. We should use mock objects instead to speed up testing. Testing the integration can be done with a few, select tests, but the majority of the logic can be tested with mocks.",-0.03877777778,-0.03877777778,negative
camel,6826,summary,Use Mock Objects Instead of Live HazelcastInstances to Speed Up Testing,test_debt,expensive_tests,"Fri, 4 Oct 2013 13:51:25 +0000","Sat, 11 Jul 2015 18:27:31 +0000","Sat, 11 Jul 2015 18:27:21 +0000",55744556,Use Mock Objects Instead of Live HazelcastInstances to Speed Up Testing,-0.4,-0.4,neutral
camel,7923,comment_1,"It doesn't fail each time. It's only failing a few times. But by having too many ""fragile"" test, it's annoying to do a build because of the many many attempts you need.",test_debt,flaky_test,"Fri, 17 Oct 2014 05:30:28 +0000","Sat, 18 Oct 2014 17:30:55 +0000","Sat, 18 Oct 2014 17:30:55 +0000",129627,"It doesn't fail each time. It's only failing a few times. But by having too many ""fragile"" test, it's annoying to do a build because of the many many attempts you need.",-0.3333333333,-0.3333333333,negative
camel,8068,comment_2,Thanks for the patch. We do like when there is an unit test with code changes so we can verify this.,test_debt,lack_of_tests,"Fri, 21 Nov 2014 10:01:45 +0000","Fri, 21 Nov 2014 16:01:04 +0000","Fri, 21 Nov 2014 14:48:14 +0000",17189,Thanks for the patch. We do like when there is an unit test with code changes so we can verify this.,0.2,0.2,positive
camel,8068,comment_3,"Currently I'm not able to build the test, they complain about missing documentation... Also compilation fails on my pc becuase MailSorter uses Java 7 style and the compile level seems java 1.6 ... So I'm currently try to sort out how to build camel at all from the git checkout :-(",test_debt,lack_of_tests,"Fri, 21 Nov 2014 10:01:45 +0000","Fri, 21 Nov 2014 16:01:04 +0000","Fri, 21 Nov 2014 14:48:14 +0000",17189,"Currently I'm not able to build the test, they complain about missing documentation... Also compilation fails on my pc becuase MailSorter uses Java 7 style and the compile level seems java 1.6 ... So I'm currently try to sort out how to build camel at all from the git checkout",-0.4576,-0.472,negative
camel,8879,comment_1,Would you be able to attach an unit test also?,test_debt,lack_of_tests,"Wed, 17 Jun 2015 12:56:35 +0000","Sun, 21 Feb 2016 10:19:02 +0000","Sun, 21 Feb 2016 10:19:02 +0000",21504147,Would you be able to attach an unit test also?,0.688,0.688,neutral
camel,9226,comment_2,Ah good to hear. You are welcome to submit your unit test as a PR / .patch file then we can add it to camel-metrics so we have it there too.,test_debt,lack_of_tests,"Thu, 15 Oct 2015 08:14:35 +0000","Fri, 16 Oct 2015 12:57:40 +0000","Fri, 16 Oct 2015 12:57:40 +0000",103385,Ah good to hear. You are welcome to submit your unit test as a PR / .patch file then we can add it to camel-metrics so we have it there too.,0.5086666667,0.5086666667,positive
hadoop,10067,comment_2,looks better. The version declaration needs to go into for unified versions across the entire project. HADOOP-9594 is an example of this,architecture_debt,violation_of_modularity,"Thu, 24 Oct 2013 18:07:53 +0000","Mon, 24 Feb 2014 20:58:13 +0000","Thu, 14 Nov 2013 09:51:17 +0000",1784604,looks better. The version declaration needs to go into hadoop-project/pom.xml for unified versions across the entire project. HADOOP-9594 is an example of this,0.2833333333,0.3,positive
hadoop,10067,comment_3,Moved version declaration into,architecture_debt,violation_of_modularity,"Thu, 24 Oct 2013 18:07:53 +0000","Mon, 24 Feb 2014 20:58:13 +0000","Thu, 14 Nov 2013 09:51:17 +0000",1784604,Moved version declaration into hadoop-project.pom.xml,0,0,neutral
hadoop,10106,comment_3,You seemed to have unnecessarily moved the doRead() function's location in the server.java file. Can you please leave it in its original place in the file and please resubmit the patch.,architecture_debt,violation_of_modularity,"Sat, 16 Nov 2013 00:41:24 +0000","Mon, 24 Feb 2014 20:58:37 +0000","Mon, 16 Dec 2013 22:14:35 +0000",2669591,You seemed to have unnecessarily moved the doRead() function's location in the server.java file. Can you please leave it in its original place in the file and please resubmit the patch.,0.02222222222,0.02222222222,negative
hadoop,10508,comment_6,"Actually, , can you move the test from to +1 once it is addressed.",architecture_debt,violation_of_modularity,"Tue, 15 Apr 2014 20:36:20 +0000","Fri, 15 Aug 2014 05:39:34 +0000","Tue, 29 Apr 2014 06:08:40 +0000",1157540,"Actually, chrili, can you move the test from hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/security to hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop? +1 once it is addressed.",0.5,0.25,neutral
hadoop,11219,comment_4,"Netty 3 is EOL and the last Netty 3 was released on 6/2016, more than 3 years old. We can't afford if Netty is found to have security vulnerability one day. Time to prioritize this work.",architecture_debt,using_obsolete_technology,"Thu, 23 Oct 2014 00:23:40 +0000","Sun, 11 Feb 2024 10:57:54 +0000","Thu, 8 Jun 2023 19:23:44 +0000",272228404,"Netty 3 is EOL and the last Netty 3 was released on 6/2016, more than 3 years old. We can't afford if Netty is found to have security vulnerability one day. Time to prioritize this work.",0.3333333333,0.3333333333,negative
hadoop,1251,comment_1,"This method is not specific to TaskTracker, i.e., it should work fine with LocalRunner too, right? So there ought to be a better place to put it. JobConf? JobClient?",architecture_debt,violation_of_modularity,"Thu, 12 Apr 2007 05:54:52 +0000","Thu, 2 May 2013 02:29:04 +0000","Mon, 16 Apr 2007 22:57:10 +0000",406938,"This method is not specific to TaskTracker, i.e., it should work fine with LocalRunner too, right? So there ought to be a better place to put it. JobConf? JobClient?",0.25225,0.25225,neutral
hadoop,12721,comment_5,"I think including some common shellprofiles is a great idea. At the same time, it's increasingly evident that hadoop-tools needs to get broken up. e.g., HADOOP-12556.",architecture_debt,violation_of_modularity,"Mon, 18 Jan 2016 08:37:07 +0000","Mon, 5 Dec 2016 15:49:43 +0000","Wed, 23 Mar 2016 20:50:30 +0000",5660003,"I think including some common shellprofiles is a great idea. At the same time, it's increasingly evident that hadoop-tools needs to get broken up. e.g., HADOOP-12556.",0.25,0.25,neutral
hadoop,12923,description,Some code is used only by tests. Let's relocate them.,architecture_debt,violation_of_modularity,"Mon, 14 Mar 2016 18:43:25 +0000","Tue, 30 Aug 2016 01:17:15 +0000","Mon, 14 Mar 2016 22:58:15 +0000",15290,Some code is used only by tests. Let's relocate them.,0,0,neutral
hadoop,12923,summary,Move the test code in ipc.Client to test,architecture_debt,violation_of_modularity,"Mon, 14 Mar 2016 18:43:25 +0000","Tue, 30 Aug 2016 01:17:15 +0000","Mon, 14 Mar 2016 22:58:15 +0000",15290,Move the test code in ipc.Client to test,0,0,neutral
hadoop,13233,comment_0,Moved to Hadoop Common project because this code is in hadoop-common module.,architecture_debt,violation_of_modularity,"Thu, 2 Jun 2016 14:10:25 +0000","Mon, 13 Feb 2017 18:51:34 +0000","Mon, 13 Feb 2017 18:38:06 +0000",22134461,Moved to Hadoop Common project because this code is in hadoop-common module.,0,0,neutral
hadoop,13386,description,Avro 1.8.x makes generated classes serializable which makes them much easier to use with Spark. It would be great to upgrade Avro to 1.8.x,architecture_debt,using_obsolete_technology,"Mon, 18 Jul 2016 23:23:06 +0000","Sat, 27 Jan 2024 06:37:07 +0000","Sat, 26 Mar 2022 11:32:04 +0000",179410138,Avro 1.8.x makes generated classes serializable which makes them much easier to use with Spark. It would be great to upgrade Avro to 1.8.x Fix CVE-2021-43045,0.190625,0.190625,positive
hadoop,14692,description,We should upgrade Apache RAT to something modern.,architecture_debt,using_obsolete_technology,"Thu, 27 Jul 2017 17:35:36 +0000","Thu, 27 Jul 2017 20:34:13 +0000","Thu, 27 Jul 2017 20:06:42 +0000",9066,We should upgrade Apache RAT to something modern.,0.525,0.525,neutral
hadoop,16265,comment_1,Maybe I should move it to hadoop-common project.,architecture_debt,violation_of_modularity,"Thu, 18 Apr 2019 01:57:25 +0000","Mon, 22 Apr 2019 15:32:30 +0000","Mon, 22 Apr 2019 15:22:36 +0000",393911,Maybe I should move it to hadoop-common project.,0,0,neutral
hadoop,2850,description,"Currently, the HOD allocation operation does the following distinct steps - allocate a requested number of nodes, [optionally] transfer a hadoop tarball and install it, then provision hadoop by bringing up the appropriate daemons. It would be nice to separate these layers so each of them could be done independently. This would lead to a very great flexibility in users using the cluster. For e.g. one could allocate a certain number of nodes, then have hod bring up one version, then bring it down, then repeat this again and so on.",architecture_debt,violation_of_modularity,"Sun, 17 Feb 2008 10:31:56 +0000","Wed, 18 May 2011 22:03:36 +0000","Wed, 18 May 2011 22:03:36 +0000",102511900,"Currently, the HOD allocation operation does the following distinct steps - allocate a requested number of nodes, [optionally] transfer a hadoop tarball and install it, then provision hadoop by bringing up the appropriate daemons. It would be nice to separate these layers so each of them could be done independently. This would lead to a very great flexibility in users using the cluster. For e.g. one could allocate a certain number of nodes, then have hod bring up one version, then bring it down, then repeat this again and so on.",0.53225,0.53225,neutral
hadoop,5141,comment_4,"It doesn't have any specific dependency on the version, all we need is the latest version, since the vesion of json.jar that we are using currently is missing some classes.",architecture_debt,using_obsolete_technology,"Thu, 29 Jan 2009 09:08:08 +0000","Thu, 1 Oct 2009 07:37:54 +0000","Thu, 1 Oct 2009 07:37:54 +0000",21162586,"1. check that the 20080701 version isn't what is actually needed Giri, did you check this? It doesn't have any specific dependency on the version, all we need is the latest version, since the vesion of json.jar that we are using currently is missing some classes.",-0.2,-0.1,neutral
hadoop,6297,description,"The zlib library supports the ability to perform two types of flushes when deflating data. It can perform both a Z_SYNC_FLUSH, which forces all input to be written as output and byte-aligned and resets the Huffman coding, and it also supports a Z_FULL_FLUSH, which does the same thing but additionally resets the compression dictionary. The Hadoop wrapper for the zlib library does not support either of these two methods. Adding support should be fairly trivial. An additional deflate method that takes a fourth ""flush"" parameter, and a modification to the native c code to accept this fourth parameter and pass it along to the zlib library. I can submit a patch for this if desired. It should be noted that the native SUN Java API is likewise missing this functionality, as has been noted for over a decade here:",architecture_debt,using_obsolete_technology,"Tue, 6 Oct 2009 15:01:24 +0000","Mon, 4 Aug 2014 21:17:39 +0000","Mon, 4 Aug 2014 21:17:39 +0000",152345775,"The zlib library supports the ability to perform two types of flushes when deflating data. It can perform both a Z_SYNC_FLUSH, which forces all input to be written as output and byte-aligned and resets the Huffman coding, and it also supports a Z_FULL_FLUSH, which does the same thing but additionally resets the compression dictionary. The Hadoop wrapper for the zlib library does not support either of these two methods. Adding support should be fairly trivial. An additional deflate method that takes a fourth ""flush"" parameter, and a modification to the native c code to accept this fourth parameter and pass it along to the zlib library. I can submit a patch for this if desired. It should be noted that the native SUN Java API is likewise missing this functionality, as has been noted for over a decade here: http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4206909",0.2714285714,0.2714285714,neutral
hadoop,6374,description,The recent change to mapred-queues.xml that causes many mapreduce tests to break unless you delete out of your build tree is bad. We need to make sure that nothing in conf is used in the unit tests. One potential solution is to copy the templates into build/test/conf and use that instead.,architecture_debt,violation_of_modularity,"Sat, 14 Nov 2009 00:29:35 +0000","Tue, 24 Aug 2010 20:40:53 +0000","Thu, 21 Jan 2010 05:58:46 +0000",5894951,The recent change to mapred-queues.xml that causes many mapreduce tests to break unless you delete conf/mapred-queues.xml out of your build tree is bad. We need to make sure that nothing in conf is used in the unit tests. One potential solution is to copy the templates into build/test/conf and use that instead.,-0.1,-0.08,negative
hadoop,6374,summary,JUnit tests should never depend on anything in conf,architecture_debt,violation_of_modularity,"Sat, 14 Nov 2009 00:29:35 +0000","Tue, 24 Aug 2010 20:40:53 +0000","Thu, 21 Jan 2010 05:58:46 +0000",5894951,JUnit tests should never depend on anything in conf,0,0,neutral
hadoop,8245,comment_0,"For problem #1, the solution is the same as is already done in some other test cases. We just need to add a workaround to clear the ZK MBeans before running the tearDown method. It's a hack, but in the absense of a fix for ZOOKEEPER-1438, it's about all we can do. I spent some time investigating problem #2. The bug is as follows: - these test cases create a new and call on it before running the main body of the tests. Although they don't call {{joinElection()}}, the creation of the elector does create a {{zkClient}} object with an associated Watcher. - in the test case, we shut down and restart ZK. This causes the above Watcher instance to fire its Disconnected and then Connected events. There was a bug in the handling of the Connected event that would cause it to re-monitor the lock znode regardless of whether it was previously in the election. - So, when ZK comes back up, there was not two but *three* electors racing for the lock. However, two of the electors actually corresponded to the same dummy service. In some cases this race would be resolved in such a way that the test timed out. I don't think this is a problem in practice, since the ""formatZK"" call runs in its own JVM in the current code. However, it's worth fixing to get the tests to not be flaky, and to have a more reasonable behavior. There are several fixes to be done: - Add extra asserts for to catch cases where we might accidentally re-join the election when we weren't supposed to be in it. - Fix the handling of the ""Connected"" event to only re-join if the elector wants to be in the election - Cause exceptions thrown by watcher callbacks to be propagated back as fatal errors Will post a patch momentarily.",architecture_debt,using_obsolete_technology,"Wed, 4 Apr 2012 03:04:16 +0000","Wed, 4 Apr 2012 19:21:28 +0000","Wed, 4 Apr 2012 19:21:28 +0000",58632,"For problem #1, the solution is the same as is already done in some other test cases. We just need to add a workaround to clear the ZK MBeans before running the tearDown method. It's a hack, but in the absense of a fix for ZOOKEEPER-1438, it's about all we can do. I spent some time investigating problem #2. The bug is as follows: these test cases create a new ActiveStandbyElector, and call ActiveStandbyElector.ensureBaseNode() on it before running the main body of the tests. Although they don't call joinElection(), the creation of the elector does create a zkClient object with an associated Watcher. in the testZookeeperFailure test case, we shut down and restart ZK. This causes the above Watcher instance to fire its Disconnected and then Connected events. There was a bug in the handling of the Connected event that would cause it to re-monitor the lock znode regardless of whether it was previously in the election. So, when ZK comes back up, there was not two but three electors racing for the lock. However, two of the electors actually corresponded to the same dummy service. In some cases this race would be resolved in such a way that the test timed out. I don't think this is a problem in practice, since the ""formatZK"" call runs in its own JVM in the current code. However, it's worth fixing to get the tests to not be flaky, and to have a more reasonable behavior. There are several fixes to be done: Add extra asserts for wantToBeInElection to catch cases where we might accidentally re-join the election when we weren't supposed to be in it. Fix the handling of the ""Connected"" event to only re-join if the elector wants to be in the election Cause exceptions thrown by watcher callbacks to be propagated back as fatal errors Will post a patch momentarily.",-0.001743589744,-0.05194117647,neutral
hadoop,8288,description,"Courtesy Philip Su, we found that were not being used at all. The configuration exists but is never used. Its also mentioned in mapred-default.xml and . Also the method in Shell.java is now useless and can be removed.",architecture_debt,using_obsolete_technology,"Tue, 17 Apr 2012 17:35:19 +0000","Fri, 7 Sep 2012 21:01:45 +0000","Thu, 19 Apr 2012 16:15:01 +0000",167982,"Courtesy Philip Su, we found that (mapred.child.ulimit, mapreduce.map.ulimit, mapreduce.reduce.ulimit) were not being used at all. The configuration exists but is never used. Its also mentioned in mapred-default.xml and templates/../mapred-site.xml . Also the method getUlimitMemoryCommand in Shell.java is now useless and can be removed.",-0.08333333333,-0.03846153846,negative
hadoop,9763,comment_4,"Nicholas, this should be moved to hadoop-common, right?",architecture_debt,violation_of_modularity,"Sun, 21 Jul 2013 10:16:02 +0000","Tue, 27 Aug 2013 22:06:38 +0000","Wed, 24 Jul 2013 06:49:54 +0000",246832,"Nicholas, this should be moved to hadoop-common, right?",0.527,0.527,neutral
hadoop,9763,comment_5,GSet and LightWeightGSet are currently in hdfs. I agree that we should move them to common. Let's do it in a separated issue?,architecture_debt,violation_of_modularity,"Sun, 21 Jul 2013 10:16:02 +0000","Tue, 27 Aug 2013 22:06:38 +0000","Wed, 24 Jul 2013 06:49:54 +0000",246832,GSet and LightWeightGSet are currently in hdfs. I agree that we should move them to common. Let's do it in a separated issue?,0.06666666667,0.06666666667,neutral
hadoop,10067,description,Compiling for Fedora revels a missing declaration for This is the result of a missing explicit dependency on jsr305.,build_debt,under-declared_dependencies,"Thu, 24 Oct 2013 18:07:53 +0000","Mon, 24 Feb 2014 20:58:13 +0000","Thu, 14 Nov 2013 09:51:17 +0000",1784604,Compiling for Fedora revels a missing declaration for javax.annotation.Nullable. This is the result of a missing explicit dependency on jsr305.,-0.1,-0.0875,negative
hadoop,10067,summary,Missing POM dependency on jsr305,build_debt,under-declared_dependencies,"Thu, 24 Oct 2013 18:07:53 +0000","Mon, 24 Feb 2014 20:58:13 +0000","Thu, 14 Nov 2013 09:51:17 +0000",1784604,Missing POM dependency on jsr305,-0.4,-0.4,negative
hadoop,10819,comment_2,"Looks like it could maybe be this error, in which case it is a missing build-time dependency in hadoop",build_debt,under-declared_dependencies,"Sat, 12 Jul 2014 07:28:34 +0000","Tue, 4 Dec 2018 06:53:26 +0000","Tue, 4 Dec 2018 06:53:26 +0000",138756292,"Looks like it could maybe be this error, in which case it is a missing build-time dependency in hadoop http://stackoverflow.com/questions/19823184/maven-compiler-plugin-3-x",-0.4,-0.4,negative
hadoop,13386,comment_7,"1.9.0 has removed a couple of dependencies, including upgrading to Jackson 2.x. We also got rid of paranamer. Avro does only depend on Jackson, Jackson-Databind and Commons-compress. You can see the dependencies here:",build_debt,over-declared_dependencies,"Mon, 18 Jul 2016 23:23:06 +0000","Sat, 27 Jan 2024 06:37:07 +0000","Sat, 26 Mar 2022 11:32:04 +0000",179410138,"1.9.0 has removed a couple of dependencies, including upgrading to Jackson 2.x. We also got rid of paranamer. Avro does only depend on Jackson, Jackson-Databind and Commons-compress. You can see the dependencies here: https://mvnrepository.com/artifact/org.apache.avro/avro/1.8.2 https://mvnrepository.com/artifact/org.apache.avro/avro/1.9.0",0,0,neutral
hadoop,13386,comment_8,"OK. We will need to tag as incompatible for precompiled avro code. But it is better, and with fewer dependencies, actually an improvement on what we get today.",build_debt,over-declared_dependencies,"Mon, 18 Jul 2016 23:23:06 +0000","Sat, 27 Jan 2024 06:37:07 +0000","Sat, 26 Mar 2022 11:32:04 +0000",179410138,"OK. We will need to tag as incompatible for precompiled avro code. But it is better, and with fewer dependencies, actually an improvement on what we get today.",0.4236666667,0.4236666667,positive
hadoop,14359,comment_2,"+1, the patch looks good to me. I confirmed that the modules do not have transitive commons-httpclient dependency. Unfortunately, not yet. Now hadoop-yarn-project and module have transitive commons-httpclient dependency via hbase-server. * hadoop-yarn-project - HBASE-16267 removed the dependency and the fix version is 2.0.0.",build_debt,over-declared_dependencies,"Thu, 27 Apr 2017 08:06:23 +0000","Wed, 2 Oct 2019 17:16:03 +0000","Mon, 1 May 2017 06:24:10 +0000",339467,"+1, the patch looks good to me. I confirmed that the modules do not have transitive commons-httpclient dependency. I'd like to get rid of commons-httpclient and make downstream applications aware it will no longer appear in Hadoop code. Unfortunately, not yet. Now hadoop-yarn-project and yarn-server-timelineserver-hbase module have transitive commons-httpclient dependency via hbase-server. hadoop-yarn-project -> hadoop-yarn-server -> hbase-server:1.2.4 -> commons-httpclient:3.1.0 HBASE-16267 removed the dependency and the fix version is 2.0.0.",0.0052,0.08414285714,positive
hadoop,14479,comment_13,"The problem was that Intel didn't have anything that wasn't locked behind an email wall to get to the source tree. It was pretty much impossible for us to get anything that was reliable. FWIW, this isn't the only native component that isn't getting tested due to missing dependencies or just plain brokenness in the CMakefiles. I'll likely upgrade the Dockerfile bits to handle multiple OSes and architectures at some point. When I do that, dependencies will almost certainly get revisited. (I don't think anyone else is really paying attention to this stuff. Looking forward to the Jenkins upgrade, cuz a lot of non-Yetus stuff is just going to flat out break.)",build_debt,build_others,"Fri, 2 Jun 2017 14:27:32 +0000","Sat, 19 Aug 2017 02:50:34 +0000","Thu, 29 Jun 2017 03:28:35 +0000",2293263,"We're supposed to be running the ISA-L tests as part of precommit, so I'm not sure what happened here. The problem was that Intel didn't have anything that wasn't locked behind an email wall to get to the source tree. It was pretty much impossible for us to get anything that was reliable. FWIW, this isn't the only native component that isn't getting tested due to missing dependencies or just plain brokenness in the CMakefiles. I'll likely upgrade the Dockerfile bits to handle multiple OSes and architectures at some point. When I do that, dependencies will almost certainly get revisited. (I don't think anyone else is really paying attention to this stuff. Looking forward to the Jenkins upgrade, cuz a lot of non-Yetus stuff is just going to flat out break.)",0.1785238095,0.1562083333,negative
hadoop,14634,description,"A long time ago, HADOOP-9342 removed jline from being included in the Hadoop distribution. Since then, more modules have added Zookeeper, and are pulling in jline again. Recommend excluding jline from the main Hadoop pom in order to prevent subsequent additions of Zookeeper dependencies from doing this again.",build_debt,over-declared_dependencies,"Fri, 7 Jul 2017 22:37:41 +0000","Mon, 10 Jul 2017 17:04:34 +0000","Sat, 8 Jul 2017 11:20:57 +0000",45796,"A long time ago, HADOOP-9342 removed jline from being included in the Hadoop distribution. Since then, more modules have added Zookeeper, and are pulling in jline again. Recommend excluding jline from the main Hadoop pom in order to prevent subsequent additions of Zookeeper dependencies from doing this again.",0.07777777778,0.07777777778,neutral
hadoop,16332,description,HADOOP-16085 added a dependency on apache httpcore just to get a constant of an http error code. This is a needless dependency which can only cause problems downstream. replace the external constant with an internal one and remove the new dependency.,build_debt,over-declared_dependencies,"Tue, 28 May 2019 10:16:12 +0000","Thu, 22 Aug 2019 04:06:33 +0000","Tue, 28 May 2019 21:53:34 +0000",41842,HADOOP-16085 added a dependency on apache httpcore just to get a constant of an http error code. This is a needless dependency which can only cause problems downstream. replace the external constant with an internal one and remove the new dependency.,-0.2833333333,-0.2833333333,negative
hadoop,3477,comment_1,"More details: == tar tzvf |awk '{print $6}'|sort|uniq -c|grep -v '1 ' 2 2 2 2 2 2 == This occurs because those files are included twice in build.xml, because there is no explicit exclude on those dirs, like there is for the others. The attached patch(1-line) adds such an exclude. This is keeping me from using the downloaded tarball as the orig.tar.gz for a debian package.",build_debt,build_others,"Sun, 1 Jun 2008 23:09:33 +0000","Fri, 22 Aug 2008 19:48:19 +0000","Wed, 11 Jun 2008 21:56:42 +0000",859629,"More details: == adam@zoot:~/code/upstream/nutch$ tar tzvf hadoop-0.17.0.tar.gz |awk ' {print $6} '|sort|uniq -c|grep -v '1 ' 2 hadoop-0.17.0/contrib/hod/bin/VERSION 2 hadoop-0.17.0/contrib/hod/bin/checknodes 2 hadoop-0.17.0/contrib/hod/bin/hod 2 hadoop-0.17.0/contrib/hod/bin/hodcleanup 2 hadoop-0.17.0/contrib/hod/bin/hodring 2 hadoop-0.17.0/contrib/hod/bin/ringmaster == This occurs because those files are included twice in build.xml, because there is no explicit exclude on those dirs, like there is for the others. The attached patch(1-line) adds such an exclude. This is keeping me from using the downloaded tarball as the orig.tar.gz for a debian package.",-0.05833333333,-0.01590909091,neutral
hadoop,3477,summary,release tar.gz contains duplicate files,build_debt,build_others,"Sun, 1 Jun 2008 23:09:33 +0000","Fri, 22 Aug 2008 19:48:19 +0000","Wed, 11 Jun 2008 21:56:42 +0000",859629,release tar.gz contains duplicate files,0,0,negative
hadoop,5775,description,"as war target read user-certs.xml and from If a user set this environment and have some files in it, it could potentially cause the unit test to fail if the conf files does not match what the unit test needs.",build_debt,build_others,"Tue, 5 May 2009 23:25:38 +0000","Tue, 24 Aug 2010 20:37:27 +0000","Fri, 26 Jun 2009 23:06:12 +0000",4491634,"as war target read user-certs.xml and user-permissions.xml from $HDFSPROXY_CONF_DIR. If a user set this environment and have some files in it, it could potentially cause the unit test to fail if the conf files does not match what the unit test needs.",-0.4,-0.2,neutral
hadoop,5775,summary,HdfsProxy Unit Test should not depend on HDFSPROXY_CONF_DIR environment,build_debt,build_others,"Tue, 5 May 2009 23:25:38 +0000","Tue, 24 Aug 2010 20:37:27 +0000","Fri, 26 Jun 2009 23:06:12 +0000",4491634,HdfsProxy Unit Test should not depend on HDFSPROXY_CONF_DIR environment,0,0,neutral
hadoop,6492,comment_6,Avro 1.3-specific dependencies had been inadvertently put in patch #2. Trying again with those removed.,build_debt,over-declared_dependencies,"Thu, 14 Jan 2010 04:40:07 +0000","Tue, 24 Aug 2010 20:41:30 +0000","Sat, 16 Jan 2010 01:08:23 +0000",160096,Avro 1.3-specific dependencies had been inadvertently put in patch #2. Trying again with those removed.,-0.45,-0.45,negative
hadoop,6538,description,"The default value of is ""kerberos"". It makes sense for that to be ""simple"" since not all users have kerberos infrastructure set up, and it is inconvenient to have it set it to ""simple"" manually.",build_debt,build_others,"Thu, 4 Feb 2010 00:53:07 +0000","Tue, 24 Aug 2010 20:41:47 +0000","Thu, 4 Feb 2010 07:46:34 +0000",24807,"The default value of ""hadoop.security.authentication"" is ""kerberos"". It makes sense for that to be ""simple"" since not all users have kerberos infrastructure set up, and it is inconvenient to have it set it to ""simple"" manually.",-0.45,-0.225,neutral
hadoop,6879,comment_5,"Apparently there's no Ivy system profile in Common (unlike Hdfs and MR). However, this dependency needs to go to test instead of master.",build_debt,build_others,"Sat, 24 Jul 2010 07:27:29 +0000","Mon, 12 Dec 2011 06:20:05 +0000","Mon, 4 Oct 2010 05:10:42 +0000",6212593,"I think, this jar file needs to go to test->system profile, not master. Apparently there's no Ivy system profile in Common (unlike Hdfs and MR). However, this dependency needs to go to test instead of master.",0.25,0.1666666667,neutral
hadoop,6879,description,com.jcraft  jsch 0.1.42 version needs to be included in the build. This is needed to facilitate implementation of some system (Herriot) testcases . Please include this in ivy. jsch is originally located in,build_debt,under-declared_dependencies,"Sat, 24 Jul 2010 07:27:29 +0000","Mon, 12 Dec 2011 06:20:05 +0000","Mon, 4 Oct 2010 05:10:42 +0000",6212593,http://mvnrepository.com/ com.jcraft  jsch 0.1.42 version needs to be included in the build. This is needed to facilitate implementation of some system (Herriot) testcases . Please include this in ivy. jsch is originally located in http://www.jcraft.com/jsch/,0.04,0.04,neutral
hadoop,7161,comment_0,"oro is a transitive dependency from commons-net. Feel free to remove it from ivy.xml though, as it'll get pulled in anyway :)",build_debt,over-declared_dependencies,"Thu, 3 Mar 2011 19:20:30 +0000","Tue, 30 Aug 2016 01:34:01 +0000","Thu, 21 Jan 2016 17:48:17 +0000",154218467,"oro is a transitive dependency from commons-net. Feel free to remove it from ivy.xml though, as it'll get pulled in anyway",0.3666666667,0.2333333333,neutral
hadoop,7161,comment_1,"-01 Remove Jakarta Oro from pom - remove oro from dependency management HADOOP-8278 removed Oro from the dependency list for hadoop-common, but left it in the dependency management section of hadoop-project. commons-net stopped using oro in 2.x (and we rely on a much later version).",build_debt,over-declared_dependencies,"Thu, 3 Mar 2011 19:20:30 +0000","Tue, 30 Aug 2016 01:34:01 +0000","Thu, 21 Jan 2016 17:48:17 +0000",154218467,"-01 Remove Jakarta Oro from pom remove oro from dependency management HADOOP-8278 removed Oro from the dependency list for hadoop-common, but left it in the dependency management section of hadoop-project. commons-net stopped using oro in 2.x (and we rely on a much later version).",-0.06666666667,-0.06666666667,negative
hadoop,7161,description,"Best I can tell we never use the ""oro"" dependency, but it's been in ivy.xml forever. Does anyone know any reason we might need it?",build_debt,over-declared_dependencies,"Thu, 3 Mar 2011 19:20:30 +0000","Tue, 30 Aug 2016 01:34:01 +0000","Thu, 21 Jan 2016 17:48:17 +0000",154218467,"Best I can tell we never use the ""oro"" dependency, but it's been in ivy.xml forever. Does anyone know any reason we might need it?",0.2916666667,0.2916666667,neutral
hadoop,7161,summary,Remove unnecessary oro package from dependency management section,build_debt,over-declared_dependencies,"Thu, 3 Mar 2011 19:20:30 +0000","Tue, 30 Aug 2016 01:34:01 +0000","Thu, 21 Jan 2016 17:48:17 +0000",154218467,Remove unnecessary oro package from dependency management section,0,0,negative
hadoop,8364,description,"Three different compile flags, compile.native, compile.c++, and compile.libhdfs, turn on or off different subcomponent builds, but they are generally all off or all on and there's no evident need for three different ways to do things. Also, in build.xml, jsvc and task-controller are included in targets ""package"" and ""bin-package"" as sub-ant tasks, while librecordio is included as a simple dependency. We should work through these and get them done in one understandable way. This is a matter of maintainability and understandability, and therefore robustness under future changes in build.xml. No substantial change in functionality is proposed.",build_debt,build_others,"Sun, 6 May 2012 21:02:35 +0000","Fri, 15 May 2015 17:58:56 +0000","Fri, 15 May 2015 17:58:56 +0000",95374581,"Three different compile flags, compile.native, compile.c++, and compile.libhdfs, turn on or off different architecture-specific subcomponent builds, but they are generally all off or all on and there's no evident need for three different ways to do things. Also, in build.xml, jsvc and task-controller are included in targets ""package"" and ""bin-package"" as sub-ant tasks, while librecordio is included as a simple dependency. We should work through these and get them done in one understandable way. This is a matter of maintainability and understandability, and therefore robustness under future changes in build.xml. No substantial change in functionality is proposed.",0.01,0.01,neutral
hadoop,8921,comment_7,"I've personally wasted over an hour to figure out that option has to be explicitly disabled before it skips running autoconf/automake (in create-configure part). The older approach was bad for someone who did a git checkout and ran a build without paying attention to the docs about native library compatibility. I'd say the optimizations (i.e non-core features) can be skipped over for a clean build on platforms where the code won't compile / isn't supported. Every single attempt to build on a Mac would result in a failed compilation, until someone discovers the -Dcompile.native option. Clean builds on ""ant compile"" should be encouraged (on any platform) without RTFMing for a -D option. Of course on platforms where the native code is indeed supported, it would error out if say, JNI headers can't be found.",build_debt,build_others,"Fri, 12 Oct 2012 09:36:01 +0000","Wed, 19 Mar 2014 21:13:40 +0000","Wed, 19 Mar 2014 21:13:40 +0000",45229059,"I've personally wasted over an hour to figure out that option has to be explicitly disabled before it skips running autoconf/automake (in create-configure part). The older approach was bad for someone who did a git checkout and ran a build without paying attention to the docs about native library compatibility. I'd say the optimizations (i.e non-core features) can be skipped over for a clean build on platforms where the code won't compile / isn't supported. Every single attempt to build on a Mac would result in a failed compilation, until someone discovers the -Dcompile.native option. Clean builds on ""ant compile"" should be encouraged (on any platform) without RTFMing for a -D option. Of course on platforms where the native code is indeed supported, it would error out if say, JNI headers can't be found.",-0.1196428571,-0.1196428571,negative
hadoop,9056,comment_1,"Looks good in general! I have the following questions and comments. # It seems this patch only contains changes that made compiling hadoop.dll available on Windows. Some functionality was missing. For example, POSIX.chmod was not ported over from branch-1-win; so were some other IO methods. I also notice some new functions were introduced in trunk compared with branch-1. For example, and What is the plan for those missing functions? Do we plan to port them over in other JIRAs? # file is not needed. # Snappy does not work on Windows right now. It may be better to exclude snappy related files from Windows build. # I think it is better to use WINDOWS instead of  _WIN32 macro in some places because we explicitly defined the UNIX and WINDOWS macros in the beginning of It will make it easier in the future if we want to change the definition of the macros. # Some changes in SecureIOUtils and Datanode are not ported over. Again I think this is related to point 1 above.",build_debt,build_others,"Fri, 16 Nov 2012 23:43:43 +0000","Thu, 2 May 2013 02:30:56 +0000","Wed, 12 Dec 2012 19:15:45 +0000",2230322,"Looks good in general! I have the following questions and comments. It seems this patch only contains changes that made compiling hadoop.dll available on Windows. Some functionality was missing. For example, POSIX.chmod was not ported over from branch-1-win; so were some other IO methods. I also notice some new functions were introduced in trunk compared with branch-1. For example, posixFadviseIfPossible() and POSIX.posix_fadvis(). What is the plan for those missing functions? Do we plan to port them over in other JIRAs? native.vcxproj.user file is not needed. Snappy does not work on Windows right now. It may be better to exclude snappy related files from Windows build. I think it is better to use WINDOWS instead of  _WIN32 macro in some places because we explicitly defined the UNIX and WINDOWS macros in the beginning of org_apache_hadoop.h. It will make it easier in the future if we want to change the definition of the macros. Some changes in SecureIOUtils and Datanode are not ported over. Again I think this is related to point 1 above.",-0.00078125,0.01193181818,neutral
hadoop,10106,description,"INFO IPC Server listener on 8020: readAndProcess from client 10.115.201.46 threw exception Unknown out of band call #-2147483647 This is thrown by a reader thread, so the message should be like INFO Socket Reader #1 for port 8020: readAndProcess from client 10.115.201.46 threw exception Unknown out of band call #-2147483647 Another example is which can also be called by handler thread. When that happend, the thread name should be the handler thread, not the responder thread.",code_debt,multi-thread_correctness,"Sat, 16 Nov 2013 00:41:24 +0000","Mon, 24 Feb 2014 20:58:37 +0000","Mon, 16 Dec 2013 22:14:35 +0000",2669591,"INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: readAndProcess from client 10.115.201.46 threw exception org.apache.hadoop.ipc.RpcServerException: Unknown out of band call #-2147483647 This is thrown by a reader thread, so the message should be like INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 8020: readAndProcess from client 10.115.201.46 threw exception org.apache.hadoop.ipc.RpcServerException: Unknown out of band call #-2147483647 Another example is Responder.processResponse, which can also be called by handler thread. When that happend, the thread name should be the handler thread, not the responder thread.",0,0,neutral
hadoop,10106,summary,Incorrect thread name in RPC log messages,code_debt,multi-thread_correctness,"Sat, 16 Nov 2013 00:41:24 +0000","Mon, 24 Feb 2014 20:58:37 +0000","Mon, 16 Dec 2013 22:14:35 +0000",2669591,Incorrect thread name in RPC log messages,0,0,neutral
hadoop,10169,comment_10,Thanks for updating the patch. One last nit is that this code: can be simplified to the following so there aren't so many return statements to track:,code_debt,low_quality_code,"Tue, 17 Dec 2013 05:37:44 +0000","Thu, 12 May 2016 18:27:56 +0000","Fri, 20 Dec 2013 22:09:01 +0000",318677,Thanks for updating the patch. One last nit is that this code: can be simplified to the following so there aren't so many return statements to track:,0.5125,0.5125,positive
hadoop,10169,description,"When i looked into a HBase JvmMetric impl, just found this synchronized seems not essential.",code_debt,dead_code,"Tue, 17 Dec 2013 05:37:44 +0000","Thu, 12 May 2016 18:27:56 +0000","Fri, 20 Dec 2013 22:09:01 +0000",318677,"When i looked into a HBase JvmMetric impl, just found this synchronized seems not essential.",0.5,0.5,neutral
hadoop,10169,summary,remove the unnecessary synchronized in JvmMetrics class,code_debt,dead_code,"Tue, 17 Dec 2013 05:37:44 +0000","Thu, 12 May 2016 18:27:56 +0000","Fri, 20 Dec 2013 22:09:01 +0000",318677,remove the unnecessary  synchronized in JvmMetrics class,0,0,negative
hadoop,10214,comment_0,"A simple fix. I think it was introduced by: r1556103 | vinodkv | 2014-01-07 09:56:11 +0800 (Tue, 07 Jan 2014) | 2 lines YARN-1029. Added embedded leader election in the ResourceManager. Contributed by Karthik Kambatla. The above change made public, so the multi-threaded warning be observed.",code_debt,multi-thread_correctness,"Thu, 9 Jan 2014 04:10:55 +0000","Thu, 12 May 2016 18:23:20 +0000","Thu, 9 Jan 2014 06:38:25 +0000",8850,"A simple fix. I think it was introduced by: r1556103 | vinodkv | 2014-01-07 09:56:11 +0800 (Tue, 07 Jan 2014) | 2 lines YARN-1029. Added embedded leader election in the ResourceManager. Contributed by Karthik Kambatla. The above change made terminateConnection() public, so the multi-threaded warning be observed.",-0.12,-0.12,neutral
hadoop,10214,description,"When i worked at HADOOP-9420, found the unrelated findbugs warning:",code_debt,low_quality_code,"Thu, 9 Jan 2014 04:10:55 +0000","Thu, 12 May 2016 18:23:20 +0000","Thu, 9 Jan 2014 06:38:25 +0000",8850,"When i worked at HADOOP-9420, found the unrelated findbugs warning: https://builds.apache.org/job/PreCommit-HADOOP-Build/3408//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-common.html",-0.6,-0.6,neutral
hadoop,10214,summary,Fix multithreaded correctness warnings in,code_debt,low_quality_code,"Thu, 9 Jan 2014 04:10:55 +0000","Thu, 12 May 2016 18:23:20 +0000","Thu, 9 Jan 2014 06:38:25 +0000",8850,Fix multithreaded correctness warnings in ActiveStandbyElector,-0.0185,-0.0185,neutral
hadoop,10295,comment_14,"Thanks for the review, Nicholas and Sangjin! , that is originally implicitly contained in the FileSystem#create call (see boolean, int, short, long, Progressable)). I just pulled it out to make the code not too long.",code_debt,low_quality_code,"Mon, 27 Jan 2014 22:43:52 +0000","Thu, 4 Sep 2014 01:16:46 +0000","Fri, 31 Jan 2014 00:01:18 +0000",263846,"Thanks for the review, Nicholas and Sangjin! sjlee0, that is originally implicitly contained in the FileSystem#create call (see FileSystem#create(Path, boolean, int, short, long, Progressable)). I just pulled it out to make the code not too long.",0.1333333333,0.1333333333,positive
hadoop,10295,comment_4,"Thanks for the comment ! That's right. I also found this problem in my patch. I personally like your idea in HADOOP-10297. That can simplify the logic there. However, FileChecksum is a public API marked as stable, to add a new abstract method there may cause incompatibility (e.g., other ppl may have implemented their own FileChecksum). A workaround here can be adding getChecksumOpt() to FileChecksum and let it return null. Totally agree. Actually I've added a new unit test in my 001 patch, and the new unit test is very similar to yours :) I thought about this problem. To me checksum type may be a little bit different from other file attributes, since other file attributes are all metadata stored in NN. Thus in my first patch I just add a new option. But now I think to put the checksum type in the FileAttribute enum should be more clear. Currently I have a 001 patch which fixes the CreateFlag bug and adds a unit test. My original plan is to post it after I finish system test in my local cluster. But since you've worked on this issue for some time and already have a decent patch, I'd like to review your patch and commit it when it is ready.",code_debt,complex_code,"Mon, 27 Jan 2014 22:43:52 +0000","Thu, 4 Sep 2014 01:16:46 +0000","Fri, 31 Jan 2014 00:01:18 +0000",263846,"Thanks for the comment laurentgo! EnumSet.of(CreateFlag.OVERWRITE) is not equivalent of setting overwrite argument to true. From DistributedFileSystem, it is EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE) That's right. I also found this problem in my patch. MD5MD5CRC32GzipFileChecksum and MD5MD5CRC32CastagnoliFileChecksum are probably HDFS specific I personally like your idea in HADOOP-10297. That can simplify the logic there. However, FileChecksum is a public API marked as stable, to add a new abstract method there may cause incompatibility (e.g., other ppl may have implemented their own FileChecksum). A workaround here can be adding getChecksumOpt() to FileChecksum and let it return null. Having a test to check if the option actually works would be a nice to have Totally agree. Actually I've added a new unit test in my 001 patch, and the new unit test is very similar to yours it may be better to extend FileAttribute enum I thought about this problem. To me checksum type may be a little bit different from other file attributes, since other file attributes are all metadata stored in NN. Thus in my first patch I just add a new option. But now I think to put the checksum type in the FileAttribute enum should be more clear. Currently I have a 001 patch which fixes the CreateFlag bug and adds a unit test. My original plan is to post it after I finish system test in my local cluster. But since you've worked on this issue for some time and already have a decent patch, I'd like to review your patch and commit it when it is ready.",0.1037555556,0.06785714286,positive
hadoop,10295,comment_6,"Besides the concern on FileChecksum, some other comments on the current patch: # We may want to change ""checksum"" to ""checksumtype"" in the changes of PRESERVE_STATUS and FileAttribute. # We actually do not need to pass a FileChecksum to In if we need to preserve the checksum type, we get the checksum type of the source file and we reuse this checksum in compareCheckSums(). In that case we only need to call once (note that getFileChecksum is very costly). # We should use in the following change (see boolean, int, short, long, Progressable)) # The new added unit test does not cover there scenario where source files have different REAL checksum types (CRC32 and CRC32C), in which case copy with preserving checksum type should succeed and the original checksum types should be preserved in the target FS. We should add unit tests for this. # There are some unnecessary whilespace and blank line changes.",code_debt,low_quality_code,"Mon, 27 Jan 2014 22:43:52 +0000","Thu, 4 Sep 2014 01:16:46 +0000","Fri, 31 Jan 2014 00:01:18 +0000",263846,"Besides the concern on FileChecksum, some other comments on the current patch: We may want to change ""checksum"" to ""checksumtype"" in the changes of PRESERVE_STATUS and FileAttribute. We actually do not need to pass a FileChecksum to RetriableFileCopyCommand. In RetriableFileCopyCommand#doCopy, if we need to preserve the checksum type, we get the checksum type of the source file and we reuse this checksum in compareCheckSums(). In that case we only need to call sourceFS.getFileChecksum once (note that getFileChecksum is very costly). We should use ""FsPermission.getFileDefault().applyUMask(FsPermission.getUMask(getConf()))"" in the following change (see FileSystem#create(Path, boolean, int, short, long, Progressable)) The new added unit test does not cover there scenario where source files have different REAL checksum types (CRC32 and CRC32C), in which case copy with preserving checksum type should succeed and the original checksum types should be preserved in the target FS. We should add unit tests for this. There are some unnecessary whilespace and blank line changes.",0.06666666667,0.03636363636,neutral
hadoop,10343,description,in we print logs at info level when we drop responses. This causes lot of noise on the console.,code_debt,low_quality_code,"Thu, 13 Feb 2014 19:05:48 +0000","Thu, 10 Apr 2014 13:11:36 +0000","Thu, 13 Feb 2014 22:03:44 +0000",10676,in LossyRetryInvocationHandler we print logs at info level when we drop responses. This causes lot of noise on the console.,0.0875,0.0875,neutral
hadoop,1034,comment_1,"More generally, in a bunch of hadoop code, only IOException are caught. (eg: same issue on I think a little cleanup is needed, especially while catching an exception in a run() of a Thread...",code_debt,low_quality_code,"Fri, 23 Feb 2007 13:45:50 +0000","Wed, 8 Jul 2009 16:42:18 +0000","Fri, 23 Feb 2007 20:27:58 +0000",24128,"More generally, in a bunch of hadoop code, only IOException are caught. (eg: same issue on DataNode.DataXceiveServer.run()) I think a little cleanup is needed, especially while catching an exception in a run() of a Thread...",0.25,0.125,negative
hadoop,1034,comment_2,"Agreed. All threads should catch Throwable at the top level and log them. *Sigh* I thought we had gone through all of the threads and fixed that problem. If you see any more, please file them as bugs.",code_debt,low_quality_code,"Fri, 23 Feb 2007 13:45:50 +0000","Wed, 8 Jul 2009 16:42:18 +0000","Fri, 23 Feb 2007 20:27:58 +0000",24128,"Agreed. All threads should catch Throwable at the top level and log them. Sigh I thought we had gone through all of the threads and fixed that problem. If you see any more, please file them as bugs.",0.1,0.1,negative
hadoop,10353,description,"The class uses a plain {{HashMap}} for caching. When the number of inserted values exceeds the the map's load threshold, it triggers a rehash. During this time, a different thread that performs a get operation on a previously inserted key can obtain a null value instead of the actual value associated with that key. The result is a NPE potentially being thrown when calling protocol)}} concurrently.",code_debt,multi-thread_correctness,"Thu, 20 Feb 2014 18:41:33 +0000","Thu, 4 Sep 2014 01:16:46 +0000","Thu, 27 Feb 2014 19:17:50 +0000",606977,"The FsUrlStreamHandlerFactory class uses a plain HashMap for caching. When the number of inserted values exceeds the the map's load threshold, it triggers a rehash. During this time, a different thread that performs a get operation on a previously inserted key can obtain a null value instead of the actual value associated with that key. The result is a NPE potentially being thrown when calling FsUrlStreamHandlerFactory#createURLStreamHandler(String protocol) concurrently.",-0.0625,-0.0625,neutral
hadoop,10432,description,"The method is private and takes a configuration to fetch a hardcoded property. Having a public method to resolve a verifier based on the provided value will enable getting a verifier based on the verifier constant (DEFAULT, STRICT, STRICT_IE6, ALLOW_ALL).",code_debt,low_quality_code,"Tue, 25 Mar 2014 06:08:09 +0000","Thu, 12 May 2016 18:26:59 +0000","Wed, 9 Apr 2014 19:40:50 +0000",1344761,"The SSFactory.getHostnameVerifier() method is private and takes a configuration to fetch a hardcoded property. Having a public method to resolve a verifier based on the provided value will enable getting a verifier based on the verifier constant (DEFAULT, DEFAULT_AND_LOCALHOST, STRICT, STRICT_IE6, ALLOW_ALL).",0.075,0.006666666667,neutral
hadoop,10485,comment_1,The v0 patch introduces no functionality changes. It only removes the unused classes.,code_debt,dead_code,"Wed, 9 Apr 2014 04:35:25 +0000","Fri, 13 May 2016 05:11:02 +0000","Wed, 9 Apr 2014 18:13:29 +0000",49084,The v0 patch introduces no functionality changes. It only removes the unused classes.,0,0,neutral
hadoop,10485,comment_2,"+1. Thank you, Haohui, for the code cleaning up.",code_debt,low_quality_code,"Wed, 9 Apr 2014 04:35:25 +0000","Fri, 13 May 2016 05:11:02 +0000","Wed, 9 Apr 2014 18:13:29 +0000",49084,"+1. Thank you, Haohui, for the code cleaning up.",0.2,0.2,positive
hadoop,10485,description,Hadoop-streaming no longer requires many classes in o.a.h.record. This jira removes the dead code.,code_debt,dead_code,"Wed, 9 Apr 2014 04:35:25 +0000","Fri, 13 May 2016 05:11:02 +0000","Wed, 9 Apr 2014 18:13:29 +0000",49084,Hadoop-streaming no longer requires many classes in o.a.h.record. This jira removes the dead code.,-0.3,-0.3,neutral
hadoop,10485,summary,Remove dead classes in hadoop-streaming,code_debt,dead_code,"Wed, 9 Apr 2014 04:35:25 +0000","Fri, 13 May 2016 05:11:02 +0000","Wed, 9 Apr 2014 18:13:29 +0000",49084,Remove dead classes in hadoop-streaming,-0.6,-0.6,neutral
hadoop,10496,description,"{{FileSink}} opens a file. If the {{MetricsSystem}} is shutdown, then the sink is discarded and the file is never closed, causing a file descriptor leak.",code_debt,low_quality_code,"Sun, 13 Apr 2014 16:29:21 +0000","Thu, 12 May 2016 18:22:38 +0000","Mon, 14 Apr 2014 04:32:51 +0000",43410,"FileSink opens a file. If the MetricsSystem is shutdown, then the sink is discarded and the file is never closed, causing a file descriptor leak.",-0.1,-0.1,neutral
hadoop,10496,summary,Metrics system FileSink can leak file descriptor.,code_debt,low_quality_code,"Sun, 13 Apr 2014 16:29:21 +0000","Thu, 12 May 2016 18:22:38 +0000","Mon, 14 Apr 2014 04:32:51 +0000",43410,Metrics system FileSink can leak file descriptor.,-0.2,-0.2,negative
hadoop,10498,comment_7,"Thanks Daryn for the confirmation. A nice to have. I do not have strong argument for this. The header name could be configurable instead of hard coding to ""X-Forwarded-For"". The default value being ""X-Forwarded-For"".",code_debt,low_quality_code,"Mon, 14 Apr 2014 16:07:30 +0000","Thu, 12 May 2016 18:21:56 +0000","Tue, 15 Apr 2014 15:27:58 +0000",84028,"daryn Thanks Daryn for the confirmation. A nice to have. I do not have strong argument for this. The header name could be configurable instead of hard coding to ""X-Forwarded-For"". The default value being ""X-Forwarded-For"".",0.025,0.025,positive
hadoop,10499,comment_19,"Hi, Sorry for the inconvenience this caused. Is it possible to patch HBase to stop calling the 3-arg form of that was removed? The third argument was unused anyway, so this won't change any functionality in HBase. Additionally, this class is currently annotated {{Private}}, so I expected we didn't need to check for downstream impacts before I committed. That means one of two additional changes needs to happen: # Change HBase to stop calling altogether. I'm not really sure what alternative you would have though, so maybe this isn't feasible. # Change the annotation to {{LimitedPrivate}} for HBase, so devs know to check for downstream impacts on interface-breaking changes in the future. What are your thoughts on this?",code_debt,low_quality_code,"Mon, 14 Apr 2014 17:10:42 +0000","Wed, 3 Sep 2014 20:36:28 +0000","Wed, 16 Apr 2014 23:18:20 +0000",194858,"Hi, yuzhihong@gmail.com. Sorry for the inconvenience this caused. Is it possible to patch HBase to stop calling the 3-arg form of ProxyUsers#authorize that was removed? The third argument was unused anyway, so this won't change any functionality in HBase. Additionally, this class is currently annotated Private, so I expected we didn't need to check for downstream impacts before I committed. That means one of two additional changes needs to happen: Change HBase to stop calling ProxyUsers#authorize altogether. I'm not really sure what alternative you would have though, so maybe this isn't feasible. Change the annotation to LimitedPrivate for HBase, so devs know to check for downstream impacts on interface-breaking changes in the future. What are your thoughts on this?",-0.253125,-0.225,negative
hadoop,10499,comment_3,"Although it makes sense to remove an unused parameter, I wonder if the parameter should be honored? It would allow different rpc services to have different proxy superuser configurations as opposed to the current static configuration... It probably made sense when daemons generally had only one rpc service. I don't have a strong opinion because I suppose it could be by another jira.",code_debt,low_quality_code,"Mon, 14 Apr 2014 17:10:42 +0000","Wed, 3 Sep 2014 20:36:28 +0000","Wed, 16 Apr 2014 23:18:20 +0000",194858,"Although it makes sense to remove an unused parameter, I wonder if the parameter should be honored? It would allow different rpc services to have different proxy superuser configurations as opposed to the current static configuration... It probably made sense when daemons generally had only one rpc service. I don't have a strong opinion because I suppose it could be re-added/implemented by another jira.",0.03883333333,0.03883333333,neutral
hadoop,10499,comment_6,"I believe , the static nature of the ProxyUser capability should be addressed separately, if there is a requirement for different services to have different ProxyUser capabilities. HADOOP-10448 would make it easier since it will move the state out of ProxyUsers. I am trying to clean up the interface exposed by _ProxyUsers_ in order to work on HADOOP-10448 and that's why this jira.",code_debt,low_quality_code,"Mon, 14 Apr 2014 17:10:42 +0000","Wed, 3 Sep 2014 20:36:28 +0000","Wed, 16 Apr 2014 23:18:20 +0000",194858,"I believe , the static nature of the ProxyUser capability should be addressed separately, if there is a requirement for different services to have different ProxyUser capabilities. HADOOP-10448 would make it easier since it will move the state out of ProxyUsers. I am trying to clean up the interface exposed by ProxyUsers in order to work on HADOOP-10448 and that's why this jira.",0.05541666667,0.05541666667,neutral
hadoop,10499,description,The Configuration parameter is not used in the authorize() function. It can be removed and callers can be updated. Attaching the simple patch which removes the unused _conf_ parameter and updates the callers. The ProxyUsers is defined as a private audience and so there shouldn't be any external callers.,code_debt,dead_code,"Mon, 14 Apr 2014 17:10:42 +0000","Wed, 3 Sep 2014 20:36:28 +0000","Wed, 16 Apr 2014 23:18:20 +0000",194858,The Configuration parameter is not used in the authorize() function. It can be removed and callers can be updated. Attaching the simple patch which removes the unused conf parameter and updates the callers. The ProxyUsers is defined as a private audience and so there shouldn't be any external callers.,0.125,0.125,neutral
hadoop,10499,summary,Remove unused parameter from,code_debt,dead_code,"Mon, 14 Apr 2014 17:10:42 +0000","Wed, 3 Sep 2014 20:36:28 +0000","Wed, 16 Apr 2014 23:18:20 +0000",194858,Remove unused parameter from ProxyUsers.authorize(),0,0,neutral
hadoop,10501,description,All the other methods accessing handlers are synchronized methods.,code_debt,multi-thread_correctness,"Mon, 14 Apr 2014 23:36:34 +0000","Sat, 7 Mar 2015 23:17:57 +0000","Sat, 7 Mar 2015 23:17:57 +0000",28251683,All the other methods accessing handlers are synchronized methods.,0,0,neutral
hadoop,10501,summary,accesses handlers without synchronization,code_debt,multi-thread_correctness,"Mon, 14 Apr 2014 23:36:34 +0000","Sat, 7 Mar 2015 23:17:57 +0000","Sat, 7 Mar 2015 23:17:57 +0000",28251683,Server#getHandlers() accesses handlers without synchronization,0,0,neutral
hadoop,10673,comment_10,"Thanks for the updating, Ming. It looks better approach to me. One additional point: if we can make the variables final, we can remove null check in {{Server#stop()}}.",code_debt,low_quality_code,"Mon, 9 Jun 2014 18:01:13 +0000","Mon, 1 Dec 2014 03:07:25 +0000","Tue, 15 Jul 2014 23:15:40 +0000",3129267,"Thanks for the updating, Ming. It looks better approach to me. One additional point: if we can make the variables final, we can remove null check in Server#stop().",0.3,0.3,positive
hadoop,10673,comment_7,", thank you for the suggestion. I think this improvement is useful, too. One minor comment: both {{rpcMetrics}} and are not final field, so how about adding null check before accessing them?",code_debt,low_quality_code,"Mon, 9 Jun 2014 18:01:13 +0000","Mon, 1 Dec 2014 03:07:25 +0000","Tue, 15 Jul 2014 23:15:40 +0000",3129267,"mingma, thank you for the suggestion. I think this improvement is useful, too. One minor comment: both rpcMetrics and rpcDetailedMetrics are not final field, so how about adding null check before accessing them?",0.307,0.307,positive
hadoop,10681,comment_17,Can we do the same for {{Lz4Compressor}} and I noticed a significant performance overhead using {{Lz4Compressor}} for compared to due to the same problem.,code_debt,slow_algorithm,"Wed, 11 Jun 2014 17:41:42 +0000","Thu, 8 Sep 2016 06:49:18 +0000","Sun, 5 Oct 2014 14:49:19 +0000",10012057,Can we do the same for Lz4Compressor and Bzip2Compressor? I noticed a significant performance overhead using Lz4Compressor for hbase.client.rpc.compressor compared to SnappyCompressor due to the same problem.,-0.1,-0.04,negative
hadoop,10681,comment_1,"Added a SnappyCode impl into hive, which should come before in the classpath for hive+tez. Tested out TPC-H Query5, which has a spill-merge on the JOIN. Query times went from 539.8 seconds to 464.89 seconds, mostly from speedup to a single reducer stage.",code_debt,slow_algorithm,"Wed, 11 Jun 2014 17:41:42 +0000","Thu, 8 Sep 2016 06:49:18 +0000","Sun, 5 Oct 2014 14:49:19 +0000",10012057,"Added a SnappyCode impl into hive, which should come before in the classpath for hive+tez. Tested out TPC-H Query5, which has a spill-merge on the JOIN. Query times went from 539.8 seconds to 464.89 seconds, mostly from speedup to a single reducer stage.",0.1271666667,0.1271666667,neutral
hadoop,10681,comment_7,Address findbugs warnings,code_debt,low_quality_code,"Wed, 11 Jun 2014 17:41:42 +0000","Thu, 8 Sep 2016 06:49:18 +0000","Sun, 5 Oct 2014 14:49:19 +0000",10012057,Address findbugs warnings,-0.6,-0.6,negative
hadoop,10681,description,"The current implementation of SnappyCompressor spends more time within the java loop of copying from the user buffer into the direct buffer allocated to the compressor impl, than the time it takes to compress the buffers. The bottleneck was found to be java monitor code inside SnappyCompressor. The methods are neatly inlined by the JIT into the parent caller which unfortunately does not flatten out the synchronized blocks. The loop does a write of small byte[] buffers (each IFile key+value). I counted approximately 6 monitor enter/exit blocks per k-v pair written.",code_debt,slow_algorithm,"Wed, 11 Jun 2014 17:41:42 +0000","Thu, 8 Sep 2016 06:49:18 +0000","Sun, 5 Oct 2014 14:49:19 +0000",10012057,"The current implementation of SnappyCompressor spends more time within the java loop of copying from the user buffer into the direct buffer allocated to the compressor impl, than the time it takes to compress the buffers. The bottleneck was found to be java monitor code inside SnappyCompressor. The methods are neatly inlined by the JIT into the parent caller (BlockCompressorStream::write), which unfortunately does not flatten out the synchronized blocks. The loop does a write of small byte[] buffers (each IFile key+value). I counted approximately 6 monitor enter/exit blocks per k-v pair written.",0.005,0.005,neutral
hadoop,1072,comment_1,"This is an incompatible change to a public API. So, if we choose to make it, we must deprecate the existing name for at least one release prior to removing it. Personally, I'm not convinced the naming hygiene is worth the hassle in this case, but I would not veto it.",code_debt,low_quality_code,"Tue, 6 Mar 2007 23:18:03 +0000","Thu, 11 Aug 2011 18:57:59 +0000","Thu, 11 Aug 2011 18:57:59 +0000",139865996,"This is an incompatible change to a public API. So, if we choose to make it, we must deprecate the existing name for at least one release prior to removing it. Personally, I'm not convinced the naming hygiene is worth the hassle in this case, but I would not veto it.",-0.02011111111,-0.02011111111,negative
hadoop,1072,description,"extends IOException. It's name should follow the Java naming convention for Exceptions, and thus be",code_debt,low_quality_code,"Tue, 6 Mar 2007 23:18:03 +0000","Thu, 11 Aug 2011 18:57:59 +0000","Thu, 11 Aug 2011 18:57:59 +0000",139865996,"org.apache.hadoop.ipc.RPC$VersionMismatch extends IOException. It's name should follow the Java naming convention for Exceptions, and thus be VersionMismatchException.",0.1,0.03333333333,neutral
hadoop,10748,description,Currently HttpServer2 loads the JspServlet by default. It should be removed as JSP support is no longer required.,code_debt,dead_code,"Tue, 24 Jun 2014 17:56:38 +0000","Fri, 24 Apr 2015 22:49:05 +0000","Wed, 27 Aug 2014 20:31:47 +0000",5538909,Currently HttpServer2 loads the JspServlet by default. It should be removed as JSP support is no longer required.,-0.05,-0.05,negative
hadoop,10752,comment_2,"Minor nit: I'd really like for this to say that the ifdef is for ARM. At least, I'd never guess that this was ARM without a for a while.",code_debt,low_quality_code,"Thu, 26 Jun 2014 08:54:13 +0000","Tue, 31 Mar 2015 08:43:07 +0000","Tue, 31 Mar 2015 08:43:06 +0000",24018533,"Minor nit: I'd really like for this to say that the ifdef is for ARM. At least, I'd never guess that this was ARM without a hint/searching/pondering for a while.",0,0,neutral
hadoop,10752,description,"This patch adds support for hardware crc for ARM's new 64 bit architecture. The patch is completely conditionalized on __arch64__ For the moment I have only done the non pipelined version as the hw I have only has 1 crc execute unit. Some initial benchmarks on terasort give sw crc: 107 sec hw crc: 103 sec The performance improvement is quite small, but this is limited by the fact that I am using early stage hw which is not performant. I have also built it on x86 and I think the change is fairly safe for other architectures because post conditionalization the src is identical on other architectures. This is the first patch I have submitted for Hadoop so I would welcome any feedback and help.",code_debt,slow_algorithm,"Thu, 26 Jun 2014 08:54:13 +0000","Tue, 31 Mar 2015 08:43:07 +0000","Tue, 31 Mar 2015 08:43:06 +0000",24018533,"This patch adds support for hardware crc for ARM's new 64 bit architecture. The patch is completely conditionalized on _arch64_ For the moment I have only done the non pipelined version as the hw I have only has 1 crc execute unit. Some initial benchmarks on terasort give sw crc: 107 sec hw crc: 103 sec The performance improvement is quite small, but this is limited by the fact that I am using early stage hw which is not performant. I have also built it on x86 and I think the change is fairly safe for other architectures because post conditionalization the src is identical on other architectures. This is the first patch I have submitted for Hadoop so I would welcome any feedback and help.",0.2042,0.2042,neutral
hadoop,10770,comment_0,This is belongs to a string of patches built on top of each other: * HADOOP-10817 proxyuser logic supporting custom prefix properties * HADOOP-10799 HTTP delegation token built in client/server authentication classes * HADOOP-10800 HttpFS using HADOOP-10799 and removing custom code * HADOOP-10835 HTTP proxyuser logic built in client/server authentication classes * HADOOP-10836 HttpFS using HADOOP-10835 and removing custom code * HADOOP-10770 KMS delegation support using HADOOP-10799 * HADOOP-10698 KMS proxyuser support using HADOOP-10835,code_debt,dead_code,"Tue, 1 Jul 2014 19:22:55 +0000","Thu, 12 May 2016 18:23:38 +0000","Fri, 15 Aug 2014 05:04:10 +0000",3836475,This is belongs to a string of patches built on top of each other: HADOOP-10817 proxyuser logic supporting custom prefix properties HADOOP-10799 HTTP delegation token built in client/server authentication classes HADOOP-10800 HttpFS using HADOOP-10799 and removing custom code HADOOP-10835 HTTP proxyuser logic built in client/server authentication classes HADOOP-10836 HttpFS using HADOOP-10835 and removing custom code HADOOP-10770 KMS delegation support using HADOOP-10799 HADOOP-10698 KMS proxyuser support using HADOOP-10835,0.3,0.3,neutral
hadoop,10770,comment_5,"Patch looks pretty good to me, Tucu. Only one little nit - seems like you've got some excessive indentation in +1 otherwise.",code_debt,low_quality_code,"Tue, 1 Jul 2014 19:22:55 +0000","Thu, 12 May 2016 18:23:38 +0000","Fri, 15 Aug 2014 05:04:10 +0000",3836475,"Patch looks pretty good to me, Tucu. Only one little nit - seems like you've got some excessive indentation in KMS#generateEncryptedKeys. +1 otherwise.",0.244,0.1626666667,positive
hadoop,10822,description,"HttpFS implements HTTP proxyuser support inline in httpfs code. For HADOOP-10698 we need similar functionality for KMS. Not to duplicate code, we should refactor existing code to common. We should also leverage HADOOP-10817.",code_debt,low_quality_code,"Mon, 14 Jul 2014 22:37:26 +0000","Wed, 13 Aug 2014 20:25:48 +0000","Wed, 13 Aug 2014 20:25:48 +0000",2584102,"HttpFS implements HTTP proxyuser support inline in httpfs code. For HADOOP-10698 we need similar functionality for KMS. Not to duplicate code, we should refactor existing code to common. We should also leverage HADOOP-10817.",0.1,0.1,neutral
hadoop,10904,comment_7,"Please fix in this jira or subtask, to not violate URI syntax. Specifically: The ""@"" is RFC defined to separate userinfo and host:port in an authority. I've been meaning to take advantage of the userinfo for a legit purpose (well beyond the scope of this discussion) and an ""abuse"" like this will likely conflict. A more URI-friendly approach is using subschemes:",code_debt,low_quality_code,"Wed, 30 Jul 2014 18:41:06 +0000","Tue, 21 Oct 2014 00:06:05 +0000","Tue, 21 Oct 2014 00:04:53 +0000",7104227,"Please fix CredentialProvider#unnestUri, in this jira or subtask, to not violate URI syntax. Specifically: For example, ""myscheme://hdfs@nn/my/path"" is converted to ""hdfs://nn/my/path"". The ""@"" is RFC defined to separate userinfo and host:port in an authority. I've been meaning to take advantage of the userinfo for a legit purpose (well beyond the scope of this discussion) and an ""abuse"" like this will likely conflict. A more URI-friendly approach is using subschemes: ""myscheme+hdfs://nn/my/path"".",0.31525,0.1544666667,negative
hadoop,11013,comment_1,a) adds --debug option b) reworks CLASSPATH export to be consistent/safer,code_debt,low_quality_code,"Wed, 27 Aug 2014 22:53:39 +0000","Thu, 12 May 2016 18:22:47 +0000","Thu, 28 Aug 2014 17:36:37 +0000",67378,a) adds --debug option b) reworks CLASSPATH export to be consistent/safer,0.4,0.4,neutral
hadoop,11013,comment_6,"-01: * Replaced all of the if's with a function * Added more messages that fill in some of the blanks (e.g., when does get added?) * Because I'm never happy, even with my own code",code_debt,low_quality_code,"Wed, 27 Aug 2014 22:53:39 +0000","Thu, 12 May 2016 18:22:47 +0000","Thu, 28 Aug 2014 17:36:37 +0000",67378,"-01: Replaced all of the if's with a function Added more messages that fill in some of the blanks (e.g., when does HADOOP_NAMENODE_OPTS get added?) Because I'm never happy, even with my own code",-0.375,-0.375,negative
hadoop,11013,description,"As part of HADOOP-9902, java execution across many different shell bits were consolidated down to (effectively) two routines. Prior to calling those two routines, the CLASSPATH is exported. This export should really be getting handled in the exec function and not in the individual shell bits. Additionally, it would be good if there was: so that bash -x would show the content of the classpath or even a '--debug classpath' option that would echo the classpath to the screen prior to java exec to help with debugging.",code_debt,low_quality_code,"Wed, 27 Aug 2014 22:53:39 +0000","Thu, 12 May 2016 18:22:47 +0000","Thu, 28 Aug 2014 17:36:37 +0000",67378,"As part of HADOOP-9902, java execution across many different shell bits were consolidated down to (effectively) two routines. Prior to calling those two routines, the CLASSPATH is exported. This export should really be getting handled in the exec function and not in the individual shell bits. Additionally, it would be good if there was: so that bash x would show the content of the classpath or even a '-debug classpath' option that would echo the classpath to the screen prior to java exec to help with debugging.",0.1531666667,0.1531666667,neutral
hadoop,11013,summary,"CLASSPATH handling should be consolidated, debuggable",code_debt,low_quality_code,"Wed, 27 Aug 2014 22:53:39 +0000","Thu, 12 May 2016 18:22:47 +0000","Thu, 28 Aug 2014 17:36:37 +0000",67378,"CLASSPATH handling should be consolidated, debuggable",0,0,neutral
hadoop,11014,summary,Potential resource leak in due to unclosed stream,code_debt,low_quality_code,"Thu, 28 Aug 2014 00:53:52 +0000","Fri, 10 Apr 2015 20:04:29 +0000","Wed, 25 Mar 2015 08:03:03 +0000",18083351,Potential resource leak in JavaKeyStoreProvider due to unclosed stream,-0.2,-0.2,negative
hadoop,11063,description,"Windows has a maximum path length of 260 characters. KMS includes several long class file names. During packaging and creation of the distro, these paths get even longer because of prepending the standard war directory structure and our share/hadoop/etc. structure. The end result is that the final paths are longer than 260 characters, making it impossible to deploy a distro on Windows.",code_debt,low_quality_code,"Thu, 4 Sep 2014 17:13:32 +0000","Mon, 1 Dec 2014 03:09:25 +0000","Thu, 4 Sep 2014 19:12:30 +0000",7138,"Windows has a maximum path length of 260 characters. KMS includes several long class file names. During packaging and creation of the distro, these paths get even longer because of prepending the standard war directory structure and our share/hadoop/etc. structure. The end result is that the final paths are longer than 260 characters, making it impossible to deploy a distro on Windows.",0.00625,0.00625,neutral
hadoop,11063,summary,"KMS cannot deploy on Windows, because class names are too long.",code_debt,low_quality_code,"Thu, 4 Sep 2014 17:13:32 +0000","Mon, 1 Dec 2014 03:09:25 +0000","Thu, 4 Sep 2014 19:12:30 +0000",7138,"KMS cannot deploy on Windows, because class names are too long.",-0.5,-0.5,negative
hadoop,11103,summary,Clean up RemoteException,code_debt,low_quality_code,"Wed, 17 Sep 2014 19:51:26 +0000","Tue, 30 Aug 2016 01:31:55 +0000","Tue, 19 May 2015 09:13:47 +0000",21043341,Clean up RemoteException,0.4,0.4,neutral
hadoop,11117,comment_1,stack/message of little or no value,code_debt,low_quality_code,"Mon, 22 Sep 2014 20:42:40 +0000","Wed, 8 Oct 2014 16:11:45 +0000","Wed, 8 Oct 2014 16:11:45 +0000",1366145,stack/message of little or no value,0,0,negative
hadoop,11117,comment_5,"I really wish we could catch these and not throw a stack trace to figure out why/what/where it broke. From an end user perspective, these stacks are completely unfriendly.",code_debt,low_quality_code,"Mon, 22 Sep 2014 20:42:40 +0000","Wed, 8 Oct 2014 16:11:45 +0000","Wed, 8 Oct 2014 16:11:45 +0000",1366145,"I really wish we could catch these and not throw a stack trace to figure out why/what/where it broke. From an end user perspective, these stacks are completely unfriendly.",0.06575,0.06575,negative
hadoop,11117,comment_8,"Some of the test failures are spurious, the only regression appears to be These tests are failing because the test code is doing an {{assertEquals}} on the expected string; the propagation of the underlying exception message is breaking this comparison. Fix is what could have been done in the first place: use as the probe",code_debt,low_quality_code,"Mon, 22 Sep 2014 20:42:40 +0000","Wed, 8 Oct 2014 16:11:45 +0000","Wed, 8 Oct 2014 16:11:45 +0000",1366145,"Some of the test failures are spurious, the only regression appears to be TestUserGroupInformation These tests are failing because the test code is doing an assertEquals on the expected string; the propagation of the underlying exception message is breaking this comparison. Fix is what could have been done in the first place: use String.contains() as the probe",-0.2445,-0.163,negative
hadoop,11117,comment_9,patch -002 which patches the test to make it less brittle to exception strings,code_debt,low_quality_code,"Mon, 22 Sep 2014 20:42:40 +0000","Wed, 8 Oct 2014 16:11:45 +0000","Wed, 8 Oct 2014 16:11:45 +0000",1366145,patch -002 which patches the test to make it less brittle to exception strings,0,0,neutral
hadoop,11117,description,"If something is failing with kerberos login, should fail with useful information. But not all exceptions from the inner code are caught and converted to LoginException. Those exceptions that aren't wrapped have their text and stack trace lost somewhere in the javax code, leaving on the text ""login failed"" and a stack trace of no value whatsoever.",code_debt,low_quality_code,"Mon, 22 Sep 2014 20:42:40 +0000","Wed, 8 Oct 2014 16:11:45 +0000","Wed, 8 Oct 2014 16:11:45 +0000",1366145,"If something is failing with kerberos login, UserGroupInformation.loginUserFromKeytabAndReturnUGI() should fail with useful information. But not all exceptions from the inner code are caught and converted to LoginException. Those exceptions that aren't wrapped have their text and stack trace lost somewhere in the javax code, leaving on the text ""login failed"" and a stack trace of no value whatsoever.",-1.39E-17,-0.0625,negative
hadoop,11201,comment_4,"Ah, now it makes sense. In that case, I think renaming to {{justPaths}} would help clarify the code.",code_debt,low_quality_code,"Tue, 14 Oct 2014 08:39:59 +0000","Fri, 10 Apr 2015 20:04:30 +0000","Wed, 19 Nov 2014 01:13:11 +0000",3083592,"The main thing going on there is extracting paths from, potentially absolute, input URI's. Ah, now it makes sense. In that case, I think renaming to justPaths would help clarify the code.",0.2,0.1333333333,positive
hadoop,11309,comment_4,", thanks for the patch. It looks good to me. One minor nit: it might be good to enclose the last condition (the one that checks for the nested class) in parentheses for clarity:",code_debt,low_quality_code,"Sun, 16 Nov 2014 07:03:04 +0000","Fri, 10 Apr 2015 20:04:45 +0000","Tue, 18 Nov 2014 17:25:56 +0000",210172,"jira.shegalov, thanks for the patch. It looks good to me. One minor nit: it might be good to enclose the last condition (the one that checks for the nested class) in parentheses for clarity:",0.588,0.441,positive
hadoop,11309,comment_5,"Thanks, , for review! Clarity is a subjective matter. I find redundant parentheses distracting especially when indentation has already been used to emphasize the grouping of operators as in this case. I leave it to committers if they have a strong preference one way or another.",code_debt,low_quality_code,"Sun, 16 Nov 2014 07:03:04 +0000","Fri, 10 Apr 2015 20:04:45 +0000","Tue, 18 Nov 2014 17:25:56 +0000",210172,"Thanks, sjlee0, for review! Clarity is a subjective matter. I find redundant parentheses distracting especially when indentation has already been used to emphasize the grouping of operators as in this case. I leave it to committers if they have a strong preference one way or another.",0.23175,0.23175,neutral
hadoop,1130,comment_4,The closeAll() method should also remove ClientFinalizer from shutdown hooks to avoid the ClassLoader (so all Class(es) loaded by the ClassLoader) that have loaded hadoop being kept in memory.,code_debt,low_quality_code,"Mon, 19 Mar 2007 13:57:16 +0000","Wed, 8 Jul 2009 16:42:21 +0000","Tue, 13 Nov 2007 08:17:42 +0000",20629226,The closeAll() method should also remove ClientFinalizer from shutdown hooks to avoid the ClassLoader (so all Class(es) loaded by the ClassLoader) that have loaded hadoop being kept in memory.,-0.2,-0.2,neutral
hadoop,1130,description,The ClientFinalizer shutdown hook is not used. This can lead to severe memory leaks if you use the DFSClient in a dynamic class loading context (such as in a webapp) as the class is retained in the memory by the shutdown hook. It retains also the current ClassLoader and thus all classes loaded by the ClassLoader. (Threads put in the shutdown hook are never garbage collected).,code_debt,low_quality_code,"Mon, 19 Mar 2007 13:57:16 +0000","Wed, 8 Jul 2009 16:42:21 +0000","Tue, 13 Nov 2007 08:17:42 +0000",20629226,The ClientFinalizer shutdown hook is not used. This can lead to severe memory leaks if you use the DFSClient in a dynamic class loading context (such as in a webapp) as the DFSClient.ClientFinalizer class is retained in the memory by the shutdown hook. It retains also the current ClassLoader and thus all classes loaded by the ClassLoader. (Threads put in the shutdown hook are never garbage collected).,0.0625,0.06,neutral
hadoop,1130,summary,Remove unused ClientFinalizer in DFSClient,code_debt,dead_code,"Mon, 19 Mar 2007 13:57:16 +0000","Wed, 8 Jul 2009 16:42:21 +0000","Tue, 13 Nov 2007 08:17:42 +0000",20629226,Remove unused ClientFinalizer in DFSClient,0,0,neutral
hadoop,11355,comment_2,"+1 LGTM, small nit for the future, you can use when checking exception text. Will commit shortly, thanks Arun.",code_debt,low_quality_code,"Fri, 5 Dec 2014 18:05:17 +0000","Fri, 24 Apr 2015 22:49:00 +0000","Fri, 5 Dec 2014 20:02:02 +0000",7005,"+1 LGTM, small nit for the future, you can use GenericTestUtils.assertExceptionContains when checking exception text. Will commit shortly, thanks Arun.",0.15,0.1,positive
hadoop,11379,comment_0,In this patch I fixed warnings raised by findbugs 3 against hadoop-auth and They're all encoding related warnings.,code_debt,low_quality_code,"Tue, 9 Dec 2014 19:07:55 +0000","Fri, 24 Apr 2015 22:48:55 +0000","Tue, 9 Dec 2014 21:09:20 +0000",7285,In this patch I fixed warnings raised by findbugs 3 against hadoop-auth and hadoop-auth-examples. They're all encoding related warnings.,-0.6,-0.6,neutral
hadoop,11379,description,"When locally run findbugs 3.0, there are new warnings generated. This Jira aims to address the new warnings in hadoop-auth and",code_debt,low_quality_code,"Tue, 9 Dec 2014 19:07:55 +0000","Fri, 24 Apr 2015 22:48:55 +0000","Tue, 9 Dec 2014 21:09:20 +0000",7285,"When locally run findbugs 3.0, there are new warnings generated. This Jira aims to address the new warnings in hadoop-auth and hadoop-auth-examples.",-0.6,-0.6,neutral
hadoop,11379,summary,Fix new findbugs warnings in hadoop-auth*,code_debt,low_quality_code,"Tue, 9 Dec 2014 19:07:55 +0000","Fri, 24 Apr 2015 22:48:55 +0000","Tue, 9 Dec 2014 21:09:20 +0000",7285,Fix new findbugs warnings in hadoop-auth*,-0.6,-0.6,neutral
hadoop,11384,description,"When locally run findbugs 3.0, there are new warnings generated. This Jira aims to address the new warnings in hadoop-openstack.",code_debt,low_quality_code,"Tue, 9 Dec 2014 21:25:17 +0000","Wed, 10 Dec 2014 00:53:37 +0000","Wed, 10 Dec 2014 00:53:37 +0000",12500,"When locally run findbugs 3.0, there are new warnings generated. This Jira aims to address the new warnings in hadoop-openstack.",-0.6,-0.6,neutral
hadoop,11384,summary,Fix new findbugs warnings in hadoop-openstack,code_debt,low_quality_code,"Tue, 9 Dec 2014 21:25:17 +0000","Wed, 10 Dec 2014 00:53:37 +0000","Wed, 10 Dec 2014 00:53:37 +0000",12500,Fix new findbugs warnings in hadoop-openstack,-0.6,-0.6,negative
hadoop,11421,comment_0,"Thanks for working on this Colin. It'll be nice to swap this in where we can, JDK7 does a much better job at exposing filesystem APIs. I wonder if we should really return a ChunkedArrayList here. It only implements a subset of the AbstractList interface, and this is a pretty general-purpose method. For huge dirs, we should probably just be using the DirectoryStream iterator directly. I do see the use of these helper functions for quick-and-dirty listings though. I'd be okay providing variants of these functions that return a ChunkedArrayList, but it seems like the default should just be a normal ArrayList. Couple other things: * Need {{<p/* I read the docs at and it'd be nice to do like the example and unwrap the into an IOException.",code_debt,low_quality_code,"Wed, 17 Dec 2014 19:06:07 +0000","Fri, 10 Apr 2015 20:04:24 +0000","Wed, 17 Dec 2014 23:18:56 +0000",15169,"Thanks for working on this Colin. It'll be nice to swap this in where we can, JDK7 does a much better job at exposing filesystem APIs. I wonder if we should really return a ChunkedArrayList here. It only implements a subset of the AbstractList interface, and this is a pretty general-purpose method. For huge dirs, we should probably just be using the DirectoryStream iterator directly. I do see the use of these helper functions for quick-and-dirty listings though. I'd be okay providing variants of these functions that return a ChunkedArrayList, but it seems like the default should just be a normal ArrayList. Couple other things: Need <p/> tag for javadoc linebreak I read the docs at http://docs.oracle.com/javase/7/docs/api/java/nio/file/DirectoryStream.html and it'd be nice to do like the example and unwrap the DirectoryIteratorException into an IOException.",0.373625,0.373625,positive
hadoop,11421,comment_2,"I think maybe later will become more general-purpose. But you're right; for now, we better use {{ArrayList}}. ok Yeah, that's important... io errors should result in io exceptions. Looks like is a probably in order to conform to the {{Iterator}} interface. I removed the variant that returns a list of File, since I found that the JDK6 file listing interfaces actually returned an array of String, so returning a list of String is compatible-ish.",code_debt,low_quality_code,"Wed, 17 Dec 2014 19:06:07 +0000","Fri, 10 Apr 2015 20:04:24 +0000","Wed, 17 Dec 2014 23:18:56 +0000",15169,"I wonder if we should really return a ChunkedArrayList here. It only implements a subset of the AbstractList interface, and this is a pretty general-purpose method. For huge dirs, we should probably just be using the DirectoryStream iterator directly. I do see the use of these helper functions for quick-and-dirty listings though. I think maybe later ChunkedArrayList will become more general-purpose. But you're right; for now, we better use ArrayList. Need <p/> tag for javadoc linebreak ok I read the docs at http://docs.oracle.com/javase/7/docs/api/java/nio/file/DirectoryStream.html and it'd be nice to do like the example and unwrap the DirectoryIteratorException into an IOException. Yeah, that's important... io errors should result in io exceptions. Looks like DirectoryIteratorException is a RuntimeException... probably in order to conform to the Iterator interface. I removed the variant that returns a list of File, since I found that the JDK6 file listing interfaces actually returned an array of String, so returning a list of String is compatible-ish.",0.3752,0.3625666667,neutral
hadoop,11421,description,"We should have a drop-in replacement for File#listDir that doesn't hide IOExceptions, and which returns a ChunkedArrayList rather than a single large array.",code_debt,low_quality_code,"Wed, 17 Dec 2014 19:06:07 +0000","Fri, 10 Apr 2015 20:04:24 +0000","Wed, 17 Dec 2014 23:18:56 +0000",15169,"We should have a drop-in replacement for File#listDir that doesn't hide IOExceptions, and which returns a ChunkedArrayList rather than a single large array.",0,0,neutral
hadoop,11544,comment_2,Thanks for the comment . I removed the entry of from core-default.xml because the default (NeverSampler) is defined in I would like to left the description in Tracing.apt.vm because users still can use for setting trace sampler though the config key should be specified as + rather than in java code.,code_debt,low_quality_code,"Wed, 4 Feb 2015 02:53:38 +0000","Fri, 10 Apr 2015 20:04:43 +0000","Wed, 4 Feb 2015 12:22:06 +0000",34108,"Thanks for the comment ajisakaa. I removed the entry of ""hadoop.htrace.sampler"" from core-default.xml because the default (NeverSampler) is defined in org.apache.htrace.SamplerBuilder. I would like to left the description in Tracing.apt.vm because users still can use ""hadoop.htrace.sampler"" for setting trace sampler though the config key should be specified as TraceUtils.HTRACE_CONF_PREFIX + SamplerBuilder.SAMPLER_CONF_KEY rather than CommonConfigurationKeys.HADOOP_TRACE_SAMPLER in java code.",0.072625,0.07873333333,neutral
hadoop,11544,description,are no longer used.,code_debt,low_quality_code,"Wed, 4 Feb 2015 02:53:38 +0000","Fri, 10 Apr 2015 20:04:43 +0000","Wed, 4 Feb 2015 12:22:06 +0000",34108,CommonConfigurationKeys.HADOOP_TRACE_SAMPLER* are no longer used.,0,0,neutral
hadoop,11544,summary,Remove unused configuration keys for tracing,code_debt,low_quality_code,"Wed, 4 Feb 2015 02:53:38 +0000","Fri, 10 Apr 2015 20:04:43 +0000","Wed, 4 Feb 2015 12:22:06 +0000",34108,Remove unused configuration keys for tracing,0,0,neutral
hadoop,11607,comment_1,"+1 Note that as s3a uses SLF4J for its log API, it can switch to {{log.info(""item {}"", value)}} for terser/more efficient logging logic (we still recommend wrapping debug statements though, just to skip string creation. If you switch to that mode, I'd be even happier. But I'm OK with what you've done so far",code_debt,low_quality_code,"Tue, 17 Feb 2015 21:30:07 +0000","Fri, 10 Apr 2015 20:04:36 +0000","Fri, 20 Feb 2015 21:59:09 +0000",260942,"+1 Note that as s3a uses SLF4J for its log API, it can switch to log.info(""item {}"", value) for terser/more efficient logging logic (we still recommend wrapping debug statements though, just to skip string creation. If you switch to that mode, I'd be even happier. But I'm OK with what you've done so far",0.272625,0.272625,positive
hadoop,11607,description,"{{S3AFileSystem}} generates INFO level logs in {{open}} and {{rename}}, which are not necessary.",code_debt,low_quality_code,"Tue, 17 Feb 2015 21:30:07 +0000","Fri, 10 Apr 2015 20:04:36 +0000","Fri, 20 Feb 2015 21:59:09 +0000",260942,"S3AFileSystem generates INFO level logs in open and rename, which are not necessary.",0.5,0.5,neutral
hadoop,11607,summary,Reduce log spew in S3AFileSystem,code_debt,low_quality_code,"Tue, 17 Feb 2015 21:30:07 +0000","Fri, 10 Apr 2015 20:04:36 +0000","Fri, 20 Feb 2015 21:59:09 +0000",260942,Reduce log spew in S3AFileSystem,0,0,neutral
hadoop,11658,comment_2,Thanks  for the report and the patch! Two comments: * Can we use in * Some inserted lines are longer than 80 characters. Would you render these?,code_debt,low_quality_code,"Mon, 2 Mar 2015 05:47:26 +0000","Fri, 10 Apr 2015 20:04:33 +0000","Mon, 2 Mar 2015 09:15:54 +0000",12508,Thanks drankye for the report and the patch! Two comments: Can we use IO_COMPRESSION_CODECS_KEY in TestCodecFactory.java? Some inserted lines are longer than 80 characters. Would you render these?,0.1333333333,0.18,positive
hadoop,11690,description,There appears to be an opportunity of code reduction by re-implementing Groups to use the,code_debt,duplicated_code,"Sun, 8 Mar 2015 23:27:19 +0000","Sat, 1 Sep 2018 20:32:21 +0000","Sat, 1 Sep 2018 20:32:21 +0000",109976702,There appears to be an opportunity of code reduction by re-implementing Groups to use the IdMappingServiceProvider.,0.4,0.4,neutral
hadoop,11740,comment_0,"This initial patch simply removes {{ErasureEncoder}} and {{ErasureDecoder}}. I think the following further simplifications are possible: # We can get rid of {{ErasureCoder}} since it has a single subclass now # Similarly, maybe we can get rid of since provides enough abstraction anyway # If {{ECBlockGroup}} can provide erased indices, we can further combine encoding and decoding classes",code_debt,low_quality_code,"Mon, 23 Mar 2015 20:10:41 +0000","Wed, 30 Sep 2015 19:38:02 +0000","Fri, 3 Apr 2015 22:23:49 +0000",958388,"This initial patch simply removes ErasureEncoder and ErasureDecoder. I think the following further simplifications are possible: We can get rid of ErasureCoder since it has a single subclass now (AbstractErasureCoder Similarly, maybe we can get rid of ErasureCodingStep since AbstractErasureCodingStep provides enough abstraction anyway If ECBlockGroup can provide erased indices, we can further combine encoding and decoding classes",0.375,0.375,neutral
hadoop,11740,comment_1,"Thanks  for the good thoughts. Yes it's often a good question to think about either interface or abstract class in Java. My feeling is that if it's in a framework, subject to be pluggable and implemented by customers, an interface would be good to have. So I guess we could keep {{ErasureCoder}} interface, and convert interface to a class. I'm not sure, as erased indices have to be be computed according to input blocks and output blocks just for decoders, and encoders don't have the related logics. Currently in RS coder the decoding is rather simple but I believe it will be much complicated for codes like LRC and Hitchhiker, so separating encoding class and decoding class is desired.",code_debt,low_quality_code,"Mon, 23 Mar 2015 20:10:41 +0000","Wed, 30 Sep 2015 19:38:02 +0000","Fri, 3 Apr 2015 22:23:49 +0000",958388,"Thanks zhz for the good thoughts. We can get rid of ErasureCoder since it has a single subclass now (AbstractErasureCoder) Yes it's often a good question to think about either interface or abstract class in Java. My feeling is that if it's in a framework, subject to be pluggable and implemented by customers, an interface would be good to have. So I guess we could keep ErasureCoder interface, and convert ErasureCodingStep interface to a class. If ECBlockGroup can provide erased indices, we can further combine encoding and decoding classes I'm not sure, as erased indices have to be be computed according to input blocks and output blocks just for decoders, and encoders don't have the related logics. Currently in RS coder the decoding is rather simple but I believe it will be much complicated for codes like LRC and Hitchhiker, so separating encoding class and decoding class is desired.",0.3328888889,0.3679583333,positive
hadoop,11740,comment_7,"Thanks for the update. I looked the new patch, just two minor comments: 1. In the test codes, may be better to use {{ErasureCoder}} instead of or since the interface type is good enough, which is why we're here. With this refining, from caller's point of view, nothing different from between encoder and decoder, so it should use the common interface. 2. Those unnecessary Javadoc are there to conform Javadoc conventions and format. In future someone may fill them. I suggest we don't remove them, you can find so many in the project.",code_debt,low_quality_code,"Mon, 23 Mar 2015 20:10:41 +0000","Wed, 30 Sep 2015 19:38:02 +0000","Fri, 3 Apr 2015 22:23:49 +0000",958388,"Thanks for the update. I looked the new patch, just two minor comments: 1. In the test codes, may be better to use ErasureCoder instead of AbstractErasureEncoder or AbstractErasureDecoder since the interface type is good enough, which is why we're here. With this refining, from caller's point of view, nothing different from between encoder and decoder, so it should use the common interface. 2. Those unnecessary Javadoc are there to conform Javadoc conventions and format. In future someone may fill them. I suggest we don't remove them, you can find so many in the project.",0.14375,0.14375,neutral
hadoop,11837,comment_4,LGTM. Minor nits: Change to +1 once addressed.,code_debt,low_quality_code,"Wed, 15 Apr 2015 19:43:59 +0000","Fri, 24 Apr 2015 22:49:17 +0000","Fri, 17 Apr 2015 18:00:35 +0000",166596,LGTM. Minor nits: Change to +1 once addressed.,0.25,0.25,neutral
hadoop,11880,comment_1,"Looking at the tests, turns out some tests do expect hard-coded paths for minidfs cluster, so the cluster comes back up in the same run this is going to be fun. Either revert with a hard-coded path and expect parallel tests to fail, or extend the dfs builder to add an operation to set the subdir, which would be retained over a test case/test suite. The revert is the short-term option, but mini dfs will need to be fixed for reliable hdfs test runs",code_debt,low_quality_code,"Mon, 27 Apr 2015 14:52:15 +0000","Fri, 1 Sep 2017 15:22:29 +0000","Fri, 1 Sep 2017 15:22:29 +0000",74133014,"Looking at the tests, turns out some tests do expect hard-coded paths for minidfs cluster, so the cluster comes back up in the same run this is going to be fun. Either revert with a hard-coded path and expect parallel tests to fail, or extend the dfs builder to add an operation to set the subdir, which would be retained over a test case/test suite. The revert is the short-term option, but mini dfs will need to be fixed for reliable hdfs test runs",0.3443333333,0.3443333333,positive
hadoop,1192,comment_0,The checksum patch did not overwrite the method getContentLength in This leads to the use of the default slower version of getContentLength defined in FileSystem. This patch fixed the du problem. It also fixed the dus problem by declaring the totalSize to be long.,code_debt,slow_algorithm,"Mon, 2 Apr 2007 20:42:22 +0000","Wed, 8 Jul 2009 16:42:24 +0000","Tue, 3 Apr 2007 18:17:31 +0000",77709,The checksum patch did not overwrite the method getContentLength in DistributedFileSystem. This leads to the use of the default slower version of getContentLength defined in FileSystem. This patch fixed the du problem. It also fixed the dus problem by declaring the totalSize to be long.,-0.2666666667,-0.2,neutral
hadoop,1192,comment_1,"+1 Code looks fine. While reviewing this I noticed that FsShell.dus() seem to be doing one level recursion into the paths it wants to du. I would have thought we either need not recurse at all or to recurse recurse fully. This patch does not change this logic. Hairong is taking a look at this. If there is some issue here (either correctness or just code efficiency), that could be a different jira.",code_debt,complex_code,"Mon, 2 Apr 2007 20:42:22 +0000","Wed, 8 Jul 2009 16:42:24 +0000","Tue, 3 Apr 2007 18:17:31 +0000",77709,"+1 Code looks fine. While reviewing this I noticed that FsShell.dus() seem to be doing one level recursion into the paths it wants to du. I would have thought we either need not recurse at all or to recurse recurse fully. This patch does not change this logic. Hairong is taking a look at this. If there is some issue here (either correctness or just code efficiency), that could be a different jira.",0.2804285714,0.2804285714,neutral
hadoop,1192,description,"After installing 0.12.2, we notice that the du command takes a much longer time to execute than the previous release. Another problem is that dus prints a negative value.",code_debt,slow_algorithm,"Mon, 2 Apr 2007 20:42:22 +0000","Wed, 8 Jul 2009 16:42:24 +0000","Tue, 3 Apr 2007 18:17:31 +0000",77709,"After installing 0.12.2, we notice that the du command takes a much longer time to execute than the previous release. Another problem is that dus prints a negative value.",-0.2,-0.2,negative
hadoop,1192,summary,Du command takes a noticable longer time to execute on a large dfs than the 0.11 release,code_debt,slow_algorithm,"Mon, 2 Apr 2007 20:42:22 +0000","Wed, 8 Jul 2009 16:42:24 +0000","Tue, 3 Apr 2007 18:17:31 +0000",77709,Du command takes a noticable longer time to execute on a large dfs than the 0.11 release,0,0,neutral
hadoop,11966,description,"HADOOP-11464 reinstated support for running the bash scripts through Cygwin. The logic involves setting a {{cygwin}} flag variable to indicate if the script is executing through Cygwin. The flag is set in all of the interactive scripts: {{hadoop}}, {{hdfs}}, {{yarn}} and {{mapred}}. The flag is not set through hadoop-daemon.sh though. This can cause an erroneous overwrite of {{HADOOP_HOME}} and inside hadoop-config.sh.",code_debt,low_quality_code,"Tue, 12 May 2015 23:08:26 +0000","Fri, 6 Jan 2017 00:48:56 +0000","Wed, 13 May 2015 19:27:51 +0000",73165,"HADOOP-11464 reinstated support for running the bash scripts through Cygwin. The logic involves setting a cygwin flag variable to indicate if the script is executing through Cygwin. The flag is set in all of the interactive scripts: hadoop, hdfs, yarn and mapred. The flag is not set through hadoop-daemon.sh though. This can cause an erroneous overwrite of HADOOP_HOME and JAVA_LIBRARY_PATH inside hadoop-config.sh.",-0.07142857143,-0.07142857143,neutral
hadoop,12002,comment_3,"It'd be better to loop over all the executables and print out all of them that aren't found, then give the -1 and return. Right now, if someone is missing several of them they'll probably have to run through multiple times.",code_debt,low_quality_code,"Tue, 19 May 2015 22:55:18 +0000","Thu, 5 Nov 2015 22:39:39 +0000","Thu, 5 Nov 2015 22:39:39 +0000",14687061,"It'd be better to loop over all the executables and print out all of them that aren't found, then give the -1 and return. Right now, if someone is missing several of them they'll probably have to run through multiple times.",0.28175,0.28175,neutral
hadoop,12002,comment_4,"Thanks , I revised the patch. -02 * cover Sean's feedback * make temporary variables in for loops local * examine the executable paths with -f, not only -x",code_debt,low_quality_code,"Tue, 19 May 2015 22:55:18 +0000","Thu, 5 Nov 2015 22:39:39 +0000","Thu, 5 Nov 2015 22:39:39 +0000",14687061,"Thanks busbey, I revised the patch. -02 cover Sean's feedback make temporary variables in for loops local examine the executable paths with -f, not only -x",0.5125,0.5125,neutral
hadoop,12002,comment_7,"* Generate a -1 jira table for every missing executable. This simplifies the code and gets rid of a lot of excess variables. * We can drop the extra line feed, so just just use echo instead of the more complex printf * This isn't java, we don't need camelCase or really long names. ;) (e.g., findbugsExecutable) [Yes, i recognize I didn't purge them out the first time, but we should make the effort to get rid of them when we can.] * Remove the extra check: -x should fail if the file doesn't exist.",code_debt,complex_code,"Tue, 19 May 2015 22:55:18 +0000","Thu, 5 Nov 2015 22:39:39 +0000","Thu, 5 Nov 2015 22:39:39 +0000",14687061,"Generate a -1 jira table for every missing executable. This simplifies the code and gets rid of a lot of excess variables. We can drop the extra line feed, so just just use echo instead of the more complex printf This isn't java, we don't need camelCase or really long names. (e.g., findbugsExecutable) [Yes, i recognize I didn't purge them out the first time, but we should make the effort to get rid of them when we can.] Remove the extra check: -x should fail if the file doesn't exist.",0.0325,0.0125,neutral
hadoop,12135,comment_1,"-00: * removed hadoop hard-codes * added a flag to turn on/off asf license * supports multiple projects, but only as a single file * support for ranges of versions * more of my inability to write python",code_debt,low_quality_code,"Sat, 27 Jun 2015 16:04:02 +0000","Tue, 30 Aug 2016 01:26:02 +0000","Mon, 6 Jul 2015 22:49:38 +0000",801936,"-00: removed hadoop hard-codes added a flag to turn on/off asf license supports multiple projects, but only as a single file support for ranges of versions more of my inability to write python",0.025,0.025,negative
hadoop,12135,comment_5,+1 we can make things more pythonic in follow-ons.,code_debt,low_quality_code,"Sat, 27 Jun 2015 16:04:02 +0000","Tue, 30 Aug 2016 01:26:02 +0000","Mon, 6 Jul 2015 22:49:38 +0000",801936,+1 we can make things more pythonic in follow-ons.,0,0,neutral
hadoop,12135,summary,cleanup releasedocmaker,code_debt,dead_code,"Sat, 27 Jun 2015 16:04:02 +0000","Tue, 30 Aug 2016 01:26:02 +0000","Mon, 6 Jul 2015 22:49:38 +0000",801936,cleanup releasedocmaker,0,0,neutral
hadoop,12268,comment_2,"Nice catch, ! The fix looks good to me. Two comments: 1. Would you remove unused imports in and 2. Would you please fix the following javadoc comment? ""concat"" should be ""append"".",code_debt,low_quality_code,"Fri, 24 Jul 2015 07:54:01 +0000","Tue, 30 Aug 2016 01:24:49 +0000","Sat, 1 Aug 2015 04:59:35 +0000",680734,"Nice catch, zxu! The fix looks good to me. Two comments: 1. Would you remove unused imports in TestHDFSContractAppend and AbstractContractAppendTest? 2. Would you please fix the following javadoc comment? ""concat"" should be ""append"".",0.3001666667,0.2572857143,positive
hadoop,12368,description,"These are test base classes that need to be subclassed to run, can mark as abstract.",code_debt,low_quality_code,"Mon, 31 Aug 2015 23:44:54 +0000","Tue, 30 Aug 2016 01:23:38 +0000","Tue, 1 Sep 2015 01:19:41 +0000",5687,"These are test base classes that need to be subclassed to run, can mark as abstract.",0,0,neutral
hadoop,12460,comment_2,"Corrected the checkstyle errors and white spaces & Attached the patch, Please review",code_debt,low_quality_code,"Tue, 6 Oct 2015 14:07:39 +0000","Tue, 30 Aug 2016 01:22:43 +0000","Mon, 19 Oct 2015 06:47:59 +0000",1096820,"Corrected the checkstyle errors and white spaces & Attached the patch, Please review",-0.1,-0.1,neutral
hadoop,12484,comment_11,"This part is triggering a Checkstyle warning: We can clean that up by changing to: I realize there are existing instances of empty statements like this in the class, but let's avoid introducing new instances. The mvninstall failure appears to be a side effect of something in the bats testing of the bash scripts. It's unrelated to this patch. Since you need to upload one more patch revision to address the Checkstyle warning, let's do one more test run and see if it happens again. If it does, then I'll follow up. The license check warning is caused by a test copying a file to a location that it shouldn't, which is then covered by the license check. I'll follow up separately on that.",code_debt,low_quality_code,"Thu, 15 Oct 2015 22:32:28 +0000","Tue, 30 Aug 2016 01:22:18 +0000","Thu, 22 Oct 2015 21:32:55 +0000",601227,"This part is triggering a Checkstyle warning: We can clean that up by changing to: I realize there are existing instances of empty statements like this in the NativeAzureFileSystem class, but let's avoid introducing new instances. The mvninstall failure appears to be a side effect of something in the bats testing of the bash scripts. It's unrelated to this patch. Since you need to upload one more patch revision to address the Checkstyle warning, let's do one more test run and see if it happens again. If it does, then I'll follow up. The license check warning is caused by a test copying a file to a location that it shouldn't, which is then covered by the license check. I'll follow up separately on that.",-0.2571428571,-0.2571428571,negative
hadoop,12484,comment_13,"This looks good overall, aside from the remaining whitespace and Checkstyle problems. I'm attaching patch v06, which is the same thing with the whitespace and Checkstyle warnings fixed.",code_debt,low_quality_code,"Thu, 15 Oct 2015 22:32:28 +0000","Tue, 30 Aug 2016 01:22:18 +0000","Thu, 22 Oct 2015 21:32:55 +0000",601227,"This looks good overall, aside from the remaining whitespace and Checkstyle problems. I'm attaching patch v06, which is the same thing with the whitespace and Checkstyle warnings fixed.",-0.206,-0.206,positive
hadoop,12484,comment_14,Thanks a lot Chris. Apologise for the whitespace and checkstyle issues; I was about to put in a new patch myself but since you have already done it I will let Jenkins run again. What are the next steps here after this? Is any further action required from my end?,code_debt,low_quality_code,"Thu, 15 Oct 2015 22:32:28 +0000","Tue, 30 Aug 2016 01:22:18 +0000","Thu, 22 Oct 2015 21:32:55 +0000",601227,Thanks a lot Chris. Apologise for the whitespace and checkstyle issues; I was about to put in a new patch myself but since you have already done it I will let Jenkins run again. What are the next steps here after this? Is any further action required from my end?,0.26875,0.26875,neutral
hadoop,12484,comment_4,", there is one more problem here: At this point, it's possible that {{lease}} is still {{null}} if the earlier {{acquireLease}} call threw an exception. I recommend checking this for null before calling {{free}}. Otherwise, it will cause a",code_debt,low_quality_code,"Thu, 15 Oct 2015 22:32:28 +0000","Tue, 30 Aug 2016 01:22:18 +0000","Thu, 22 Oct 2015 21:32:55 +0000",601227,"gouravk, there is one more problem here: At this point, it's possible that lease is still null if the earlier acquireLease call threw an exception. I recommend checking this for null before calling free. Otherwise, it will cause a NullPointerException.",0.08016666667,0.08016666667,negative
hadoop,12505,comment_10,"Yes, because it means unpredictable behavior. Unpredictable behavior almost always turns into a security hole. It's trivial to construct a group that turns into ../.. (or whatever) in the path structure if I'm interpreting the output of hadoop fs -ls. That's very very bad. (that said: it'd be an awesome crack. Change the default user's group and watch everyone nuke their own files...) The NFS folks added some code to do it, but didn't really integrate it correctly. Expedience always trumps correctness. :(",code_debt,low_quality_code,"Fri, 23 Oct 2015 16:08:23 +0000","Mon, 28 Mar 2016 16:48:44 +0000","Mon, 28 Mar 2016 16:16:19 +0000",13565276,"I'm curious then about what is your stance on JniBasedUnixGroupsMapping. Do you see it as a bug that it works correctly with non-Unix-compliant names? Yes, because it means unpredictable behavior. Unpredictable behavior almost always turns into a security hole. It's trivial to construct a group that turns into ../.. (or whatever) in the path structure if I'm interpreting the output of hadoop fs -ls. That's very very bad. (that said: it'd be an awesome crack. Change the default user's group and watch everyone nuke their own files...) In Hadoop, we don't have access to a canonical UID/GID, The NFS folks added some code to do it, but didn't really integrate it correctly. Expedience always trumps correctness.",-0.068,0.0188,negative
hadoop,12505,comment_5,"The Hadoop code itself does not consistently enforce POSIX compliance for user names or group names. It's more a function of the hosting OS. For example, Harsh has pointed out that for operators using the JNI-based implementation instead of shell-based, Hadoop users already can show up with membership in groups that don't have a POSIX-compliant name. I've seen that for Windows deployments, and it sounds like the use case here was a Linux deployment connected to Active Directory. The group mapping alone is not sufficient to enforce POSIX compliance either. This is only used to populate the user's set of groups. It does not control other input paths for group names. For example, WebHDFS and Java API calls can accept names with spaces. Here is an example, tested on Mac. This kind of data of course complicates parsing of shell output, and I imagine many operators would prefer to enforce POSIX-compliant names by policy. However, I don't believe Hadoop has taken responsibility for that enforcement. I don't think the existing implementation of really provides any POSIX compliance benefits, at least not intentionally. In the example given, it would split the group ""Domain Users"" on spaces and decide to put the user into 2 groups: ""Domain"" and ""Users"". While those split names don't have spaces, they're also still not POSIX compliant because of the capital letters, and more importantly, they're completely erroneous. Hopefully the split isn't putting anyone into a real group where they don't really belong. I see this as a bug rather than a POSIX compliance feature. I would prefer to see the -1 lifted and have the bug fixed. That said, I also see it as low priority, since the majority of deployments I see use the JNI-based implementation now, which does not have the bug.",code_debt,low_quality_code,"Fri, 23 Oct 2015 16:08:23 +0000","Mon, 28 Mar 2016 16:48:44 +0000","Mon, 28 Mar 2016 16:16:19 +0000",13565276,"The Hadoop code itself does not consistently enforce POSIX compliance for user names or group names. It's more a function of the hosting OS. For example, Harsh has pointed out that for operators using the JNI-based implementation instead of shell-based, Hadoop users already can show up with membership in groups that don't have a POSIX-compliant name. I've seen that for Windows deployments, and it sounds like the use case here was a Linux deployment connected to Active Directory. The group mapping alone is not sufficient to enforce POSIX compliance either. This is only used to populate the user's set of groups. It does not control other input paths for group names. For example, WebHDFS and Java FileSystem#setOwner API calls can accept names with spaces. Here is an example, tested on Mac. This kind of data of course complicates parsing of shell output, and I imagine many operators would prefer to enforce POSIX-compliant names by policy. However, I don't believe Hadoop has taken responsibility for that enforcement. I don't think the existing implementation of ShellBasedUnixGroupsMapping really provides any POSIX compliance benefits, at least not intentionally. In the example given, it would split the group ""Domain Users"" on spaces and decide to put the user into 2 groups: ""Domain"" and ""Users"". While those split names don't have spaces, they're also still not POSIX compliant because of the capital letters, and more importantly, they're completely erroneous. Hopefully the split isn't putting anyone into a real group where they don't really belong. I see this as a bug rather than a POSIX compliance feature. I would prefer to see the -1 lifted and have the bug fixed. That said, I also see it as low priority, since the majority of deployments I see use the JNI-based implementation now, which does not have the bug.",-0.02106944444,-0.02106944444,neutral
hadoop,1254,comment_1,"TestCheckpoint has one case where it creates a MiniDFSCluster but doesn't wait for it to be active. I'll file a patch for this. I wonder if this started showing up due to some new speed up or slow down in starting a NameNode and/or a DataNode, perhaps introduced by HADOOP-971 or HADOOP-1189...",code_debt,slow_algorithm,"Thu, 12 Apr 2007 17:50:45 +0000","Wed, 8 Jul 2009 16:42:25 +0000","Fri, 13 Apr 2007 17:43:25 +0000",85960,"TestCheckpoint has one case where it creates a MiniDFSCluster but doesn't wait for it to be active. I'll file a patch for this. I wonder if this started showing up due to some new speed up or slow down in starting a NameNode and/or a DataNode, perhaps introduced by HADOOP-971 or HADOOP-1189...",0.0235,0.0235,negative
hadoop,12701,comment_0,"After the fix, mvn for entire Hadoop tree took 01:27 min on my Macbook Pro. In comparison, it took 38.677 s before.",code_debt,slow_algorithm,"Mon, 11 Jan 2016 01:03:40 +0000","Tue, 30 Aug 2016 01:20:10 +0000","Sat, 7 May 2016 00:21:36 +0000",10106276,"After the fix, mvn checkstyle:checkstyle for entire Hadoop tree took 01:27 min on my Macbook Pro. In comparison, it took 38.677 s before.",0,0,neutral
hadoop,12701,description,Test source files are not checked by checkstyle because Maven checkstyle plugin parameter is *false* by default. Propose to enable checkstyle on test source files in order to improve the quality of unit tests.,code_debt,low_quality_code,"Mon, 11 Jan 2016 01:03:40 +0000","Tue, 30 Aug 2016 01:20:10 +0000","Sat, 7 May 2016 00:21:36 +0000",10106276,Test source files are not checked by checkstyle because Maven checkstyle plugin parameter includeTestSourceDirectory is false by default. Propose to enable checkstyle on test source files in order to improve the quality of unit tests.,0.0625,0.0625,neutral
hadoop,12721,comment_2,"-1 1) Adding hadoop-tools to the default path is going to break all sorts of user-level things due to the amount of transitive dependencies. 2) This will *greatly* impact the startup of time of commands by including a bunch of class files that will never get used. Instead of slamming in everything, users should be using shell profiles to include the jars they actually need. Another thing that would be good to do is to break up the hadoop tools dir to be per component.",code_debt,low_quality_code,"Mon, 18 Jan 2016 08:37:07 +0000","Mon, 5 Dec 2016 15:49:43 +0000","Wed, 23 Mar 2016 20:50:30 +0000",5660003,"-1 1) Adding hadoop-tools to the default path is going to break all sorts of user-level things due to the amount of transitive dependencies. 2) This will greatly impact the startup of time of commands by including a bunch of class files that will never get used. A simple solution is to add those jars into the classpath of the cmds. Suggestions are welcomed~ Instead of slamming in everything, users should be using shell profiles to include the jars they actually need. Another thing that would be good to do is to break up the hadoop tools dir to be per component.",0.194,0.2752,negative
hadoop,12733,comment_2,"RE: ASF license warnings This looks like a bunch of generated files, not checked in files.",code_debt,low_quality_code,"Sat, 23 Jan 2016 07:16:45 +0000","Wed, 4 Jan 2017 23:04:55 +0000","Wed, 4 Jan 2017 05:21:00 +0000",29973855,"RE: ASF license warnings This looks like a bunch of generated files, not checked in files.",-0.6,-0.6,negative
hadoop,12733,description,The following variables appear to no longer be used.,code_debt,dead_code,"Sat, 23 Jan 2016 07:16:45 +0000","Wed, 4 Jan 2017 23:04:55 +0000","Wed, 4 Jan 2017 05:21:00 +0000",29973855,The following variables appear to no longer be used. io.seqfile.lazydecompress io.seqfile.sorter.recordlimit,0,0,neutral
hadoop,12829,comment_1,"Thank you, . I can't think of any reason why this thread should swallow without a trace. It is not performing any operations that should inherently generate INE, as far as I can see, and if someone else sends an INE we ought to... interrupt the thread. Can you include the stack trace so that this exception has a better chance of getting noticed? Also, capitalize the error? +1 pending those changes",code_debt,low_quality_code,"Sat, 20 Feb 2016 01:19:42 +0000","Mon, 16 Dec 2019 10:04:45 +0000","Tue, 23 Feb 2016 19:33:42 +0000",324840,"Thank you, gchanan. I can't think of any reason why this thread should swallow InterruptedException without a trace. It is not performing any operations that should inherently generate INE, as far as I can see, and if someone else sends an INE we ought to... interrupt the thread. Can you include the stack trace so that this exception has a better chance of getting noticed? Also, capitalize the error? +1 pending those changes",0.008333333333,0.008333333333,negative
hadoop,12829,description,"The implemented in HADOOP-12107 swallows interrupt exceptions. Over in Solr/Sentry land, we run thread leak checkers on our test code, which passed before this change and fails after it. Here's a sample report: And here's an indication that the interrupt is being ignored: This is inconsistent with how other long-running threads in hadoop, i.e. PeerCache respond to being interrupted. The argument for doing this in HADOOP-12107 is given as I'm unclear on what ""spurious wakeup"" means and it is not mentioned in So, I believe this thread should respect interruption.",code_debt,low_quality_code,"Sat, 20 Feb 2016 01:19:42 +0000","Mon, 16 Dec 2019 10:04:45 +0000","Tue, 23 Feb 2016 19:33:42 +0000",324840,"The StatisticsDataReferenceCleaner, implemented in HADOOP-12107 swallows interrupt exceptions. Over in Solr/Sentry land, we run thread leak checkers on our test code, which passed before this change and fails after it. Here's a sample report: And here's an indication that the interrupt is being ignored: This is inconsistent with how other long-running threads in hadoop, i.e. PeerCache respond to being interrupted. The argument for doing this in HADOOP-12107 is given as (https://issues.apache.org/jira/browse/HADOOP-12107?focusedCommentId=14598397&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14598397): Cleaner#run Catch and log InterruptedException in the while loop, such that thread does not die on a spurious wakeup. It's safe since it's a daemon thread. I'm unclear on what ""spurious wakeup"" means and it is not mentioned in https://docs.oracle.com/javase/tutorial/essential/concurrency/interrupt.html: A thread sends an interrupt by invoking interrupt on the Thread object for the thread to be interrupted. For the interrupt mechanism to work correctly, the interrupted thread must support its own interruption. So, I believe this thread should respect interruption.",-0.3218125,0.003514880952,negative
hadoop,12829,summary,swallows interrupt exceptions,code_debt,low_quality_code,"Sat, 20 Feb 2016 01:19:42 +0000","Mon, 16 Dec 2019 10:04:45 +0000","Tue, 23 Feb 2016 19:33:42 +0000",324840,StatisticsDataReferenceCleaner swallows interrupt exceptions,-0.4,-0.4,neutral
hadoop,12864,comment_1,"Actually, it looks like bin/rcc should have been removed with HADOOP-10474.",code_debt,dead_code,"Wed, 2 Mar 2016 18:16:48 +0000","Wed, 29 Jun 2016 03:30:40 +0000","Tue, 28 Jun 2016 21:25:19 +0000",10206511,"Actually, it looks like bin/rcc should have been removed with HADOOP-10474.",0,0,negative
hadoop,12888,comment_0,"Hadoop is notoriously bad for security manager support (See HADOOP-5731) ... though being able to go securely client-side would be good. As you note: server-side requirements shouldn't impact client side. # what happens if you try to use webhdfs rather than hdfs:// ? # show us the stack trace? Shell is used client-side to detect OS, there is a languishing patch to isolate OS checks...someone needs to refresh that patch and we can get it into trunk. It's also critical on windows. Which field is causing the problem?",code_debt,low_quality_code,"Mon, 29 Feb 2016 22:53:19 +0000","Tue, 30 Aug 2016 01:17:40 +0000","Wed, 16 Mar 2016 14:32:53 +0000",1352374,"Hadoop is notoriously bad for security manager support (See HADOOP-5731) ... though being able to go securely client-side would be good. As you note: server-side requirements shouldn't impact client side. what happens if you try to use webhdfs rather than hdfs:// ? show us the stack trace? Shell is used client-side to detect OS, there is a languishing patch to isolate OS checks...someone needs to refresh that patch and we can get it into trunk. It's also critical on windows. Which field is causing the problem?",-0.0559,-0.0559,negative
hadoop,12888,comment_11,Yet another update to fix checkstyle errors.,code_debt,low_quality_code,"Mon, 29 Feb 2016 22:53:19 +0000","Tue, 30 Aug 2016 01:17:40 +0000","Wed, 16 Mar 2016 14:32:53 +0000",1352374,Yet another update to fix checkstyle errors.,-0.4,-0.4,neutral
hadoop,12888,comment_15,"I went for debug: 1. to be consistent (similar to isSetsid 2. just like on windows on the other code paths, having these features disabled, on the client, has no impact. On info they would simply add noise and confusion in my opinion.",code_debt,low_quality_code,"Mon, 29 Feb 2016 22:53:19 +0000","Tue, 30 Aug 2016 01:17:40 +0000","Wed, 16 Mar 2016 14:32:53 +0000",1352374,"I went for debug: 1. to be consistent (similar to isSetsid 2. just like on windows on the other code paths, having these features disabled, on the client, has no impact. On info they would simply add noise and confusion in my opinion.",0,0,neutral
hadoop,12888,comment_8,...afraid you'll have to look at the checkstyle issues. Don't worry about the test and whitespace complaints,code_debt,low_quality_code,"Mon, 29 Feb 2016 22:53:19 +0000","Tue, 30 Aug 2016 01:17:40 +0000","Wed, 16 Mar 2016 14:32:53 +0000",1352374,...afraid you'll have to look at the checkstyle issues. Don't worry about the test and whitespace complaints,-0.04025,-0.04025,negative
hadoop,12888,comment_9,Fix checkstyle errors.,code_debt,low_quality_code,"Mon, 29 Feb 2016 22:53:19 +0000","Tue, 30 Aug 2016 01:17:40 +0000","Wed, 16 Mar 2016 14:32:53 +0000",1352374,Fix checkstyle errors.,-0.4,-0.4,negative
hadoop,12888,description,"HDFS _client_ requires dangerous permission, in particular _execute_ on _all files_ despite only trying to connect to an HDFS cluster. A full list (for both Hadoop 1 and 2) is available here along with the place in code where they occur. While it is understandable for some permissions to be used, requiring {{FilePermission <<ALL FILES To make matters worse, the code is executed to initialize a field so in case the permissions is not granted, the VM fails with which is unrecoverable. Ironically enough, on Windows this problem does not appear since the code simply bypasses it and initializes the field with a fall back value ({{false}}). A quick fix would be to simply take into account that the JVM {{SecurityManager}} might be active and the permission not granted or that the external process fails and use a fall back value. A proper and long-term fix would be to minimize the use of permissions for hdfs client since it is simply not required. A client should be as light as possible and not have the server requirements leaked onto.",code_debt,low_quality_code,"Mon, 29 Feb 2016 22:53:19 +0000","Tue, 30 Aug 2016 01:17:40 +0000","Wed, 16 Mar 2016 14:32:53 +0000",1352374,"HDFS client requires dangerous permission, in particular execute on all files despite only trying to connect to an HDFS cluster. A full list (for both Hadoop 1 and 2) is available here along with the place in code where they occur. While it is understandable for some permissions to be used, requiring FilePermission <<ALL FILES>> execute to simply initialize a class field Shell which in the end is not used (since it's just a client) simply compromises the entire security system. To make matters worse, the code is executed to initialize a field so in case the permissions is not granted, the VM fails with InitializationError which is unrecoverable. Ironically enough, on Windows this problem does not appear since the code simply bypasses it and initializes the field with a fall back value (false). A quick fix would be to simply take into account that the JVM SecurityManager might be active and the permission not granted or that the external process fails and use a fall back value. A proper and long-term fix would be to minimize the use of permissions for hdfs client since it is simply not required. A client should be as light as possible and not have the server requirements leaked onto.",-0.017,-0.014875,neutral
hadoop,12946,comment_0,Patch 001: * Make the thread object static. * Use a reference count to track how many instances depend on the thread. * Ensure the thread is started on the first start call. * Ensure the thread is only stopped on the last stop call * Move AbstractService specific test cases from TestJvmMetrics to a new TestAbstractService class. * Add JvmPauseMonitor test cases.,code_debt,low_quality_code,"Sun, 20 Mar 2016 18:14:55 +0000","Tue, 10 Jan 2017 01:41:24 +0000","Tue, 10 Jan 2017 01:41:24 +0000",25514789,Patch 001: Make the thread object static. Use a reference count to track how many instances depend on the thread. Ensure the thread is started on the first start call. Ensure the thread is only stopped on the last stop call Move AbstractService specific test cases from TestJvmMetrics to a new TestAbstractService class. Add JvmPauseMonitor test cases.,-0.1170333333,-0.1170333333,neutral
hadoop,12952,description,"The examples for building distributions include how to create one without any documentation. But it includes the javadoc stage in the build, which is very slow. Adding skips that phase, and helps round out the parameters to a build.",code_debt,slow_algorithm,"Tue, 22 Mar 2016 14:51:50 +0000","Tue, 30 Aug 2016 01:17:05 +0000","Wed, 23 Mar 2016 05:17:16 +0000",51926,"The examples for building distributions include how to create one without any documentation. But it includes the javadoc stage in the build, which is very slow. Adding -Dmaven.javadoc.skip=true skips that phase, and helps round out the parameters to a build.",0.1333333333,0.0973,neutral
hadoop,13030,comment_11,Pretty much this exact same code exists in httpfs.sh ....,code_debt,duplicated_code,"Sat, 16 Apr 2016 01:20:05 +0000","Tue, 30 Aug 2016 01:16:21 +0000","Thu, 28 Apr 2016 00:14:36 +0000",1032871,Pretty much this exact same code exists in httpfs.sh ....,0.19075,0.19075,neutral
hadoop,13030,comment_2,Patch 2 fixes the shellcheck warnings.,code_debt,low_quality_code,"Sat, 16 Apr 2016 01:20:05 +0000","Tue, 30 Aug 2016 01:16:21 +0000","Thu, 28 Apr 2016 00:14:36 +0000",1032871,Patch 2 fixes the shellcheck warnings.,-0.6,-0.6,neutral
hadoop,13030,comment_4,"Patch 3 adds my first part of comment to the function, for better readability.",code_debt,low_quality_code,"Sat, 16 Apr 2016 01:20:05 +0000","Tue, 30 Aug 2016 01:16:21 +0000","Thu, 28 Apr 2016 00:14:36 +0000",1032871,"Patch 3 adds my first part of comment to the function, for better readability.",0.5,0.5,positive
hadoop,13030,description,{{kms.sh}} currently cannot handle special characters.,code_debt,low_quality_code,"Sat, 16 Apr 2016 01:20:05 +0000","Tue, 30 Aug 2016 01:16:21 +0000","Thu, 28 Apr 2016 00:14:36 +0000",1032871,kms.sh currently cannot handle special characters.,0,0,negative
hadoop,13030,summary,Handle special characters in passwords in KMS startup script,code_debt,low_quality_code,"Sat, 16 Apr 2016 01:20:05 +0000","Tue, 30 Aug 2016 01:16:21 +0000","Thu, 28 Apr 2016 00:14:36 +0000",1032871,Handle special characters in passwords in KMS startup script,0,0,neutral
hadoop,13039,comment_4,"Hi , I think we can remove this string ""as potentially malicious"". The patch looks good otherwise. Thanks!",code_debt,dead_code,"Tue, 19 Apr 2016 22:33:30 +0000","Fri, 6 Jan 2017 01:01:38 +0000","Wed, 27 Apr 2016 03:49:39 +0000",623769,"Hi liuml07, I think we can remove this string ""as potentially malicious"". The patch looks good otherwise. Thanks!",0.392,0.392,positive
hadoop,13138,comment_2,", thank you for the patch. This looks good to me. I think it will be ready to go after addressing the Checkstyle nitpicks.",code_debt,low_quality_code,"Thu, 12 May 2016 08:29:58 +0000","Wed, 2 Jan 2019 22:15:55 +0000","Wed, 18 May 2016 16:42:45 +0000",547967,"vinayrpet, thank you for the patch. This looks good to me. I think it will be ready to go after addressing the Checkstyle nitpicks.",0.5628333333,0.5628333333,positive
hadoop,13138,comment_3,Attaching the patch with fixed checkstyle nits,code_debt,low_quality_code,"Thu, 12 May 2016 08:29:58 +0000","Wed, 2 Jan 2019 22:15:55 +0000","Wed, 18 May 2016 16:42:45 +0000",547967,Attaching the patch with fixed checkstyle nits,0,0,neutral
hadoop,13158,description,The {{cannedACL}} field of {{S3AFileSystem}} can be {{null}}. The {{toString}} implementation has an unguarded call to so there is a risk of,code_debt,low_quality_code,"Mon, 16 May 2016 18:33:41 +0000","Tue, 30 Aug 2016 01:14:48 +0000","Tue, 17 May 2016 12:20:53 +0000",64032,"The cannedACL field of S3AFileSystem can be null. The toString implementation has an unguarded call to cannedACL.toString(), so there is a risk of NullPointerException.",-0.25625,-0.3416666667,neutral
hadoop,13233,comment_4,"TestKDiag succeeded on my machine, I suspect it's a random failure. checkstyle could be fixed, but it would either look weird (indented differently than the other lines in Stat.DESCRIPTION) or I would have to reindent a few more lines.",code_debt,low_quality_code,"Thu, 2 Jun 2016 14:10:25 +0000","Mon, 13 Feb 2017 18:51:34 +0000","Mon, 13 Feb 2017 18:38:06 +0000",22134461,"TestKDiag succeeded on my machine, I suspect it's a random failure. checkstyle could be fixed, but it would either look weird (indented differently than the other lines in Stat.DESCRIPTION) or I would have to reindent a few more lines.",-0.1333333333,-0.1333333333,negative
hadoop,13233,comment_8,+1 No need to fix checkstyle warning. Failed tests not reproducible.,code_debt,low_quality_code,"Thu, 2 Jun 2016 14:10:25 +0000","Mon, 13 Feb 2017 18:51:34 +0000","Mon, 13 Feb 2017 18:38:06 +0000",22134461,+1 No need to fix checkstyle warning. Failed tests not reproducible.,0.1,0.1,negative
hadoop,13233,summary,help of stat is confusing,code_debt,low_quality_code,"Thu, 2 Jun 2016 14:10:25 +0000","Mon, 13 Feb 2017 18:51:34 +0000","Mon, 13 Feb 2017 18:38:06 +0000",22134461,help of stat is confusing,-0.0185,-0.0185,negative
hadoop,13353,comment_2,"I see that when the IOException is thrown, the log message does not preserve its stacktrace. Could you also update the fix to include the stacktrace? Basically, change it to:",code_debt,low_quality_code,"Fri, 8 Jul 2016 02:29:15 +0000","Wed, 2 Oct 2019 17:13:30 +0000","Fri, 5 Aug 2016 23:38:48 +0000",2495373,"I see that when the IOException is thrown, the log message does not preserve its stacktrace. Could you also update the fix to include the stacktrace? Basically, change it to:",0,0,negative
hadoop,13353,comment_3,"Thanks for the initial patch. I created a v02 patch based on the initial one, adding a regression test, removed redundant code and improved error logging.",code_debt,complex_code,"Fri, 8 Jul 2016 02:29:15 +0000","Wed, 2 Oct 2019 17:13:30 +0000","Fri, 5 Aug 2016 23:38:48 +0000",2495373,"Thanks liangvls@gmail.com for the initial patch. I created a v02 patch based on the initial one, adding a regression test, removed redundant code and improved error logging.",0.2,0.2,positive
hadoop,13353,comment_5,Sorry for late reply because I was on vacation. Thanks  for the v2 patch. But some code about datanode blockpool in the v2 patch seems irrelevant ?,code_debt,low_quality_code,"Fri, 8 Jul 2016 02:29:15 +0000","Wed, 2 Oct 2019 17:13:30 +0000","Fri, 5 Aug 2016 23:38:48 +0000",2495373,Sorry for late reply because I was on vacation. Thanks weichiu for the v2 patch. But some code about datanode blockpool in the v2 patch seems irrelevant ?,-0.03333333333,-0.03333333333,negative
hadoop,13529,comment_5,some incremental updates: 1. add docs 2. remove UserInfo 3. remove comments for overrie function 4. codestyle issue,code_debt,low_quality_code,"Mon, 22 Aug 2016 01:05:09 +0000","Mon, 6 Nov 2017 07:53:16 +0000","Fri, 26 Aug 2016 04:40:22 +0000",358513,kakagou some incremental updates: 1. add docs 2. remove UserInfo 3. remove comments for overrie function 4. codestyle issue,0,0,neutral
hadoop,13529,description,1. argument and variant naming 2. abstract utility class 3. add some comments 4. adjust some configuration 5. fix TODO 6. remove unnecessary commets 7. some bug fix 8. add some unit test,code_debt,low_quality_code,"Mon, 22 Aug 2016 01:05:09 +0000","Mon, 6 Nov 2017 07:53:16 +0000","Fri, 26 Aug 2016 04:40:22 +0000",358513,1. argument and variant naming 2. abstract utility class 3. add some comments 4. adjust some configuration 5. fix TODO 6. remove unnecessary commets 7. some bug fix 8. add some unit test,0,0,neutral
hadoop,13638,comment_11,That was copied from an existing test case.,code_debt,duplicated_code,"Wed, 21 Sep 2016 21:04:16 +0000","Wed, 2 Oct 2019 17:15:08 +0000","Mon, 26 Sep 2016 20:12:54 +0000",428918,That was copied from an existing test case.,0,0,neutral
hadoop,13638,comment_9,Checkstyle warnings can't be removed unless we refactor test methods.,code_debt,low_quality_code,"Wed, 21 Sep 2016 21:04:16 +0000","Wed, 2 Oct 2019 17:15:08 +0000","Mon, 26 Sep 2016 20:12:54 +0000",428918,Checkstyle warnings can't be removed unless we refactor test methods.,-0.6,-0.6,neutral
hadoop,1367,comment_2,The distFrom variable has been made a ThreadLocal. This should get rid of the findbugs warnings,code_debt,multi-thread_correctness,"Mon, 14 May 2007 22:36:04 +0000","Mon, 20 Aug 2007 18:11:57 +0000","Wed, 27 Jun 2007 18:12:46 +0000",3785802,The distFrom variable has been made a ThreadLocal. This should get rid of the findbugs warnings,-0.3,-0.3,neutral
hadoop,1367,description,line 556 The distFrom field of this class appears to be accessed inconsistently with respect to synchronization.,code_debt,multi-thread_correctness,"Mon, 14 May 2007 22:36:04 +0000","Mon, 20 Aug 2007 18:11:57 +0000","Wed, 27 Jun 2007 18:12:46 +0000",3785802,org.apache.hadoop.net.NetworkTopology.java line 556 The distFrom field of this class appears to be accessed inconsistently with respect to synchronization.,0.009,0.0015,neutral
hadoop,1367,summary,Inconsistent synchronization of locked 50% of time,code_debt,multi-thread_correctness,"Mon, 14 May 2007 22:36:04 +0000","Mon, 20 Aug 2007 18:11:57 +0000","Wed, 27 Jun 2007 18:12:46 +0000",3785802,Inconsistent synchronization of NetworkTopology.distFrom; locked 50% of time,0,0,neutral
hadoop,13768,comment_4,"Looking at the following codes: 1. Please give {{l}} a more readable name. 2. Can you give some comments to explain some bit about the procedure? I (probably others) wouldn't know why it's like that without querying the SDK's manual. I know it now, there're 2 modes in the operation, one mode to return successfully deleted objects, and the other returning the deleting-failed objects. You're using the latter, and use a loop to try some times to delete and delete the failed-to-delete objects.",code_debt,low_quality_code,"Fri, 28 Oct 2016 08:27:17 +0000","Fri, 24 Nov 2017 08:43:36 +0000","Fri, 10 Feb 2017 06:56:06 +0000",9066529,"Looking at the following codes: 1. Please give l a more readable name. 2. Can you give some comments to explain some bit about the procedure? I (probably others) wouldn't know why it's like that without querying the SDK's manual. I know it now, there're 2 modes in the ossClient.deleteObjects operation, one mode to return successfully deleted objects, and the other returning the deleting-failed objects. You're using the latter, and use a loop to try some times to delete and delete the failed-to-delete objects.",-0.0125,-0.0109375,neutral
hadoop,14092,comment_5,Thanks for the review and commit! Wrong file name is even worse than other misspelled words because folks might get puzzled when they follow instructions and fail to get the result.,code_debt,low_quality_code,"Fri, 17 Feb 2017 06:04:54 +0000","Fri, 21 Apr 2017 21:38:34 +0000","Sat, 18 Feb 2017 18:25:36 +0000",130842,Thanks stevel@apache.org for the review and commit! Wrong file name is even worse than other misspelled words because folks might get puzzled when they follow instructions and fail to get the result.,-0.05625,-0.05625,negative
hadoop,14351,comment_4,Thanks  for reviewing the patch. Attached another patch containing the checkstyle fixes excluding the fix for the {{FileLength}} related warning. The below findbugs warning is unrelated to changes done in the patch which was introduced in [Commit of HADOOP-10809,code_debt,low_quality_code,"Tue, 25 Apr 2017 15:53:17 +0000","Wed, 26 Apr 2017 23:53:39 +0000","Wed, 26 Apr 2017 20:58:05 +0000",104688,"Thanks liuml07 for reviewing the patch. Attached another patch containing the checkstyle fixes excluding the fix for the FileLength related warning. The below findbugs warning is unrelated to changes done in the patch which was introduced in Commit #2217e2f8ff418b88eac6ad36cafe3a9795a11f40 of HADOOP-10809 Useless object stored in variable keysToUpdateAsFolder of method org.apache.hadoop.fs.azure.NativeAzureFileSystem.mkdirs(Path, FsPermission, boolean)",-0.1333333333,-0.0045,positive
hadoop,14359,comment_3,Thanks for the pointer! I really wish we could get rid of commons-httpclient in Hadoop 3. ran mvn dependency:tree and I see commons-httpclient 3 is exposed in two jars: [INFO] [INFO] +- [INFO] | +- [INFO] | | \- [INFO] | +- [INFO] | +- [INFO] [INFO] +- [INFO] | +- [INFO] | | \- [INFO] | +- [INFO] | +-,code_debt,dead_code,"Thu, 27 Apr 2017 08:06:23 +0000","Wed, 2 Oct 2019 17:16:03 +0000","Mon, 1 May 2017 06:24:10 +0000",339467,Thanks for the pointer! I really wish we could get rid of commons-httpclient in Hadoop 3. ran mvn dependency:tree and I see commons-httpclient 3 is exposed in two jars: [INFO] org.apache.hadoop:hadoop-yarn-server-timelineservice-hbase:jar:3.0.0-alpha3-SNAPSHOT [INFO] +- org.apache.hbase:hbase-server:jar:1.2.4:compile [INFO] | +- org.apache.hbase:hbase-procedure:jar:1.2.4:compile [INFO] | | - org.apache.hbase:hbase-common:jar:tests:1.2.4:test [INFO] | +- org.apache.hbase:hbase-prefix-tree:jar:1.2.4:runtime [INFO] | +- commons-httpclient:commons-httpclient:jar:3.1:compile [INFO] org.apache.hadoop:hadoop-yarn-server-timelineservice-hbase-tests:jar:3.0.0-alpha3-SNAPSHOT [INFO] +- org.apache.hbase:hbase-server:jar:1.2.4:compile [INFO] | +- org.apache.hbase:hbase-procedure:jar:1.2.4:compile [INFO] | | - org.apache.hbase:hbase-common:jar:tests:1.2.4:test [INFO] | +- org.apache.hbase:hbase-prefix-tree:jar:1.2.4:runtime [INFO] | +- commons-httpclient:commons-httpclient:jar:3.1:compile,0.1333333333,0.01481481481,positive
hadoop,14359,description,commons-httpclient dependency was removed in HADOOP-10105 but there are some settings to shade commons-httpclient. Probably they can be safely removed.,code_debt,dead_code,"Thu, 27 Apr 2017 08:06:23 +0000","Wed, 2 Oct 2019 17:16:03 +0000","Mon, 1 May 2017 06:24:10 +0000",339467,commons-httpclient dependency was removed in HADOOP-10105 but there are some settings to shade commons-httpclient. Probably they can be safely removed.,0.1,0.1,neutral
hadoop,14359,summary,Remove unnecessary shading of commons-httpclient,code_debt,dead_code,"Thu, 27 Apr 2017 08:06:23 +0000","Wed, 2 Oct 2019 17:16:03 +0000","Mon, 1 May 2017 06:24:10 +0000",339467,Remove unnecessary shading of commons-httpclient,0,0,negative
hadoop,1453,comment_4,Both your patches are for whereas your comment says that there are *two* places where a redundant exists() call is made: and ChecksumFileSystem. Can you pl clarify?,code_debt,low_quality_code,"Fri, 1 Jun 2007 23:24:43 +0000","Wed, 8 Jul 2009 16:42:29 +0000","Wed, 20 Jun 2007 21:30:25 +0000",1634742,Both your patches are for DistributedFileSystem whereas your comment says that there are two places where a redundant exists() call is made: DistributedFileSystem and ChecksumFileSystem. Can you pl clarify?,0,0,neutral
hadoop,1453,description,{{exists(f)}} adds extra namenode interaction that is not really required. Open is a critical DFS call.,code_debt,low_quality_code,"Fri, 1 Jun 2007 23:24:43 +0000","Wed, 8 Jul 2009 16:42:29 +0000","Wed, 20 Jun 2007 21:30:25 +0000",1634742,exists(f) adds extra namenode interaction that is not really required. Open is a critical DFS call.,0,0,neutral
hadoop,1459,comment_17,In the medium term we should move all of Hadoop to use IP addresses instead of hostnames. I've filed the relevant bug in HADOOP-1487.,code_debt,low_quality_code,"Mon, 4 Jun 2007 10:09:30 +0000","Wed, 8 Jul 2009 16:41:57 +0000","Mon, 18 Jun 2007 20:39:50 +0000",1247420,In the medium term we should move all of Hadoop to use IP addresses instead of hostnames. I've filed the relevant bug in HADOOP-1487.,0,0,neutral
hadoop,1459,comment_3,"Could you please remove redundant import if you change DatanodeInfo. I think this is the right fix for the time being, although I'm not happy that # we should keep 2 different identifications for the nodes and that # we have different ways of node identification in different components of hadoop. My proposition would be to return back to hostnames instead of ip addresses. But this of course belongs to a different issue.",code_debt,dead_code,"Mon, 4 Jun 2007 10:09:30 +0000","Wed, 8 Jul 2009 16:41:57 +0000","Mon, 18 Jun 2007 20:39:50 +0000",1247420,"Could you please remove redundant import org.apache.hadoop.io.UTF8 if you change DatanodeInfo. I think this is the right fix for the time being, although I'm not happy that we should keep 2 different identifications for the nodes and that we have different ways of node identification in different components of hadoop. My proposition would be to return back to hostnames instead of ip addresses. But this of course belongs to a different issue.",-0.01025,-0.005125,neutral
hadoop,14634,comment_1,"+1, committed. Thanks ray. We all hate spurious JARs sneaking in. the ones we deliberately add are troublesome enough",code_debt,low_quality_code,"Fri, 7 Jul 2017 22:37:41 +0000","Mon, 10 Jul 2017 17:04:34 +0000","Sat, 8 Jul 2017 11:20:57 +0000",45796,"+1, committed. Thanks ray. We all hate spurious JARs sneaking in. the ones we deliberately add are troublesome enough",0.205,0.205,negative
hadoop,14634,comment_4,Thanks for the commit Steve! It's going to be good to have all these sorts of things cleaned up in time for the final Hadoop 3 release.,code_debt,low_quality_code,"Fri, 7 Jul 2017 22:37:41 +0000","Mon, 10 Jul 2017 17:04:34 +0000","Sat, 8 Jul 2017 11:20:57 +0000",45796,Thanks for the commit Steve! It's going to be good to have all these sorts of things cleaned up in time for the final Hadoop 3 release.,0.538,0.538,positive
hadoop,14870,comment_2,"Got the tests working here...they were actually running, though the use of JUnit4 rules to name threads was amplifying confusion. Seen a test failure",code_debt,low_quality_code,"Fri, 15 Sep 2017 17:50:21 +0000","Fri, 27 Oct 2017 11:57:26 +0000","Fri, 27 Oct 2017 11:57:11 +0000",3607610,"Got the tests working here...they were actually running, though the use of JUnit4 rules to name threads was amplifying confusion. Seen a test failure",0.06666666667,0.06666666667,negative
hadoop,14942,comment_1,LGTM +1 cleanup code is always good to make robust,code_debt,low_quality_code,"Wed, 11 Oct 2017 00:41:37 +0000","Fri, 20 Oct 2017 22:01:24 +0000","Fri, 20 Oct 2017 21:29:45 +0000",852488,LGTM +1 cleanup code is always good to make robust,0.588,0.588,positive
hadoop,14942,description,"Over in HBASE-18975, we observed the following: came from second line below: in which case jobFS was null. A check against null should be added.",code_debt,low_quality_code,"Wed, 11 Oct 2017 00:41:37 +0000","Fri, 20 Oct 2017 22:01:24 +0000","Fri, 20 Oct 2017 21:29:45 +0000",852488,"Over in HBASE-18975, we observed the following: NullPointerException came from second line below: in which case jobFS was null. A check against null should be added.",0,0,neutral
hadoop,14942,summary,DistCp#cleanup() should check whether jobFS is null,code_debt,low_quality_code,"Wed, 11 Oct 2017 00:41:37 +0000","Fri, 20 Oct 2017 22:01:24 +0000","Fri, 20 Oct 2017 21:29:45 +0000",852488,DistCp#cleanup() should check whether jobFS is null,0,0,neutral
hadoop,1536,comment_0,This is related to HADOOP-1283. This patch removed the implementation of file level locks from HDFS.,code_debt,dead_code,"Wed, 27 Jun 2007 07:09:32 +0000","Wed, 8 Jul 2009 17:05:59 +0000","Fri, 29 Jun 2007 16:34:28 +0000",206696,This is related to HADOOP-1283. This patch removed the implementation of file level locks from HDFS.,0,0,neutral
hadoop,1536,comment_1,HADOOP-932 was filed about removing the all of the locking interface. It probably time to fix it.,code_debt,dead_code,"Wed, 27 Jun 2007 07:09:32 +0000","Wed, 8 Jul 2009 17:05:59 +0000","Fri, 29 Jun 2007 16:34:28 +0000",206696,HADOOP-932 was filed about removing the all of the locking interface. It probably time to fix it.,0,0,negative
hadoop,15476,comment_1,"Thanks for catching this . Can we just remove this log message? There is another message below: For wildcard address, localAddr will be null. So perhaps we can fix this other log message to print wildcard if localAddr is null.",code_debt,low_quality_code,"Thu, 17 May 2018 19:31:54 +0000","Wed, 1 Aug 2018 20:07:56 +0000","Wed, 1 Aug 2018 19:34:06 +0000",6566532,"Thanks for catching this ajayydv. Can we just remove this log message? There is another message below: For wildcard address, localAddr will be null. So perhaps we can fix this other log message to print wildcard if localAddr is null.",0.1125,0.1125,neutral
hadoop,15486,comment_0,There's no need for a config. Just make the lock always be fair. Updates to the topology should be rare compared to block placements so the throughput degradation of the fair lock will be minimal.,code_debt,multi-thread_correctness,"Mon, 21 May 2018 19:28:58 +0000","Mon, 23 Jul 2018 17:14:03 +0000","Wed, 23 May 2018 17:35:51 +0000",166013,There's no need for a config. Just make the lock always be fair. Updates to the topology should be rare compared to block placements so the throughput degradation of the fair lock will be minimal.,0.1666666667,0.1666666667,neutral
hadoop,15486,description,"Whenever a datanode is restarted, the registration call after the restart received by NameNode lands in via requires write lock on This registration thread is getting starved by flood of calls, which are triggered by clients those who were writing to the restarted datanode. The registration call which is waiting for write lock on is holding write lock on causing all the other RPC calls which require the lock on wait. We can make lock fair so that the registration thread will not starve.",code_debt,multi-thread_correctness,"Mon, 21 May 2018 19:28:58 +0000","Mon, 23 Jul 2018 17:14:03 +0000","Wed, 23 May 2018 17:35:51 +0000",166013,"Whenever a datanode is restarted, the registration call after the restart received by NameNode lands in NetworkTopology#add via DatanodeManager#registerDatanode requires write lock on NetworkTopology#netLock. This registration thread is getting starved by flood of FSNamesystem.getAdditionalDatanode calls, which are triggered by clients those who were writing to the restarted datanode. The registration call which is waiting for write lock on NetworkTopology#netLock is holding write lock on FSNamesystem#fsLock, causing all the other RPC calls which require the lock on FSNamesystem#fsLock wait. We can make NetworkTopology#netLock lock fair so that the registration thread will not starve.",0.1666666667,0.1,neutral
hadoop,15569,comment_0,"Patch 001 * document assumed roles better, in particularly, the permissions you need for S3Guard read and S3Guard admin. * Change structure to avoid listing which commands need which perms, and be a bit expansive in what is needed, to line up for future changes (e.g. get/set object tags) * I have tested the s3guard stuff with an assumed role.",code_debt,low_quality_code,"Thu, 28 Jun 2018 19:19:44 +0000","Tue, 10 Jul 2018 16:58:49 +0000","Tue, 10 Jul 2018 16:58:49 +0000",1028345,"Patch 001 document assumed roles better, in particularly, the permissions you need for S3Guard read and S3Guard admin. Change structure to avoid listing which commands need which perms, and be a bit expansive in what is needed, to line up for future changes (e.g. get/set object tags) I have tested the s3guard stuff with an assumed role.",-0.0949,-0.0949,neutral
hadoop,15742,comment_7,"Other than checkstyle issue, +1 for others.",code_debt,low_quality_code,"Tue, 11 Sep 2018 08:04:59 +0000","Tue, 18 Sep 2018 03:14:56 +0000","Tue, 18 Sep 2018 03:12:47 +0000",587268,"Other than checkstyle issue, +1 for others.",0,0,neutral
hadoop,15742,description,Currently we don't log the info of ipc backoff. It will look good to print this as well so that makes users know if we enable this.,code_debt,low_quality_code,"Tue, 11 Sep 2018 08:04:59 +0000","Tue, 18 Sep 2018 03:14:56 +0000","Tue, 18 Sep 2018 03:12:47 +0000",587268,Currently we don't log the info of ipc backoff. It will look good to print this as well so that makes users know if we enable this.,0.3178333333,0.3178333333,neutral
hadoop,15859,comment_0,Attached a patch that removes the JNI setting of the remaining field per Ben's analysis above and cleans up the naming re: objects vs. classes in the JNI function arguments.,code_debt,low_quality_code,"Tue, 16 Oct 2018 18:31:51 +0000","Wed, 7 Nov 2018 01:34:32 +0000","Wed, 17 Oct 2018 19:48:06 +0000",90975,Attached a patch that removes the JNI setting of the remaining field per Ben's analysis above and cleans up the naming re: objects vs. classes in the JNI function arguments.,0,0,neutral
hadoop,15859,description,"As a follow up to HADOOP-15820, I was doing more testing on ZSTD compression and still encountered segfaults in the JVM in HBase after that fix. I took a deeper look and realized there is still another bug, which looks like it's that we are actually [calling on the ""remaining"" variable on the class itself (instead of an instance of that class) because the Java stub for the native C init() function [is marked leading to memory corruption and a crash during GC later. Initially I thought we would fix this by changing the Java init() method to be non-static, but it looks like the ""remaining"" setInt() call is actually unnecessary anyway, because in reset() we [set ""remaining"" to 0 right after calling the JNI init() So init() doesn't have to be changed to an instance method, we can leave it as static, but remove the JNI init() call's ""remaining"" setInt() call altogether. Furthermore we should probably clean up the class/instance distinction in the C file because that's what led to this confusion. There are some other methods where the distinction is incorrect or ambiguous, we should fix them to prevent this from happening again. I talked to  who further pointed out the ZStandardCompressor also has similar problems and needs to be fixed too.",code_debt,low_quality_code,"Tue, 16 Oct 2018 18:31:51 +0000","Wed, 7 Nov 2018 01:34:32 +0000","Wed, 17 Oct 2018 19:48:06 +0000",90975,"As a follow up to HADOOP-15820, I was doing more testing on ZSTD compression and still encountered segfaults in the JVM in HBase after that fix. I took a deeper look and realized there is still another bug, which looks like it's that we are actually calling setInt() on the ""remaining"" variable on the ZStandardDecompressor class itself (instead of an instance of that class) because the Java stub for the native C init() function is marked static, leading to memory corruption and a crash during GC later. Initially I thought we would fix this by changing the Java init() method to be non-static, but it looks like the ""remaining"" setInt() call is actually unnecessary anyway, because in ZStandardDecompressor.java's reset() we set ""remaining"" to 0 right after calling the JNI init() call. So ZStandardDecompressor.java init() doesn't have to be changed to an instance method, we can leave it as static, but remove the JNI init() call's ""remaining"" setInt() call altogether. Furthermore we should probably clean up the class/instance distinction in the C file because that's what led to this confusion. There are some other methods where the distinction is incorrect or ambiguous, we should fix them to prevent this from happening again. I talked to jlowe who further pointed out the ZStandardCompressor also has similar problems and needs to be fixed too.",0.07601388889,0.05213333333,negative
hadoop,16044,comment_0,"Following up for -HADOOP-15662, c-ompared to L173: - Updated the format to: *Unknown host name: %s. Retrying to resolve the host name...* - Replace *""ex.getMessage()*"" with although they both return host name string, but the *getHost()* seems to be more readable code. - Removed the else if logic as it breaks the current exception trace. Test: testUnknownHost() would take about 14 minutes due to the retry, I verified the warning format in the console: I haven't come up with a good way to retrieve the log msg for test, any suggestions? Testes against US west account passed: All tests passed my US west account: XNS account oauth 35, 0, Errors: 0, Skipped: 0 324, 0, Errors: 0, Skipped: 22 168, 0, Errors: 0, Skipped: 21 XNS account sharedKey: 35, 0, Errors: 0, Skipped: 0 324, 0, Errors: 0, Skipped: 20 168, 0, Errors: 0, Skipped: 15 non-xns account sharedKe: 35, 0, Errors: 0, Skipped: 0 324, 0, Errors: 0, Skipped: 206 168, 0, Errors: 0, Skipped: 15",code_debt,low_quality_code,"Fri, 11 Jan 2019 18:02:56 +0000","Mon, 14 Jan 2019 20:03:40 +0000","Mon, 14 Jan 2019 19:47:06 +0000",265450,"Following up for -HADOOP-15662, c-ompared to HADOOP-15662-002.patch: L173: Updated the format to: Unknown host name: %s. Retrying to resolve the host name... Replace ""ex.getMessage()"" with ""httpOperation.getUrl().getHost())"", although they both return host name string, but the getHost()seems to be more readable code. Removed the else if logic as it breaks the current exception trace. Test: testUnknownHost() would take about 14 minutes due to the retry, I verified the warning format in the console: I haven't come up with a good way to retrieve the log msg for test, any suggestions? Testes against US west account passed: All tests passed my US west account: XNS account oauth Tests run: 35, Failures: 0, Errors: 0, Skipped: 0 Tests run: 324, Failures: 0, Errors: 0, Skipped: 22 Tests run: 168, Failures: 0, Errors: 0, Skipped: 21 XNS account sharedKey: Tests run: 35, Failures: 0, Errors: 0, Skipped: 0 Tests run: 324, Failures: 0, Errors: 0, Skipped: 20 Tests run: 168, Failures: 0, Errors: 0, Skipped: 15 non-xns account sharedKe: Tests run: 35, Failures: 0, Errors: 0, Skipped: 0 Tests run: 324, Failures: 0, Errors: 0, Skipped: 206 Tests run: 168, Failures: 0, Errors: 0, Skipped: 15",-0.02505,-0.01565625,neutral
hadoop,16044,summary,ABFS: Better exception handling of DNS errors followup,code_debt,low_quality_code,"Fri, 11 Jan 2019 18:02:56 +0000","Mon, 14 Jan 2019 20:03:40 +0000","Mon, 14 Jan 2019 19:47:06 +0000",265450,ABFS: Better exception handling of DNS errors followup,0.05,0.05,neutral
hadoop,16226,comment_1,thanks for the patch . Maybe use a simple double-slash comment to avoid this checkstyle issue: +1 pending ^,code_debt,low_quality_code,"Mon, 1 Apr 2019 23:59:49 +0000","Sun, 5 Apr 2020 08:54:10 +0000","Wed, 3 Apr 2019 04:19:35 +0000",101986,thanks for the patch ajisakaa. Maybe use a simple double-slash comment to avoid this checkstyle issue: +1 pending ^,0.05,0.05,positive
hadoop,16226,comment_2,Thanks  for the review. Added a period to fix the checkstyle warning.,code_debt,low_quality_code,"Mon, 1 Apr 2019 23:59:49 +0000","Sun, 5 Apr 2020 08:54:10 +0000","Wed, 3 Apr 2019 04:19:35 +0000",101986,Thanks jira.shegalov for the review. Added a period to fix the checkstyle warning.,-0.1,-0.06666666667,positive
hadoop,16265,comment_0,"It is a little wired to return only the raw literal number when default time units and literal number is provided. It may mislead us to unexpected behave if we just take into account its name and parameters. Though changing long 10 to string '10s' will return expected result, I think it should return the same result when long 10 and default time unit SECOND is given.",code_debt,low_quality_code,"Thu, 18 Apr 2019 01:57:25 +0000","Mon, 22 Apr 2019 15:32:30 +0000","Mon, 22 Apr 2019 15:22:36 +0000",393911,"It is a little wired to return only the raw literal number when default time units and literal number is provided. It may mislead us to unexpected behave if we just take into account its name and parameters.Though changing long 10 to string '10s' will return expected result, I think it should return the same result when long 10 and default time unit SECOND is given.",-0.5333333333,-0.5333333333,neutral
hadoop,16265,comment_4,Thanks  for catching this! Should we use {{defaultUnit}} instead of in the latest patch v2?,code_debt,low_quality_code,"Thu, 18 Apr 2019 01:57:25 +0000","Mon, 22 Apr 2019 15:32:30 +0000","Mon, 22 Apr 2019 15:22:36 +0000",393911,Thanks starphin for catching this! Should we use defaultUnit instead of TimeUnit.SECONDS in the latest patch v2?,0.225,0.15,neutral
hadoop,16265,description,"When call getTimeDuration like this: {color:#333333}If ""nn.interval"" is set manually or configured in xml file, 10000 will be retrurned.{color} If not, 10 will be returned while 10000 is expected. The logic is not consistent.",code_debt,low_quality_code,"Thu, 18 Apr 2019 01:57:25 +0000","Mon, 22 Apr 2019 15:32:30 +0000","Mon, 22 Apr 2019 15:22:36 +0000",393911,"When call getTimeDuration like this: conf.getTimeDuration(""nn.interval"", 10, TimeUnit.SECONDS, TimeUnit.MILLISECONDS); If ""nn.interval"" is set manually or configured in xml file, 10000 will be retrurned. If not, 10 will be returned while 10000 is expected. The logic is not consistent.",0.25,0,neutral
hadoop,16265,summary,is not consistent between default value and manual settings.,code_debt,low_quality_code,"Thu, 18 Apr 2019 01:57:25 +0000","Mon, 22 Apr 2019 15:32:30 +0000","Mon, 22 Apr 2019 15:22:36 +0000",393911,Configuration#getTimeDuration is not consistent between default value and manual settings.,0.5,0.5,neutral
hadoop,16409,description,"currently requires a qualified URI (e.g. s3a://bucket/path) which is how I see this being used most immediately, but it also make sense for someone to just be able to configure /path, if all of their buckets follow that pattern, or if they're providing configuration already in a bucket-specific context (e.g. job-level configs, etc.) Just need to qualify whatever is passed in to allowAuthoritative to make that work. Also, in HADOOP-16396 Gabor pointed out a few whitepace nits that I neglected to fix before merging.",code_debt,low_quality_code,"Wed, 3 Jul 2019 20:28:55 +0000","Fri, 19 Jul 2019 18:28:30 +0000","Mon, 8 Jul 2019 17:28:05 +0000",421150,"fs.s3a.authoritative.path currently requires a qualified URI (e.g. s3a://bucket/path) which is how I see this being used most immediately, but it also make sense for someone to just be able to configure /path, if all of their buckets follow that pattern, or if they're providing configuration already in a bucket-specific context (e.g. job-level configs, etc.) Just need to qualify whatever is passed in to allowAuthoritative to make that work. Also, in HADOOP-16396 Gabor pointed out a few whitepace nits that I neglected to fix before merging.",-0.052,-0.0208,neutral
hadoop,16435,comment_8,I don't think this is about a single session - these metrics are only used in the [IPC If you create an server and then shut it down - would you expect its metrics to be retained? (IMHO: if it silently retaines any information related to the actual instance that's a resource leak),code_debt,low_quality_code,"Wed, 17 Jul 2019 09:08:42 +0000","Tue, 30 Jul 2019 00:46:46 +0000","Tue, 30 Jul 2019 00:46:46 +0000",1093084,I don't think this is about a single session - these metrics are only used in the IPC Server. If you create an server and then shut it down - would you expect its metrics to be retained? (IMHO: if it silently retaines any information related to the actual instance that's a resource leak),-0.152,-0.032,neutral
hadoop,16504,comment_4,"The problem now is that since listen queue is full, TCP can drop many packet and resend packet after a while. So tcp connection to be slow or even timed out. Changing this default value can reduce client connection timeout when the request is large for nn or dn. Update the v1 patch and upload it. Thank you.",code_debt,slow_algorithm,"Sat, 10 Aug 2019 16:46:31 +0000","Wed, 2 Oct 2019 17:16:03 +0000","Thu, 15 Aug 2019 22:21:35 +0000",452104,"weichiu The problem now is that since listen queue is full, TCP can drop many packet and resend packet after a while. So tcp connection to be slow or even timed out. Changing this default value can reduce client connection timeout when the request is large for nn or dn. Update the v1 patch and upload it. Thank you.",-0.08,-0.08,negative
hadoop,16523,summary,Minor spell mistake in comment (PR#388),code_debt,low_quality_code,"Tue, 20 Aug 2019 20:35:13 +0000","Tue, 20 Aug 2019 21:46:25 +0000","Tue, 20 Aug 2019 20:36:34 +0000",81,Minor spell mistake in comment (PR#388),-0.4,-0.4,negative
hadoop,1961,comment_2,There are some more inconsistencies in copyToLocal() : For example : I will fix this as well. There is also an extra isDirectory() that is not required.,code_debt,low_quality_code,"Thu, 27 Sep 2007 23:33:00 +0000","Wed, 8 Jul 2009 16:42:11 +0000","Fri, 5 Oct 2007 00:09:01 +0000",606961,There are some more inconsistencies in copyToLocal() : For example : I will fix this as well. There is also an extra isDirectory() that is not required.,0.3155,0.3155,negative
hadoop,1961,comment_3,"There are at least three versions of copyToLocal : one in FsShell, one in ChecksumFileSystem, and FileUtil. All of these implement same logic and recursion.. but are slightly different. FsShell and ChecksumFS versions in turn invoke FileUtil version. We should remove FsShell and ChecksumFS, at least in 0.15 or 0.16.",code_debt,duplicated_code,"Thu, 27 Sep 2007 23:33:00 +0000","Wed, 8 Jul 2009 16:42:11 +0000","Fri, 5 Oct 2007 00:09:01 +0000",606961,"There are at least three versions of copyToLocal : one in FsShell, one in ChecksumFileSystem, and FileUtil. All of these implement same logic and recursion.. but are slightly different. FsShell and ChecksumFS versions in turn invoke FileUtil version. We should remove FsShell and ChecksumFS, at least in 0.15 or 0.16.",0,0,neutral
hadoop,1961,comment_8,"+1 Patch looks good. Below are some minor thoughts. - I totally agree that there are redundant copyToLocal methods. See also HADOOP-1544. - In the case of rename failing, we know that the tmp file is good but cannot be renamed to dst. User can easily rename the tmp file. For the other exception cases, user don't know what to do to fix the problem. If we remove ""deleteOnExit"" flag, then we cannot easily tell whether the tmp file is perfect or not. - Should we not use deprecated API anymore? e.g.",code_debt,low_quality_code,"Thu, 27 Sep 2007 23:33:00 +0000","Wed, 8 Jul 2009 16:42:11 +0000","Fri, 5 Oct 2007 00:09:01 +0000",606961,"+1 Patch looks good. Below are some minor thoughts. I totally agree that there are redundant copyToLocal methods. See also HADOOP-1544. In the case of rename failing, we know that the tmp file is good but cannot be renamed to dst. User can easily rename the tmp file. For the other exception cases, user don't know what to do to fix the problem. If we remove ""deleteOnExit"" flag, then we cannot easily tell whether the tmp file is perfect or not. Should we not use deprecated API anymore? e.g. srcFS.isDirectory(src), srcFS.listPaths(src)",0.152,0.08866666667,positive
hadoop,2077,comment_1,"Minor change in formatting of the output, which now looks like:",code_debt,low_quality_code,"Thu, 18 Oct 2007 18:51:57 +0000","Wed, 8 Jul 2009 16:52:29 +0000","Mon, 14 Jan 2008 18:28:01 +0000",7601764,"Minor change in formatting of the output, which now looks like:",0,0,neutral
hadoop,2148,comment_2,This patch optimizes and so that they perform the data-node blockMap lookup only once. The patch is pretty straightforward. I don't think we should benchmark this.,code_debt,slow_algorithm,"Sat, 3 Nov 2007 00:12:52 +0000","Wed, 8 Jul 2009 16:42:46 +0000","Wed, 19 Mar 2008 22:48:25 +0000",11918133,This patch optimizes FSDataset.getBlockFile() and FSDataset.getLength() so that they perform the data-node blockMap lookup only once. The patch is pretty straightforward. I don't think we should benchmark this.,0.06666666667,0.04,neutral
hadoop,2148,comment_4,This fixes findBugs warnings. I could not reproduce test timeout in This test has a lot of test cases. My suspicion is that if Hudson runs slow it could run out of time on this.,code_debt,low_quality_code,"Sat, 3 Nov 2007 00:12:52 +0000","Wed, 8 Jul 2009 16:42:46 +0000","Wed, 19 Mar 2008 22:48:25 +0000",11918133,This fixes findBugs warnings. I could not reproduce test timeout in TestDFSStorageStateRecovery. This test has a lot of test cases. My suspicion is that if Hudson runs slow it could run out of time on this.,-0.2,-0.15,negative
hadoop,2148,comment_6,+1 This patch looks good. It removes a duplicate block map look up.,code_debt,duplicated_code,"Sat, 3 Nov 2007 00:12:52 +0000","Wed, 8 Jul 2009 16:42:46 +0000","Wed, 19 Mar 2008 22:48:25 +0000",11918133,+1 This patch looks good. It removes a duplicate block map look up.,0.288,0.288,positive
hadoop,2148,description,"first verifies that the block is valid and then returns the file name corresponding to the block. Doing that it performs the data-node blockMap lookup twice. Only one lookup is needed here. This is important since the data-node blockMap is big. Another observation is that data-nodes do not need the blockMap at all. File names can be derived from the block IDs, there is no need to hold Block to File mapping in memory.",code_debt,slow_algorithm,"Sat, 3 Nov 2007 00:12:52 +0000","Wed, 8 Jul 2009 16:42:46 +0000","Wed, 19 Mar 2008 22:48:25 +0000",11918133,"FSDataset.getBlockFile() first verifies that the block is valid and then returns the file name corresponding to the block. Doing that it performs the data-node blockMap lookup twice. Only one lookup is needed here. This is important since the data-node blockMap is big. Another observation is that data-nodes do not need the blockMap at all. File names can be derived from the block IDs, there is no need to hold Block to File mapping in memory.",-0.01111111111,-0.009523809524,neutral
hadoop,2148,summary,Inefficient,code_debt,low_quality_code,"Sat, 3 Nov 2007 00:12:52 +0000","Wed, 8 Jul 2009 16:42:46 +0000","Wed, 19 Mar 2008 22:48:25 +0000",11918133,Inefficient FSDataset.getBlockFile(),-0.75,-0.375,negative
hadoop,2208,comment_10,"Some comments: You don't have to call setSendCounters in JobInProgress.java, and, you probably should rename the APIs getSendCounters and setSendCounters to something more intuitive.",code_debt,low_quality_code,"Thu, 15 Nov 2007 04:47:40 +0000","Wed, 8 Jul 2009 16:52:35 +0000","Thu, 3 Jan 2008 08:16:17 +0000",4246117,"Some comments: You don't have to call setSendCounters in JobInProgress.java, and, you probably should rename the APIs getSendCounters and setSendCounters to something more intuitive.",0,0,neutral
hadoop,2208,comment_12,"I'm pretty worried about the approach of this patch. It takes it from always sending the current values for the counters to just sending the ones that changed. That doesn't seem like an optimization that is likely to be important. Have you run large jobs that show this is important? My concern is that sending the deltas makes the system very vulnerable to losing or duplicating a message. My preference would be to have a boolean in the TaskStatus whether it should be sending the counters or not, but always send the current values of all counters. I'd also recommend against the current sendCounters and doSendCounters. I think your original names were better: Maybe they should be something like:",code_debt,low_quality_code,"Thu, 15 Nov 2007 04:47:40 +0000","Wed, 8 Jul 2009 16:52:35 +0000","Thu, 3 Jan 2008 08:16:17 +0000",4246117,"I'm pretty worried about the approach of this patch. It takes it from always sending the current values for the counters to just sending the ones that changed. That doesn't seem like an optimization that is likely to be important. Have you run large jobs that show this is important? My concern is that sending the deltas makes the system very vulnerable to losing or duplicating a message. My preference would be to have a boolean in the TaskStatus whether it should be sending the counters or not, but always send the current values of all counters. I'd also recommend against the current sendCounters and doSendCounters. I think your original names were better: {get,set}SendCounters. Maybe they should be something like: {get,set} IncludeCounters...",-0.01175,-0.01044444444,negative
hadoop,2208,comment_8,"I see a potential bug with this patch: the TaskTracker _caches_ counters sent by the child-task till it's sent out to the JobTracker via the heartbeat. Hence, we need to *merge* the ones received from the child in not just over-write them as-is today. Minor nit: the merge of the counters probably belongs to rather than ...",code_debt,low_quality_code,"Thu, 15 Nov 2007 04:47:40 +0000","Wed, 8 Jul 2009 16:52:35 +0000","Thu, 3 Jan 2008 08:16:17 +0000",4246117,"I see a potential bug with this patch: the TaskTracker caches counters sent by the child-task till it's sent out to the JobTracker via the heartbeat. Hence, we need to merge the ones received from the child in TaskStatus.statusUpdate, not just over-write them as-is today. Minor nit: the merge of the counters probably belongs to TaskInProgress.recomputeProgress rather than TaskInProgress.updateStatus ...",0,0,neutral
hadoop,2402,comment_0,"Inserted a 64k to Effects compression similar to block, lzo compressed SequenceFile (~20%). For comparison, lzop (command line compression utility backed by lzo lib) uses 256k blocks (and a different file format) and compresses the same 100M sample by 60%. As an aside, with this applied, Zip compressed text files are written approximately 10% faster; uncompressed text about 4% faster.",code_debt,slow_algorithm,"Tue, 11 Dec 2007 02:03:47 +0000","Thu, 2 May 2013 02:29:11 +0000","Thu, 24 Jan 2008 23:23:49 +0000",3878402,"Inserted a 64k BufferedOutputStream to TextOutputFormat::getRecordWriter. Effects compression similar to block, lzo compressed SequenceFile (~20%). For comparison, lzop (command line compression utility backed by lzo lib) uses 256k blocks (and a different file format) and compresses the same 100M sample by 60%. As an aside, with this applied, Zip compressed text files are written approximately 10% faster; uncompressed text about 4% faster.",-0.06666666667,-0.05,neutral
hadoop,2402,comment_10,"Removed most whitespace changes, except those reformatting sections with lines greater than 80 chars",code_debt,low_quality_code,"Tue, 11 Dec 2007 02:03:47 +0000","Thu, 2 May 2013 02:29:11 +0000","Thu, 24 Jan 2008 23:23:49 +0000",3878402,"Removed most whitespace changes, except those reformatting sections with lines greater than 80 chars",0.5,0.5,neutral
hadoop,2402,comment_2,"I think this is probably the right approach, to not require the codecs themselves to return buffered streams. But the size of the buffer should be rather than a fixed 64k, no?",code_debt,low_quality_code,"Tue, 11 Dec 2007 02:03:47 +0000","Thu, 2 May 2013 02:29:11 +0000","Thu, 24 Jan 2008 23:23:49 +0000",3878402,"I think this is probably the right approach, to not require the codecs themselves to return buffered streams. But the size of the buffer should be io.file.buffer.size, rather than a fixed 64k, no?",0.2635,0.1054,neutral
hadoop,2402,comment_4,"We can't compress multiple buffers together with the lzo codec? It only compresses buffer-at-a-time? If so, then it should do the buffering & set the buffer size, since this is an lzo-specific issue. I don't think it's a bug for a codec to return a buffered stream if a particular buffer size is required to get good performance from that codec. If it's impossible to get lzo to compress data across buffers, and 64k or larger is required to get good compression, then it should mandate that buffer size, perhaps adding a new configuration parameter. Separately, we should consider whether to (a) unilaterally add an io.file.buffer.size buffer in TextOutputFormat, since it helps other codecs, or (b) assume that all codecs return appropriately buffered streams, and add a buffer in the Zip codec if it improves performance. If a io.file.buffer.size buffer=4k gives somewhat improved Zip performance, and a 64k buffer gives even better performance, I think that's okay. Performance should improve a bit by increasing at the expense of chewing up more memory per open file. The default setting should be for decent performance with minimal memory use.",code_debt,slow_algorithm,"Tue, 11 Dec 2007 02:03:47 +0000","Thu, 2 May 2013 02:29:11 +0000","Thu, 24 Jan 2008 23:23:49 +0000",3878402,"> For Lzo, this means it will compress no more than 4k at a time, yielding even less than 20% compression. We can't compress multiple buffers together with the lzo codec? It only compresses buffer-at-a-time? If so, then it should do the buffering & set the buffer size, since this is an lzo-specific issue. > This might be a better place to add some buffering, but then the codec will be returning a buffered stream. I don't think it's a bug for a codec to return a buffered stream if a particular buffer size is required to get good performance from that codec. If it's impossible to get lzo to compress data across buffers, and 64k or larger is required to get good compression, then it should mandate that buffer size, perhaps adding a new configuration parameter. Separately, we should consider whether to (a) unilaterally add an io.file.buffer.size buffer in TextOutputFormat, since it helps other codecs, or (b) assume that all codecs return appropriately buffered streams, and add a buffer in the Zip codec if it improves performance. If a io.file.buffer.size buffer=4k gives somewhat improved Zip performance, and a 64k buffer gives even better performance, I think that's okay. Performance should improve a bit by increasing io.file.buffer.size, at the expense of chewing up more memory per open file. The default setting should be for decent performance with minimal memory use.",0.09347777778,0.09510833333,neutral
hadoop,2402,comment_9,"Mostly looks ok, but there are too many unrelated white-space changes - hence, I'm cancelling this patch.",code_debt,low_quality_code,"Tue, 11 Dec 2007 02:03:47 +0000","Thu, 2 May 2013 02:29:11 +0000","Thu, 24 Jan 2008 23:23:49 +0000",3878402,"Mostly looks ok, but there are too many unrelated white-space changes - hence, I'm cancelling this patch.",0.75,0.75,negative
hadoop,2851,comment_3,Talked to Chris: these debug messages do not seem useful. So they are better removed. removed the debug messages and fixed all unchecked warning in Configuration.,code_debt,low_quality_code,"Tue, 19 Feb 2008 16:34:00 +0000","Tue, 28 Jul 2009 01:14:28 +0000","Tue, 28 Jul 2009 01:14:28 +0000",45304828,Talked to Chris: these debug messages do not seem useful. So they are better removed. 2851_20081103.patch: removed the debug messages and fixed all unchecked warning in Configuration.,-0.2,-0.15,negative
hadoop,2851,description,The constructors for the Configuration object contains a superfluous logging message that logs an IOException whenever logging is enabled for the debug level. Basically both constructors have the statement: if { } I can' t see any reason for it to be there and it just ends up leaving bogus IOExceptions in log files. It looks like its an old debug print statement which has accidentally been left in.,code_debt,low_quality_code,"Tue, 19 Feb 2008 16:34:00 +0000","Tue, 28 Jul 2009 01:14:28 +0000","Tue, 28 Jul 2009 01:14:28 +0000",45304828,"The constructors for the Configuration object contains a superfluous logging message that logs an IOException whenever logging is enabled for the debug level. Basically both constructors have the statement: if (LOG.isDebugEnabled()) { LOG.debug(StringUtils.stringifyException(new IOException(""config()""))); } I can' t see any reason for it to be there and it just ends up leaving bogus IOExceptions in log files. It looks like its an old debug print statement which has accidentally been left in.",-0.654,-0.327,negative
hadoop,2851,summary,Bogus logging messages in Configration object constructors,code_debt,low_quality_code,"Tue, 19 Feb 2008 16:34:00 +0000","Tue, 28 Jul 2009 01:14:28 +0000","Tue, 28 Jul 2009 01:14:28 +0000",45304828,Bogus logging messages in Configration object constructors,-0.781,-0.781,neutral
hadoop,289,comment_3,Please replace the getLocalizedMessage and implicit toString with calls to which includes both the message and the call stack. The call stack helps a lot in finding and debugging the problem.,code_debt,low_quality_code,"Thu, 8 Jun 2006 09:34:01 +0000","Wed, 8 Jul 2009 16:41:54 +0000","Sat, 10 Jun 2006 00:16:56 +0000",139375,"Please replace the getLocalizedMessage and implicit toString with calls to StringUtils.stringifyException, which includes both the message and the call stack. The call stack helps a lot in finding and debugging the problem.",0.1,0.06666666667,neutral
hadoop,289,description,- Datanode needs to catch when registering otherwise it goes down the same way as when the namenode is not available (HADOOP-282). - need to be caught for all non-registering requests. The data node should be shutdown in this case. Otherwise it will loop infinitely and consume namenode resources.,code_debt,low_quality_code,"Thu, 8 Jun 2006 09:34:01 +0000","Wed, 8 Jul 2009 16:41:54 +0000","Sat, 10 Jun 2006 00:16:56 +0000",139375,Datanode needs to catch SocketTimeoutException when registering otherwise it goes down the same way as when the namenode is not available (HADOOP-282). UnregisteredDatanodeException need to be caught for all non-registering requests. The data node should be shutdown in this case. Otherwise it will loop infinitely and consume namenode resources.,-0.09733333333,-0.073,neutral
hadoop,2965,description,From internal bug from Arkady.. Map reduce jobs have names. The log files use mysterious names like They should use real job names so that users can find the logs they need.,code_debt,low_quality_code,"Fri, 7 Mar 2008 09:34:51 +0000","Wed, 18 May 2011 22:03:41 +0000","Wed, 18 May 2011 22:03:41 +0000",100873730,From internal bug from Arkady.. ============================= Map reduce jobs have names. The log files use mysterious names like jobfailures.jsp_jobid_job_200712150418_0001_kind_map_cause_failed.html They should use real job names so that users can find the logs they need.,-0.5,-0.35725,negative
hadoop,3081,comment_1,"The non-static f, FsPermission permission) and the static fs, Path dir, FsPermission permission) look similar but they are different in semantic: - The non-static f, FsPermission permission) likes mkdir in Unix, it makes a directory with *umask*. - However, the static fs, Path dir, FsPermission permission) makes a directory with *absolute permission*. So I think it is better to change one of the method name (probably the static one), otherwise, it would be confusing.",code_debt,low_quality_code,"Mon, 24 Mar 2008 17:37:05 +0000","Wed, 8 Jul 2009 16:42:58 +0000","Fri, 19 Sep 2008 23:29:59 +0000",15486774,"The non-static FileSystem.mkdirs(Path f, FsPermission permission) and the static FileSystem.mkdirs(FileSystem fs, Path dir, FsPermission permission) look similar but they are different in semantic: The non-static FileSystem.mkdirs(Path f, FsPermission permission) likes mkdir in Unix, it makes a directory with umask. However, the static FileSystem.mkdirs(FileSystem fs, Path dir, FsPermission permission) makes a directory with absolute permission. So I think it is better to change one of the method name (probably the static one), otherwise, it would be confusing.",-0.3603333333,-0.2315238095,neutral
hadoop,3198,comment_1,Some comments. 1) Declare a _private static final_ for {{MAX_DFS_RETRIES}} and initialize it to 10. Use this in the for loop. 2) Remove extra spaces after {{reporter}} (line 14 of the patch) 3) Sleeping for 1 sec needs to be argued. Btw a log message is required before waiting. 4) Some extra code slipped in (regarding the log message). 5) After 10 retries we should throw the exception rather than silently coming out of the loop (leading to null pointer exception). +_Points to ponder_+ Can we do a timeout based stuff where we wait for _shuffle-run-time / 2_ before bailing out and having multiple retries within this timeout. This will somehow make sure that we dont kill the reducer too early.,code_debt,low_quality_code,"Sat, 5 Apr 2008 19:30:54 +0000","Wed, 8 Jul 2009 16:52:43 +0000","Thu, 17 Apr 2008 16:42:38 +0000",1026704,Some comments. 1) Declare a private static final for MAX_DFS_RETRIES and initialize it to 10. Use this in the for loop. 2) Remove extra spaces after reporter (line 14 of the patch) 3) Sleeping for 1 sec needs to be argued. Btw a log message is required before waiting. 4) Some extra code slipped in (regarding the log message). 5) After 10 retries we should throw the exception rather than silently coming out of the loop (leading to null pointer exception). Points to ponder Can we do a timeout based stuff where we wait for shuffle-run-time / 2 before bailing out and having multiple retries within this timeout. This will somehow make sure that we dont kill the reducer too early.,0.06944444444,0.06944444444,neutral
hadoop,3198,comment_5,This will lead to very unmaintainable code. We absolutely do not want to have nested retries for different contexts.,code_debt,low_quality_code,"Sat, 5 Apr 2008 19:30:54 +0000","Wed, 8 Jul 2009 16:52:43 +0000","Thu, 17 Apr 2008 16:42:38 +0000",1026704,This will lead to very unmaintainable code. We absolutely do not want to have nested retries for different contexts.,-0.1,-0.1,negative
hadoop,3337,comment_12,"DatanodeDescriptor is not sent over RPC and is not supposed to. You can never get DatanodeDescriptor on the other end. DatanodeDescriptor is sort of a name-node private class. Although the actual class is DatanodeDescriptor, rpc serializes the base class DatanodeInfo using its Writable implementation and sends the latter over the network. The problem here is that the serialization intended for DatanodeDescriptor (which is only serialized to disk) is mixed with the serialization of DatanodeInfo (which should be used only for rpc). We have been through this before. I think we should introduce 2 new static methods in the DatanodeDescriptor that would provide serialization to disk.",code_debt,low_quality_code,"Fri, 2 May 2008 00:36:44 +0000","Wed, 8 Jul 2009 16:43:04 +0000","Sun, 4 May 2008 19:29:32 +0000",240768,"DatanodeDescriptor is not sent over RPC and is not supposed to. You can never get DatanodeDescriptor on the other end. DatanodeDescriptor is sort of a name-node private class. Although the actual class is DatanodeDescriptor, rpc serializes the base class DatanodeInfo using its Writable implementation and sends the latter over the network. The problem here is that the serialization intended for DatanodeDescriptor (which is only serialized to disk) is mixed with the serialization of DatanodeInfo (which should be used only for rpc). We have been through this before. I think we should introduce 2 new static methods in the DatanodeDescriptor that would provide serialization to disk.",-0.1178571429,-0.1178571429,negative
hadoop,3337,comment_2,"This patch works on my old file system image. Minor comments, please - remove import of UTF8 - provide comments on the 2 new methods *FSEditLog() explaining what they are for.",code_debt,low_quality_code,"Fri, 2 May 2008 00:36:44 +0000","Wed, 8 Jul 2009 16:43:04 +0000","Sun, 4 May 2008 19:29:32 +0000",240768,"This patch works on my old file system image. Minor comments, please remove import of UTF8 provide comments on the 2 new methods *FSEditLog() explaining what they are for.",0.4,0.4,neutral
hadoop,3377,description,"A minor cleanup. In the TaskRunner, we can read : ""When hadoop moves to JDK1.5, replace with String#replace"" This patch do so, removing ~30 lines.",code_debt,low_quality_code,"Tue, 13 May 2008 08:14:50 +0000","Wed, 8 Jul 2009 16:52:47 +0000","Fri, 16 May 2008 00:15:30 +0000",230440,"A minor cleanup. In the TaskRunner, we can read : ""When hadoop moves to JDK1.5, replace [TaskRunner.replaceAll] with String#replace"" This patch do so, removing ~30 lines.",0,0,neutral
hadoop,3491,description,catches a general Exception and logs them no matter what. It should explicitly catch the and exit gracefully.,code_debt,low_quality_code,"Thu, 5 Jun 2008 00:15:38 +0000","Wed, 8 Jul 2009 16:43:08 +0000","Sat, 7 Jun 2008 00:00:23 +0000",171885,ResolutionMonitor.run() catches a general Exception and logs them no matter what. It should explicitly catch the InterruptedException and exit gracefully.,-0.1,-0.06666666667,neutral
hadoop,3501,description,"As of HADOOP-2095, InMemoryFileSystem is no longer used. Its design was optimized for a particular application and it is thus not a good general-purpose RAM-based FileSystem implementation, so it ought to be removed.",code_debt,dead_code,"Thu, 5 Jun 2008 16:47:42 +0000","Thu, 13 Jan 2011 04:52:23 +0000","Fri, 6 Jun 2008 20:14:41 +0000",98819,"As of HADOOP-2095, InMemoryFileSystem is no longer used. Its design was optimized for a particular application and it is thus not a good general-purpose RAM-based FileSystem implementation, so it ought to be removed.",-0.388,-0.388,negative
hadoop,3560,description,Archvies creates a bunch of empty part files sometimes which are not necessary.,code_debt,low_quality_code,"Fri, 13 Jun 2008 20:20:56 +0000","Wed, 8 Jul 2009 16:41:31 +0000","Tue, 17 Jun 2008 05:15:34 +0000",291278,Archvies creates a bunch of empty part files sometimes which are not necessary.,0.15,0.15,negative
hadoop,3560,summary,Archvies sometimes create empty part files.,code_debt,low_quality_code,"Fri, 13 Jun 2008 20:20:56 +0000","Wed, 8 Jul 2009 16:41:31 +0000","Tue, 17 Jun 2008 05:15:34 +0000",291278,Archvies sometimes create empty part files.,-0.2,-0.2,neutral
hadoop,3649,comment_4,"This looks right. I made some minor changes to the patch: # removed method which is not used anywhere, and # moved into FSNamesystem, which imo it belongs to based on the structure of our name-node class hierarchy.",code_debt,dead_code,"Thu, 26 Jun 2008 17:28:58 +0000","Wed, 8 Jul 2009 16:43:12 +0000","Tue, 1 Jul 2008 02:37:01 +0000",378483,"This looks right. I made some minor changes to the patch: removed CorruptReplicasMap.size() method which is not used anywhere, and moved CorruptReplicasMap.invalidateCorruptReplicas() into FSNamesystem, which imo it belongs to based on the structure of our name-node class hierarchy.",0.2635,0.13175,neutral
hadoop,3836,comment_0,The test output says The output directory was not cleaned up after the test is done.,code_debt,low_quality_code,"Fri, 25 Jul 2008 20:13:51 +0000","Wed, 8 Jul 2009 16:52:55 +0000","Fri, 1 Aug 2008 22:45:04 +0000",613873,The test output says The output directory was not cleaned up after the test is done.,0,0,negative
hadoop,3849,description,"The 'notify' done by causes the to immediately run to the JobTracker On a 3500 node cluster, I saw that each TaskTracker calls multiple times per-second. This caused the JobTracker's RPC queues to back-up resulting in each RPC spending more than 120s in the queue - leading to shuffle proceeding very very slowly.",code_debt,slow_algorithm,"Tue, 29 Jul 2008 04:58:34 +0000","Wed, 8 Jul 2009 16:52:56 +0000","Tue, 29 Jul 2008 17:52:13 +0000",46419,"The 'notify' done by FetchStatus.getMapEvents causes the MapEventsFetcherThread to immediately run to the JobTracker (getTaskCompletionEvents). On a 3500 node cluster, I saw that each TaskTracker calls JobTracker.getTaskCompletionEvents multiple times per-second. This caused the JobTracker's RPC queues to back-up resulting in each RPC spending more than 120s in the queue - leading to shuffle proceeding very very slowly.",0.2,0.08,neutral
hadoop,3957,comment_0,fixed unchecked warnings. Removed some not-so-useful methods and the use of deprecated API.,code_debt,dead_code,"Fri, 15 Aug 2008 00:06:03 +0000","Wed, 26 Sep 2012 13:59:09 +0000","Mon, 18 Aug 2008 20:54:41 +0000",334118,3957_20080814.patch: fixed unchecked warnings. Removed some not-so-useful methods and the use of deprecated API.,-0.05,-0.03333333333,negative
hadoop,3957,description,There are a few javac warning in DistCp and TestCopyFiles.,code_debt,low_quality_code,"Fri, 15 Aug 2008 00:06:03 +0000","Wed, 26 Sep 2012 13:59:09 +0000","Mon, 18 Aug 2008 20:54:41 +0000",334118,There are a few javac warning in DistCp and TestCopyFiles.,-0.6,-0.6,neutral
hadoop,3957,summary,Fix javac warnings in DistCp and the corresponding tests,code_debt,low_quality_code,"Fri, 15 Aug 2008 00:06:03 +0000","Wed, 26 Sep 2012 13:59:09 +0000","Mon, 18 Aug 2008 20:54:41 +0000",334118,Fix javac warnings in DistCp and the corresponding tests,-0.6,-0.6,neutral
hadoop,4436,description,"Consider a bucket with the following object names: * / * /foo * foo//bar NativeS3FileSystem treats an object named ""/"" as a directory. Doing an ""fs -lsr"" causes an infinite loop. I suggest we change NativeS3FileSystem to handle these by ignoring any such ""invalid"" names. Thoughts?",code_debt,low_quality_code,"Fri, 17 Oct 2008 01:06:08 +0000","Tue, 30 Jun 2015 07:22:34 +0000","Fri, 16 Jan 2015 13:32:33 +0000",197209585,"Consider a bucket with the following object names: / /foo foo//bar NativeS3FileSystem treats an object named ""/"" as a directory. Doing an ""fs -lsr"" causes an infinite loop. I suggest we change NativeS3FileSystem to handle these by ignoring any such ""invalid"" names. Thoughts?",-0.2603333333,-0.2603333333,neutral
hadoop,4436,summary,S3 object names with arbitrary slashes confuse NativeS3FileSystem,code_debt,low_quality_code,"Fri, 17 Oct 2008 01:06:08 +0000","Tue, 30 Jun 2015 07:22:34 +0000","Fri, 16 Jan 2015 13:32:33 +0000",197209585,S3 object names with arbitrary slashes confuse NativeS3FileSystem,-0.2724,-0.2724,negative
hadoop,4576,comment_10,Changing visibility of the constructor and removing the deprecation. ant test-patch output for the patch is :,code_debt,dead_code,"Mon, 3 Nov 2008 05:38:44 +0000","Wed, 8 Jul 2009 16:40:30 +0000","Fri, 5 Dec 2008 05:14:19 +0000",2763335,Changing visibility of the constructor and removing the deprecation. ant test-patch output for the patch is :,-0.25,-0.25,neutral
hadoop,4576,comment_5,"Comments : : 1) No need to import {{AtomicInteger}}. Also plz remove extra diffs. : 1) You cant simply change the constructor def. Overload it and deprecate the other if needed. 2) Extra diffs w.r.t. 3) Use {{StringUtils}} for formatting time. 1) The ordering doesnt seem right. You submit 5 jobs, try to assign tasks (which should be no-op) and then you init the jobs. 2) Shouldnt we also check/test the timing issue that after _poll-interval_ units of time the values are correct. Something like - add jobs - check the queue-sched-info - allow the jobs to be inited by the poller i.e wait for _poller-interval_ time - check again to see if the change is made. 2) Plz mark the start and end of a new sub-test using comments",code_debt,low_quality_code,"Mon, 3 Nov 2008 05:38:44 +0000","Wed, 8 Jul 2009 16:40:30 +0000","Fri, 5 Dec 2008 05:14:19 +0000",2763335,"Comments : JobQueuesManager.java : 1) No need to import AtomicInteger. Also plz remove extra diffs. CapacityTaskScheduler.java : 1) You cant simply change the constructor def. Overload it and deprecate the other if needed. 2) Extra diffs w.r.t. 3) + sb.append(String.format(""* Scheduling information can be off by "" + + ""maximum of %d seconds\n"", pollingInterval/1000)); Use StringUtils for formatting time. TestCapacityScheduler.java 1) + scheduler.assignTasks(tracker(""tt1"")); // heartbeat + p.selectJobsToInitialize(); The ordering doesnt seem right. You submit 5 jobs, try to assign tasks (which should be no-op) and then you init the jobs. 2) Shouldnt we also check/test the timing issue that after poll-interval units of time the values are correct. Something like add jobs check the queue-sched-info allow the jobs to be inited by the poller i.e wait for poller-interval time check again to see if the change is made. 2) Plz mark the start and end of a new sub-test using comments",-0.1152,-0.0329375,neutral
hadoop,4576,comment_9,"I think it is not necessary to deprecate the constructor of SchedulingInfo. It is a private static inner class and hence nobody should be affected ? It would just add more code to maintain. Can you please submit a new patch removing the deprecation and just changing the api ? Please also remember to remove the checks for null in the toString method, as the object is not expected to be null after this change. BTW, I spoke to Amar about this and we agree on this point now.",code_debt,dead_code,"Mon, 3 Nov 2008 05:38:44 +0000","Wed, 8 Jul 2009 16:40:30 +0000","Fri, 5 Dec 2008 05:14:19 +0000",2763335,"I think it is not necessary to deprecate the constructor of SchedulingInfo. It is a private static inner class and hence nobody should be affected ? It would just add more code to maintain. Can you please submit a new patch removing the deprecation and just changing the api ? Please also remember to remove the checks for null in the toString method, as the object is not expected to be null after this change. BTW, I spoke to Amar about this and we agree on this point now.",-0.02808333333,-0.02808333333,neutral
hadoop,4576,description,"The UI for capacity scheduler displays 'pending tasks' counts. However the capacity scheduler does not update these counts to be the actual values for optimization purposes, for e.g. to avoid walking all pending jobs on all heartbeats. Hence this information is not very accurate. Also, while 'running tasks' counts are useful to compare against capacities and limits, 'pending tasks' counts do not add too much user value. A better count to display would be the number of running and pending jobs.",code_debt,low_quality_code,"Mon, 3 Nov 2008 05:38:44 +0000","Wed, 8 Jul 2009 16:40:30 +0000","Fri, 5 Dec 2008 05:14:19 +0000",2763335,"The UI for capacity scheduler displays 'pending tasks' counts. However the capacity scheduler does not update these counts to be the actual values for optimization purposes, for e.g. to avoid walking all pending jobs on all heartbeats. Hence this information is not very accurate. Also, while 'running tasks' counts are useful to compare against capacities and limits, 'pending tasks' counts do not add too much user value. A better count to display would be the number of running and pending jobs.",-0.03666666667,-0.03666666667,negative
hadoop,4634,summary,220 javac compiler warnings,code_debt,low_quality_code,"Tue, 11 Nov 2008 18:25:52 +0000","Thu, 20 Nov 2008 23:20:27 +0000","Wed, 12 Nov 2008 01:58:08 +0000",27136,220 javac compiler warnings,-0.6,-0.6,negative
hadoop,481,comment_2,Instead of passing a long[] you should pass a struct that implements Writable. Probably TaskMetrics would be a good name for this.,code_debt,low_quality_code,"Thu, 24 Aug 2006 22:48:37 +0000","Wed, 8 Jul 2009 16:51:53 +0000","Tue, 18 Sep 2007 19:38:45 +0000",33684608,Instead of passing a long[] you should pass a struct that implements Writable. Probably TaskMetrics would be a good name for this.,0.388,0.388,neutral
hadoop,4884,description,The tool tip of Time series chart for Chukwa is formatting date as: day/month/year hour:minute:second The date format should change to: year/month/day hour:minute:second,code_debt,low_quality_code,"Tue, 16 Dec 2008 19:13:15 +0000","Wed, 8 Jul 2009 16:40:49 +0000","Thu, 8 Jan 2009 08:26:43 +0000",1948408,The tool tip of Time series chart for Chukwa is formatting date as: day/month/year hour:minute:second The date format should change to: year/month/day hour:minute:second,0,0,neutral
hadoop,4941,comment_0,remove the deprecated methods.,code_debt,dead_code,"Wed, 24 Dec 2008 19:01:56 +0000","Tue, 24 Aug 2010 20:34:43 +0000","Thu, 1 Jan 2009 18:07:36 +0000",687940,4941_20081224.patch: remove the deprecated methods.,0,0,neutral
hadoop,4985,comment_0,"remove unnecessary ""throws IOException""",code_debt,dead_code,"Wed, 7 Jan 2009 02:10:41 +0000","Wed, 12 Aug 2020 23:02:16 +0000","Mon, 12 Jan 2009 21:18:36 +0000",500875,"4985_20090106.patch: remove unnecessary ""throws IOException""",0,0,negative
hadoop,4985,description,"In FSDirectory, quite a few methods are unnecessary declared with ""throws IOException"". In some cases, it can just be removed without causing any compilation problem. In some other cases, it can be replaced with a specific subclass like",code_debt,low_quality_code,"Wed, 7 Jan 2009 02:10:41 +0000","Wed, 12 Aug 2020 23:02:16 +0000","Mon, 12 Jan 2009 21:18:36 +0000",500875,"In FSDirectory, quite a few methods are unnecessary declared with ""throws IOException"". In some cases, it can just be removed without causing any compilation problem. In some other cases, it can be replaced with a specific subclass like QuotaExceededException.",0.1333333333,0.1333333333,neutral
hadoop,4985,summary,IOException is abused in FSDirectory,code_debt,low_quality_code,"Wed, 7 Jan 2009 02:10:41 +0000","Wed, 12 Aug 2020 23:02:16 +0000","Mon, 12 Jan 2009 21:18:36 +0000",500875,IOException is abused in FSDirectory,-0.6,-0.6,negative
hadoop,5076,description,In the log4j appender always rewrite the file instead of append to the log file. This should be changed to append to ensure the metrics log file is streamed correctly.,code_debt,low_quality_code,"Sat, 17 Jan 2009 01:42:28 +0000","Wed, 8 Jul 2009 16:40:47 +0000","Tue, 10 Feb 2009 02:40:06 +0000",2077058,"In Log4jMetricsContext, the log4j appender always rewrite the file instead of append to the log file. This should be changed to append to ensure the metrics log file is streamed correctly.",0.175,0.175,neutral
hadoop,5097,comment_0,"- change JspHelper.fsn from ""static"" to ""private final"" - change a few non-static methods in JspHelper to static",code_debt,low_quality_code,"Wed, 21 Jan 2009 22:37:59 +0000","Tue, 24 Aug 2010 20:35:10 +0000","Mon, 2 Feb 2009 18:43:39 +0000",1022740,"5097_20090121.patch: change JspHelper.fsn from ""static"" to ""private final"" change a few non-static methods in JspHelper to static",-0.4375,-0.2916666667,neutral
hadoop,5097,comment_1,added javadoc and re-organized some methods.,code_debt,low_quality_code,"Wed, 21 Jan 2009 22:37:59 +0000","Tue, 24 Aug 2010 20:35:10 +0000","Mon, 2 Feb 2009 18:43:39 +0000",1022740,5097_20090126.patch: added javadoc and re-organized some methods.,0,0,neutral
hadoop,5097,comment_2,"# Remove comment in FSNamesystem /** NameNode RPC address */ # I do not very much like that you add JspHelper as a NameNode members. I understand the intention is not to instantiate objects on the web UI, but may be we should rather consider making most of the methods / fields of JspHelper static instead of constructing the object. In any case it is better not to introduce helper variables in NamNode and DataNode.",code_debt,low_quality_code,"Wed, 21 Jan 2009 22:37:59 +0000","Tue, 24 Aug 2010 20:35:10 +0000","Mon, 2 Feb 2009 18:43:39 +0000",1022740,"Remove comment in FSNamesystem /** NameNode RPC address */ I do not very much like that you add JspHelper as a NameNode members. I understand the intention is not to instantiate objects on the web UI, but may be we should rather consider making most of the methods / fields of JspHelper static instead of constructing the object. In any case it is better not to introduce helper variables in NamNode and DataNode.",-0.07283333333,-0.07283333333,negative
hadoop,5097,comment_3,changed all JspHelp methods to static and removed /** NameNode RPC address */,code_debt,low_quality_code,"Wed, 21 Jan 2009 22:37:59 +0000","Tue, 24 Aug 2010 20:35:10 +0000","Mon, 2 Feb 2009 18:43:39 +0000",1022740,5097_20090127.patch: changed all JspHelp methods to static and removed /** NameNode RPC address */,-0.875,-0.4375,neutral
hadoop,5097,comment_4,should be done in static. Thank Suresh for catching this.,code_debt,low_quality_code,"Wed, 21 Jan 2009 22:37:59 +0000","Tue, 24 Aug 2010 20:35:10 +0000","Mon, 2 Feb 2009 18:43:39 +0000",1022740,5097_20090127b.patch: UnixUserGroupInformation.saveToConf() should be done in static. Thank Suresh for catching this.,-0.2125,-0.1416666667,neutral
hadoop,5097,description,"There is another static FSNamesystem variable, fsn, declared in JspHelper. We should remove it.",code_debt,dead_code,"Wed, 21 Jan 2009 22:37:59 +0000","Tue, 24 Aug 2010 20:35:10 +0000","Mon, 2 Feb 2009 18:43:39 +0000",1022740,"There is another static FSNamesystem variable, fsn, declared in JspHelper. We should remove it.",-0.4375,-0.4375,negative
hadoop,511,summary,mapred.reduce.tasks not used,code_debt,dead_code,"Wed, 6 Sep 2006 17:36:06 +0000","Wed, 8 Jul 2009 16:51:56 +0000","Thu, 24 May 2007 06:42:07 +0000",22424761,mapred.reduce.tasks not used,0,0,neutral
hadoop,5298,comment_4,"There is no difference in the logic. I agree that two tests are similar. The codes were duplicated in HADOOP-4284. It needs more works to refactor the codes now. Since these are only tests, I suggest we leave it and do the refactoring in the future.",code_debt,duplicated_code,"Fri, 20 Feb 2009 16:38:04 +0000","Thu, 23 Apr 2009 19:18:04 +0000","Mon, 9 Mar 2009 22:15:41 +0000",1489057,"> It looks like HADOOP-4695 is addressing the same issue, but uses a different approach. Is the difference significant? The two tests are nearly identical and could probably share more code than they currently do. There is no difference in the logic. I agree that two tests are similar. The codes were duplicated in HADOOP-4284. It needs more works to refactor the codes now. Since these are only tests, I suggest we leave it and do the refactoring in the future.",0.12,0.125,neutral
hadoop,537,comment_0,"We shouldn't need a clean-X target for every compile-X target, since compile-X targets should place things in the build directory and the build directory should be removed by the single clean target. We should also try not to clutter the top-level build.xml. If a separate clean-X target is required for productive development in some subtree, then it should be placed in a separate build.xml or Makefile in that source subtree. It can then remove appropriate items from the top-level build directory. But, ideally, the top-level 'clean' target should be able to remove all generated items by simply removing the top-level build directory.",code_debt,low_quality_code,"Fri, 15 Sep 2006 00:43:20 +0000","Wed, 8 Jul 2009 17:05:59 +0000","Mon, 25 Sep 2006 23:08:59 +0000",944739,"We shouldn't need a clean-X target for every compile-X target, since compile-X targets should place things in the build directory and the build directory should be removed by the single clean target. We should also try not to clutter the top-level build.xml. If a separate clean-X target is required for productive development in some subtree, then it should be placed in a separate build.xml or Makefile in that source subtree. It can then remove appropriate items from the top-level build directory. But, ideally, the top-level 'clean' target should be able to remove all generated items by simply removing the top-level build directory.",0.1589285714,0.1589285714,neutral
hadoop,537,comment_1,"This is an orthogonal approach but I like it too. In this case we should not have any clean-X targets at all. Some more details on this particular issue. The real problem is that ""compile-libhdfs"" creates temporary files in the source directory. Sure they need to be cleaned, but they should not be created there in the first place. I think this have already been discussed, don't remember where.",code_debt,low_quality_code,"Fri, 15 Sep 2006 00:43:20 +0000","Wed, 8 Jul 2009 17:05:59 +0000","Mon, 25 Sep 2006 23:08:59 +0000",944739,"This is an orthogonal approach but I like it too. In this case we should not have any clean-X targets at all. Some more details on this particular issue. The real problem is that ""compile-libhdfs"" creates temporary files in the source directory. Sure they need to be cleaned, but they should not be created there in the first place. I think this have already been discussed, don't remember where.",-0.06666666667,-0.06666666667,neutral
hadoop,537,comment_2,Here's a patch which ensures that no temporary files are created in the src/c++/libhdfs; including .o/.so etc. They are all put in build/libhdfs. In fact I've ensured that even the doxygen generated docs go to build/libhdfs/docs and can be packaged aptly. Konstantin - can you please confirm that it works for you? thanks. I've kept the 'clean-libhdfs' target around in build.xml so that devs working on libhdfs can use it without needing to invoke the top-level 'clean' which nukes the 'build' directory.,code_debt,low_quality_code,"Fri, 15 Sep 2006 00:43:20 +0000","Wed, 8 Jul 2009 17:05:59 +0000","Mon, 25 Sep 2006 23:08:59 +0000",944739,Here's a patch which ensures that no temporary files are created in the src/c++/libhdfs; including .o/.so etc. They are all put in build/libhdfs. In fact I've ensured that even the doxygen generated docs go to build/libhdfs/docs and can be packaged aptly. Konstantin - can you please confirm that it works for you? thanks. I've kept the 'clean-libhdfs' target around in build.xml so that devs working on libhdfs can use it without needing to invoke the top-level 'clean' which nukes the 'build' directory.,0.1814814815,0.1814814815,neutral
hadoop,537,description,"It produces the following: BUILD FAILED Execute failed: CreateProcess: make clean error=2 Besides, I would propose to have clean-* target for every compile-* target in build.xml. Some people probably don't build libhdfs or contrib, so why should they clean it.",code_debt,low_quality_code,"Fri, 15 Sep 2006 00:43:20 +0000","Wed, 8 Jul 2009 17:05:59 +0000","Mon, 25 Sep 2006 23:08:59 +0000",944739,"It produces the following: BUILD FAILED Hadoop\build.xml:496: Execute failed: java.io.IOException: CreateProcess: make clean error=2 Besides, I would propose to have clean-* target for every compile-* target in build.xml. Some people probably don't build libhdfs or contrib, so why should they clean it.",0.1066666667,-0.04444444444,negative
hadoop,5465,comment_1,"Two bugs in DFS contributed to the problem: (1). DataNode does not sync on modification to the counter ""xmitsInProgress"", which keeps track of the number of replication in progress. When two threads update the counter concurrently, race condition may occurs. The counter may change to be a non-zero value when no replication is going on. (2). Each DN is configured to have at most 2 replications in progress. When DN notifies NN that it has 1 replication in progress, NN should be able to send one block replication request to DN. But NN wrongly interprets the counter as the number of targets. When it sees that the block is scheduled to 2 targets but DN can only take 1, it sends an empty replication request to DN. As a result, blocking all replications from this DataNode. If the DataNode is the only source of an under-replicated block, the block will never get replicated. Fixing either (1) or (2) could fix the problem. I think (1) is more fundamental so I will fix (1) in this jira and file a different jira to fix (2).",code_debt,multi-thread_correctness,"Wed, 11 Mar 2009 23:03:36 +0000","Wed, 8 Jul 2009 16:43:33 +0000","Fri, 13 Mar 2009 19:59:42 +0000",161766,"Two bugs in DFS contributed to the problem: (1). DataNode does not sync on modification to the counter ""xmitsInProgress"", which keeps track of the number of replication in progress. When two threads update the counter concurrently, race condition may occurs. The counter may change to be a non-zero value when no replication is going on. (2). Each DN is configured to have at most 2 replications in progress. When DN notifies NN that it has 1 replication in progress, NN should be able to send one block replication request to DN. But NN wrongly interprets the counter as the number of targets. When it sees that the block is scheduled to 2 targets but DN can only take 1, it sends an empty replication request to DN. As a result, blocking all replications from this DataNode. If the DataNode is the only source of an under-replicated block, the block will never get replicated. Fixing either (1) or (2) could fix the problem. I think (1) is more fundamental so I will fix (1) in this jira and file a different jira to fix (2).",-0.07723076923,-0.07723076923,neutral
hadoop,561,description,"one replica of a file should be written locally if possible. That's currently not the case. Copying a 1GB file using hadoop dfs -cp running on one of the cluster nodes, all the blocks were written to remote nodes, as seen by fsck -files -blocks -locations on the newly created file. as long as there is sufficient space locally, a local copy has significant performance benefits.",code_debt,slow_algorithm,"Tue, 26 Sep 2006 16:50:35 +0000","Wed, 8 Jul 2009 16:42:04 +0000","Fri, 20 Oct 2006 17:33:19 +0000",2076164,"one replica of a file should be written locally if possible. That's currently not the case. Copying a 1GB file using hadoop dfs -cp running on one of the cluster nodes, all the blocks were written to remote nodes, as seen by fsck -files -blocks -locations on the newly created file. as long as there is sufficient space locally, a local copy has significant performance benefits.",0.05208333333,0.05208333333,neutral
hadoop,564,description,"Minor nit, but it seems that we should choose a protocol name that is likely not to conflict with other distributed FS projects. HDFS seems less likely to. Right now this will be trivial to change. Just wanted to socialize this before doing the search and replace. PS right now the dfs: usage is not consistent and has crept into a couple of new features the y! team is working on (caching of files and distcp).",code_debt,low_quality_code,"Thu, 28 Sep 2006 03:42:53 +0000","Wed, 8 Jul 2009 16:42:03 +0000","Wed, 21 Feb 2007 22:36:13 +0000",12682400,"Minor nit, but it seems that we should choose a protocol name that is likely not to conflict with other distributed FS projects. HDFS seems less likely to. Right now this will be trivial to change. Just wanted to socialize this before doing the search and replace. PS right now the dfs: usage is not consistent and has crept into a couple of new features the y! team is working on (caching of files and distcp).",0.3423333333,0.3423333333,neutral
hadoop,5657,comment_4,It'd help code readability if some comments are added on why two values are emitted per map and the logic the testcase is employing for validation (verbose comments for the arithmetic is what I mean *smile*).,code_debt,low_quality_code,"Sat, 11 Apr 2009 00:16:20 +0000","Tue, 24 Aug 2010 20:37:05 +0000","Wed, 29 Apr 2009 04:11:21 +0000",1569301,It'd help code readability if some comments are added on why two values are emitted per map and the logic the testcase is employing for validation (verbose comments for the arithmetic is what I mean smile).,0.4,0.4,neutral
hadoop,5657,comment_6,Adds some extra comments; no functional changes,code_debt,low_quality_code,"Sat, 11 Apr 2009 00:16:20 +0000","Tue, 24 Aug 2010 20:37:05 +0000","Wed, 29 Apr 2009 04:11:21 +0000",1569301,Adds some extra comments; no functional changes,0,0,neutral
hadoop,5771,comment_3,Patch looked good overall. Am uploading a new patch with slight modifications to the above patch with: - Better java code comments - Better code formatting and code-cleanup - And a small fix in because the earlier code was making the streaming jobs only run with LocalJobRunner.,code_debt,low_quality_code,"Tue, 5 May 2009 06:11:08 +0000","Tue, 24 Aug 2010 20:37:25 +0000","Mon, 11 May 2009 16:37:16 +0000",555968,Patch looked good overall. Am uploading a new patch with slight modifications to the above patch with: Better java code comments Better code formatting and code-cleanup And a small fix in TestStreamingAsDifferentUser because the earlier code was making the streaming jobs only run with LocalJobRunner.,0.638,0.638,positive
hadoop,5824,comment_0,+1 for removing it if it is not used.,code_debt,dead_code,"Wed, 13 May 2009 21:53:52 +0000","Mon, 3 Feb 2014 10:40:35 +0000","Thu, 14 May 2009 18:13:11 +0000",73159,+1 for removing it if it is not used.,0,0,neutral
hadoop,5824,description,Operation OP_READ_METADATA exists on the datanode streaming interface. But it is not used by any client code and is currently not protected by access token. Should it be removed?,code_debt,dead_code,"Wed, 13 May 2009 21:53:52 +0000","Mon, 3 Feb 2014 10:40:35 +0000","Thu, 14 May 2009 18:13:11 +0000",73159,Operation OP_READ_METADATA exists on the datanode streaming interface. But it is not used by any client code and is currently not protected by access token. Should it be removed?,0.05,0.05,negative
hadoop,6145,comment_7,srcFs.exists(src) and both call We could call once in FsShell.delete(..) and save a rpc.,code_debt,low_quality_code,"Thu, 9 Jul 2009 08:02:37 +0000","Fri, 2 Jul 2010 04:49:07 +0000","Tue, 14 Jul 2009 17:42:05 +0000",466768,srcFs.exists(src) and srcFs.isDirectory(src) both call srcFs.getFileStatus(src). We could call srcFs.getFileStatus(src) once in FsShell.delete(..) and save a rpc.,0.1333333333,0.05714285714,neutral
hadoop,6151,comment_5,"* The unit test should use JUnit4 test annotations instead of JUnit3 TestCase * looks useful for debugging, but should probably be left out * The static \*Bytes fields should be final * The @return docs for ""needsQuoting"" could be more explicit",code_debt,low_quality_code,"Wed, 15 Jul 2009 16:24:17 +0000","Tue, 24 Aug 2010 20:39:04 +0000","Fri, 18 Sep 2009 16:33:30 +0000",5616553,"The unit test should use JUnit4 test annotations instead of JUnit3 TestCase HttpServer::printRequest looks useful for debugging, but should probably be left out The static *Bytes fields should be final The @return docs for ""needsQuoting"" could be more explicit",0.04166666667,0.18125,neutral
hadoop,6182,comment_0,"with this patch the releaseaudit warnings is down to 1. [rat:report] 1 Unknown Licenses [rat:report] [rat:report] Unapproved licenses: [rat:report] this is an empty file. I'm not sure why we have this empty java files. If we can remove this empty java file, we can get rid of this releaaseaudit warning. Thanks,",code_debt,dead_code,"Mon, 10 Aug 2009 05:55:27 +0000","Tue, 24 Aug 2010 20:39:20 +0000","Mon, 17 Aug 2009 04:27:18 +0000",599511,"with this patch the releaseaudit warnings is down to 1. [rat:report] 1 Unknown Licenses [rat:report] ******************************* [rat:report] Unapproved licenses: [rat:report] src/java/org/apache/hadoop/fs/LengthFileChecksum.java LengthFileChecksum.java this is an empty file. I'm not sure why we have this empty java files. If we can remove this empty java file, we can get rid of this releaaseaudit warning. Thanks,",-0.1092,-0.1208571429,negative
hadoop,6182,comment_7,"Looks like the original patch inserted text blocks into the XML files ahead of the block: <?xml version=""1.0""? <?xml-stylesheet type=""text/xsl"" I corrected this for checkstyle in HADOOP-6185, but because the error is pervasive, it fails a wide variety of unit tests.",code_debt,low_quality_code,"Mon, 10 Aug 2009 05:55:27 +0000","Tue, 24 Aug 2010 20:39:20 +0000","Mon, 17 Aug 2009 04:27:18 +0000",599511,"Looks like the original patch inserted text blocks into the XML files ahead of the block: <?xml version=""1.0""?> <?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?> I corrected this for checkstyle in HADOOP-6185, but because the error is pervasive, it fails a wide variety of unit tests.",-0.15,-0.1,negative
hadoop,6182,comment_8,Let's remove the file separately once the build is stable.,code_debt,dead_code,"Mon, 10 Aug 2009 05:55:27 +0000","Tue, 24 Aug 2010 20:39:20 +0000","Mon, 17 Aug 2009 04:27:18 +0000",599511,"> Looks like LengthFileChecksum was removed in HADOOP-3981, although the source file wasn't deleted. ... Let's remove the file separately once the build is stable.",0.4,0.2,neutral
hadoop,6182,description,As of now rats tool shows 111 RA warnings [rat:report] Summary [rat:report] - [rat:report] Notes: 18 [rat:report] Binaries: 118 [rat:report] Archives: 33 [rat:report] Standards: 942 [rat:report] [rat:report] Apache Licensed: 820 [rat:report] Generated Documents: 11 [rat:report] [rat:report] JavaDocs are generated and so license header is optional [rat:report] Generated files do not required license headers [rat:report] [rat:report] 111 Unknown Licenses [rat:report] [rat:report],code_debt,low_quality_code,"Mon, 10 Aug 2009 05:55:27 +0000","Tue, 24 Aug 2010 20:39:20 +0000","Mon, 17 Aug 2009 04:27:18 +0000",599511,As of now rats tool shows 111 RA warnings [rat:report] Summary [rat:report] ------- [rat:report] Notes: 18 [rat:report] Binaries: 118 [rat:report] Archives: 33 [rat:report] Standards: 942 [rat:report] [rat:report] Apache Licensed: 820 [rat:report] Generated Documents: 11 [rat:report] [rat:report] JavaDocs are generated and so license header is optional [rat:report] Generated files do not required license headers [rat:report] [rat:report] 111 Unknown Licenses [rat:report] [rat:report] *******************************,0.0125,0.0125,neutral
hadoop,6182,summary,Adding Apache License Headers and reduce releaseaudit warnings to zero,code_debt,low_quality_code,"Mon, 10 Aug 2009 05:55:27 +0000","Tue, 24 Aug 2010 20:39:20 +0000","Mon, 17 Aug 2009 04:27:18 +0000",599511,Adding Apache License Headers and reduce releaseaudit warnings to zero,-0.6,-0.6,neutral
hadoop,6188,comment_2,I'd suggest to declare method's parameters final: it will improve readability and make testing easier.,code_debt,low_quality_code,"Tue, 11 Aug 2009 17:26:56 +0000","Tue, 24 Aug 2010 20:39:21 +0000","Tue, 11 Aug 2009 21:14:17 +0000",13641,I'd suggest to declare method's parameters final: it will improve readability and make testing easier.,0.4,0.4,positive
hadoop,6188,comment_5,+1 on the new patch. I also manually tested that with the new jar TestHDFSTrash no longer fails. I'm not overly concerned with the non-final parameters as the function is just a few lines and the variables aren't being abused. Of note is the necessity of checking for null from listStatus. This is to avoid the fact that the listStatus implementation differs between and I've opened HDFS-538 to address this.,code_debt,low_quality_code,"Tue, 11 Aug 2009 17:26:56 +0000","Tue, 24 Aug 2010 20:39:21 +0000","Tue, 11 Aug 2009 21:14:17 +0000",13641,+1 on the new patch. I also manually tested that with the new jar TestHDFSTrash no longer fails. I'm not overly concerned with the non-final parameters as the function is just a few lines and the variables aren't being abused. Of note is the necessity of checking for null from listStatus. This is to avoid the fact that the listStatus implementation differs between Local/ChecksumFileSystem and DistributedFileSystem. I've opened HDFS-538 to address this.,0.1563,0.13025,neutral
hadoop,6198,comment_0,I'd argue that we may want to support both interfaces. The fact that the current path-based filtering retrieves a FileStatus object first is an implementation detail and it is conceivable that a different FS implementation may be more efficient to support path-based filtering than file-status based filtering.,code_debt,slow_algorithm,"Tue, 18 Aug 2009 05:46:36 +0000","Thu, 24 Jul 2014 19:57:11 +0000","Thu, 24 Jul 2014 19:57:11 +0000",155657435,I'd argue that we may want to support both interfaces. The fact that the current path-based filtering retrieves a FileStatus object first is an implementation detail and it is conceivable that a different FS implementation may be more efficient to support path-based filtering than file-status based filtering.,0.1095,0.1095,neutral
hadoop,6198,comment_2,I am not sure why it would be a burden for FS implementors. listStatus would be implemented by FileSystem and FS implementors should only override them when the default implementation is less efficient (and the implementors have the incentive of making them fast).,code_debt,slow_algorithm,"Tue, 18 Aug 2009 05:46:36 +0000","Thu, 24 Jul 2014 19:57:11 +0000","Thu, 24 Jul 2014 19:57:11 +0000",155657435,I am not sure why it would be a burden for FS implementors. listStatus would be implemented by FileSystem and FS implementors should only override them when the default implementation is less efficient (and the implementors have the incentive of making them fast).,-0.05,-0.05,neutral
hadoop,6220,comment_5,"Would adding to the throws list of HttpServer::start, and rethrowing, be clearer? should be reserved for, well, interrupted I/O. Have you seen wrapped in logs or other evidence that would support this change? It is difficult to evaluate this without corresponding changes to the callers. What one might do with this is speculative in its current form.",code_debt,low_quality_code,"Thu, 27 Aug 2009 12:01:14 +0000","Thu, 12 May 2016 18:22:56 +0000","Wed, 28 Sep 2011 20:38:10 +0000",65867816,"If we want callers to distinguish InterruptedExceptions from IOEs, then this exception should be extracted [...] Would adding InterruptedException to the throws list of HttpServer::start, and rethrowing, be clearer? InterruptedIOException should be reserved for, well, interrupted I/O. Have you seen wrapped InterruptedExceptions in logs or other evidence that would support this change? It is difficult to evaluate this without corresponding changes to the callers. What one might do with this is speculative in its current form.",-0.2536666667,-0.1505416667,neutral
hadoop,6229,comment_2,Took Nikolas' suggestion. Created new class Turns out mapred already has such class. After this patch is committed I will remove mapred's one and make it use this one (from common). Also I will need to change HDFS to use this new exception instead of FileNotFound.,code_debt,low_quality_code,"Wed, 2 Sep 2009 00:40:39 +0000","Wed, 26 Apr 2017 08:44:34 +0000","Mon, 7 Sep 2009 13:27:32 +0000",478013,Took Nikolas' suggestion. Created new class FileAlreadyExestsException. Turns out mapred already has such class. After this patch is committed I will remove mapred's one and make it use this one (from common). Also I will need to change HDFS to use this new exception instead of FileNotFound.,0.125,0.1,neutral
hadoop,6229,comment_4,I believe the original code isn't defensive enough and there's a possibility for NPE to be thrown if {{Path f}} happens to be {{null}}. I'd add an extra check for this as the very first line of the method. Looks good otherwise.,code_debt,low_quality_code,"Wed, 2 Sep 2009 00:40:39 +0000","Wed, 26 Apr 2017 08:44:34 +0000","Mon, 7 Sep 2009 13:27:32 +0000",478013,I believe the original code isn't defensive enough and there's a possibility for NPE to be thrown if Path f happens to be null. I'd add an extra check for this as the very first line of the method. Looks good otherwise.,0.092,0.092,negative
hadoop,6229,comment_8,"+1 This will be tested by the test being introduced in HDFS-303, right? Minor nit: we tend not to add serialVersionUID fields. (Any warnings in IDEs should be turned off.)",code_debt,low_quality_code,"Wed, 2 Sep 2009 00:40:39 +0000","Wed, 26 Apr 2017 08:44:34 +0000","Mon, 7 Sep 2009 13:27:32 +0000",478013,"+1 This will be tested by the test being introduced in HDFS-303, right? Minor nit: we tend not to add serialVersionUID fields. (Any warnings in IDEs should be turned off.)",-0.085,-0.085,neutral
hadoop,6283,summary,The exception meessage in is not clear,code_debt,low_quality_code,"Wed, 23 Sep 2009 21:55:43 +0000","Tue, 24 Aug 2010 20:40:22 +0000","Thu, 8 Oct 2009 18:38:03 +0000",1284140,The exception meessage in FileUtil$HardLink.getLinkCount(..) is not clear,0,0,neutral
hadoop,6297,comment_3,"Inexplicably, the test it failed is the test I added. It looks like the native library isn't getting initialized properly by the testing machine, for some reason. I can run the test just fine locally using ant test. Also, I'm confused by the audit warning. What does that warning mean?",code_debt,low_quality_code,"Tue, 6 Oct 2009 15:01:24 +0000","Mon, 4 Aug 2014 21:17:39 +0000","Mon, 4 Aug 2014 21:17:39 +0000",152345775,"Inexplicably, the test it failed is the test I added. It looks like the native library isn't getting initialized properly by the testing machine, for some reason. I can run the test just fine locally using ant test. Also, I'm confused by the audit warning. What does that warning mean?",-0.22,-0.22,negative
hadoop,6297,comment_4,"The test should pass even if the native libraries aren't loaded, as some platforms (Windows, MacOS) don't come with the native libs installed. The error in the unit test: may be a problem with Hudson, but you may want to be certain your patch works on a clean checkout of trunk. It means that the patch adds files without headers licensing them to Apache. TestZlib.java, in this case. Small nits on the testcase: * Please use the JUnit4 conventions, rather than JUnit3; instead of extending TestCase, import the org.junit classes and use annotations (see examples in src/test, e.g. TestCodec) * This can simply call {{Assert.fail()}} rather than using the boolean, or even better, just allow the exception to escape the method",code_debt,low_quality_code,"Tue, 6 Oct 2009 15:01:24 +0000","Mon, 4 Aug 2014 21:17:39 +0000","Mon, 4 Aug 2014 21:17:39 +0000",152345775,"It looks like the native library isn't getting initialized properly by the testing machine, for some reason. I can run the test just fine locally using ant test. The test should pass even if the native libraries aren't loaded, as some platforms (Windows, MacOS) don't come with the native libs installed. The error in the unit test: may be a problem with Hudson, but you may want to be certain your patch works on a clean checkout of trunk. Also, I'm confused by the audit warning. What does that warning mean? It means that the patch adds files without headers licensing them to Apache. TestZlib.java, in this case. Small nits on the testcase: Please use the JUnit4 conventions, rather than JUnit3; instead of extending TestCase, import the org.junit classes and use annotations (see examples in src/test, e.g. TestCodec) This can simply call Assert.fail() rather than using the boolean, or even better, just allow the exception to escape the method",0.040625,-0.03125,neutral
hadoop,6304,comment_6,"Hey Arun. MAPREDUCE-2238 was caused by the use of these same APIs in What was happening was something like the following: - Thread A: 755) - Thread B: 755) The way they got interleaved was something like: - B: set userlogs/attempt_x to 000 - A: set userlogs/ to 000 - B: try to restore permissions on attempt_x, but fail since it can't traverse the path - A: set userlogs/ back to 755 - The attempt_x directory is left at 000 or some other ""incomplete"" permissions where any following Hudson runs won't be able to delete it. This same problem can happen in a real cluster too. So I think the pattern used in this patch (same as the one in is dangerous because it's not atomic. This was just one manifestation of the issue. I agree JNI is difficult but this is a very simple use of it. No buffer management, no complicated errors to handle, no strange system APIs. Let's have the JNI discussion over in HADOOP-7110 though.",code_debt,multi-thread_correctness,"Fri, 9 Oct 2009 06:07:30 +0000","Fri, 21 Jan 2011 08:34:28 +0000","Fri, 21 Jan 2011 08:34:28 +0000",40530418,"Hey Arun. MAPREDUCE-2238 was caused by the use of these same APIs in Localizer.PermissionsHandler.setPermissions. What was happening was something like the following: Thread A: setPermissions(""/path/to/userlogs"", 755) Thread B: setPermissions(""/path/to/userlogs/attempt_x"", 755) The way they got interleaved was something like: B: set userlogs/attempt_x to 000 A: set userlogs/ to 000 B: try to restore permissions on attempt_x, but fail since it can't traverse the path A: set userlogs/ back to 755 The attempt_x directory is left at 000 or some other ""incomplete"" permissions where any following Hudson runs won't be able to delete it. This same problem can happen in a real cluster too. So I think the pattern used in this patch (same as the one in Localizer.PermissionsHandler.setPermissions) is dangerous because it's not atomic. This was just one manifestation of the issue. I agree JNI is difficult but this is a very simple use of it. No buffer management, no complicated errors to handle, no strange system APIs. Let's have the JNI discussion over in HADOOP-7110 though.",-0.12775,-0.07861538462,neutral
hadoop,6313,comment_2,This patch removes Syncable implementation in RawLocalFileSystem and makes the default implementation of hflush & hsync to be flush in FSDataOutputStream.,code_debt,duplicated_code,"Wed, 14 Oct 2009 17:19:27 +0000","Tue, 24 Aug 2010 20:40:35 +0000","Fri, 30 Oct 2009 19:53:35 +0000",1391648,This patch removes Syncable implementation in RawLocalFileSystem and makes the default implementation of hflush & hsync to be flush in FSDataOutputStream.,-0.5,-0.5,negative
hadoop,6364,comment_4,"# It might make sense to have options for the namenode and job tracker, something with different config names for each, so a single configuration can define both. # It could still be important for some people to say ""come up on all interfaces, localhost included"", which could imply it should be a list. # There's also the problem that the namenode has historically been very fussy about what hostname was used to refer to it",code_debt,low_quality_code,"Thu, 5 Nov 2009 17:30:50 +0000","Thu, 22 Mar 2012 06:02:21 +0000","Wed, 2 Nov 2011 18:22:22 +0000",62815892,"It might make sense to have options for the namenode and job tracker, something with different config names for each, so a single configuration can define both. It could still be important for some people to say ""come up on all interfaces, localhost included"", which could imply it should be a list. There's also the problem that the namenode has historically been very fussy about what hostname was used to refer to it",-0.1687777778,-0.1687777778,neutral
hadoop,6413,comment_3,Updated to JUnit 4 (annotation) style,code_debt,low_quality_code,"Sat, 5 Dec 2009 00:41:05 +0000","Tue, 24 Aug 2010 20:41:09 +0000","Tue, 15 Dec 2009 00:39:08 +0000",863883,Updated to JUnit 4 (annotation) style,0,0,neutral
hadoop,6435,description,"The public RPC.waitForProxy() method waits for Long.MAX_VALUE before giving up, ignores all interrupt requests. This is excessive. The version of the method that is package scoped should be made public. Interrupt swallowing is covered in HADOOP-6221 and can be done as a separate patch",code_debt,low_quality_code,"Fri, 11 Dec 2009 17:58:55 +0000","Thu, 2 May 2013 02:29:27 +0000","Wed, 23 Dec 2009 19:36:38 +0000",1042663,"The public RPC.waitForProxy() method waits for Long.MAX_VALUE before giving up, ignores all interrupt requests. This is excessive. The version of the method that is package scoped should be made public. Interrupt swallowing is covered in HADOOP-6221 and can be done as a separate patch",-0.07222222222,-0.07222222222,negative
hadoop,6457,comment_0,"Do note that users can be in more than one group, so hadoop.group.name should probably be hadoop.group.names, and be comma-delimited.",code_debt,low_quality_code,"Fri, 18 Dec 2009 10:32:04 +0000","Tue, 29 Jul 2014 21:24:15 +0000","Tue, 29 Jul 2014 21:24:15 +0000",145536731,"Do note that users can be in more than one group, so hadoop.group.name should probably be hadoop.group.names, and be comma-delimited.",-0.1,-0.1,neutral
hadoop,6556,description,Cleanup some of the methods of FileContext,code_debt,low_quality_code,"Thu, 11 Feb 2010 00:44:07 +0000","Wed, 26 Apr 2017 08:46:27 +0000","Wed, 26 Apr 2017 08:46:27 +0000",227347340,Cleanup some of the methods of FileContext,0,0,neutral
hadoop,6584,comment_8,Small patch to fix javadoc warnings introduced into Y20 branch. Not to be committed.,code_debt,low_quality_code,"Mon, 22 Feb 2010 02:12:58 +0000","Mon, 12 Dec 2011 06:20:06 +0000","Sat, 3 Jul 2010 00:03:05 +0000",11310607,Small patch to fix javadoc warnings introduced into Y20 branch. Not to be committed.,-0.55,-0.55,negative
hadoop,659,comment_3,"Name-node needs some form of the global map of blocks in order to be able to remove blocks from neededReplications e.g. during block reports. We should also use a parallel list view of blocks that lists them in the priority order. Since we plan to have only 2 priority levels, we can add first-priority items to the beginning of the list and low-priority to the end. That way when we iterate the list we will first choose higher priority blocks automatically. Add and remove to/from the list are both O(1), so the performance remains the same as we have now. Iterating through the list is linear, which is better than what we have now O(n log n). I agree the main purpose of the issue is to replicate blocks that are about to become extinct. But if we can improve map/reduce performance by treating replication of config and jar files similarly, why not do that?",code_debt,slow_algorithm,"Tue, 31 Oct 2006 20:29:31 +0000","Wed, 8 Jul 2009 16:42:06 +0000","Thu, 25 Jan 2007 22:59:13 +0000",7439382,"Name-node needs some form of the global map of blocks in order to be able to remove blocks from neededReplications e.g. during block reports. We should also use a parallel list view of blocks that lists them in the priority order. Since we plan to have only 2 priority levels, we can add first-priority items to the beginning of the list and low-priority to the end. That way when we iterate the list we will first choose higher priority blocks automatically. Add and remove to/from the list are both O(1), so the performance remains the same as we have now. Iterating through the list is linear, which is better than what we have now O(n log n). I agree the main purpose of the issue is to replicate blocks that are about to become extinct. But if we can improve map/reduce performance by treating replication of config and jar files similarly, why not do that?",0.0772,0.0772,neutral
hadoop,6632,comment_3,A minor fix for the MR side to reuse filesystem handles,code_debt,low_quality_code,"Sun, 14 Mar 2010 04:28:53 +0000","Mon, 12 Dec 2011 06:19:10 +0000","Tue, 20 Jul 2010 00:51:35 +0000",11046162,A minor fix for the MR side to reuse filesystem handles,0,0,neutral
hadoop,6639,description,"When FileSystem cache is enabled, FileSystem.get(..) will call which is a synchronized method. If the lookup fails, a new instance will be initialized. Depends on the FileSystem subclass implementation, the initialization may take a long time. In such case, the FileSystem.Cache lock will be hold and all calls to FileSystem.get(..) by other threads will be blocked for a long time. In particular, the initialization may take a long time since there are retries. It is even worst if the socket timeout is set to a large value.",code_debt,slow_algorithm,"Wed, 17 Mar 2010 22:58:43 +0000","Wed, 17 Mar 2010 23:17:35 +0000","Wed, 17 Mar 2010 23:17:35 +0000",1132,"When FileSystem cache is enabled, FileSystem.get(..) will call FileSystem.Cache.get(..), which is a synchronized method. If the lookup fails, a new instance will be initialized. Depends on the FileSystem subclass implementation, the initialization may take a long time. In such case, the FileSystem.Cache lock will be hold and all calls to FileSystem.get(..) by other threads will be blocked for a long time. In particular, the DistributedFileSystem initialization may take a long time since there are retries. It is even worst if the socket timeout is set to a large value.",-0.1333333333,-0.1090909091,neutral
hadoop,6644,description,util.Shell method name - should use common naming convention,code_debt,low_quality_code,"Fri, 19 Mar 2010 06:23:43 +0000","Fri, 28 May 2010 11:23:01 +0000","Wed, 26 May 2010 18:56:50 +0000",5920387,util.Shell getGROUPS_FOR_USER_COMMAND method name - should use common naming convention,0,0,neutral
hadoop,6644,summary,util.Shell method name - should use common naming convention,code_debt,low_quality_code,"Fri, 19 Mar 2010 06:23:43 +0000","Fri, 28 May 2010 11:23:01 +0000","Wed, 26 May 2010 18:56:50 +0000",5920387,util.Shell getGROUPS_FOR_USER_COMMAND method name - should use common naming convention,0,0,neutral
hadoop,6658,description,"Packages, classes and methods that are marked with the or annotation should not appear in the public Javadoc. Developer Javadoc generated using the ""javadoc-dev"" ant target should continue to generate Javadoc for all program elements.",code_debt,low_quality_code,"Wed, 24 Mar 2010 04:14:18 +0000","Wed, 28 Mar 2018 20:17:32 +0000","Thu, 22 Apr 2010 20:48:54 +0000",2565276,"Packages, classes and methods that are marked with the InterfaceAudience.Private or InterfaceAudience.LimitedPrivate annotation should not appear in the public Javadoc. Developer Javadoc generated using the ""javadoc-dev"" ant target should continue to generate Javadoc for all program elements.",-0.3125,-0.15625,neutral
hadoop,6658,summary,Exclude Public elements in generated Javadoc,code_debt,low_quality_code,"Wed, 24 Mar 2010 04:14:18 +0000","Wed, 28 Mar 2018 20:17:32 +0000","Thu, 22 Apr 2010 20:48:54 +0000",2565276,Exclude  Public elements in  generated Javadoc,-0.4125,-0.4125,neutral
hadoop,6709,comment_2,+1 Looks good to me. Nit: please put @Deprecated for getBlockSize and getLength on their own line to be consistent with the other methods.,code_debt,low_quality_code,"Thu, 15 Apr 2010 23:39:00 +0000","Fri, 9 Dec 2016 10:45:39 +0000","Thu, 29 Apr 2010 19:57:04 +0000",1196284,+1 Looks good to me. Nit: please put @Deprecated for getBlockSize and getLength on their own line to be consistent with the other methods.,0.488,0.488,positive
hadoop,6709,comment_4,More than a Nit. We have to leave the deprecation in. The goal of this Jira is to merely reinstate deleted methods. Old deprecation should remain.,code_debt,low_quality_code,"Thu, 15 Apr 2010 23:39:00 +0000","Fri, 9 Dec 2016 10:45:39 +0000","Thu, 29 Apr 2010 19:57:04 +0000",1196284,> Nit: please put @Deprecated More than a Nit. We have to leave the deprecation in. The goal of this Jira is to merely reinstate deleted methods. Old deprecation should remain.,-0.2125,-0.1625,neutral
hadoop,6709,description,"To make the FileSystem API in the 0.21 release (which should be treated as a minor release) compatible with 0.20 the deprecated methods removed in HADOOP-4779 need to be re-instated. They should still be marked as deprecated, however.",code_debt,low_quality_code,"Thu, 15 Apr 2010 23:39:00 +0000","Fri, 9 Dec 2016 10:45:39 +0000","Thu, 29 Apr 2010 19:57:04 +0000",1196284,"To make the FileSystem API in the 0.21 release (which should be treated as a minor release) compatible with 0.20 the deprecated methods removed in HADOOP-4779 need to be re-instated. They should still be marked as deprecated, however.",0.453,0.453,neutral
hadoop,6727,comment_1,"Just updating throws clauses and java docs, hence the lack of tests.",code_debt,low_quality_code,"Tue, 27 Apr 2010 23:42:05 +0000","Tue, 24 Aug 2010 20:42:57 +0000","Mon, 3 May 2010 17:39:29 +0000",496644,"Just updating throws clauses and java docs, hence the lack of tests.",-0.4,-0.4,negative
hadoop,6727,description,"HADOOP-6537 added to the throws clause and java docs of FileContext public APIs. FileContext fully resolves symbolic links, exception is only used internally by FSLinkResolver. I'll attach a patch which updates the APIs and javadoc.",code_debt,low_quality_code,"Tue, 27 Apr 2010 23:42:05 +0000","Tue, 24 Aug 2010 20:42:57 +0000","Mon, 3 May 2010 17:39:29 +0000",496644,"HADOOP-6537 added UnresolvedLinkException to the throws clause and java docs of FileContext public APIs. FileContext fully resolves symbolic links, UnresolvedLinkException exception is only used internally by FSLinkResolver. I'll attach a patch which updates the APIs and javadoc.",0.1333333333,0.1333333333,neutral
hadoop,6727,summary,Remove from public FileContext APIs,code_debt,low_quality_code,"Tue, 27 Apr 2010 23:42:05 +0000","Tue, 24 Aug 2010 20:42:57 +0000","Mon, 3 May 2010 17:39:29 +0000",496644,Remove UnresolvedLinkException from public FileContext APIs,0,0,neutral
hadoop,6730,comment_6,"Review of the patch: * Lines 47-52: These notes aren't necessary since the test writers should know about @Before and @After. Also, the provided methods (setUp() and tearDown()) conflict with the provided names. * Lines 55 & 56: These values should be all capitalized and final. Alternatively, it may be good to provide these values via functions so that implementing classes may override as needed. * Line 64: catch should be on the same line as closing brace, per our coding style * Line 70: fc should not be declared static. It is the responsibility of each implementing class to provide an instance of it. Also, fc should probably be protected * Line 91: Please provide assert message for assertion failure * Since this is a test that the copy method worked, we should prove it by comparing the contents of the copied file with the original. * Lines 135-138: This code: is a no-op and should be removed. Nits: * Rather than Assert.assertTrue() you can do a static import of * Line 57: Remove extra line * Line 47: ""Since this a junit 4"" ->""Since this is a junit 4 test""",code_debt,low_quality_code,"Wed, 28 Apr 2010 18:50:09 +0000","Mon, 12 Dec 2011 06:19:03 +0000","Sat, 1 May 2010 21:09:36 +0000",267567,"Review of the patch: Lines 47-52: These notes aren't necessary since the test writers should know about @Before and @After. Also, the provided methods (setUp() and tearDown()) conflict with the provided names. Lines 55 & 56: These values should be all capitalized and final. Alternatively, it may be good to provide these values via functions so that implementing classes may override as needed. Line 64: catch should be on the same line as closing brace, per our coding style Line 70: fc should not be declared static. It is the responsibility of each implementing class to provide an instance of it. Also, fc should probably be protected Line 91: Please provide assert message for assertion failure Since this is a test that the copy method worked, we should prove it by comparing the contents of the copied file with the original. Lines 135-138: This code: is a no-op and should be removed. Nits: Rather than Assert.assertTrue() you can do a static import of org.junit.Assert. {assertTrue|*} . Line 57: Remove extra line Line 47: ""Since this a junit 4"" ->""Since this is a junit 4 test""",0.17135,0.1223928571,neutral
hadoop,6730,comment_8,Updated patch with changes. 1. Removed unnecessary tearDown method. 2. Changed file read write to bytes from String.,code_debt,dead_code,"Wed, 28 Apr 2010 18:50:09 +0000","Mon, 12 Dec 2011 06:19:03 +0000","Sat, 1 May 2010 21:09:36 +0000",267567,Updated patch with changes. 1. Removed unnecessary tearDown method. 2. Changed file read write to bytes from String.,0,0,neutral
hadoop,6826,comment_8,"hi amar, this API never made it to any ""release"" of Hadoop. Better to remove it now than to ship a release with a bad API.",code_debt,low_quality_code,"Tue, 15 Jun 2010 21:21:00 +0000","Thu, 2 May 2013 02:29:32 +0000","Tue, 22 Jun 2010 04:15:36 +0000",543276,"hi amar, this API never made it to any ""release"" of Hadoop. Better to remove it now than to ship a release with a bad API.",-0.025,-0.025,negative
hadoop,6839,comment_4,Looks good overall. Please fix the formatting (white space are inconsistent) and make exception and error messages more meaningful. Submit the patch for verification whenever is ready.,code_debt,low_quality_code,"Fri, 25 Jun 2010 08:24:21 +0000","Thu, 2 May 2013 02:29:31 +0000","Wed, 14 Jul 2010 18:47:11 +0000",1678970,Looks good overall. Please fix the formatting (white space are inconsistent) and make exception and error messages more meaningful. Submit the patch for verification whenever is ready.,0.4808888889,0.4808888889,positive
hadoop,6839,description,"Develop a new method for getting the user list. Method signature is public ArrayList<StringAdd new attribute in system-test.xml file for getting userlist path. For submitting the jobs as different user a proxy user id is needed. So,get the available users from a userlist and then pass the user as proxy instead of hardcoding user id in a test.",code_debt,low_quality_code,"Fri, 25 Jun 2010 08:24:21 +0000","Thu, 2 May 2013 02:29:31 +0000","Wed, 14 Jul 2010 18:47:11 +0000",1678970,"Develop a new method for getting the user list. Method signature is public ArrayList<String> getHadoopMultiUsersList() throws IOException; Add new attribute in system-test.xml file for getting userlist path. For submitting the jobs as different user a proxy user id is needed. So,get the available users from a userlist and then pass the user as proxy instead of hardcoding user id in a test.",0,0,neutral
hadoop,6869,comment_3,"- Looks good with one nit. I'd suggest to have methods with fewer arguments simply call ones with more args. E.g. in this case instead of duplicating their exact implementation. You have done similar in already. - Also, in you already have a ref to the filesystem so you can just call its {{createFile}} methods instead of implementing your own logic just call to have new file created for you. - also you are writing some content into this file which seems to be an inadvertent side-effect. Doesn't look like a good idea.",code_debt,low_quality_code,"Wed, 21 Jul 2010 17:05:23 +0000","Mon, 27 Sep 2010 04:14:15 +0000","Wed, 28 Jul 2010 03:23:10 +0000",555467,"Looks good with one nit. I'd suggest to have methods with fewer arguments simply call ones with more args. E.g. in this case instead of duplicating their exact implementation. You have done similar in DaemonProtocolAspect already. Also, in DaemonProtocolAspect you already have a ref to the filesystem so you can just call its createFile methods instead of implementing your own logic just call fs.createNewFile() to have new file created for you. also you are writing some content into this file which seems to be an inadvertent side-effect. Doesn't look like a good idea.",0.2126,0.132875,positive
hadoop,6869,comment_5,"Hi Cos, I do agree with you. However my requirement is creating the user define files and folders in task attempt id folder with different kind of permissions while job is running. Later while cleaning up, i just wanted make sure whether all the user defined files and folders are cleaned up or not for that job. So that I have defined the above methods for fulfilling my requirement. As my understand, i don't think so, the FileSystem is providing the no such straight forward and createNewFolder) like you mentioned. In the four methods, the first method creates the files with full permission.Suppose if user wants to give his own permissions while creating file in that case he can use the second method.Same way for folder creation also. I will remove this part in the code.",code_debt,dead_code,"Wed, 21 Jul 2010 17:05:23 +0000","Mon, 27 Sep 2010 04:14:15 +0000","Wed, 28 Jul 2010 03:23:10 +0000",555467,"Hi Cos, I do agree with you. However my requirement is creating the user define files and folders in task attempt id folder with different kind of permissions while job is running. Later while cleaning up, i just wanted make sure whether all the user defined files and folders are cleaned up or not for that job. So that I have defined the above methods for fulfilling my requirement. As my understand, i don't think so, the FileSystem is providing the no such straight forward methods(createNewFile and createNewFolder) like you mentioned. In the four methods, the first method creates the files with full permission.Suppose if user wants to give his own permissions while creating file in that case he can use the second method.Same way for folder creation also. also you are writing some content into this file which seems to be an inadvertent side-effect. Doesn't look like a good idea I will remove this part in the code.",0.1111111111,0.0724,neutral
hadoop,6869,comment_6,This reasoning sounds like an argument to move this functionality up the stack to the MR cluster concrete implementation. But let's suppose they are useful enough and let them be in the Common. I do understand that. What I have said is that you need a generic enough method which does most of needed functionality (i.e. create a file with specified permissions) and a wrapper around it which creates a file with all permissions. The implementation of the latter is essentially a call to the former with *all* permissions being passed. This is already done in this very patch for the implementation of methods. Look at your own code.,code_debt,low_quality_code,"Wed, 21 Jul 2010 17:05:23 +0000","Mon, 27 Sep 2010 04:14:15 +0000","Wed, 28 Jul 2010 03:23:10 +0000",555467,"my requirement is creating the user define files and folders in task attempt id folder with different kind of permissions while job is running This reasoning sounds like an argument to move this functionality up the stack to the MR cluster concrete implementation. But let's suppose they are useful enough and let them be in the Common. In the four methods, the first method creates the files with full permission.Suppose if user wants to give his own permissions while creating file in that case he can use the second method.Same way for folder creation also. I do understand that. What I have said is that you need a generic enough method which does most of needed functionality (i.e. create a file with specified permissions) and a wrapper around it which creates a file with all permissions. The implementation of the latter is essentially a call to the former with all permissions being passed. This is already done in this very patch for the implementation of createFolder(..) methods. Look at your own code.",0.2142857143,0.15,neutral
hadoop,6879,comment_10,Base implementing method should use {{portNumber}} variable instead of a constant.,code_debt,low_quality_code,"Sat, 24 Jul 2010 07:27:29 +0000","Mon, 12 Dec 2011 06:20:05 +0000","Mon, 4 Oct 2010 05:10:42 +0000",6212593,Base implementing method should use portNumber variable instead of a constant.,0,0,neutral
hadoop,6879,comment_13,Minor nits: 1. The method needs to be implemented or removed: 2. There shouldn't be any @author tags in the javadoc. Other than that patch looks fine.,code_debt,dead_code,"Sat, 24 Jul 2010 07:27:29 +0000","Mon, 12 Dec 2011 06:20:05 +0000","Mon, 4 Oct 2010 05:10:42 +0000",6212593,Minor nits: 1. The method needs to be implemented or removed: 2. There shouldn't be any @author tags in the javadoc. Other than that patch looks fine.,0.1,0.1,negative
hadoop,6988,comment_1,"At a minimum, -1 on the environment variable. Shouldn't HADOOP_CLIENT_OPTS be sufficient for passing extra -D params? We have an abundance of environment variables that users can't handle as it is.",code_debt,low_quality_code,"Mon, 4 Oct 2010 23:28:30 +0000","Tue, 24 May 2011 19:12:40 +0000","Tue, 24 May 2011 19:12:40 +0000",20029450,"At a minimum, -1 on the environment variable. Shouldn't HADOOP_CLIENT_OPTS be sufficient for passing extra -D params? We have an abundance of environment variables that users can't handle as it is.",-0.2083333333,-0.2083333333,negative
hadoop,6988,comment_3,"Grr. I really wish we'd stop creating pet environment variables. This is ridiculous. Can we remove this env var as part of this JIRA? What takes precendence the env var or the jobconf setting? What is the interaction? If the answer is ""we have to look at the code"" then we've failed. It makes much more sense to have to support a comma delimited set (to be consistent with the rest of the job conf. Never mind that colon is the traditional directory delimiter on OS X.)",code_debt,low_quality_code,"Mon, 4 Oct 2010 23:28:30 +0000","Tue, 24 May 2011 19:12:40 +0000","Tue, 24 May 2011 19:12:40 +0000",20029450,"Grr. I really wish we'd stop creating pet environment variables. This is ridiculous. Can we remove this env var as part of this JIRA? What takes precendence the env var or the mapreduce.job.credentials.binary jobconf setting? What is the interaction? If the answer is ""we have to look at the code"" then we've failed. It makes much more sense to have mapreduce.job.credentials.binary to support a comma delimited set (to be consistent with the rest of the job conf. Never mind that colon is the traditional directory delimiter on OS X.)",-0.06666666667,-0.04,negative
hadoop,6988,comment_4,"The environment variable should *not* be multi-valued. It is used to communicate the job's token store to sub-processes of the task. Since a task can't be in more than one job, there isn't any need. What is the use case for having multiple token files? The rest of the lists use commas, so this should be the same. Wouldn't it be easier to write a tool that allows you to combine multiple token files together into a single one?",code_debt,low_quality_code,"Mon, 4 Oct 2010 23:28:30 +0000","Tue, 24 May 2011 19:12:40 +0000","Tue, 24 May 2011 19:12:40 +0000",20029450,"The environment variable should not be multi-valued. It is used to communicate the job's token store to sub-processes of the task. Since a task can't be in more than one job, there isn't any need. What is the use case for having multiple token files? The rest of the lists use commas, so this should be the same. Wouldn't it be easier to write a tool that allows you to combine multiple token files together into a single one?",-0.08333333333,-0.08333333333,neutral
hadoop,715,comment_0,Here is a simple fix... I've also renamed the 'hadoop.log.dir' variable in ant to 'test.log.dir' since it is used only in the 'test' target for consistency and to ensure people don't get confused.,code_debt,low_quality_code,"Tue, 14 Nov 2006 03:44:53 +0000","Fri, 1 Dec 2006 22:43:36 +0000","Tue, 14 Nov 2006 21:53:35 +0000",65322,Here is a simple fix... I've also renamed the 'hadoop.log.dir' variable in ant to 'test.log.dir' since it is used only in the 'test' target for consistency and to ensure people don't get confused.,0.06,0.06,neutral
hadoop,7375,comment_0,AFS#getFileStatus is now called instead of which means fixRelativePart is no longer used to make the path absolute in FileContext relative to the working dir before passing the path to AFS right? Nit: lines 568 and 2231 need indenting. Otherwise looks great.,code_debt,low_quality_code,"Fri, 10 Jun 2011 05:54:28 +0000","Tue, 15 Nov 2011 00:50:44 +0000","Sun, 12 Jun 2011 01:36:27 +0000",157319,"AFS#getFileStatus is now called instead of FileContext#getFileStatus, which means fixRelativePart is no longer used to make the path absolute in FileContext relative to the working dir before passing the path to AFS right? Nit: lines 568 and 2231 need indenting. Otherwise looks great.",0.01216666667,0.01216666667,neutral
hadoop,7375,comment_4,"Ah, never mind, I thought it was previously calling FC#getFileStatus. +1 feel free to address the nits directly in the commit since it's just indentation.",code_debt,low_quality_code,"Fri, 10 Jun 2011 05:54:28 +0000","Tue, 15 Nov 2011 00:50:44 +0000","Sun, 12 Jun 2011 01:36:27 +0000",157319,"Ah, never mind, I thought it was previously calling FC#getFileStatus. +1 feel free to address the nits directly in the commit since it's just indentation.",0.1,0.1,neutral
hadoop,758,comment_3,"The exception in the bug is that last exception that that occurred. It masks the first exception that would be a better indicator of the problem. ReduceTask.java (around line 313)Looks like try { /* run reducer */ } finally { /* close some streams */ } The above trace and the one in HADOOP-757 both are in finally {} and mask the exception in try {}. I will submit a patch that prints the exception thrown in try {} if finally block throws one. While trying reproduce the above trace I managed to produce ""Bad File Descriptor"" exception in HADOOP-757. In summary, it looks like these failures are possible with low tmp spaces but we don't log the exceptions that were triggered initially.",code_debt,low_quality_code,"Wed, 29 Nov 2006 05:41:43 +0000","Wed, 8 Jul 2009 16:42:11 +0000","Thu, 25 Jan 2007 21:31:03 +0000",4981760,"The exception in the bug is that last exception that that occurred. It masks the first exception that would be a better indicator of the problem. ReduceTask.java (around line 313)Looks like try { /* run reducer */ } finally { /* close some streams */ } The above trace and the one in HADOOP-757 both are in finally {} and mask the exception in try {}. I will submit a patch that prints the exception thrown in try {} if finally block throws one. While trying reproduce the above trace I managed to produce ""Bad File Descriptor"" exception in HADOOP-757. In summary, it looks like these failures are possible with low tmp spaces but we don't log the exceptions that were triggered initially.",-0.1571428571,-0.1571428571,neutral
hadoop,758,comment_7,"... This is exactly what I had in my devel code. The method ('double code path') in the patch was Owen's preferred approach. If we don't need to do all the cleanups (may be because this will actually close the Java process and open a new one for the new task), then we don't need finally at all.",code_debt,low_quality_code,"Wed, 29 Nov 2006 05:41:43 +0000","Wed, 8 Jul 2009 16:42:11 +0000","Thu, 25 Jan 2007 21:31:03 +0000",4981760,"> Wouldn't something like the following work? > IOException ioe = null; > try { > ... body ... ... This is exactly what I had in my devel code. The method ('double code path') in the patch was Owen's preferred approach. If we don't need to do all the cleanups (may be because this will actually close the Java process and open a new one for the new task), then we don't need finally at all.",0.1666666667,-0.025,neutral
hadoop,7684,comment_7,rpm package doesnt seem to include the historyserver and secondarynamenode init scripts. looks like and init.d script should be added to the list of init.d scripts in the spec file.,code_debt,low_quality_code,"Tue, 27 Sep 2011 01:02:35 +0000","Wed, 19 Oct 2011 00:26:06 +0000","Tue, 4 Oct 2011 00:45:09 +0000",603754,rpm package doesnt seem to include the historyserver and secondarynamenode init scripts. looks like hadoop-historyserver and hadoop-secondarynamnode init.d script should be added to the list of init.d scripts in the spec file.,0,0,neutral
hadoop,7786,comment_13,"I think ""dfs.block.size"" introduced in HADOOP-4952 was just a mistake. I think the intention was to have it ""dfs.blocksize"", same as in HDFS. I see a bunch of other file system keys in FsConfig, that duplicate their HDFS counterparts, but the naming is consistent across them. So I assume that extra dot between block and size was a typo. Having said that I think it is not a good idea to duplicate key definitions. The reason is exactly the typos or inconsistent renaming of those properties or the default values. E.g. in common = 32 MB}} while in HDFS = 64 MB}} This is really messy. The only method from {{FsConfig}} that is used in the code is I propose to remove everything else from {{FsConfig}} in order to avoid confusion. The rational behind this is that {{FsConfig}} should only contain keys that are specified in core-site.xml. The keys that belolng to hdf-site.xml shoud be described in {{DFSConfigKeys}}. It also looks that Tom's documentation change HDFS-671 describes keys consistently with this assumption.",code_debt,duplicated_code,"Fri, 6 May 2011 04:48:15 +0000","Mon, 12 Dec 2011 06:19:17 +0000","Tue, 1 Nov 2011 17:12:53 +0000",15510278,"I think ""dfs.block.size"" introduced in HADOOP-4952 was just a mistake. I think the intention was to have it ""dfs.blocksize"", same as in HDFS. I see a bunch of other file system keys in FsConfig, that duplicate their HDFS counterparts, but the naming is consistent across them. So I assume that extra dot between block and size was a typo. Having said that I think it is not a good idea to duplicate key definitions. The reason is exactly the typos or inconsistent renaming of those properties or the default values. E.g. in common FsConfig.FS_DEFAULT_BLOCK_SIZE = 32 MB while in HDFS DFSConfigKeys.DFS_BLOCK_SIZE_DEFAULT = 64 MB This is really messy. The only method from FsConfig that is used in the code is getDefaultFsURI(). I propose to remove everything else from FsConfig in order to avoid confusion. The rational behind this is that FsConfig should only contain keys that are specified in core-site.xml. The keys that belolng to hdf-site.xml shoud be described in DFSConfigKeys. It also looks that Tom's documentation change HDFS-671 describes keys consistently with this assumption.",-0.101,-0.09821052632,negative
hadoop,7786,comment_14,"I agree with Konst, we should remove the overlap between FsConfig and DFSCOnfig.",code_debt,duplicated_code,"Fri, 6 May 2011 04:48:15 +0000","Mon, 12 Dec 2011 06:19:17 +0000","Tue, 1 Nov 2011 17:12:53 +0000",15510278,"I agree with Konst, we should remove the overlap between FsConfig and DFSCOnfig.",0.2,0.2,neutral
hadoop,7786,comment_17,"Eli, the patch looks great. One thing: Instead of It should be +1 other than that. Findbugs come from protobuf packages untouched here. Don't understand why jenkins reported them as new.",code_debt,low_quality_code,"Fri, 6 May 2011 04:48:15 +0000","Mon, 12 Dec 2011 06:19:17 +0000","Tue, 1 Nov 2011 17:12:53 +0000",15510278,"Eli, the patch looks great. One thing: Instead of aConf.get(FS_DEFAULT_NAME_KEY) It should be aConf.get(FS_DEFAULT_NAME_KEY, FS_DEFAULT_NAME_DEFAULT) +1 other than that. Findbugs come from protobuf packages untouched here. Don't understand why jenkins reported them as new.",0.15,0.05833333333,positive
hadoop,7786,description,"HADOOP-4952 added a couple HDFS-specific configuration values to common (the block size and the replication factor) that conflict with the HDFS values (eg have the wrong defaults, wrong key name), are not used by common or hdfs and should be removed. After removing these I noticed the rest of FsConfig is only used once outside a test, and isn't tagged as a public API, I think we can remove it entirely.",code_debt,low_quality_code,"Fri, 6 May 2011 04:48:15 +0000","Mon, 12 Dec 2011 06:19:17 +0000","Tue, 1 Nov 2011 17:12:53 +0000",15510278,"HADOOP-4952 added a couple HDFS-specific configuration values to common (the block size and the replication factor) that conflict with the HDFS values (eg have the wrong defaults, wrong key name), are not used by common or hdfs and should be removed. After removing these I noticed the rest of FsConfig is only used once outside a test, and isn't tagged as a public API, I think we can remove it entirely.",-0.06,-0.06,negative
hadoop,7796,summary,HADOOP-7773 introduced 7 new findbugs warnings,code_debt,low_quality_code,"Wed, 2 Nov 2011 04:14:13 +0000","Tue, 10 Mar 2015 02:33:07 +0000","Mon, 21 Nov 2011 19:17:37 +0000",1695804,HADOOP-7773 introduced 7 new findbugs warnings,-0.6,-0.6,neutral
hadoop,7924,comment_1,"- maybe we should rename FailoverController to or even just ManualFailover? IMO has always had a connotation of being some kind of daemon. - in the preFailoverCheck, I'd expect us to support the case where ""svc1"" is down -- ie the admin has noticed the active crashed, and therefore wants to initiate failover to svc2. We should treat this the same as a failure in calling transitionToStandby - i.e fence and continue. - {{svc1}} and {{svc2}} aren't very descriptive parameter names - maybe {{fromSvc}} and {{toSvc}} or {{oldActive}} and {{newActive}}? The javadoc in {{failover(...)}} is also incorrect here for {{svc2Name}} - in no need to duplicate the standard exception javadoc from its parent class",code_debt,low_quality_code,"Wed, 14 Dec 2011 09:40:08 +0000","Fri, 6 Jan 2012 14:38:48 +0000","Thu, 5 Jan 2012 21:13:44 +0000",1942416,"maybe we should rename FailoverController to ManualFailoverController? or even just ManualFailover? IMO ""FailoverController"" has always had a connotation of being some kind of daemon. in the preFailoverCheck, I'd expect us to support the case where ""svc1"" is down  ie the admin has noticed the active crashed, and therefore wants to initiate failover to svc2. We should treat this the same as a failure in calling transitionToStandby - i.e fence and continue. svc1 and svc2 aren't very descriptive parameter names - maybe fromSvc and toSvc or oldActive and newActive? The javadoc in failover(...) is also incorrect here for svc2Name in FailoverFailedException, no need to duplicate the standard exception javadoc from its parent class",0.03816666667,0.02895238095,negative
hadoop,7971,comment_6,Updated with better error msg and exit status .,code_debt,low_quality_code,"Thu, 12 Jan 2012 20:16:35 +0000","Mon, 5 Mar 2012 02:48:49 +0000","Tue, 17 Jan 2012 07:01:11 +0000",384276,Updated with better error msg and exit status .,0.05,0.05,neutral
hadoop,7974,comment_0,Patch that uses a get-parent call instead of hacking with strings.,code_debt,low_quality_code,"Fri, 13 Jan 2012 23:50:17 +0000","Mon, 5 Mar 2012 02:49:12 +0000","Wed, 8 Feb 2012 02:09:28 +0000",2168351,Patch that uses a get-parent call instead of hacking with strings.,0,0,neutral
hadoop,7985,description,"I use this command ""mvn -Pdist -P-cbuild -DskipTests install"" to build. Without ANY changes in code, running this command takes 1:32. It seems to me this is too long. Investigate if this time can be reduced drastically.",code_debt,slow_algorithm,"Thu, 19 Jan 2012 21:14:41 +0000","Thu, 15 Aug 2013 19:02:33 +0000","Thu, 15 Aug 2013 19:02:33 +0000",49585672,"I use this command ""mvn -Pdist -P-cbuild -Dmaven.javadoc.skip -DskipTests install"" to build. Without ANY changes in code, running this command takes 1:32. It seems to me this is too long. Investigate if this time can be reduced drastically.",0,0,negative
hadoop,7985,summary,maven build should be super fast when there are no changes,code_debt,slow_algorithm,"Thu, 19 Jan 2012 21:14:41 +0000","Thu, 15 Aug 2013 19:02:33 +0000","Thu, 15 Aug 2013 19:02:33 +0000",49585672,maven build should be super fast when there are no changes,0.542,0.542,neutral
hadoop,8084,comment_2,No I haven't done benchmarks. But it seemed to make sense to avoid a copy if it could be avoided.,code_debt,slow_algorithm,"Fri, 17 Feb 2012 01:31:13 +0000","Wed, 23 May 2012 20:15:43 +0000","Tue, 21 Feb 2012 05:20:49 +0000",359376,No I haven't done benchmarks. But it seemed to make sense to avoid a copy if it could be avoided.,-0.1,-0.1,neutral
hadoop,8124,comment_0,"- removes Syncable.sync(); - removes the deprecated - removes unnecessary ""throws IOException"" declarations.",code_debt,dead_code,"Thu, 1 Mar 2012 00:09:47 +0000","Thu, 12 May 2016 18:22:41 +0000","Thu, 1 Mar 2012 23:53:06 +0000",85399,"c8124_20120229.patch: removes Syncable.sync(); removes the deprecated FSDataOutputStream(out); removes unnecessary ""throws IOException"" declarations.",0.25,0.1666666667,neutral
hadoop,8124,description,The Syncable.sync() was deprecated in 0.21. We should remove it.,code_debt,dead_code,"Thu, 1 Mar 2012 00:09:47 +0000","Thu, 12 May 2016 18:22:41 +0000","Thu, 1 Mar 2012 23:53:06 +0000",85399,The Syncable.sync() was deprecated in 0.21. We should remove it.,0.1666666667,0.1666666667,negative
hadoop,8124,summary,Remove the deprecated Syncable.sync() method,code_debt,dead_code,"Thu, 1 Mar 2012 00:09:47 +0000","Thu, 12 May 2016 18:22:41 +0000","Thu, 1 Mar 2012 23:53:06 +0000",85399,Remove the deprecated Syncable.sync() method,0.25,0.25,neutral
hadoop,8168,comment_5,I think you meant {{Math.min}}. Although I'd suggest maybe something like this to avoid spurious whitespace:,code_debt,low_quality_code,"Tue, 13 Mar 2012 18:38:36 +0000","Thu, 11 Oct 2012 17:45:07 +0000","Thu, 28 Jun 2012 17:44:02 +0000",9241526,I think you meant Math.min. Although I'd suggest maybe something like this to avoid spurious whitespace:,-0.1445,-0.1445,neutral
hadoop,8168,comment_7,"Hi Daryn, Your fix looks a bit cleaner than mine, thanks! -Eugene",code_debt,low_quality_code,"Tue, 13 Mar 2012 18:38:36 +0000","Thu, 11 Oct 2012 17:45:07 +0000","Thu, 28 Jun 2012 17:44:02 +0000",9241526,"Hi Daryn, Your fix looks a bit cleaner than mine, thanks! -Eugene",0.2,0.2,positive
hadoop,8209,comment_3,"Patch looks pretty good to me, Eli. Just a few small comments: # Not obvious to me why we have these static version methods in the Storage class, which themselves just delegate to static methods of the VersionInfo class. # Recommend adding additional detail to the AssertionErrors, including the revisions and versions that didn't match. # Recommend adding an explanation to the DN log message about why the communication is being allowed, e.g.: ""... because versions match exactly ('"" + version + ""') and is enabled."" Ditto for TT. # Similarly the log message explaining why communication isn't being allowed might mention whether the check failed because of strict revision checking, or relaxed version checking. # Why call the new method ""getInfoVersion"" in JobTracker? getVersion, as was done in Storage, seems to make more sense to me. # In I don't think you actually test that different revisions are still disallowed by default, since you change both the revision and version simultaneously in the test.",code_debt,low_quality_code,"Sun, 25 Mar 2012 22:17:38 +0000","Wed, 24 Oct 2012 19:12:09 +0000","Thu, 12 Apr 2012 22:06:37 +0000",1554539,"Patch looks pretty good to me, Eli. Just a few small comments: Not obvious to me why we have these static version methods in the Storage class, which themselves just delegate to static methods of the VersionInfo class. Recommend adding additional detail to the AssertionErrors, including the revisions and versions that didn't match. Recommend adding an explanation to the DN log message about why the communication is being allowed, e.g.: ""... because versions match exactly ('"" + version + ""') and hadoop.relaxed.worker.version.check is enabled."" Ditto for TT. Similarly the log message explaining why communication isn't being allowed might mention whether the check failed because of strict revision checking, or relaxed version checking. Why call the new method ""getInfoVersion"" in JobTracker? getVersion, as was done in Storage, seems to make more sense to me. In TestTaskTrackerVersionCheck#testDefaultVersionCheck, I don't think you actually test that different revisions are still disallowed by default, since you change both the revision and version simultaneously in the test.",0.1753888889,0.1521923077,neutral
hadoop,8209,comment_5,"I reviewed the delta, and it largely looks good. One tiny nit: looks like you variously spelled the word ""disallow"" either as ""dissallow"" or ""dissalow"". Patch looks good otherwise - +1. Please do also run the branch-1 test suite on the latest patch before committing.",code_debt,low_quality_code,"Sun, 25 Mar 2012 22:17:38 +0000","Wed, 24 Oct 2012 19:12:09 +0000","Thu, 12 Apr 2012 22:06:37 +0000",1554539,"I reviewed the delta, and it largely looks good. One tiny nit: looks like you variously spelled the word ""disallow"" either as ""dissallow"" or ""dissalow"". Patch looks good otherwise - +1. Please do also run the branch-1 test suite on the latest patch before committing.",0.438,0.438,positive
hadoop,8210,comment_8,"I think it's worth changing the return type of this function to LinkedHashSet, so it's clear that the ordering here is on purpose. Perhaps also add a comment here saying something like: - Nits: please un-abbreviate ""first"" for better readability. Also, ""e.g."" instead of ""Eg."" -- or just say ""For example"" - I think it's more idiomatic to just put the postincrement inside the []s - - there's a small spurious whitespace change in NetUtils.java - looks like the pom change is still in this patch (redundant with HADOOP-8211)",code_debt,low_quality_code,"Mon, 26 Mar 2012 00:10:15 +0000","Thu, 2 May 2013 02:29:51 +0000","Mon, 2 Apr 2012 19:07:17 +0000",673022,"I think it's worth changing the return type of this function to LinkedHashSet, so it's clear that the ordering here is on purpose. Perhaps also add a comment here saying something like: Nits: please un-abbreviate ""first"" for better readability. Also, ""e.g."" instead of ""Eg.""  or just say ""For example"" I think it's more idiomatic to just put the postincrement inside the []s there's a small spurious whitespace change in NetUtils.java looks like the pom change is still in this patch (redundant with HADOOP-8211)",0.0728,0.0728,neutral
hadoop,8222,description,"it would be nice if the jsvc pid file were properly namespaced in /var/run to avoid collisions with other jsvc instances jsvc uses /var/run/jsvc.pid for the datanode pid file. If that's configurable, it should be configured: /var/run/jsvc.pid is sorely not namespaced. if [ = ""true"" ]; then -errfile &1 -outfile else # Even though we are trying to run a non-detached datanode, # jsvc will not write to stdout/stderr, so we have to pipe # it and tail the logfile. -errfile &1 -outfile $log_path"" echo Non-detached jsvc output piping to: $log_path touch $log_path tail -f $log_path & fi And the relevant argument is '-pidfile'",code_debt,low_quality_code,"Thu, 29 Mar 2012 01:39:11 +0000","Mon, 30 Jun 2014 23:04:24 +0000","Fri, 30 Mar 2012 01:03:23 +0000",84252,"it would be nice if the jsvc pid file were properly namespaced in /var/run to avoid collisions with other jsvc instances jsvc uses /var/run/jsvc.pid for the datanode pid file. If that's configurable, it should be configured: /var/run/jsvc.pid is sorely not namespaced. if [ ""$_HADOOP_DAEMON_DETACHED"" = ""true"" ]; then _JSVC_FLAGS=""-pidfile $_HADOOP_DAEMON_PIDFILE -errfile &1 -outfile $_HADOOP_DAEMON_OUT"" else Even though we are trying to run a non-detached datanode, jsvc will not write to stdout/stderr, so we have to pipe it and tail the logfile. log_path=/tmp/jsvc_${COMMAND}.$$ _JSVC_FLAGS=""-nodetach -errfile &1 -outfile $log_path"" echo Non-detached jsvc output piping to: $log_path touch $log_path tail -f $log_path & fi And the relevant argument is '-pidfile' (http://linux.die.net/man/1/jsvc).",0.0223,0.004666666667,neutral
hadoop,8283,description,Tests in projects other than common need to be able to change whether the token service uses an ip or a host.,code_debt,low_quality_code,"Mon, 16 Apr 2012 15:32:16 +0000","Fri, 7 Sep 2012 21:01:44 +0000","Mon, 16 Apr 2012 16:12:57 +0000",2441,Tests in projects other than common need to be able to change whether the token service uses an ip or a host.,0.094,0.094,neutral
hadoop,8288,summary,Remove references of mapred.child.ulimit etc. since they are not being used any more,code_debt,dead_code,"Tue, 17 Apr 2012 17:35:19 +0000","Fri, 7 Sep 2012 21:01:45 +0000","Thu, 19 Apr 2012 16:15:01 +0000",167982,Remove references of mapred.child.ulimit etc. since they are not being used any more,0,0,negative
hadoop,8303,comment_0,Patch that cleans all warnings of hadoop-streaming sub-module save for usage of MiniMRCluster and DistributedCache and one instance of StreamJob's non usual constructor cause a job reference is still required there.,code_debt,low_quality_code,"Sun, 22 Apr 2012 17:44:54 +0000","Thu, 12 May 2016 18:27:52 +0000","Sun, 2 Dec 2012 05:02:52 +0000",19307878,Patch that cleans all warnings of hadoop-streaming sub-module save for usage of MiniMRCluster and DistributedCache and one instance of StreamJob's non usual constructor cause a job reference is still required there.,0.027,0.027,neutral
hadoop,8303,comment_2,"A few things: # Looks like even Hadoop-Common QA does not pick up hadoop-tools changes. # The eclipse's auto unused import fixer removed RecRecord imports which caused TestIO not to compile. I fixed those manually in this new patch. # TestStreaming had a change in original patch that caused it to lose info about the running jobs and such. I tweaked the change to use a non ToolRunner approach, and now work nicely again - minus deprecation warnings. Since Hadoop QA bot can't run the tests, here is the test result manually run, with patch applied: {{cd mvn clean test}}",code_debt,low_quality_code,"Sun, 22 Apr 2012 17:44:54 +0000","Thu, 12 May 2016 18:27:52 +0000","Sun, 2 Dec 2012 05:02:52 +0000",19307878,"A few things: Looks like even Hadoop-Common QA does not pick up hadoop-tools changes. The eclipse's auto unused import fixer removed RecRecord imports which caused TestIO not to compile. I fixed those manually in this new patch. TestStreaming had a change in original patch that caused it to lose info about the running jobs and such. I tweaked the change to use a non ToolRunner approach, and TestStreamingCounters/Combiner now work nicely again - minus deprecation warnings. Since Hadoop QA bot can't run the tests, here is the test result manually run, with patch applied: cd hadoop-tools/hadoop-streaming; mvn clean test",0.03888888889,0.03888888889,negative
hadoop,8303,description,Clean up a bunch of existing javac warnings in hadoop-streaming module.,code_debt,low_quality_code,"Sun, 22 Apr 2012 17:44:54 +0000","Thu, 12 May 2016 18:27:52 +0000","Sun, 2 Dec 2012 05:02:52 +0000",19307878,Clean up a bunch of existing javac warnings in hadoop-streaming module.,-0.1,-0.1,neutral
hadoop,8303,summary,Clean up hadoop-streaming,code_debt,low_quality_code,"Sun, 22 Apr 2012 17:44:54 +0000","Thu, 12 May 2016 18:27:52 +0000","Sun, 2 Dec 2012 05:02:52 +0000",19307878,Clean up hadoop-streaming,0.4,0.4,neutral
hadoop,8304,comment_12,"That's good comments on compatibility of this change. I saw evolving tag there but not consider people extends DNSToSwitchMapping as well (just thought ScriptBased, TableMapping and cached are good enough). I won't say original interface cause some performance headache as the time of resolving rack info can be overwhelmed comparing with the whole flow (replica placement or task scheduling). However, it is more easy to use for major consumers of original interface which are expecting to resolve individual host. Do you see any scenario to resolve a list of host? (not counting the unit test) Eli, I don't understand the question of last comment there as I just want to fix the interface here. :)",code_debt,slow_algorithm,"Mon, 23 Apr 2012 10:35:52 +0000","Wed, 23 May 2012 20:15:50 +0000","Tue, 8 May 2012 21:14:32 +0000",1334320,"That's good comments on compatibility of this change. I saw evolving tag there but not consider people extends DNSToSwitchMapping as well (just thought ScriptBased, TableMapping and cached are good enough). I won't say original interface cause some performance headache as the time of resolving rack info can be overwhelmed comparing with the whole flow (replica placement or task scheduling). However, it is more easy to use for major consumers of original interface which are expecting to resolve individual host. Do you see any scenario to resolve a list of host? (not counting the unit test) Eli, I don't understand the question of last comment there as I just want to fix the interface here.",0.4284285714,0.4331666667,positive
hadoop,8316,comment_0,"Patch attached. - update to NullAppender in log4j.properties, it is set explicitly by default in the bin and env scripts, so this is mostly a nop - and now default to the NullAppender in log4j.properties. Update hdfs.audit.logger in hadoop-env.sh to match. This is being made configurable in HADOOP-8224. mapred.audit.logger is not set in the bin or env scripts and is dead code, filed HADOOP-8392 for that (and to hookup RM/NM). Testing, verified the hdfs audit log is no longer automatically created and logged to when run from a tarball install.",code_debt,dead_code,"Thu, 26 Apr 2012 02:34:24 +0000","Thu, 11 Oct 2012 17:45:05 +0000","Fri, 11 May 2012 19:25:54 +0000",1356690,"Patch attached. update hadoop.security.logger to NullAppender in log4j.properties, it is set explicitly by default in the bin and env scripts, so this is mostly a nop hdfs/mapred.audit.logger and now default to the NullAppender in log4j.properties. Update hdfs.audit.logger in hadoop-env.sh to match. This is being made configurable in HADOOP-8224. mapred.audit.logger is not set in the bin or env scripts and is dead code, filed HADOOP-8392 for that (and to hookup RM/NM). Testing, verified the hdfs audit log is no longer automatically created and logged to when run from a tarball install.",0.05,0.005882352941,neutral
hadoop,8341,description,Now that the precommit build can test hadoop-tools we need to fix or filter the many findbugs warnings that are popping up in there.,code_debt,low_quality_code,"Tue, 1 May 2012 20:09:22 +0000","Thu, 12 May 2016 18:25:56 +0000","Tue, 8 May 2012 13:24:35 +0000",580513,Now that the precommit build can test hadoop-tools we need to fix or filter the many findbugs warnings that are popping up in there.,-0.6,-0.6,neutral
hadoop,8358,comment_0,"This patch is in HADOOP cause its slightly wide/crossproject. Lemme know if I should split it though. Changes summary: * The NodeManager log (from where I noticed this first) showed this cause there was a hdfs-default.xml config set for this deprecated prop, and its web filter loaded that up. Cleaned up hdfs-default.xml. * The old property lookup existed in JspHelper in HDFS. Changed that to use the new property. * Cleaned up the usage of constants for this property, via classes instead of its own constant refs. * Added new prop and default to core-default.xml",code_debt,low_quality_code,"Fri, 4 May 2012 07:44:56 +0000","Thu, 11 Oct 2012 17:45:09 +0000","Mon, 28 May 2012 15:42:38 +0000",2102262,"This patch is in HADOOP cause its slightly wide/crossproject. Lemme know if I should split it though. Changes summary: The NodeManager log (from where I noticed this first) showed this cause there was a hdfs-default.xml config set for this deprecated prop, and its web filter loaded that up. Cleaned up hdfs-default.xml. The old property lookup existed in JspHelper in HDFS. Changed that to use the new property. Cleaned up the usage of constants for this property, via CommonConfigurationKeys classes instead of its own constant refs. Added new prop and default to core-default.xml",-0.1363636364,-0.1363636364,neutral
hadoop,8358,comment_4,Any further comments on the patch? Its quite trivial a change and helps remove unnecessary WARN noise.,code_debt,dead_code,"Fri, 4 May 2012 07:44:56 +0000","Thu, 11 Oct 2012 17:45:09 +0000","Mon, 28 May 2012 15:42:38 +0000",2102262,Any further comments on the patch? Its quite trivial a change and helps remove unnecessary WARN noise.,0.4375,0.4375,neutral
hadoop,8358,description,"Looks easy to fix, and we should avoid using old config params that we ourselves deprecated.",code_debt,low_quality_code,"Fri, 4 May 2012 07:44:56 +0000","Thu, 11 Oct 2012 17:45:09 +0000","Mon, 28 May 2012 15:42:38 +0000",2102262,"Looks easy to fix, and we should avoid using old config params that we ourselves deprecated.",0.046,0.046,positive
hadoop,8405,description,"The ZKFC code wasn't previously terminating the ZK connection in all cases where it should (eg after a failed startup or after formatting ZK). This didn't cause a problem for CLI usage, since the process exited afterwards, but caused the test results to get clouded with a lot of ""Reconecting to ZK"" messages, which make the logs hard to read.",code_debt,low_quality_code,"Wed, 16 May 2012 22:43:46 +0000","Thu, 17 May 2012 00:39:07 +0000","Thu, 17 May 2012 00:39:07 +0000",6921,"The ZKFC code wasn't previously terminating the ZK connection in all cases where it should (eg after a failed startup or after formatting ZK). This didn't cause a problem for CLI usage, since the process exited afterwards, but caused the test results to get clouded with a lot of ""Reconecting to ZK"" messages, which make the logs hard to read.",0.2,0.2,neutral
hadoop,8405,summary,ZKFC tests leak ZK instances,code_debt,low_quality_code,"Wed, 16 May 2012 22:43:46 +0000","Thu, 17 May 2012 00:39:07 +0000","Thu, 17 May 2012 00:39:07 +0000",6921,ZKFC tests leak ZK instances,-0.2,-0.2,neutral
hadoop,8431,comment_6,"It should just print the usage. Eg in the following I'd remove the ERROR log, the backtrace and the ""Invalid arguments"" log.",code_debt,low_quality_code,"Thu, 24 May 2012 01:51:09 +0000","Thu, 11 Oct 2012 17:45:04 +0000","Fri, 7 Sep 2012 01:27:55 +0000",9157006,"It should just print the usage. Eg in the following I'd remove the ERROR log, the IllegalArgumentException backtrace and the ""Invalid arguments"" log.",-0.2,-0.2,neutral
hadoop,8548,description,Precommit builds show an incorrect link for javac warnings. Note that 'trunk' appears twice. Do we need $(basename $BASEDIR) in the following? Other places don't have it.,code_debt,low_quality_code,"Mon, 2 Jul 2012 18:11:46 +0000","Thu, 12 May 2016 18:25:38 +0000","Mon, 2 Jul 2012 19:36:05 +0000",5059,Precommit builds show an incorrect link for javac warnings. Note that 'trunk' appears twice. Do we need $(basename $BASEDIR) in the following? Other places don't have it.,-0.15,-0.15,negative
hadoop,8597,comment_6,Jenkins says that should be static.,code_debt,low_quality_code,"Sat, 14 Jul 2012 07:37:02 +0000","Fri, 15 Feb 2013 13:12:35 +0000","Tue, 11 Sep 2012 20:48:25 +0000",5145083,Jenkins says that org.apache.hadoop.fs.shell.Display$AvroFileInputStream should be static.,-0.875,-0.1458333333,neutral
hadoop,8633,comment_4,The changes look OK. I'm not sure I really like TargetFileSystem. I just don't see a lot of benefit from making TargetFileSystem. I think it would be cleaner to just have your own list of Paths to delete and then delete them in the finally block. Having it be a FileSystem just seems confusing to me. Because TargetFileSystem is never added to the FileSystem cache I assume that if a System.exit is called somewhere while processing the command the TargetFileSystem will not be closed and the files will not actually be deleted on exit. Is this important?,code_debt,low_quality_code,"Mon, 30 Jul 2012 14:17:13 +0000","Thu, 12 May 2016 18:25:59 +0000","Wed, 1 Aug 2012 14:04:40 +0000",172047,The changes look OK. I'm not sure I really like TargetFileSystem. I just don't see a lot of benefit from making TargetFileSystem. I think it would be cleaner to just have your own list of Paths to delete and then delete them in the finally block. Having it be a FileSystem just seems confusing to me. Because TargetFileSystem is never added to the FileSystem cache I assume that if a System.exit is called somewhere while processing the command the TargetFileSystem will not be closed and the files will not actually be deleted on exit. Is this important?,0.015625,0.015625,negative
hadoop,8819,comment_8,"In this code if {{created}} is true it enters the if condition and then executes If {{created}} is false that part of the code is not entered. Aaron, can you add details on why this would change the behavior?",code_debt,low_quality_code,"Sun, 16 Sep 2012 16:31:48 +0000","Thu, 12 May 2016 18:22:15 +0000","Mon, 17 Sep 2012 17:11:22 +0000",88774,"Note that the change in FTPFileSystem actually is a behavior change, and perhaps an incompatible one. All of the rest of these changes seem harmless, but that one seems a little suspect. In this code if created is true it enters the if condition and then executes client.makeDirectory(). If created is false that part of the code is not entered. Aaron, can you add details on why this would change the behavior?",0.09325,0.02883333333,neutral
hadoop,8819,description,Should use && instead of & in a few places in,code_debt,low_quality_code,"Sun, 16 Sep 2012 16:31:48 +0000","Thu, 12 May 2016 18:22:15 +0000","Mon, 17 Sep 2012 17:11:22 +0000",88774,"Should use && instead of & in a few places in FTPFileSystem,FTPInputStream,S3InputStream,ViewFileSystem,ViewFs.",0,0,neutral
hadoop,8819,summary,Should use && instead of & in a few places in,code_debt,low_quality_code,"Sun, 16 Sep 2012 16:31:48 +0000","Thu, 12 May 2016 18:22:15 +0000","Mon, 17 Sep 2012 17:11:22 +0000",88774,"Should use && instead of  & in a few places in FTPFileSystem,FTPInputStream,S3InputStream,ViewFileSystem,ViewFs",0,0,neutral
hadoop,8843,comment_4,"Looks good to me, except for one small nit: it would be worth a javadoc comment on OLD_CHECKPOINT saying something like: /** Format of checkpoint directories used prior to Hadoop 0.23. */ and then a small comment at the point where it's used, like // Check for old-style checkpoint directories left over after an upgrade from Hadoop 1.x Otherwise I think people may be confused what ""old"" refers to, a few years down the line.",code_debt,low_quality_code,"Tue, 25 Sep 2012 20:27:48 +0000","Wed, 3 Sep 2014 23:21:16 +0000","Wed, 26 Sep 2012 17:39:25 +0000",76297,"Looks good to me, except for one small nit: it would be worth a javadoc comment on OLD_CHECKPOINT saying something like: /** Format of checkpoint directories used prior to Hadoop 0.23. */ and then a small comment at the point where it's used, like // Check for old-style checkpoint directories left over after an upgrade from Hadoop 1.x Otherwise I think people may be confused what ""old"" refers to, a few years down the line.",0.2556666667,0.2556666667,positive
hadoop,8866,comment_1,"While you're at it, why not get rid of size() and numeric iteration too? eg:",code_debt,low_quality_code,"Tue, 25 Sep 2012 22:41:01 +0000","Fri, 15 Feb 2013 13:11:50 +0000","Sat, 29 Sep 2012 01:00:52 +0000",267591,"While you're at it, why not get rid of size() and numeric iteration too? eg:",0,0,neutral
hadoop,8866,description,"does O(N) calls LinkedList#get() in a loop, rather than using an iterator. This makes query O(N^2), rather than O(N).",code_debt,slow_algorithm,"Tue, 25 Sep 2012 22:41:01 +0000","Fri, 15 Feb 2013 13:11:50 +0000","Sat, 29 Sep 2012 01:00:52 +0000",267591,"SampleQuantiles#query() does O(N) calls LinkedList#get() in a loop, rather than using an iterator. This makes query O(N^2), rather than O(N).",0,0,neutral
hadoop,8895,comment_1,"Daryn, Thanks for your input. Though it is not absolutely necessary for {{TokenRenewer}} to be an interface, I felt it would make things simpler when working on HDFS-4009. For instance, if a could implement {{TokenRenewer}}, we might not need Now that we are removing completely - {{HADOOP-8891}} - it won't be immediately applicable. However, I was not sure why it should be an abstract class. Do you think we should leave it as is? Is there an advantage of abstract class over interface? I am uploading a patch with my proposed changes.",code_debt,low_quality_code,"Sat, 6 Oct 2012 03:55:55 +0000","Mon, 3 Nov 2014 18:33:59 +0000","Tue, 9 Oct 2012 18:33:46 +0000",311871,"Daryn, Thanks for your input. Though it is not absolutely necessary for TokenRenewer to be an interface, I felt it would make things simpler when working on HDFS-4009. For instance, if a WebHdfsFileSystem could implement TokenRenewer, we might not need DelegationTokenRenewer.Renewable. Now that we are removing DelegationTokenRenewer completely - HADOOP-8891 - it won't be immediately applicable. However, I was not sure why it should be an abstract class. Do you think we should leave it as is? Is there an advantage of abstract class over interface? I am uploading a patch with my proposed changes.",0.128,0.09955555556,neutral
hadoop,8921,comment_6,"The patch looks good. Even though is redundant in the command ""ant compile-native compile-native"" can do the job itself), it's still good to be consistent. One minor thing is the os-arch check. Original code doesn't check the x86_64/x86 availability, native code compilation can fail on certain platforms(such as MacOS). At lease the developer will notice the failure. With the os-arch check in the patch, the ""ant compile-native"" command will report success but actually didn't do the work. The original behavior seems more intuitive to me.",code_debt,complex_code,"Fri, 12 Oct 2012 09:36:01 +0000","Wed, 19 Mar 2014 21:13:40 +0000","Wed, 19 Mar 2014 21:13:40 +0000",45229059,"The patch looks good. Even though ""-Dcompile.native=true"" is redundant in the command ""ant compile-native -Dcompile.native=true""(""ant compile-native"" can do the job itself), it's still good to be consistent. One minor thing is the os-arch check. Original code doesn't check the x86_64/x86 availability, native code compilation can fail on certain platforms(such as MacOS). At lease the developer will notice the failure. With the os-arch check in the patch, the ""ant compile-native"" command will report success but actually didn't do the work. The original behavior seems more intuitive to me.",0.09928571429,0.1423888889,positive
hadoop,8929,comment_2,"Sorry, should have explained the patch in more detail: - made it Comparable and changed the HashMap to TreeMap so that the printout is in ascending percentile order. Given that this map is always very small, and snapshot() is only called once a minute or so, the runtime/memory differences between treemap and hashmap should be negligible. - changed the behavior to return null instead of throw, because all the catching, etc, got pretty ugly. In implementing toString, I figured I'd clean up the other behavior along the way.",code_debt,low_quality_code,"Mon, 15 Oct 2012 19:46:19 +0000","Thu, 12 May 2016 18:22:35 +0000","Tue, 16 Oct 2012 06:07:18 +0000",37259,"Sorry, should have explained the patch in more detail: made it Comparable and changed the HashMap to TreeMap so that the printout is in ascending percentile order. Given that this map is always very small, and snapshot() is only called once a minute or so, the runtime/memory differences between treemap and hashmap should be negligible. changed the behavior to return null instead of throw, because all the catching, etc, got pretty ugly. In implementing toString, I figured I'd clean up the other behavior along the way.",-0.06666666667,-0.05,neutral
hadoop,8929,description,"The new SampleQuantiles class is useful in the context of benchmarks, but currently there is no way to print it out outside the context of a metrics sink. It would be nice to have a convenient way to stringify it for logging, etc. Also: - made it Comparable and changed the HashMap to TreeMap so that the printout is in ascending percentile order. Given that this map is always very small, and snapshot() is only called once a minute or so, the runtime/memory differences between treemap and hashmap should be negligible. - changed the behavior to return null instead of throw, because all the catching, etc, got pretty ugly. In implementing toString, I figured I'd clean up the other behavior along the way.",code_debt,low_quality_code,"Mon, 15 Oct 2012 19:46:19 +0000","Thu, 12 May 2016 18:22:35 +0000","Tue, 16 Oct 2012 06:07:18 +0000",37259,"The new SampleQuantiles class is useful in the context of benchmarks, but currently there is no way to print it out outside the context of a metrics sink. It would be nice to have a convenient way to stringify it for logging, etc. Also: made it Comparable and changed the HashMap to TreeMap so that the printout is in ascending percentile order. Given that this map is always very small, and snapshot() is only called once a minute or so, the runtime/memory differences between treemap and hashmap should be negligible. changed the behavior to return null instead of throw, because all the catching, etc, got pretty ugly. In implementing toString, I figured I'd clean up the other behavior along the way.",0.325,0.2708333333,neutral
hadoop,8985,description,"Currently .proto files use java_package to specify java packages in proto files, but namespace are not specified for other languages such as cpp, this causes name collision in cpp. we can add namespace declarations to avoid this. In Java, the package specifier is used as the Java package, unless you explicitly provide a option java_package in your .proto file. So the original java package will not be affected. About namespace name, how about in cpp) for all common sub-project proto files, and in cpp) for all hdfs sub-project proto files?",code_debt,low_quality_code,"Fri, 26 Oct 2012 05:53:55 +0000","Wed, 3 Sep 2014 23:11:06 +0000","Sat, 27 Oct 2012 18:48:14 +0000",132859,"Currently .proto files use java_package to specify java packages in proto files, but namespace are not specified for other languages such as cpp, this causes name collision in cpp. we can add namespace declarations to avoid this. In Java, the package specifier is used as the Java package, unless you explicitly provide a option java_package in your .proto file. So the original java package will not be affected. About namespace name, how about ""hadoop.common""(hadoop::common in cpp) for all common sub-project proto files, and ""hadoop.hdfs""(hadoop::hdfs in cpp) for all hdfs sub-project proto files?",-0.05714285714,-0.04444444444,neutral
hadoop,9254,comment_4,"+1, lgtm. One minor nit: has an extra license comment in the middle of the imports which I'll cleanup on checkin.",code_debt,low_quality_code,"Mon, 28 Jan 2013 10:06:33 +0000","Thu, 12 May 2016 18:27:10 +0000","Wed, 2 Oct 2013 20:55:31 +0000",21379738,"+1, lgtm. One minor nit: BloomFilterCommonTester has an extra license comment in the middle of the imports which I'll cleanup on checkin.",0,0,neutral
hadoop,9278,comment_0,"The invalid HAR URI is caused here in On Windows, this creates a path that includes the drive specifier. Later, this gets used to construct a HAR URI for testing, which isn't valid, because it doesn't adhere to the the protocol-host format used by HAR URIs. The file handle leak happens in This method opens the _masterindex file, but not all code paths guarantee that the file will be closed. For example, parsing an invalid version throws an exception before the _masterindex file gets closed. There is a test that covers this case, so it leaks a file handle when that test runs. On Windows, this ultimately causes tests to fail during their post-test cleanup, because Windows file locking behavior causes the delete to fail.",code_debt,low_quality_code,"Mon, 4 Feb 2013 23:00:17 +0000","Thu, 12 May 2016 18:22:52 +0000","Tue, 5 Feb 2013 21:26:06 +0000",80749,"The invalid HAR URI is caused here in TestHarFileSystemBasics: On Windows, this creates a path that includes the drive specifier. Later, this gets used to construct a HAR URI for testing, which isn't valid, because it doesn't adhere to the the protocol-host format used by HAR URIs. The file handle leak happens in HarFileSystem#HarMetaData#parseMetaData: This method opens the _masterindex file, but not all code paths guarantee that the file will be closed. For example, parsing an invalid version throws an exception before the _masterindex file gets closed. There is a test that covers this case, so it leaks a file handle when that test runs. On Windows, this ultimately causes tests to fail during their post-test cleanup, because Windows file locking behavior causes the delete to fail.",-0.1,-0.1,negative
hadoop,9278,description,fails on Windows due to invalid HAR URI and file handle leak. We need to change the tests to use valid HAR URIs and fix the file handle leak.,code_debt,low_quality_code,"Mon, 4 Feb 2013 23:00:17 +0000","Thu, 12 May 2016 18:22:52 +0000","Tue, 5 Feb 2013 21:26:06 +0000",80749,TestHarFileSystemBasics fails on Windows due to invalid HAR URI and file handle leak. We need to change the tests to use valid HAR URIs and fix the file handle leak.,-0.25,-0.25,negative
hadoop,9278,summary,HarFileSystem may leak file handle,code_debt,low_quality_code,"Mon, 4 Feb 2013 23:00:17 +0000","Thu, 12 May 2016 18:22:52 +0000","Tue, 5 Feb 2013 21:26:06 +0000",80749,HarFileSystem may leak file handle,-0.2,-0.2,negative
hadoop,9305,comment_0,"Here's a simple patch which addresses the issue. The only behavior changes are to conditionally load the AIX64LoginModule and the UsernamePrincipal classes if we're on a 64-bit AIX box, instead of the AIXLoginModule and AIXPrincipal classes. This patch also refactors the getOsPrincipalClass a little bit to reduce some code repetition. No tests are included since to test this properly would require an AIX box. I tested this manually by running with both 32-bit and 64-bit AIX clients and confirming that it works as expected, both with and without Kerberos enabled. Without the patch only 32-bit clients will work. I also ensured there are no regressions by testing the Hadoop client with both IBM Java and Sun Java on Linux both with and without Kerberos enabled. Everything worked as expected.",code_debt,duplicated_code,"Wed, 13 Feb 2013 18:48:14 +0000","Tue, 27 Aug 2013 22:06:30 +0000","Wed, 13 Feb 2013 19:48:07 +0000",3593,"Here's a simple patch which addresses the issue. The only behavior changes are to conditionally load the AIX64LoginModule and the UsernamePrincipal classes if we're on a 64-bit AIX box, instead of the AIXLoginModule and AIXPrincipal classes. This patch also refactors the getOsPrincipalClass a little bit to reduce some code repetition. No tests are included since to test this properly would require an AIX box. I tested this manually by running with both 32-bit and 64-bit AIX clients and confirming that it works as expected, both with and without Kerberos enabled. Without the patch only 32-bit clients will work. I also ensured there are no regressions by testing the Hadoop client with both IBM Java and Sun Java on Linux both with and without Kerberos enabled. Everything worked as expected.",-0.0023125,-0.0023125,neutral
hadoop,9305,comment_1,"+1 pending jenkins. ATM, what about opening a JIRA to clean this spaghetti of conditionals replacing it with a MAP and a simple struct having the needed settings?",code_debt,low_quality_code,"Wed, 13 Feb 2013 18:48:14 +0000","Tue, 27 Aug 2013 22:06:30 +0000","Wed, 13 Feb 2013 19:48:07 +0000",3593,"+1 pending jenkins. ATM, what about opening a JIRA to clean this spaghetti of conditionals replacing it with a MAP and a simple struct having the needed settings?",0.2,0.2,neutral
hadoop,9309,comment_2,"Thanks, Arpit. I tested this on Windows and Ubuntu with native build, and it worked great. Here are a couple of comments. I am +1 for the patch after removal of some unneeded #includes unless those #includes are there for some reason that I missed. (See below for details.) I believe this setting would disable the warning across the whole project, right? Another option could be to upgrade our lz4.c. It looks like there have been some recent changes to address warnings seen when compiling on Windows: I'm sure that would be a much larger scope though, so it's probably best to treat it as a separate jira. Is it necessary to add these #includes to NativeCodeLoader.c? I tried removing them, and I was still able to compile.",code_debt,dead_code,"Thu, 14 Feb 2013 21:18:45 +0000","Thu, 21 Feb 2013 18:58:39 +0000","Thu, 21 Feb 2013 18:45:01 +0000",595576,"Thanks, Arpit. I tested this on Windows and Ubuntu with native build, and it worked great. Here are a couple of comments. I am +1 for the patch after removal of some unneeded #includes unless those #includes are there for some reason that I missed. (See below for details.) I believe this setting would disable the warning across the whole project, right? Another option could be to upgrade our lz4.c. It looks like there have been some recent changes to address warnings seen when compiling on Windows: http://code.google.com/p/lz4/source/detail?r=75&path=/trunk/lz4.c I'm sure that would be a much larger scope though, so it's probably best to treat it as a separate jira. Is it necessary to add these #includes to NativeCodeLoader.c? I tried removing them, and I was still able to compile.",0.1083636364,0.1083636364,positive
hadoop,930,comment_1,Second patch with the following changes: * Send Content-MD5 header to perform message integrity checks for writes. * Fix warnings from Jets3t to do with not closing streams. * Change property names to be independent of existing S3FileSystem: and * Findbugs and formatting fixes.,code_debt,low_quality_code,"Thu, 25 Jan 2007 14:48:52 +0000","Thu, 2 May 2013 02:29:14 +0000","Fri, 6 Jun 2008 21:07:05 +0000",43049893,Second patch with the following changes: Send Content-MD5 header to perform message integrity checks for writes. Fix warnings from Jets3t to do with not closing streams. Change property names to be independent of existing S3FileSystem: fs.s3n.awsAccessKeyId and fs.s3n.awsSecretAccessKey. Findbugs and formatting fixes.,-0.2333333333,-0.0875,neutral
hadoop,930,comment_3,Fixed release audit warnings and a few formatting warnings from checkstyle.,code_debt,low_quality_code,"Thu, 25 Jan 2007 14:48:52 +0000","Thu, 2 May 2013 02:29:14 +0000","Fri, 6 Jun 2008 21:07:05 +0000",43049893,Fixed release audit warnings and a few formatting warnings from checkstyle.,-0.6,-0.6,neutral
hadoop,9336,comment_1,"The semantics will be returning the UGI of the connection, so it will always report the UGI of the original user making the connection, not of any subsequent {{UGI.doAs}} calls. However, this jira will not universally affect anything that doesn't explicitly use it. I intend for only the {{FSNamesystem}} audit calls to currently use it to reduce the significant performance bottlenecks we are encountering. I'm contemplating filing another jira for {{UGI.doAs}} to cache a stack of UGIs. That would greatly accelerate in general.",code_debt,slow_algorithm,"Tue, 26 Feb 2013 20:22:15 +0000","Thu, 12 May 2016 18:21:47 +0000","Thu, 28 Feb 2013 22:07:17 +0000",179102,"The semantics will be returning the UGI of the connection, so it will always report the UGI of the original user making the connection, not of any subsequent UGI.doAs calls. However, this jira will not universally affect anything that doesn't explicitly use it. I intend for only the FSNamesystem audit calls to currently use it to reduce the significant performance bottlenecks we are encountering. I'm contemplating filing another jira for UGI.doAs to cache a stack of UGIs. That would greatly accelerate UGI.getCurrentUser in general.",0.02857142857,0.025,neutral
hadoop,9419,description,"I recently found a bug in the gpl compression libraries that was causing map tasks for a particular job to OOM. Now granted it does not make a lot of sense for a job to use the LzopCodec for map output compression over the LzoCodec, but arguably other codecs could be doing similar things and causing the same sort of memory leaks. I propose that we do a sanity check when creating a new If the codec newly created object does not match the value from getType... it should turn off caching for that Codec.",code_debt,low_quality_code,"Tue, 19 Mar 2013 18:34:28 +0000","Tue, 19 Mar 2013 21:20:44 +0000","Tue, 19 Mar 2013 21:20:44 +0000",9976,"I recently found a bug in the gpl compression libraries that was causing map tasks for a particular job to OOM. https://github.com/omalley/hadoop-gpl-compression/issues/3 Now granted it does not make a lot of sense for a job to use the LzopCodec for map output compression over the LzoCodec, but arguably other codecs could be doing similar things and causing the same sort of memory leaks. I propose that we do a sanity check when creating a new decompressor/compressor. If the codec newly created object does not match the value from getType... it should turn off caching for that Codec.",0.07716666667,0.06575,negative
hadoop,9508,comment_1,"If the code itself is confusing, perhaps you could use this jira to add comments/fix comments.",code_debt,low_quality_code,"Thu, 25 Apr 2013 22:41:43 +0000","Thu, 12 May 2016 18:22:06 +0000","Thu, 25 Apr 2013 23:56:12 +0000",4469,"If the code itself is confusing, perhaps you could use this jira to add comments/fix comments.",-0.437,-0.437,neutral
hadoop,95,comment_4,"Posting here a similar performance test, which is related to HADOOP-72 Just because these two tests have too much in common. TestDFSIO measures performance of the cluster for reads and writes.",code_debt,duplicated_code,"Sat, 18 Mar 2006 09:31:35 +0000","Wed, 8 Jul 2009 16:41:49 +0000","Wed, 18 Oct 2006 18:31:16 +0000",18521981,"Posting here a similar performance test, which is related to HADOOP-72 Just because these two tests have too much in common. TestDFSIO measures performance of the cluster for reads and writes.",-0.25,-0.25,neutral
hadoop,9669,comment_3,"Thanks, Haohui. Some comments: 1. please try to keep the original javadoc for the same named methods 2. can you make ""State state"" as final? 3. please fix the javadoc /** check if the rest of data has more than <len""len"" is not visible in generated javadoc 4. readFixedOpaque still has a copy not sure if it's possible to generat a read-only bytebuffer from another bytebuffer 5. it would be nice to remove the extra copy for writeFixedOpaque For 4 and 5, I am ok if you think it's out of scope of this JIRA.",code_debt,low_quality_code,"Tue, 25 Jun 2013 19:43:32 +0000","Tue, 24 Sep 2013 23:33:30 +0000","Wed, 18 Sep 2013 01:03:59 +0000",7276827,"Thanks, Haohui. Some comments: 1. please try to keep the original javadoc for the same named methods 2. can you make ""State state"" as final? 3. please fix the javadoc /** check if the rest of data has more than <len> bytes */ ""len"" is not visible in generated javadoc 4. readFixedOpaque still has a copy not sure if it's possible to generat a read-only bytebuffer from another bytebuffer 5. it would be nice to remove the extra copy for writeFixedOpaque For 4 and 5, I am ok if you think it's out of scope of this JIRA.",0.1898125,0.1898125,neutral
hadoop,9669,comment_6,"+1. New patch looks good. Nit: I noticed that most NFS related RPC call response size is less than 256 bytes, so we can have as 256 instead of 512.",code_debt,low_quality_code,"Tue, 25 Jun 2013 19:43:32 +0000","Tue, 24 Sep 2013 23:33:30 +0000","Wed, 18 Sep 2013 01:03:59 +0000",7276827,"+1. New patch looks good. Nit: I noticed that most NFS related RPC call response size is less than 256 bytes, so we can have DEFAULT_INITIAL_CAPACITY as 256 instead of 512.",0.2586666667,0.092,positive
hadoop,9669,summary,Reduce the number of byte array creations and copies in XDR data manipulation,code_debt,low_quality_code,"Tue, 25 Jun 2013 19:43:32 +0000","Tue, 24 Sep 2013 23:33:30 +0000","Wed, 18 Sep 2013 01:03:59 +0000",7276827,Reduce the number of byte array creations and copies in XDR data manipulation,-0.2,-0.2,neutral
hadoop,9763,comment_13,- adds more cases to the short test; - disables the longer test; - randomizes initial value of currentTestTime and the increments; - adds slightly more comments in,code_debt,low_quality_code,"Sun, 21 Jul 2013 10:16:02 +0000","Tue, 27 Aug 2013 22:06:38 +0000","Wed, 24 Jul 2013 06:49:54 +0000",246832,c9763_20130724.patch: adds more cases to the short test; disables the longer test; randomizes initial value of currentTestTime and the increments; adds slightly more comments in TestLightWeightCache.,0,0,neutral
hadoop,9883,comment_8,"Hi Abin! Thanks for your contribution! Please refer to Matt's comment (from when I first started :-)) Also, please consider reading . The two things I'd recommend are using 2 spaces for indentation and following the 80 char limit per line. I noticed that the change was made as part of MAPREDUCE-181 . Before that it used to be what you are proposing. Do you understand why that change might / might not be relevant any more? I can review the test code once we finalize the src/main code.",code_debt,dead_code,"Mon, 19 Aug 2013 15:53:23 +0000","Tue, 12 May 2015 22:07:20 +0000","Tue, 12 May 2015 22:07:11 +0000",54540828,"Hi Abin! Thanks for your contribution! Please refer to Matt's comment (from when I first started ) https://issues.apache.org/jira/browse/HDFS-2011?focusedCommentId=13041707&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13041707 Also, please consider reading http://wiki.apache.org/hadoop/HowToContribute . The two things I'd recommend are using 2 spaces for indentation and following the 80 char limit per line. I noticed that the change was made as part of MAPREDUCE-181 . Before that it used to be what you are proposing. Do you understand why that change might / might not be relevant any more? I can review the test code once we finalize the src/main code.",0.1416666667,0.125,positive
hadoop,9896,comment_10,"Hi , I think this is not the root cause. I run restRetryProxy alone and also get the timeout, this probably has nothing to do with socket leak. Here is the error message. You can see client retried twice and got stuck",code_debt,low_quality_code,"Wed, 21 Aug 2013 23:21:54 +0000","Thu, 12 May 2016 18:21:44 +0000","Tue, 3 Sep 2013 21:10:43 +0000",1115329,"Hi chuanliu, I think this is not the root cause. I run restRetryProxy alone and also get the timeout, this probably has nothing to do with socket leak. Here is the error message. You can see client retried twice and got stuck",-0.275,-0.275,negative
hadoop,9896,comment_14,"I may found the race condition in ipc.Client and have a fix, please see the detail in [HADOOP-9916]",code_debt,low_quality_code,"Wed, 21 Aug 2013 23:21:54 +0000","Thu, 12 May 2016 18:21:44 +0000","Tue, 3 Sep 2013 21:10:43 +0000",1115329,"I may found the race condition in ipc.Client and have a fix, please see the detail in HADOOP-9916",0.1,0.1,neutral
hadoop,9929,comment_3,I don't believe the globbing code I cleaned up for v23+ has these issues. You want want to use it for reference.,code_debt,low_quality_code,"Tue, 3 Sep 2013 16:09:44 +0000","Mon, 24 Feb 2014 20:58:05 +0000","Thu, 19 Sep 2013 02:10:04 +0000",1332020,I don't believe the globbing code I cleaned up for v23+ has these issues. You want want to use it for reference.,0.1,0.1,negative
hadoop,12837,comment_3,"Thanks  for sharing the details. Yes, it is referring to a directory. Would it be possible for you to suggest some workaround since there are no plans to fix it as you mentioned.",defect_debt,uncorrected_known_defects,"Wed, 24 Feb 2016 09:22:18 +0000","Fri, 28 Apr 2017 09:57:07 +0000","Fri, 28 Apr 2017 09:57:07 +0000",37067689,"Thanks cnauroth for sharing the details. Yes, it is referring to a directory. Would it be possible for you to suggest some workaround since there are no plans to fix it as you mentioned.",0.2,0.2,neutral
hadoop,12837,comment_8,"Well, the above mentioned workaround of the _SUCCESS file works in my case since the content of the directory in question isn't expected to change after it is created. However in case of frequently updating directory contents that won't work. For that case one needs to dig deeper in the directory / sub-directories and determine mtime of each file and finally return the max value as mtime of the directory, however, that would be an expensive operation, particularly in case of huge directories / subdirectories. For now the workaround seems to be working for me. You may want to keep this ticket in backlog if this happens to find priority. Feel free to close if otherwise. Thanks guys for sharing your inputs. Regards, Jagdish",defect_debt,uncorrected_known_defects,"Wed, 24 Feb 2016 09:22:18 +0000","Fri, 28 Apr 2017 09:57:07 +0000","Fri, 28 Apr 2017 09:57:07 +0000",37067689,"Well, the above mentioned workaround of the _SUCCESS file works in my case since the content of the directory in question isn't expected to change after it is created. However in case of frequently updating directory contents that won't work. For that case one needs to dig deeper in the directory / sub-directories and determine mtime of each file and finally return the max value as mtime of the directory, however, that would be an expensive operation, particularly in case of huge directories / subdirectories. For now the workaround seems to be working for me. You may want to keep this ticket in backlog if this happens to find priority. Feel free to close if otherwise. Thanks guys for sharing your inputs. Regards, Jagdish",0.22909375,0.22909375,neutral
hadoop,1367,comment_0,This is actually protected by the FSNamesystem global lock. Have to address this Deferriissue when we go to fine-grain-locking model. Deferring to next release.,defect_debt,uncorrected_known_defects,"Mon, 14 May 2007 22:36:04 +0000","Mon, 20 Aug 2007 18:11:57 +0000","Wed, 27 Jun 2007 18:12:46 +0000",3785802,This is actually protected by the FSNamesystem global lock. Have to address this Deferriissue when we go to fine-grain-locking model. Deferring to next release.,0.15,0.15,neutral
hadoop,1367,comment_1,I do not think it is a critical bug. Deferring it to 0.14.,defect_debt,uncorrected_known_defects,"Mon, 14 May 2007 22:36:04 +0000","Mon, 20 Aug 2007 18:11:57 +0000","Wed, 27 Jun 2007 18:12:46 +0000",3785802,I do not think it is a critical bug. Deferring it to 0.14.,-0.1,-0.1,negative
hadoop,13991,comment_0,"Musaddique thank your for your post and details on a fix. I'm sorry to say we aren't going to take this. That's not because there's anything wrong with it, but because we've stopped doing any work on s3n other than any emergency security work, putting all our effort into S3a. Leaving s3n alone means that we have a reference s3 connector that is pretty much guaranteed not to have any regressions, while in s3a we can do more leading edge stuff. S3a does have retry logic, a lot built into the Amazon S3 library itself, with some extra bits to deal with things that aren't retried that well (e.g. final commit of a multipart upload). # please switch to s3a as soon as you can. If you are using Hadoop 2.7.3, its stable enough for use. # and, if you want to improve s3a, please get involved on that code, ideally look at the work in HADOOP-11694 to see what to look forward to in Hadoop 2.8, and HADOOP-13204 to see the todo list where help is really welcome and that includes help testing. thanks,",defect_debt,uncorrected_known_defects,"Sat, 14 Jan 2017 18:41:34 +0000","Fri, 24 Feb 2017 14:07:36 +0000","Fri, 24 Feb 2017 14:07:36 +0000",3525962,"Musaddique thank your for your post and details on a fix. I'm sorry to say we aren't going to take this. That's not because there's anything wrong with it, but because we've stopped doing any work on s3n other than any emergency security work, putting all our effort into S3a. Leaving s3n alone means that we have a reference s3 connector that is pretty much guaranteed not to have any regressions, while in s3a we can do more leading edge stuff. S3a does have retry logic, a lot built into the Amazon S3 library itself, with some extra bits to deal with things that aren't retried that well (e.g. final commit of a multipart upload). please switch to s3a as soon as you can. If you are using Hadoop 2.7.3, its stable enough for use. and, if you want to improve s3a, please get involved on that code, ideally look at the work in HADOOP-11694 to see what to look forward to in Hadoop 2.8, and HADOOP-13204 to see the todo list where help is really welcome and that includes help testing. thanks,",0.1673439153,0.1673439153,negative
hadoop,2776,comment_8,"I'm going to close this as won't fix. I don't think this is anything that we actually can fix here other than providing a complicated hostname mapping system for web interfaces. Part of the frustration I'm sure stems from a misunderstanding of what is actually happening: The slaves file is only used by the shell code to run ssh connections. It has absolutely zero impact on the core of Hadoop. Hadoop makes the perfectly valid assumption that the hostname the system tells us is a valid, network-connectable hostname. It is, from the inside of EC2. We have no way to know that you are attempting to connect from a completely different address that is being forwarded from some external entity. Proxying connections into a private network space is a perfectly valid solution.",defect_debt,uncorrected_known_defects,"Mon, 4 Feb 2008 16:59:32 +0000","Thu, 17 Jul 2014 20:16:05 +0000","Thu, 17 Jul 2014 20:16:05 +0000",203483793,"I'm going to close this as won't fix. I don't think this is anything that we actually can fix here other than providing a complicated hostname mapping system for web interfaces. Part of the frustration I'm sure stems from a misunderstanding of what is actually happening: The slaves file has the public names listed. The slaves file is only used by the shell code to run ssh connections. It has absolutely zero impact on the core of Hadoop. Resolving a public name inside EC2 returns the private IP (which would reverse to the internal DNS name). Hadoop makes the perfectly valid assumption that the hostname the system tells us is a valid, network-connectable hostname. It is, from the inside of EC2. We have no way to know that you are attempting to connect from a completely different address that is being forwarded from some external entity. Proxying connections into a private network space is a perfectly valid solution.",0.04791666667,0.0175,negative
hadoop,7154,comment_13,"- I know this bug is pretty old, but do you mind doing me the favor of explaining this statement: Perhaps I am revealing too much of my naivety, but what issues the vmem size presents nor the reasons are necessarily obvious to me. The reason I ask is not directly related to this JIRA nor even Hadoop. I am just trying to learn more about the glibc change and its potential impacts. I've noticed high virtual memory size in another Java-based application (a Zabbix agent process if you care) and I'm struggling slightly to decide if I should worry about it. presents what appears to me to be a rational explanation as to why the virtual memory size shouldn't matter too much. I could push on Zabbix to implement a change to set MALLOC_ARENA_MAX and I feel relatively confident the change wouldn't hurt anything but I'm not sure it would actually help anything either. The Zabbix agent appears to be performing fine and the only reason I noticed the high vmem size was because someone pointed me to this JIRA and I did an audit looking for processes with virtual memory sizes that looked suspicious. I guess the biggest problem I have with the affect the glibc change has on reported vmem size is that it seems to make vmem size meaningless where previously you could get some idea about what a process was doing from its vmem size but your comment suggests maybe there are other things I should be concerned about as well. If you could share those with me I would greatly appreciate it and perhaps others will benefit as well. Thanks!",defect_debt,uncorrected_known_defects,"Fri, 25 Feb 2011 00:48:56 +0000","Tue, 21 Apr 2015 21:28:17 +0000","Tue, 8 Mar 2011 23:51:52 +0000",1033376,"tlipcon - I know this bug is pretty old, but do you mind doing me the favor of explaining this statement: We've observed a DN process using 14GB of vmem for only 300M of resident set. This causes all kinds of nasty issues for obvious reasons. Perhaps I am revealing too much of my naivety, but what issues the vmem size presents nor the reasons are necessarily obvious to me. The reason I ask is not directly related to this JIRA nor even Hadoop. I am just trying to learn more about the glibc change and its potential impacts. I've noticed high virtual memory size in another Java-based application (a Zabbix agent process if you care) and I'm struggling slightly to decide if I should worry about it. http://journal.siddhesh.in/posts/malloc-per-thread-arenas-in-glibc.html presents what appears to me to be a rational explanation as to why the virtual memory size shouldn't matter too much. I could push on Zabbix to implement a change to set MALLOC_ARENA_MAX and I feel relatively confident the change wouldn't hurt anything but I'm not sure it would actually help anything either. The Zabbix agent appears to be performing fine and the only reason I noticed the high vmem size was because someone pointed me to this JIRA and I did an audit looking for processes with virtual memory sizes that looked suspicious. I guess the biggest problem I have with the affect the glibc change has on reported vmem size is that it seems to make vmem size meaningless where previously you could get some idea about what a process was doing from its vmem size but your comment suggests maybe there are other things I should be concerned about as well. If you could share those with me I would greatly appreciate it and perhaps others will benefit as well. Thanks!",0.05465,-0.02494444444,neutral
hadoop,8741,comment_3,On version 2.x and above this is no longer an issue. The affected documentations are 1.0.4 and 1.2.1. These versions are EOL.,defect_debt,uncorrected_known_defects,"Tue, 28 Aug 2012 14:27:13 +0000","Wed, 24 May 2017 09:57:48 +0000","Wed, 24 May 2017 09:57:47 +0000",149455834,On version 2.x and above this is no longer an issue. The affected documentations are 1.0.4 and 1.2.1. These versions are EOL.,-0.05,-0.05,neutral
hadoop,10225,description,Right now Maven javadoc and sources artifacts do not accompany Hadoop releases within Maven central. This means that one needs to checkout source code to DEBUG aspects of the codebase... this is not user friendly. The build script(s) should be amended to accommodate publication of javadoc and sources artifacts alongside pom and jar artifacts. Some history on this conversation can be seen below,design_debt,non-optimal_design,"Sun, 12 Jan 2014 14:05:14 +0000","Thu, 10 Jan 2019 18:37:42 +0000","Thu, 10 Jan 2019 18:37:42 +0000",157609948,Right now Maven javadoc and sources artifacts do not accompany Hadoop releases within Maven central. This means that one needs to checkout source code to DEBUG aspects of the codebase... this is not user friendly. The build script(s) should be amended to accommodate publication of javadoc and sources artifacts alongside pom and jar artifacts. Some history on this conversation can be seen below http://s.apache.org/7qR,-0.0245,-0.0245,negative
hadoop,10291,comment_0,The root cause is that s property used by some test cases has a side effect. The tests set The broken test case assumed that it will be set when other tests are invoked. The fix is to explicitly set the property instead of depending on the execution order.,design_debt,non-optimal_design,"Mon, 27 Jan 2014 17:04:03 +0000","Thu, 12 May 2016 18:27:23 +0000","Wed, 29 Jan 2014 04:44:35 +0000",128432,The root cause is that s property used by some test cases has a side effect. The tests set (NetUtils.addStaticResolution) The broken test case assumed that it will be set when other tests are invoked. The fix is to explicitly set the property instead of depending on the execution order.,-0.325,-0.24375,negative
hadoop,10295,comment_1,"Funny, I have been preparing a patch for this very same issue for a week. Some comments regarding your patch: * instead of a new commandline option, it may be better to extend FileAttribute enum * and are probably HDFS specific (although being available in hadoop-common). I opened HADOOP-10297 for having * Instead of doing two instanceof check, it is possible to use the super class * is not equivalent of setting overwrite argument to true. From it is * Having a test to check if the option actually works would be a nice to have (according to me) Since I also have a patch, I'll attach it to this ticket to, and let have a hadoop maintainer help us sorting them out :)",design_debt,non-optimal_design,"Mon, 27 Jan 2014 22:43:52 +0000","Thu, 4 Sep 2014 01:16:46 +0000","Fri, 31 Jan 2014 00:01:18 +0000",263846,"Funny, I have been preparing a patch for this very same issue for a week. Some comments regarding your patch: instead of a new commandline option, it may be better to extend FileAttribute enum MD5MD5CRC32GzipFileChecksum and MD5MD5CRC32CastagnoliFileChecksum are probably HDFS specific (although being available in hadoop-common). I opened HADOOP-10297 for having FileChecksum.getChecksumOpt() Instead of doing two instanceof check, it is possible to use the super class MD5MD5CRC32FileChecksum EnumSet.of(CreateFlag.OVERWRITE) is not equivalent of setting overwrite argument to true. From DistributedFileSystem, it is EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE) Having a test to check if the option actually works would be a nice to have (according to me) Since I also have a patch, I'll attach it to this ticket to, and let have a hadoop maintainer help us sorting them out",0.419875,0.170025,neutral
hadoop,10295,comment_8,"Thanks for working on this, Jing. One thing to note is that the block size needs to be identical in addition to the checksum parameters in order for the checksums to match. So it might make more sense to introduce an option to preserve the two together.",design_debt,non-optimal_design,"Mon, 27 Jan 2014 22:43:52 +0000","Thu, 4 Sep 2014 01:16:46 +0000","Fri, 31 Jan 2014 00:01:18 +0000",263846,"Thanks for working on this, Jing. One thing to note is that the block size needs to be identical in addition to the checksum parameters in order for the checksums to match. So it might make more sense to introduce an option to preserve the two together.",0.2166666667,0.2166666667,positive
hadoop,1034,description,"Only IOException is catched and logged (in warn). Every Throwable should be logged in error. Eg: a RuntimeException occurs in the writeBlock() method. The exception will not be logged, but simply ignored. The socket is closed silently and nothing and an non understandable exception will be thrown in the DFSClient sending the block....",design_debt,non-optimal_design,"Fri, 23 Feb 2007 13:45:50 +0000","Wed, 8 Jul 2009 16:42:18 +0000","Fri, 23 Feb 2007 20:27:58 +0000",24128,"Only IOException is catched and logged (in warn). Every Throwable should be logged in error. Eg: a RuntimeException occurs in the writeBlock() method. The exception will not be logged, but simply ignored. The socket is closed silently and nothing and an non understandable exception will be thrown in the DFSClient sending the block....",-0.28,-0.28,neutral
hadoop,10501,comment_2,"It should be okay as long as it is called after Server#start(). Adding {{synchronized}} won't do much good, but won't hurt either.",design_debt,non-optimal_design,"Mon, 14 Apr 2014 23:36:34 +0000","Sat, 7 Mar 2015 23:17:57 +0000","Sat, 7 Mar 2015 23:17:57 +0000",28251683,"It should be okay as long as it is called after Server#start(). Adding synchronized won't do much good, but won't hurt either.",0.6915,0.6915,neutral
hadoop,10526,summary,Chance for Stream leakage in CompressorStream,design_debt,non-optimal_design,"Mon, 5 Dec 2011 15:12:56 +0000","Wed, 3 Sep 2014 20:36:28 +0000","Mon, 21 Apr 2014 19:22:44 +0000",75010188,Chance for Stream leakage in CompressorStream,0.4,0.4,neutral
hadoop,10681,comment_3,"The patch removes unsafe synchronized blocks from the individual methods in Snappy and Zlib codecs. This synchronization is slow and when used in the most common pattern for CompressionCodec is still thread-unsafe for sharing streams with loops running across multiple threads and would ideally require explicit code { while { 0, buffer.length); } } to get correct stateful behaviour. As code exists today it is not thread-safe and does slow lock-prefix x86_64 instructions. The JNI library below in SnappyCodec.c actually does its own locking mutexes for the actual critical sections within.",design_debt,non-optimal_design,"Wed, 11 Jun 2014 17:41:42 +0000","Thu, 8 Sep 2016 06:49:18 +0000","Sun, 5 Oct 2014 14:49:19 +0000",10012057,"The patch removes unsafe synchronized blocks from the individual methods in Snappy and Zlib codecs. This synchronization is slow and when used in the most common pattern for CompressionCodec is still thread-unsafe for sharing streams with loops running across multiple threads and would ideally require explicit code synchronized(compressor) { while (!compressor.finished()) { compressor.compress(buffer, 0, buffer.length); } } to get correct stateful behaviour. As code exists today it is not thread-safe and does slow lock-prefix x86_64 instructions. The JNI library below in SnappyCodec.c actually does its own locking mutexes for the actual critical sections within.",0.04408333333,0.02938888889,negative
hadoop,10979,description,It would make adding common options to hadoop_usage output easier if some entries were auto-populated. This is similar to what happens in FsShell and other parts of the Java code.,design_debt,non-optimal_design,"Tue, 19 Aug 2014 13:39:50 +0000","Thu, 12 May 2016 18:23:14 +0000","Thu, 16 Jul 2015 23:58:54 +0000",28635544,It would make adding common options to hadoop_usage output easier if some entries were auto-populated. This is similar to what happens in FsShell and other parts of the Java code.,0,0,neutral
hadoop,1130,comment_3,"Jira keeps eating my comments on this issue. *sigh* Actually, I think you have this backwards. Since Hadoop may be used as a third party library, it must use finalizers and shutdown hooks to clean up properly. The problems with have an explicit close method that the user must call are: 1. Forcing users to remember to cleanup your library is error-prone. 2. Just because library Foo is done does not mean library Bar is done and Bar will lose when Foo calls close. That said, I think that with some relatively minor changes, we can get the effect that you want (minimal garbage after the usage of the FileSystem is done): 1. Fix HADOOP-1160 so that the FileSystem.closeAll actually closes the client. 2. Have FileSystem.closeAll clear the CACHE map via reset. 3. Have the DFSClient.close method remove the client from the ClientFinalizer. 4. Have the constructor add ""this"" to the ClientFinalizer. So after the closeAll call, you'll still have the DFSClient class loaded, but not much else. In particular, you'll have no instances of DFSClient left.",design_debt,non-optimal_design,"Mon, 19 Mar 2007 13:57:16 +0000","Wed, 8 Jul 2009 16:42:21 +0000","Tue, 13 Nov 2007 08:17:42 +0000",20629226,"Jira keeps eating my comments on this issue. sigh > As hadoop may be used as a third party library, it shouldn't install shutdown hook by itself. Actually, I think you have this backwards. Since Hadoop may be used as a third party library, it must use finalizers and shutdown hooks to clean up properly. The problems with have an explicit close method that the user must call are: 1. Forcing users to remember to cleanup your library is error-prone. 2. Just because library Foo is done does not mean library Bar is done and Bar will lose when Foo calls close. That said, I think that with some relatively minor changes, we can get the effect that you want (minimal garbage after the usage of the FileSystem is done): 1. Fix HADOOP-1160 so that the FileSystem.closeAll actually closes the client. 2. Have FileSystem.closeAll clear the CACHE map via reset. 3. Have the DFSClient.close method remove the client from the ClientFinalizer. 4. Have the constructor add ""this"" to the ClientFinalizer. So after the closeAll call, you'll still have the DFSClient class loaded, but not much else. In particular, you'll have no instances of DFSClient left.",0.00655,0.006238095238,negative
hadoop,11523,comment_3,"Hi Duo. Thank you for the patch. The current patch would acquire the lease for all {{rename}} operations, covering both block blobs and page blobs. During initial development of atomic rename, we were careful to limit the scope to only page blobs (typically used by HBase logs). Existing applications using block blobs might not be expecting the leases, so I think we need to make sure the lease acquisition only happens after checking In the error handling, if an exception is thrown from the try block, and then another exception is also thrown from freeing the lease, then this would drop the original exception and throw the exception from freeing the lease. I suspect in general the main exception from the try block is going to be more interesting for root cause analysis, so I recommend throwing that one and just logging the one from freeing the lease, like so: In addition to the unit tests, I ran the test suite against a live Azure storage account and confirmed that everything passed. The release audit warning from Jenkins is unrelated to this patch.",design_debt,non-optimal_design,"Thu, 29 Jan 2015 18:57:37 +0000","Thu, 22 Oct 2015 00:48:25 +0000","Fri, 30 Jan 2015 01:08:39 +0000",22262,"Hi Duo. Thank you for the patch. The current patch would acquire the lease for all rename operations, covering both block blobs and page blobs. During initial development of atomic rename, we were careful to limit the scope to only page blobs (typically used by HBase logs). Existing applications using block blobs might not be expecting the leases, so I think we need to make sure the lease acquisition only happens after checking AzureNativeFileSystemStore#isAtomicRenameKey. In the error handling, if an exception is thrown from the try block, and then another exception is also thrown from freeing the lease, then this would drop the original exception and throw the exception from freeing the lease. I suspect in general the main exception from the try block is going to be more interesting for root cause analysis, so I recommend throwing that one and just logging the one from freeing the lease, like so: In addition to the unit tests, I ran the test suite against a live Azure storage account and confirmed that everything passed. The release audit warning from Jenkins is unrelated to this patch.",-0.01,-0.03583333333,neutral
hadoop,11523,comment_9,"Thanks, . I have just 2 more minor nitpicks. I suggest changing this: to this: This way, we'll log the full stack trace on lease free errors and have more information for troubleshooting. This is just a minor style point, but for the following, the keyword should be inline with the closing brace. Instead of this: We do this: Instead of this: We do this:",design_debt,non-optimal_design,"Thu, 29 Jan 2015 18:57:37 +0000","Thu, 22 Oct 2015 00:48:25 +0000","Fri, 30 Jan 2015 01:08:39 +0000",22262,"Thanks, onpduo. I have just 2 more minor nitpicks. I suggest changing this: to this: This way, we'll log the full stack trace on lease free errors and have more information for troubleshooting. This is just a minor style point, but for the following, the keyword should be inline with the closing brace. Instead of this: We do this: Instead of this: We do this:",0.06,0.06,neutral
hadoop,11523,description,"In current WASB (Windows Azure Storage - Blob) implementation, when rename operation succeeds, WASB will update the parent folder's ""last modified time"" property. By default we do not acquire lease on this folder when updating its property and simply pass ""null"" to it. In HBase scenario, when doing distributed log splitting, there might be a case that multiple processes from different region servers will access the same folder, and randomly we will see this exception in regionserver's log, which makes log splitting fail. So we should acquire the lease when updating the folder property rather than pass ""null"" to it.",design_debt,non-optimal_design,"Thu, 29 Jan 2015 18:57:37 +0000","Thu, 22 Oct 2015 00:48:25 +0000","Fri, 30 Jan 2015 01:08:39 +0000",22262,"In current WASB (Windows Azure Storage - Blob) implementation, when rename operation succeeds, WASB will update the parent folder's ""last modified time"" property. By default we do not acquire lease on this folder when updating its property and simply pass ""null"" to it. In HBase scenario, when doing distributed log splitting, there might be a case that multiple processes from different region servers will access the same folder, and randomly we will see this exception in regionserver's log, which makes log splitting fail. So we should acquire the lease when updating the folder property rather than pass ""null"" to it.",-0.0875,-0.0875,neutral
hadoop,11862,comment_0,", technically KMS is a proxy for an actual key provider, that, in addition to the keys, generates EDEKs for keys and decrypts them to corresponding DEKs. It also caches the keys in memory. It delegates the Key operations to a backing keyprovider specified by the specified in the *kms-site.xml* conf file. The only concrete implementation (shipped with hadoop) of a KeyProvider is currently the Consider the following deployment scenario : # *KMS1* configured with as ""jcek://file@..."". and thus will delegate to a # *KMS2* configured with as ""kms://http@KMS1.."" and thus will delegate Key operations to KMS1 but will provide generate/decrypt operations. It also caches the Keys for faster access # *KMS3* ALSO configured with as ""kms://http@KMS1.."" and thus will, like KMS2 delegate Key operations to KMS1 but will provide generate/decrypt operations. Now if we set the to the special loadbalancing url : then all requests will be loadbalanced across KMS2 and KMS3 and keys will be shared.",design_debt,non-optimal_design,"Wed, 22 Apr 2015 02:34:43 +0000","Fri, 21 May 2021 09:03:54 +0000","Tue, 28 Jun 2016 22:47:29 +0000",37483966,"dengxiumao, technically KMS is a proxy for an actual key provider, that, in addition to creating/storing/deleting the keys, generates EDEKs for keys and decrypts them to corresponding DEKs. It also caches the keys in memory. It delegates the Key store/retrieve/delete operations to a backing keyprovider specified by the hadoop.kms.key.provider.uri specified in the kms-site.xml conf file. The only concrete implementation (shipped with hadoop) of a KeyProvider is currently the JavaKeyStoreProvider. Consider the following deployment scenario : KMS1 configured with hadoop.kms.key.provider.uri as ""jcek://file@..."". and thus will delegate to a JavaKeyStoreProvider KMS2 configured with hadoop.kms.key.provider.uri as ""kms://http@KMS1.."" and thus will delegate Key create/store/rollover/delete operations to KMS1 but will provide generate/decrypt operations. It also caches the Keys for faster access KMS3 ALSO configured with hadoop.kms.key.provider.uri as ""kms://http@KMS1.."" and thus will, like KMS2 delegate Key create/store/rollover/delete operations to KMS1 but will provide generate/decrypt operations. Now if we set the hadoop.security.key.provider.path to the special loadbalancing url : ""kms://http@KM1_HOST;KMS2_HOST:16000/kms"", then all requests will be loadbalanced across KMS2 and KMS3 and keys will be shared.",0.10625,0.1051724138,neutral
hadoop,11862,comment_1,"hi , thank you for your reply. yes, the scenario as you said, actually will be loadbalanced and will shared accross KMS instances. But, it's not High Available(HA), there are 2 senarios: 1. if the KMS1 goes down, KMS2 and KMS3 will not available. 2. if the kms.keystore file was delete, the files encrypted by the keys in kms.keystore won't be read. So, I think if keys have several replicas, like HDFS replicas mechanism, it will be really HA. ps. maybe I should modify the title more clearly.",design_debt,non-optimal_design,"Wed, 22 Apr 2015 02:34:43 +0000","Fri, 21 May 2021 09:03:54 +0000","Tue, 28 Jun 2016 22:47:29 +0000",37483966,"hi asuresh, thank you for your reply. yes, the scenario as you said, actually will be loadbalanced and will shared accross KMS instances. But, it's not High Available(HA), there are 2 senarios: 1. if the KMS1 goes down, KMS2 and KMS3 will not available. 2. if the kms.keystore file was delete, the files encrypted by the keys in kms.keystore won't be read. So, I think if keys have several replicas, like HDFS replicas mechanism, it will be really HA. ps. maybe I should modify the title more clearly.",-0.0092,-0.0092,neutral
hadoop,11862,comment_2,", Hmmm.. it is not HA for some operations (create / rollover / delete) most other operations including get / encrypt / decrypt, they should be (since the keys are actually cached by KMS.. and EDEKs generated by 1 KMS can be decrypted by the other.. if the EZ key version is in cache). But yes, i agree, there is no replica stored anywhere.. so for catastrophic failures where the backing KMS does not come up, you lose data. I would expect an enterprise deployment to use a more robust production quality key store for which you can easily write a KeyProvider and use it as a backing key store. But yes, we dont ship one with hadoop.",design_debt,non-optimal_design,"Wed, 22 Apr 2015 02:34:43 +0000","Fri, 21 May 2021 09:03:54 +0000","Tue, 28 Jun 2016 22:47:29 +0000",37483966,"dengxiumao, But, it's not High Available(HA), there are 2 senarios: Hmmm.. it is not HA for some operations (create / rollover / delete) most other operations including get / encrypt / decrypt, they should be (since the keys are actually cached by KMS.. and EDEKs generated by 1 KMS can be decrypted by the other.. if the EZ key version is in cache). But yes, i agree, there is no replica stored anywhere.. so for catastrophic failures where the backing KMS does not come up, you lose data. I would expect an enterprise deployment to use a more robust production quality key store for which you can easily write a KeyProvider and use it as a backing key store. But yes, we dont ship one with hadoop.",0.1725,0.135,neutral
hadoop,12371,comment_2,"thanks.. however .. what if I simply want to wipe out trash ? On my testing cluster, with rarely small storage, I am running jobs generating some data and deleting them over and over again, the trash easily blows up the storage and my HDFS becomes unavailable. Does it make sense to provide an force option to clean up trash ? E.g hadoop fs -expunge -f",design_debt,non-optimal_design,"Tue, 1 Sep 2015 08:39:01 +0000","Wed, 9 Sep 2015 02:24:18 +0000","Wed, 9 Sep 2015 02:24:18 +0000",668717,"xyao thanks.. however .. what if I simply want to wipe out trash ? On my testing cluster, with rarely small storage, I am running jobs generating some data and deleting them over and over again, the trash easily blows up the storage and my HDFS becomes unavailable. Does it make sense to provide an force option to clean up trash ? E.g hadoop fs -expunge -f",0.05,0.05,neutral
hadoop,12484,comment_1,"Hello . If {{acquireLease}} succeeds, but then the {{delete}} fails, then this will leak the lease. Can you please use a {{finally}} block to guarantee that the lease gets released in all code paths? You can look at other points in the class that acquire and free a lease for an existing example. Could you please review the checkstyle and whitespace warnings from the pre-commit run and fix them? If it's not feasible to write a unit test to simulate the race condition, then can you please describe any manual testing that you've done to verify the change? Thank you!",design_debt,non-optimal_design,"Thu, 15 Oct 2015 22:32:28 +0000","Tue, 30 Aug 2016 01:22:18 +0000","Thu, 22 Oct 2015 21:32:55 +0000",601227,"Hello gouravk. If acquireLease succeeds, but then the delete fails, then this will leak the lease. Can you please use a finally block to guarantee that the lease gets released in all code paths? You can look at other points in the class that acquire and free a lease for an existing example. Could you please review the checkstyle and whitespace warnings from the pre-commit run and fix them? If it's not feasible to write a unit test to simulate the race condition, then can you please describe any manual testing that you've done to verify the change? Thank you!",0.03630952381,0.03630952381,neutral
hadoop,12496,description,"hadoop-aws jar still depends on the very old 1.7.4 version of aws-java-sdk. In newer versions of SDK, there is incompatible API changes that leads to the following error when trying to use the S3A class and newer versions of sdk presents. This is because S3A is calling the method with ""int"" as the parameter type while the new SDK is expecting ""long"". This makes it impossible to use kinesis + s3a in the same process. It would be very helpful to upgrade hadoop-awas's aws-sdk version. $iwC$$iwC.<init at $iwC.<init at <init at .<init at .<clinit at .<init at .<clinit at $print(<console at Method)",design_debt,non-optimal_design,"Wed, 21 Oct 2015 01:48:25 +0000","Tue, 22 Dec 2015 21:12:16 +0000","Wed, 21 Oct 2015 10:11:49 +0000",30204,"hadoop-aws jar still depends on the very old 1.7.4 version of aws-java-sdk. In newer versions of SDK, there is incompatible API changes that leads to the following error when trying to use the S3A class and newer versions of sdk presents. This is because S3A is calling the method with ""int"" as the parameter type while the new SDK is expecting ""long"". This makes it impossible to use kinesis + s3a in the same process. It would be very helpful to upgrade hadoop-awas's aws-sdk version. java.lang.NoSuchMethodError: com.amazonaws.services.s3.transfer.TransferManagerConfiguration.setMultipartUploadThreshold(I)V at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:285) at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2653) at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92) at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687) at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371) at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295) at org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:130) at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:114) at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:104) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:29) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:34) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:36) at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:38) at $iwC$$iwC$$iwC$$iwC.<init>(<console>:40) at $iwC$$iwC$$iwC.<init>(<console>:42) at $iwC$$iwC.<init>(<console>:44) at $iwC.<init>(<console>:46) at <init>(<console>:48) at .<init>(<console>:52) at .<clinit>(<console>) at .<init>(<console>:7) at .<clinit>(<console>) at $print(<console>) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065) at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340) at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840) at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871) at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819) at org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:655) at org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:620) at org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:613) at org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57) at org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93) at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:276) at org.apache.zeppelin.scheduler.Job.run(Job.java:170) at org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:118) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)",0.06508333333,0.03305116279,neutral
hadoop,12505,comment_3,"yeah, it'll break the universe in really awful ways if we accept groups with spaces. Think about folks parsing hadoop fs -ls for example. or the REST interfaces. or ACLs. or ... ugh!",design_debt,non-optimal_design,"Fri, 23 Oct 2015 16:08:23 +0000","Mon, 28 Mar 2016 16:48:44 +0000","Mon, 28 Mar 2016 16:16:19 +0000",13565276,"yeah, it'll break the universe in really awful ways if we accept groups with spaces. Think about folks parsing hadoop fs -ls for example. or the REST interfaces. or ACLs. or ... ugh!",-0.09333333333,-0.09333333333,negative
hadoop,1251,description,I need a way to get the InputSplit from a Mapper. This is effectively a stop gap until we get context objects and can fix this much better. *smile* I introduce a static method in the TaskTracker named getInputSplit() that returns the InputSplit.,design_debt,non-optimal_design,"Thu, 12 Apr 2007 05:54:52 +0000","Thu, 2 May 2013 02:29:04 +0000","Mon, 16 Apr 2007 22:57:10 +0000",406938,I need a way to get the InputSplit from a Mapper. This is effectively a stop gap until we get context objects and can fix this much better. smile I introduce a static method in the TaskTracker named getInputSplit() that returns the InputSplit.,-0.001388888889,-0.001388888889,negative
hadoop,12520,description,"The hadoop-azure tests support execution against the live Azure Storage service if the developer specifies the key to an Azure Storage account. The configuration works by overwriting the file. This can be an error-prone process. The azure-test.xml file is checked into revision control to show an example. There is a risk that the tester could overwrite azure-test.xml containing the keys and then accidentally commit the keys to revision control. This would leak the keys to the world for potential use by an attacker. This issue proposes to use XInclude to isolate the keys into a separate file, ignored by git, which will never be committed to revision control. This is very similar to the setup already used by hadoop-aws for integration testing.",design_debt,non-optimal_design,"Tue, 27 Oct 2015 21:59:08 +0000","Tue, 3 Jan 2017 11:15:57 +0000","Wed, 28 Oct 2015 05:52:58 +0000",28430,"The hadoop-azure tests support execution against the live Azure Storage service if the developer specifies the key to an Azure Storage account. The configuration works by overwriting the src/test/resources/azure-test.xml file. This can be an error-prone process. The azure-test.xml file is checked into revision control to show an example. There is a risk that the tester could overwrite azure-test.xml containing the keys and then accidentally commit the keys to revision control. This would leak the keys to the world for potential use by an attacker. This issue proposes to use XInclude to isolate the keys into a separate file, ignored by git, which will never be committed to revision control. This is very similar to the setup already used by hadoop-aws for integration testing.",0.048,0.03909090909,neutral
hadoop,12806,description,"Trying to use the hadoop fs s3a library in AWS lambda with temporary credentials but it's not possible because of the way the is defined under Specifically the following code is used to initialise the credentials chain The above works fine when the EC2 metadata endpoint is available (i.e. running on an EC2 instance) however it doesn't work properly when the environment variables are used to define credentials as it happens in AWS Lambda. Amazon suggests to use the in AWS Lambda. To summarise and suggest an alternative I think that the could be used instead of the and that would cover the following cases: {panel} * Environment Variables - AWS_ACCESS_KEY_ID and (RECOMMENDED since they are recognized by all the AWS SDKs and CLI except for .NET), or AWS_ACCESS_KEY and AWS_SECRET_KEY (only recognized by Java SDK) * Java System Properties - aws.accessKeyId and aws.secretKey * Credential profiles file at the default location shared by all AWS SDKs and the AWS CLI * Instance profile credentials delivered through the Amazon EC2 metadata service {panel} If you think that the above change would be useful I could investigate more about what the required changes would be and submit a patch.",design_debt,non-optimal_design,"Mon, 15 Feb 2016 14:26:49 +0000","Mon, 19 Feb 2018 17:16:24 +0000","Mon, 19 Feb 2018 17:16:24 +0000",63514175,Trying to use the hadoop fs s3a library in AWS lambda with temporary credentials but it's not possible because of the way the AWSCredentialsProviderChain is defined under https://github.com/apache/hadoop/blob/29ae25801380b94442253c4202dee782dc4713f5/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java Specifically the following code is used to initialise the credentials chain The above works fine when the EC2 metadata endpoint is available (i.e. running on an EC2 instance) however it doesn't work properly when the environment variables are used to define credentials as it happens in AWS Lambda. Amazon suggests to use the EnvironmentVariableCredentialsProvider in AWS Lambda. To summarise and suggest an alternative I think that the DefaultAWSCredentialsProviderChain could be used instead of the InstanceProfileCredentialsProvider and that would cover the following cases: If you think that the above change would be useful I could investigate more about what the required changes would be and submit a patch.,0.1944444444,0.2166666667,neutral
hadoop,12811,description,The HBase's HMaster port number conflicts with Hadoop kms port number. Both uses 16000. There might be use cases user need kms and HBase present on the same cluster. The HBase is able to encrypt its HFiles but user might need KMS to encrypt other HDFS directories. Users would have to manually override the default port of either application on their cluster. It would be nice to have different default ports so kms and HBase could naturally coexist.,design_debt,non-optimal_design,"Wed, 17 Feb 2016 00:12:23 +0000","Thu, 12 May 2016 18:22:07 +0000","Thu, 14 Apr 2016 18:36:47 +0000",4991064,The HBase's HMaster port number conflicts with Hadoop kms port number. Both uses 16000. There might be use cases user need kms and HBase present on the same cluster. The HBase is able to encrypt its HFiles but user might need KMS to encrypt other HDFS directories. Users would have to manually override the default port of either application on their cluster. It would be nice to have different default ports so kms and HBase could naturally coexist.,-0.00825,-0.00825,neutral
hadoop,12829,comment_11,"Hello guys, I hit the same thread leak issue in FLINK-15239. In our use case, a Flink cluster is started w/o Hadoop dependencies, and accepts jobs submitted from various clients. If a job needs to access Hadoop, client will include Hadoop jars as its dependencies. On Flink side, we create a class loader to load all the dependencies of each job. As a result, we start a thread each time we load the Hadoop classes. While the change here made the thread respond to interrupt exception, it seems there's no one to interrupt the thread. So I guess the thread only dies when the JVM exits. If that's the case, is it possible to provide a way to explicitly interrupt the thread?",design_debt,non-optimal_design,"Sat, 20 Feb 2016 01:19:42 +0000","Mon, 16 Dec 2019 10:04:45 +0000","Tue, 23 Feb 2016 19:33:42 +0000",324840,"Hello guys, I hit the same thread leak issue in FLINK-15239. In our use case, a Flink cluster is started w/o Hadoop dependencies, and accepts jobs submitted from various clients. If a job needs to access Hadoop, client will include Hadoop jars as its dependencies. On Flink side, we create a class loader to load all the dependencies of each job. As a result, we start a StatisticsDataReferenceCleaner thread each time we load the Hadoop classes. While the change here made the thread respond to interrupt exception, it seems there's no one to interrupt the thread. So I guess the thread only dies when the JVM exits. If that's the case, is it possible to provide a way to explicitly interrupt the thread?",-0.05,-0.05,neutral
hadoop,12946,description,"Ensure at most one JVMPauseMonitor thread is running per JVM when there are multiple JVMPauseMonitor instances, e.g., in mini clusters. This will prevent redundant GC pause log messages while still maintaining one monitor thread running. This is a different way to fix HADOOP-12855.",design_debt,non-optimal_design,"Sun, 20 Mar 2016 18:14:55 +0000","Tue, 10 Jan 2017 01:41:24 +0000","Tue, 10 Jan 2017 01:41:24 +0000",25514789,"Ensure at most one JVMPauseMonitor thread is running per JVM when there are multiple JVMPauseMonitor instances, e.g., in mini clusters. This will prevent redundant GC pause log messages while still maintaining one monitor thread running. This is a different way to fix HADOOP-12855.",0.08016666667,0.08016666667,neutral
hadoop,13365,comment_0,"I think the plan of attach should be: 1. Let users keep \_OPTS in hadoop-env.sh, etc. 2. Detect if it's not already an array by checking \_OPTS[1]. 3. If not, convert to array using eval (bash 3.x compat) This way, if users want to use \_OPTS in array format (which, for long lists of options, is significantly easier to work with), they can.",design_debt,non-optimal_design,"Mon, 11 Jul 2016 15:06:56 +0000","Sat, 1 Sep 2018 20:06:01 +0000","Sat, 1 Sep 2018 20:06:01 +0000",67582745,"I think the plan of attach should be: 1. Let users keep _OPTS in hadoop-env.sh, etc. 2. Detect if it's not already an array by checking _OPTS[1]. 3. If not, convert to array using eval (bash 3.x compat) This way, if users want to use _OPTS in array format (which, for long lists of options, is significantly easier to work with), they can.",0.05714285714,0.05714285714,neutral
hadoop,13365,description,"While we are mucking with all of the _OPTS variables, this is a good time to convert them to arrays so that filesystems with spaces in them can be used.",design_debt,non-optimal_design,"Mon, 11 Jul 2016 15:06:56 +0000","Sat, 1 Sep 2018 20:06:01 +0000","Sat, 1 Sep 2018 20:06:01 +0000",67582745,"While we are mucking with all of the _OPTS variables, this is a good time to convert them to arrays so that filesystems with spaces in them can be used.",0.625,0.625,neutral
hadoop,13386,comment_10,"Real issue is that it forces people downstream to recompile their code too, if they have this version of avro on their CP. If they exclude it, then you can't use the compiled classes in Hadoop -but there aren't that many. Maybe: tag as incompatible and explain what to do: recompile or exclude",design_debt,non-optimal_design,"Mon, 18 Jul 2016 23:23:06 +0000","Sat, 27 Jan 2024 06:37:07 +0000","Sat, 26 Mar 2022 11:32:04 +0000",179410138,"Real issue is that it forces people downstream to recompile their code too, if they have this version of avro on their CP. If they exclude it, then you can't use the compiled classes in Hadoop -but there aren't that many. Maybe: tag as incompatible and explain what to do: recompile or exclude",-0.1333333333,-0.1333333333,negative
hadoop,13768,comment_6,"assuming that the file limit is always1000, why not just list the path in 1000 blocks and issue delete requests in that size. There are ultimate limits to the size of responses in path listings (max size of an HTTP request), and inevitably heap problems well before then.",design_debt,non-optimal_design,"Fri, 28 Oct 2016 08:27:17 +0000","Fri, 24 Nov 2017 08:43:36 +0000","Fri, 10 Feb 2017 06:56:06 +0000",9066529,"assuming that the file limit is always1000, why not just list the path in 1000 blocks and issue delete requests in that size. There are ultimate limits to the size of responses in path listings (max size of an HTTP request), and inevitably heap problems well before then.",-0.2948333333,-0.2948333333,neutral
hadoop,13770,comment_2,"Rev02: the original design is not desirable, as it swallowed a potential interrupt, causing to fail. Unfortunately, Java does not allow this static method to throw exception. The rev02 removed the static member variable, so that the method can throw the interrupt exception.",design_debt,non-optimal_design,"Wed, 16 Dec 2015 22:28:18 +0000","Wed, 2 Oct 2019 17:14:23 +0000","Fri, 28 Oct 2016 15:11:22 +0000",27362584,"Rev02: the original design is not desirable, as it swallowed a potential interrupt, causing TestRPCWaitForProxy.testInterruptedWaitForProxy to fail. Unfortunately, Java does not allow this static method to throw exception. The rev02 removed the static member variable, so that the method can throw the interrupt exception.",-0.4069444444,-0.425,negative
hadoop,13770,comment_6,"+1, kicked Jenkins again to get a fresh run. I also marked this as a Blocker for 2.8. We will not be allowed to remove the public member of a Public class once the public member ships in a release or we risk breaking backwards compatibility. Fortunately this public member hasn't been released yet, so we have a chance to fix it cleanly.",design_debt,non-optimal_design,"Wed, 16 Dec 2015 22:28:18 +0000","Wed, 2 Oct 2019 17:14:23 +0000","Fri, 28 Oct 2016 15:11:22 +0000",27362584,"+1, kicked Jenkins again to get a fresh run. I also marked this as a Blocker for 2.8. We will not be allowed to remove the public member of a Public class once the public member ships in a release or we risk breaking backwards compatibility. Fortunately this public member hasn't been released yet, so we have a chance to fix it cleanly.",0.309375,0.309375,neutral
hadoop,13770,description,"creates a bash shell command to verify if the system supports bash. However, its error message is misleading, and the logic should be updated. If the shell command throws an IOException, it does not imply the bash did not run successfully. If the shell command process was interrupted, its internal logic throws an which is a subclass of IOException. An example of it appeared in a recent jenkins job The test logic in starts a thread, wait it for 1 second, and interrupt the thread, expecting the thread to terminate. However, the method swallowed the interrupt, and therefore failed. The original design is not desirable, as it swallowed a potential interrupt, causing to fail. Unfortunately, Java does not allow this static method to throw exception. We should removed the static member variable, so that the method can throw the interrupt exception. The node manager should call the static method, instead of using the static member variable. This fix has an associated benefit: the tests could run faster, because it will no longer need to spawn a bash process when it uses a Shell static method variable (which happens quite often for checking what operating system Hadoop is running on)",design_debt,non-optimal_design,"Wed, 16 Dec 2015 22:28:18 +0000","Wed, 2 Oct 2019 17:14:23 +0000","Fri, 28 Oct 2016 15:11:22 +0000",27362584,"Shell.checkIsBashSupported() creates a bash shell command to verify if the system supports bash. However, its error message is misleading, and the logic should be updated. If the shell command throws an IOException, it does not imply the bash did not run successfully. If the shell command process was interrupted, its internal logic throws an InterruptedIOException, which is a subclass of IOException. An example of it appeared in a recent jenkins job https://builds.apache.org/job/PreCommit-HADOOP-Build/8257/testReport/org.apache.hadoop.ipc/TestRPCWaitForProxy/testInterruptedWaitForProxy/ The test logic in TestRPCWaitForProxy.testInterruptedWaitForProxy starts a thread, wait it for 1 second, and interrupt the thread, expecting the thread to terminate. However, the method Shell.checkIsBashSupported swallowed the interrupt, and therefore failed. The original design is not desirable, as it swallowed a potential interrupt, causing TestRPCWaitForProxy.testInterruptedWaitForProxy to fail. Unfortunately, Java does not allow this static method to throw exception. We should removed the static member variable, so that the method can throw the interrupt exception. The node manager should call the static method, instead of using the static member variable. This fix has an associated benefit: the tests could run faster, because it will no longer need to spawn a bash process when it uses a Shell static method variable (which happens quite often for checking what operating system Hadoop is running on)",-0.3013030303,-0.2529,negative
hadoop,13991,description,"NativeS3FileSystem does not support any retry management for failed uploading to S3. If due to socket timeout or any other network exception, file uploading to S3 bucket fails, then uploading fails and temporary file gets deleted. Connection reset Source) Source) This can be solved by using asynchronous retry management. We have made following modifications to NativeS3FileSystem to add the retry management, which is working fine in our product system, without any uploading failure:",design_debt,non-optimal_design,"Sat, 14 Jan 2017 18:41:34 +0000","Fri, 24 Feb 2017 14:07:36 +0000","Fri, 24 Feb 2017 14:07:36 +0000",3525962,"NativeS3FileSystem does not support any retry management for failed uploading to S3. If due to socket timeout or any other network exception, file uploading to S3 bucket fails, then uploading fails and temporary file gets deleted. java.net.SocketException: Connection reset at java.net.SocketInputStream.read(SocketInputStream.java:196) at java.net.SocketInputStream.read(SocketInputStream.java:122) at org.jets3t.service.S3Service.putObject(S3Service.java:2265) at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.storeFile(Jets3tNativeFileSystemStore.java:122) at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.fs.s3native.$Proxy8.storeFile(Unknown Source) at org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsOutputStream.close(NativeS3FileSystem.java:284) at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72) at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106) at org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.close(CBZip2OutputStream.java:737) at org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream.close(BZip2Codec.java:336) at org.apache.flume.sink.hdfs.HDFSCompressedDataStream.close(HDFSCompressedDataStream.java:155) at org.apache.flume.sink.hdfs.BucketWriter$3.call(BucketWriter.java:312) at org.apache.flume.sink.hdfs.BucketWriter$3.call(BucketWriter.java:308) at org.apache.flume.sink.hdfs.BucketWriter$9$1.run(BucketWriter.java:679) at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50) at org.apache.flume.sink.hdfs.BucketWriter$9.call(BucketWriter.java:676) This can be solved by using asynchronous retry management. We have made following modifications to NativeS3FileSystem to add the retry management, which is working fine in our product system, without any uploading failure:",0.06666666667,0.002279202279,negative
hadoop,1459,comment_5,"Note that this increases serialization cost for any DatanodeInfo tranfer, which is pretty much most RPC. This will needs a protocol version change since this won't work with prev clients/datanodes.",design_debt,non-optimal_design,"Mon, 4 Jun 2007 10:09:30 +0000","Wed, 8 Jul 2009 16:41:57 +0000","Mon, 18 Jun 2007 20:39:50 +0000",1247420,"Note that this increases serialization cost for any DatanodeInfo tranfer, which is pretty much most RPC. This will needs a protocol version change since this won't work with prev clients/datanodes.",-0.2,-0.2,negative
hadoop,1488,description,"During the last rework of the shuffle, a lot of the stages of the shuffle had auto-progress threads added, leading to system lockups when the shuffle stalls. We need to add Progressables to the sort and fetching interfaces so that if any task gets stuck, it will eventually be killed by the framework.",design_debt,non-optimal_design,"Wed, 13 Jun 2007 17:13:54 +0000","Wed, 8 Jul 2009 16:52:18 +0000","Thu, 28 Jun 2007 21:11:23 +0000",1310249,"During the last rework of the shuffle, a lot of the stages of the shuffle had auto-progress threads added, leading to system lockups when the shuffle stalls. We need to add Progressables to the sort and fetching interfaces so that if any task gets stuck, it will eventually be killed by the framework.",0.2,0.2,negative
hadoop,15577,description,Hello! distcp through 3.1.0 appears to copy files and then rename them into their final/destination filename. added support for more efficient S3 committers that do not use renames. Please update distcp to use these efficient committers and no renames. Thanks!,design_debt,non-optimal_design,"Mon, 2 Jul 2018 22:14:48 +0000","Tue, 3 Jul 2018 14:20:20 +0000","Tue, 3 Jul 2018 14:20:20 +0000",57932,Hello! distcp through 3.1.0 appears to copy files and then rename them into their final/destination filename. https://issues.apache.org/jira/browse/HADOOP-13786added support for more efficient S3 committers that do not use renames. Please update distcp to use these efficient committers and no renames. Thanks!,0.2,0.2,positive
hadoop,15645,comment_0,"Patch 001 * better setup of implementations, especially bucket config, where per-bucket settings are cleared * and both check the metastore type of the test FS, skip if not correct -and include details on store in the message. * testDiff test erases test FS in testDiff setup through rawFS and metastore FS, in case it gets contaminated This addresses the double setup of dynamodb being picked up on a local test run, the ddb metastore getting tainted with stuff which shouldn't be there, and the s3 bucket getting stuff into it which the getFileSystem() fs doesn't see/delete.",design_debt,non-optimal_design,"Wed, 1 Aug 2018 00:16:58 +0000","Mon, 13 Aug 2018 21:12:02 +0000","Mon, 13 Aug 2018 11:22:35 +0000",1076737,"Patch 001 better setup of AbstractS3GuardToolTestBase implementations, especially bucket config, where per-bucket settings are cleared ITestS3GuardToolDynamoDB and ITestS3GuardToolLocal both check the metastore type of the test FS, skip if not correct -and include details on store in the message. testDiff test erases test FS in testDiff setup through rawFS and metastore FS, in case it gets contaminated This addresses the double setup of dynamodb being picked up on a local test run, the ddb metastore getting tainted with stuff which shouldn't be there, and the s3 bucket getting stuff into it which the getFileSystem() fs doesn't see/delete.",-0.14375,-0.14375,neutral
hadoop,1586,description,"Currently, in the loop of MAX_RETRIES (set to three) attempts are made to report progress/ping or All attempt failures are counted as critical. Here I am proposing a variant - treat only ConnectException exceptions are critical and treat the others as non-critical. The other exception could be the in the case of the two RPCs. The reason why I am proposing this is that since HADOOP-1462 went in, I have been seeing quite a few unexpected 65 deaths, and with some logging it appears that they happen, most of the time, due to the in the progress RPC call (before HADOOP-1462, the return value of progress would not be checked). And when the hack described above was put in, things improved considerably. One argument that one might make against the above proposal is that the tasktracker could be faulty, when a task is not able to successfully invoke an RPC on it even though it is able to connect. If this is indeed the case, even in the current scheme of things, the only resort is to restart the tasktracker (either manually, or, the JobTracker asks it to reinitialize), and in both the cases, normal behavior of the protocol will ensure that the child task will die (since the reinited tasktracker is going to return false for the progress/ping calls).",design_debt,non-optimal_design,"Tue, 10 Jul 2007 11:55:01 +0000","Wed, 8 Jul 2009 16:52:18 +0000","Thu, 26 Jul 2007 17:49:18 +0000",1403657,"Currently, in the loop of Task.startCommunicationThread, MAX_RETRIES (set to three) attempts are made to report progress/ping (TaskUmbilicalProtocol.progress or TaskUmbilicalProtocol.ping). All attempt failures are counted as critical. Here I am proposing a variant - treat only ConnectException exceptions are critical and treat the others as non-critical. The other exception could be the SocketTimeoutException in the case of the two RPCs. The reason why I am proposing this is that since HADOOP-1462 went in, I have been seeing quite a few unexpected 65 deaths, and with some logging it appears that they happen, most of the time, due to the SocketTimeoutException in the progress RPC call (before HADOOP-1462, the return value of progress would not be checked). And when the hack described above was put in, things improved considerably. One argument that one might make against the above proposal is that the tasktracker could be faulty, when a task is not able to successfully invoke an RPC on it even though it is able to connect. If this is indeed the case, even in the current scheme of things, the only resort is to restart the tasktracker (either manually, or, the JobTracker asks it to reinitialize), and in both the cases, normal behavior of the protocol will ensure that the child task will die (since the reinited tasktracker is going to return false for the progress/ping calls).",0.03333333333,0.05757575758,neutral
hadoop,1586,summary,Progress reporting thread can afford to be slightly lenient towards exceptions other than ConnectException,design_debt,non-optimal_design,"Tue, 10 Jul 2007 11:55:01 +0000","Wed, 8 Jul 2009 16:52:18 +0000","Thu, 26 Jul 2007 17:49:18 +0000",1403657,Progress reporting thread can afford to be slightly lenient towards exceptions other than ConnectException,0.3,0.3,neutral
hadoop,16013,description,"sets up a {{Timer}} to schedule a decay of the weights it tracks: However this Timer is not set up as a daemon thread. I have seen this cause my JVM to refuse to exit when running, for example, with FairCallQueue enabled.",design_debt,non-optimal_design,"Tue, 18 Dec 2018 17:55:55 +0000","Tue, 5 Nov 2019 01:19:02 +0000","Fri, 11 Jan 2019 20:58:17 +0000",2084542,"DecayRpcScheduler sets up a Timer to schedule a decay of the weights it tracks: However this Timer is not set up as a daemon thread. I have seen this cause my JVM to refuse to exit when running, for example, NNThroughputBenchmark with FairCallQueue enabled.",-0.2,-0.2,negative
hadoop,16044,comment_3,"This is a silly question, but why is being retried? Because JVMs do have a history of caching negative DNS lookup results. Looking at the latest javadocs, is set to 10s, so it will eventually come up, but you do have to trust DNS to be updating its records Elsewhere i the code we're just treating this as unrecoverable. Which may not be correct action in a world of dynamicness...but if we do start changing this policy, we should think about doing it consistently everywhere (including HA failover events)",design_debt,non-optimal_design,"Fri, 11 Jan 2019 18:02:56 +0000","Mon, 14 Jan 2019 20:03:40 +0000","Mon, 14 Jan 2019 19:47:06 +0000",265450,"This is a silly question, but why is UnknownHostException being retried? Because JVMs do have a history of caching negative DNS lookup results. Looking at the latest javadocs, networkaddress.cache.negative.ttl is set to 10s, so it will eventually come up, but you do have to trust DNS to be updating its records Elsewhere i the code we're just treating this as unrecoverable. Which may not be correct action in a world of dynamicness...but if we do start changing this policy, we should think about doing it consistently everywhere (including HA failover events)",0.09,0.00625,negative
hadoop,16044,comment_4,"ABFS has been retrying on since it previewed because our understanding is that this exception is thrown for transient name resolution failures. Our retry policy last longer than the typical DNS TTL (or negative cache) of 5 minutes, so the driver could recover and enable a long running task to complete successfully. WASB also retries for these. I expect ADL retries too, although have not confirmed. Mostly we do this for status quo, I mean, it is less likely to cause a regression if we keep the current behavior. With that said, if you have evidence this is a bad design, we should change it. I see that we do the opposite for S3, but I don't know what led to that decision nor do I have a good sense for the behavior in the wild, so I don't know what's best. Certainly retrying is not going to increase the recovery time on the node in question.",design_debt,non-optimal_design,"Fri, 11 Jan 2019 18:02:56 +0000","Mon, 14 Jan 2019 20:03:40 +0000","Mon, 14 Jan 2019 19:47:06 +0000",265450,"ABFS has been retrying onUnknownHostException since it previewed because our understanding is that this exception is thrown for transient name resolution failures. Our retry policy last longer than the typical DNS TTL (or negative cache) of 5 minutes, so the driver could recover and enable a long running task to complete successfully.WASB also retries for these. I expect ADL retries too, although have not confirmed. Mostly we do this for status quo, I mean, it is less likely to cause a regression if we keep the current behavior. With that said, ifyou have evidence this is a bad design, we should change it. I see thatwe do the opposite for S3, but I don't know what led to that decision nor do I have a good sense for the behavior in the wild, so I don't know what's best. Certainly retrying is not going to increase the recovery time on the node in question.",-0.1557291667,-0.1557291667,neutral
hadoop,16044,comment_5,"bq With that said, if you have evidence this is a bad design, we should change it. no, I don't think it is a bad design. I'm curious. In a classic physical deployment, DNS failures are a bad sign. And, because the JVM cached -ve DNS results *forever* , spinning never fixed things. If the JVMs have stopped doing this, then in a dynamic world, this makes sense. I wondering something broader, which is: is the assumption that not worth retrying"" no longer valid? And if so, what to do about all those existing uses?",design_debt,non-optimal_design,"Fri, 11 Jan 2019 18:02:56 +0000","Mon, 14 Jan 2019 20:03:40 +0000","Mon, 14 Jan 2019 19:47:06 +0000",265450,"bq With that said, if you have evidence this is a bad design, we should change it. no, I don't think it is a bad design. I'm curious. In a classic physical deployment, DNS failures are a bad sign. And, because the JVM cached -ve DNS results forever , spinning never fixed things. If the JVMs have stopped doing this, then in a dynamic world, this makes sense. I wondering something broader, which is: is the assumption that ""UnknownHostException not worth retrying"" no longer valid? And if so, what to do about all those existing uses?",-0.1125,-0.1125,negative
hadoop,16207,comment_1,"could be more fundamental as in ""I'm not sure the committers are correctly telling S3Guard about parent directories"". After each PUT is manifest, we call finishedWrite() , but that seems to do more about purging spurious deleted files, rather than adding dir entries into S3Guard. Provided mkdirs() is called Proposed: build a list of all directories which need to exist as part of a job commit, and only create those entries The other strategy is for to mkdir on the parent. That's a bit more expensive though. Better to not worry about whether it exists and do all this stuff during job commit only",design_debt,non-optimal_design,"Tue, 26 Mar 2019 13:07:38 +0000","Fri, 4 Oct 2019 13:27:40 +0000","Fri, 4 Oct 2019 13:17:44 +0000",16589406,"could be more fundamental as in ""I'm not sure the committers are correctly telling S3Guard about parent directories"". After each PUT is manifest, we call finishedWrite() , but that seems to do more about purging spurious deleted files, rather than adding dir entries into S3Guard. Provided mkdirs() is called Proposed: build a list of all directories which need to exist as part of a job commit, and only create those entries The other strategy is for initiateMultipartUpload() to mkdir on the parent. That's a bit more expensive though. Better to not worry about whether it exists and do all this stuff during job commit only",-0.1525333333,-0.1525333333,neutral
hadoop,16265,comment_2,"Hey , this is a good catch. It looks like  and I missed this when doing HDFS-14346. Would it not be simpler to just do: It seems a little weird to me to convert a number to a string just so that we can re-parse it into a number.",design_debt,non-optimal_design,"Thu, 18 Apr 2019 01:57:25 +0000","Mon, 22 Apr 2019 15:32:30 +0000","Mon, 22 Apr 2019 15:22:36 +0000",393911,"Hey starphin, this is a good catch. It looks like csun and I missed this when doing HDFS-14346. Would it not be simpler to just do: It seems a little weird to me to convert a number to a string just so that we can re-parse it into a number.",0.2586666667,0.2586666667,positive
hadoop,16461,description,"The lock now has a ShutdownHook creation, which ends up doing which ends up doing a ""new Configuration()"" within the locked section. This indirectly hurts the cache hit scenarios as well, since if the lock on this is held, then the other section cannot be entered either. slowing down the RawLocalFileSystem when there are other threads creating HDFS FileSystem objects at the same time.",design_debt,non-optimal_design,"Wed, 24 Jul 2019 21:53:03 +0000","Tue, 1 Oct 2019 00:06:38 +0000","Fri, 26 Jul 2019 10:35:00 +0000",132117,"https://github.com/apache/hadoop/blob/2546e6ece240924af2188bb39b3954a4896e4a4f/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java#L3388 The lock now has a ShutdownHook creation, which ends up doing https://github.com/apache/hadoop/blob/2546e6ece240924af2188bb39b3954a4896e4a4f/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ShutdownHookManager.java#L205 which ends up doing a ""new Configuration()"" within the locked section. This indirectly hurts the cache hit scenarios as well, since if the lock on this is held, then the other section cannot be entered either. https://github.com/apache/tez/blob/master/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/sort/impl/TezSpillRecord.java#L65 slowing down the RawLocalFileSystem when there are other threads creating HDFS FileSystem objects at the same time.",-0.05883333333,-0.05883333333,neutral
hadoop,16504,comment_6,Looks reasonable to me. We even have customers increasing this number to 2k or even 16k. The only potential downside is it could result in more memory usage. Not sure how much it could be but I don't think that's a concern for typical deployments. I'd like to wait for a day or two for any one else to have a chance to assess & comment before I commit this patch.,design_debt,non-optimal_design,"Sat, 10 Aug 2019 16:46:31 +0000","Wed, 2 Oct 2019 17:16:03 +0000","Thu, 15 Aug 2019 22:21:35 +0000",452104,Looks reasonable to me. We even have customers increasing this number to 2k or even 16k. The only potential downside is it could result in more memory usage. Not sure how much it could be but I don't think that's a concern for typical deployments. I'd like to wait for a day or two for any one else to have a chance to assess & comment before I commit this patch.,0.1105,0.1105,neutral
hadoop,16504,description,"Because default value is too small, TCP's ListenDrop indicator along with the rpc request large. The upper limit of the system's semi-join queue is 65636 and maximum number of fully connected queues is 1024. I think this default value should be adjusted.",design_debt,non-optimal_design,"Sat, 10 Aug 2019 16:46:31 +0000","Wed, 2 Oct 2019 17:16:03 +0000","Thu, 15 Aug 2019 22:21:35 +0000",452104,"Because ipc.server.listen.queue.size default value is too small, TCP's ListenDrop indicator along with the rpc request large. The upper limit of the system's semi-join queue is 65636 and maximum number of fully connected queues is 1024. I think this default value should be adjusted.",-0.125,-0.05357142857,negative
hadoop,16607,description,"Hive deployments can use a JCEKs file to store secrets, which it sets up To be readable only by the Hive user, listing it under When it tries to create an S3A FS instance as another user, via a doAs{} clause, the S3A FS getPassword() call fails on the subsequent -even if the secret it is looking for is in the XML file or, as in the case of encryption settings, or session key undefined. I can you point the blame at hive for this -it's the one with a forbidden JCEKS file on the provider path, but I think it is easiest to fix in S3AUtils than in hive, and safer then changing Configuration. ABFS is likely to see the same problem. I propose an option to set the fallback policy. I initially thought about always handling this: Catching the exception, attempting to downgrade to Reading XML and if that fails rethrowing the caught exception. However, this will do the wrong thing if the option is completely undefined, As is common with the encryption settings. I don't want to simply default to log and continue here though, as it may be a legitimate failure -such as when you really do want to read secrets from such a source. Issue: what fallback policies? * fail: fail fast. today's policy; the default. * ignore: log and continue We could try and be clever in future. To get away with that, we would have to declare which options were considered compulsory and re-throw the caught Exception if no value was found in the XML file. That can be a future enhancement -but it is why I want the policy to be an enumeration, rather than a simple boolean. Tests: should be straightforward; set to a non-existent file and expected to be processed according to the settings.",design_debt,non-optimal_design,"Wed, 25 Sep 2019 14:15:33 +0000","Mon, 18 Nov 2019 19:42:53 +0000","Fri, 11 Oct 2019 15:26:36 +0000",1386663,"Hive deployments can use a JCEKs file to store secrets, which it sets up To be readable only by the Hive user, listing it under hadoop.credential.providers. When it tries to create an S3A FS instance as another user, via a doAs{} clause, the S3A FS getPassword() call fails on the subsequent AccessDeniedException -even if the secret it is looking for is in the XML file or, as in the case of encryption settings, or session key undefined. I can you point the blame at hive for this -it's the one with a forbidden JCEKS file on the provider path, but I think it is easiest to fix in S3AUtils than in hive, and safer then changing Configuration. ABFS is likely to see the same problem. I propose an option to set the fallback policy. I initially thought about always handling this: Catching the exception, attempting to downgrade to Reading XML and if that fails rethrowing the caught exception. However, this will do the wrong thing if the option is completely undefined, As is common with the encryption settings. I don't want to simply default to log and continue here though, as it may be a legitimate failure -such as when you really do want to read secrets from such a source. Issue: what fallback policies? fail: fail fast. today's policy; the default. ignore: log and continue We could try and be clever in future. To get away with that, we would have to declare which options were considered compulsory and re-throw the caught Exception if no value was found in the XML file. That can be a future enhancement -but it is why I want the policy to be an enumeration, rather than a simple boolean. Tests: should be straightforward; set hadoop.credential.providers to a non-existent file and expected to be processed according to the settings.",-0.00205952381,-0.00151754386,neutral
hadoop,1773,comment_6,"I don't see any error on fedora_core_x64 jdk_1.5 ant_1.7 Quoting a code snippet from build.xml <touch 2:00 pm"" <fileset dir=""${conf.dir}"" <fileset </touch What is the significance of ""01/25/1971 2:00 pm"" .. if the idea is to stamp the files with an older date.. then a better thing to do is <touch millis=""0"" <fileset dir=""${conf.dir}"" <fileset </touch This may not fix the issue.. its just a suggestion to make the build file easier to understand",design_debt,non-optimal_design,"Thu, 23 Aug 2007 22:01:21 +0000","Thu, 12 May 2016 18:22:44 +0000","Sat, 27 Apr 2013 00:33:01 +0000",179116300,"I don't see any error on fedora_core_x64 jdk_1.5 ant_1.7 Quoting a code snippet from build.xml <touch datetime=""01/25/1971 2:00 pm""> <fileset dir=""${conf.dir}"" includes=""*/.template""/> <fileset dir=""${contrib.dir}"" includes=""*/.template""/> </touch> What is the significance of ""01/25/1971 2:00 pm"" .. if the idea is to stamp the files with an older date.. then a better thing to do is <touch millis=""0""> <fileset dir=""${conf.dir}"" includes=""*/.template""/> <fileset dir=""${contrib.dir}"" includes=""*/.template""/> </touch> This may not fix the issue.. its just a suggestion to make the build file easier to understand",0.1875,0.075,neutral
hadoop,2181,comment_9,"Some comments: 1) The change in JobInProgress to do with wasRunning is problematic. In some cases, you might end up logging the split info more than once. 2) The method doesn't fit well in the StringUtils class. OTOH you could define it as a private method in TaskInProgress from where you call it.",design_debt,non-optimal_design,"Fri, 9 Nov 2007 18:27:51 +0000","Wed, 8 Jul 2009 16:52:31 +0000","Tue, 6 May 2008 11:36:27 +0000",15440916,"Some comments: 1) The change in JobInProgress to do with wasRunning is problematic. In some cases, you might end up logging the split info more than once. 2) The StringUtils.nodetoString method doesn't fit well in the StringUtils class. OTOH you could define it as a private method in TaskInProgress from where you call it.",-0.15175,-0.1214,negative
hadoop,2208,description,"Currently, We have counter updates from task tracker to job tracker on every heartbeat. Both counter name and the values are updated for every heartbeat. This can be improved by sending names and values for the first time and only the values after that. The frequency can be reduced by doing update only when the counters got changed.",design_debt,non-optimal_design,"Thu, 15 Nov 2007 04:47:40 +0000","Wed, 8 Jul 2009 16:52:35 +0000","Thu, 3 Jan 2008 08:16:17 +0000",4246117,"Currently, We have counter updates from task tracker to job tracker on every heartbeat. Both counter name and the values are updated for every heartbeat. This can be improved by sending names and values for the first time and only the values after that. The frequency can be reduced by doing update only when the counters got changed.",-0.0125,-0.0125,neutral
hadoop,2776,comment_1,"Also note that the new EC2 scripts add SOCKS support: hadoop-ec2 proxy <cluster name By installing FoxyProxy or setting you socks firewall settings, you can browse your whole cluster. that said, Nate's suggestion might be less clunky... see:",design_debt,non-optimal_design,"Mon, 4 Feb 2008 16:59:32 +0000","Thu, 17 Jul 2014 20:16:05 +0000","Thu, 17 Jul 2014 20:16:05 +0000",203483793,"Also note that the new EC2 scripts add SOCKS support: hadoop-ec2 proxy <cluster name> By installing FoxyProxy or setting you socks firewall settings, you can browse your whole cluster. that said, Nate's suggestion might be less clunky... see: https://issues.apache.org/jira/browse/HADOOP-2410",0.2,0.2,neutral
hadoop,2776,comment_7,Any more updates on this? Running Hadoop inside Amazon EC2 is super annoying because most of the links are broken (internal vs external addresses). This bug has been open for FOUR YEARS.,design_debt,non-optimal_design,"Mon, 4 Feb 2008 16:59:32 +0000","Thu, 17 Jul 2014 20:16:05 +0000","Thu, 17 Jul 2014 20:16:05 +0000",203483793,Any more updates on this? Running Hadoop inside Amazon EC2 is super annoying because most of the links are broken (internal vs external addresses). This bug has been open for FOUR YEARS.,-0.07311111111,-0.07311111111,negative
hadoop,2796,comment_0,"The proposed solution in the bug of adding a constant number to the script's exit code, in retrospect, seems like a bad idea. - It is not very intuitive. - There could be cases where because of the addition, some shells like bash which do modulo 256 on exit codes, could make the result become 0, which seems like a successful execution. - It causes an unreasonable dependency between HOD and user scripts, who need to remember this magic number. The requirements for this problem, to my understanding, are as follows: - Return a zero exit code for a completely successful operation (both hod and the script have worked fine) - Return a non-zero exit code for a failed operation (either hod or the script have failed). Users may not care for more than this. Did it work or not - In the event of a non-zero exit code where the user wants to know if his script failed, provide an easy, clear way to determine if it failed. On these lines, the attached patch does the following: - Returns a zero exit code on success. - Returns a non-zero exit code on failure of script or hod itself. - If the script returned a non-zero exit code, it writes the exit code from the script to a file 'script.exitcode' into the cluster directory. Users can simple check for this file's existence and determine if it is a script failure. - If it's a hod failure, no such file will exist.",design_debt,non-optimal_design,"Thu, 7 Feb 2008 13:12:36 +0000","Wed, 21 May 2008 20:05:50 +0000","Fri, 21 Mar 2008 14:26:42 +0000",3719646,"The proposed solution in the bug of adding a constant number to the script's exit code, in retrospect, seems like a bad idea. It is not very intuitive. There could be cases where because of the addition, some shells like bash which do modulo 256 on exit codes, could make the result become 0, which seems like a successful execution. It causes an unreasonable dependency between HOD and user scripts, who need to remember this magic number. The requirements for this problem, to my understanding, are as follows: Return a zero exit code for a completely successful operation (both hod and the script have worked fine) Return a non-zero exit code for a failed operation (either hod or the script have failed). Users may not care for more than this. Did it work or not In the event of a non-zero exit code where the user wants to know if his script failed, provide an easy, clear way to determine if it failed. On these lines, the attached patch does the following: Returns a zero exit code on success. Returns a non-zero exit code on failure of script or hod itself. If the script returned a non-zero exit code, it writes the exit code from the script to a file 'script.exitcode' into the cluster directory. Users can simple check for this file's existence and determine if it is a script failure. If it's a hod failure, no such file will exist.",0.0470952381,0.02792307692,negative
hadoop,2897,description,"Hod currently allows the user to specify and run a hadoop script after allocation, and deallocating as soon as the script is done. For e.g. hod -m 3 -z ~/hadoop.script allocates 3 nodes, and runs ~/hadoop.script, then deallocates This is a convenient way single line wrapper around 4-5 commands that users have to write themselves. We have this because: - hod 0.3 does not provide an easy way to combine these into a single operation, because of the HOD shell. - even in hod 0.4, users have to carefully write some error checking code to make sure their cluster is allocated successfully, before running the script and their HADOOP_CONF_DIR should be set correctly. - users can free up clusters as soon as they are done. The requirements make sense. But having this as part of the core hod interface seems incorrect. The interface should be an orthogonal set of commands that each just do one thing well. The script option should be converted to a simple wrapper that can be part of the hod project. This way, users can enjoy the benefits of not having to write such a script themselves, while the hod codebase can still be clean. One disadvantage if we change this is that users will need to remember one more command. But given hod 0.4 is a new interface anyway, it is better to address now, rather than later. And we can alleviate this a bit by making sure options are consistently named between hod and the wrapper script.",design_debt,non-optimal_design,"Tue, 26 Feb 2008 10:55:37 +0000","Sat, 8 Mar 2008 19:51:07 +0000","Mon, 3 Mar 2008 05:54:18 +0000",500321,"Hod currently allows the user to specify and run a hadoop script after allocation, and deallocating as soon as the script is done. For e.g. hod -m 3 -z ~/hadoop.script allocates 3 nodes, and runs ~/hadoop.script, then deallocates This is a convenient way single line wrapper around 4-5 commands that users have to write themselves. We have this because: hod 0.3 does not provide an easy way to combine these into a single operation, because of the HOD shell. even in hod 0.4, users have to carefully write some error checking code to make sure their cluster is allocated successfully, before running the script and their HADOOP_CONF_DIR should be set correctly. users can free up clusters as soon as they are done. The requirements make sense. But having this as part of the core hod interface seems incorrect. The interface should be an orthogonal set of commands that each just do one thing well. The script option should be converted to a simple wrapper that can be part of the hod project. This way, users can enjoy the benefits of not having to write such a script themselves, while the hod codebase can still be clean. One disadvantage if we change this is that users will need to remember one more command. But given hod 0.4 is a new interface anyway, it is better to address now, rather than later. And we can alleviate this a bit by making sure options are consistently named between hod and the wrapper script.",0.1542051282,0.1347833333,neutral
hadoop,2959,comment_0,"I should have pounted out the use case why this is matter. When the combining logic (reducer logic) depends on some thing that is initialized in the configure method, and of the configure method call is relative expensive (say initialize a dictionary from a file on dfs), then such an optimization makes a huge difference.",design_debt,non-optimal_design,"Fri, 7 Mar 2008 03:40:02 +0000","Wed, 8 Jul 2009 16:52:39 +0000","Fri, 7 Mar 2008 06:37:13 +0000",10631,"I should have pounted out the use case why this is matter. When the combining logic (reducer logic) depends on some thing that is initialized in the configure method, and of the configure method call is relative expensive (say initialize a dictionary from a file on dfs), then such an optimization makes a huge difference.",0.35,0.35,neutral
hadoop,2959,summary,"When a mapper needs to run a combiner, it should create one and reuse it, instead of creating one per partition per spill",design_debt,non-optimal_design,"Fri, 7 Mar 2008 03:40:02 +0000","Wed, 8 Jul 2009 16:52:39 +0000","Fri, 7 Mar 2008 06:37:13 +0000",10631,"When a mapper needs to run a combiner, it should create one and reuse it, instead of creating one per partition per spill",0,0,neutral
hadoop,3087,description,"JobInfo object is not refreshed in loadHistory.jsp if same job is accessed again. In loadhistory.jsp, the jobInfo object is stored as session attribute. And if the same job is accessed again, the object is not refreshed. This becomes a problem if first time the object accessed was during job running and it doesnt refresh even after completing the job.",design_debt,non-optimal_design,"Tue, 25 Mar 2008 07:04:48 +0000","Wed, 8 Jul 2009 16:52:41 +0000","Thu, 27 Mar 2008 12:36:32 +0000",192704,"JobInfo object is not refreshed in loadHistory.jsp if same job is accessed again. In loadhistory.jsp, the jobInfo object is stored as session attribute. And if the same job is accessed again, the object is not refreshed. This becomes a problem if first time the object accessed was during job running and it doesnt refresh even after completing the job.",-0.3518888889,-0.3518888889,neutral
hadoop,3108,comment_2,"I extended Nicholas's patch. Found one more potential NPE related to getNode() method in FSDirectory. TestPermissions failed in its original version because it was catching but was receiving a RemoteException instead. I wrote a method, which throws the cause of the RemoteException if it is of the right type. I recommend using it instead of analyzing the exception message. I should say in general that handling of RemoteExceptions e.g. in connection with is terrible. I will file a separate jira. The test was not closing file system correctly, I fixed that. And also a couple of findBugs warnings related to ignoring return values of called methods. I used the same unwrapping for canMkdirs(), canCreate() and canOpen() methods: this test is based on FileSystem API, which should not know anything about RemoteException, because e.g. LocalFileSystem does not throw this, ever. These changes affected TestDFSPermission, which in turn led to changes in DFSClient methods responsible, which report permission violations. With all the changes TestPermissions now correctly tests the case of changing permissions and owner of missing files Nicholas introduced. All tests pass on my machine except for TestDFSShell. Something is wrong with getFileInfo(). Hudson is stuck again so I cannot verify on a different machine either. Please somebody take a look.",design_debt,non-optimal_design,"Thu, 27 Mar 2008 18:26:42 +0000","Wed, 8 Jul 2009 16:43:00 +0000","Fri, 28 Mar 2008 20:55:34 +0000",95332,"I extended Nicholas's patch. Found one more potential NPE related to getNode() method in FSDirectory. TestPermissions failed in its original version because it was catching FileNotFoundException but was receiving a RemoteException instead. I wrote a unwrapRemoteException() method, which throws the cause of the RemoteException if it is of the right type. I recommend using it instead of analyzing the exception message. I should say in general that handling of RemoteExceptions e.g. in connection with FileNotFoundException is terrible. I will file a separate jira. The test was not closing file system correctly, I fixed that. And also a couple of findBugs warnings related to ignoring return values of called methods. I used the same unwrapping for canMkdirs(), canCreate() and canOpen() methods: this test is based on FileSystem API, which should not know anything about RemoteException, because e.g. LocalFileSystem does not throw this, ever. These changes affected TestDFSPermission, which in turn led to changes in DFSClient methods responsible, which report permission violations. With all the changes TestPermissions now correctly tests the case of changing permissions and owner of missing files Nicholas introduced. All tests pass on my machine except for TestDFSShell. Something is wrong with getFileInfo(). Hudson is stuck again so I cannot verify on a different machine either. Please somebody take a look.",-0.07435416667,-0.07435416667,negative
hadoop,3108,comment_5,"Since this patch is meant for 16, I think its better if it only fixes the NPEs. Changing some of the IOExceptions to FileNotFound might be ok. But any other semantic changes probably belong in 17 or trunk. For example with the patch, deleting a file can throw IOException, where i think it should return false.",design_debt,non-optimal_design,"Thu, 27 Mar 2008 18:26:42 +0000","Wed, 8 Jul 2009 16:43:00 +0000","Fri, 28 Mar 2008 20:55:34 +0000",95332,"Since this patch is meant for 16, I think its better if it only fixes the NPEs. Changing some of the IOExceptions to FileNotFound might be ok. But any other semantic changes probably belong in 17 or trunk. For example with the patch, deleting a file can throw IOException, where i think it should return false.",0.2625,0.2625,neutral
hadoop,3159,comment_5,"Digressing a little bit, the fundamental confusion seems to be that the key used by the cache needs scheme, authority, and username, but it only requires scheme and authority for look up.. so tries derive username some how. In long term, hopefully the interface itself gets fixed.",design_debt,non-optimal_design,"Thu, 3 Apr 2008 00:10:31 +0000","Thu, 17 Apr 2008 05:28:57 +0000","Fri, 4 Apr 2008 19:23:29 +0000",155578,"Digressing a little bit, the fundamental confusion seems to be that the key used by the cache needs scheme, authority, and username, but it only requires scheme and authority for look up.. so tries derive username some how. In long term, hopefully the interface itself gets fixed.",0.35,0.35,neutral
hadoop,3159,comment_8,"+1 The new patch is acceptable. Longer-term it seems to me we need a static URI) method, and the possibility to register different login methods for different URI schemes, e.g., The static method would then, when the configuration has no login information invoke the login method of the class named for that scheme, if any.",design_debt,non-optimal_design,"Thu, 3 Apr 2008 00:10:31 +0000","Thu, 17 Apr 2008 05:28:57 +0000","Fri, 4 Apr 2008 19:23:29 +0000",155578,"+1 The new patch is acceptable. Longer-term it seems to me we need a static login(Configuration, URI) method, and the possibility to register different login methods for different URI schemes, e.g., fs.login.hdfs=UnixUserGroupInformation. The static method would then, when the configuration has no login information invoke the login method of the class named for that scheme, if any.",-0.1095,-0.2188,neutral
hadoop,3198,comment_3,"HDFS client has a retry on exists. It is likely that it tried and failed the several times. That is perhaps fine for exists call in general. However, for this particular call in getRecordWriter in reduce rask, the cost of failure is too expensive. Thus, reduce task has to do something special. In this sense, I think it is reduce task's responsibility to further re-try. I am open for any suggestions to fix the problem. However, I am not convenced that re-try at rpc level is the right answer.",design_debt,non-optimal_design,"Sat, 5 Apr 2008 19:30:54 +0000","Wed, 8 Jul 2009 16:52:43 +0000","Thu, 17 Apr 2008 16:42:38 +0000",1026704,"HDFS client has a retry on exists. It is likely that it tried and failed the several times. That is perhaps fine for exists call in general. However, for this particular call in getRecordWriter in reduce rask, the cost of failure is too expensive. Thus, reduce task has to do something special. In this sense, I think it is reduce task's responsibility to further re-try. I am open for any suggestions to fix the problem. However, I am not convenced that re-try at rpc level is the right answer.",-0.0986875,-0.0986875,neutral
hadoop,3286,description,"Gridmix jobs use time suffix to differentiate output dir names. The current time granularity of second may not be sufficient, causing multiple jobs having the same output dir.",design_debt,non-optimal_design,"Mon, 21 Apr 2008 01:51:44 +0000","Wed, 21 May 2008 20:06:03 +0000","Wed, 23 Apr 2008 01:05:53 +0000",170049,"Gridmix jobs use time suffix to differentiate output dir names. The current time granularity of second may not be sufficient, causing multiple jobs having the same output dir.",0.0625,0.0625,neutral
hadoop,3286,summary,Gridmix jobs' output dir names may collide,design_debt,non-optimal_design,"Mon, 21 Apr 2008 01:51:44 +0000","Wed, 21 May 2008 20:06:03 +0000","Wed, 23 Apr 2008 01:05:53 +0000",170049,Gridmix jobs'  output dir names may collide,-0.5,-0.5,neutral
hadoop,3337,comment_4,Wouldn't this affect readFields() and write() of DatanodeDescriptor (used everywhere : RPCs etc) ? This patch looks like a problematic hack. I think this needs to be fixed better. If EditLog requires to read and write differently these different serialization should used there instead of everywhere.,design_debt,non-optimal_design,"Fri, 2 May 2008 00:36:44 +0000","Wed, 8 Jul 2009 16:43:04 +0000","Sun, 4 May 2008 19:29:32 +0000",240768,Wouldn't this affect readFields() and write() of DatanodeDescriptor (used everywhere : RPCs etc) ? This patch looks like a problematic hack. I think this needs to be fixed better. If EditLog requires to read and write differently these different serialization should used there instead of everywhere.,0.125,0.125,negative
hadoop,3337,comment_8,"Sure it is. Even if it is not, I don't think its a good practice to silently break the contract because we think the contract is not used (yet), (especially for widely used interfaces like Writables) Example use: returns {{LocatedBlocks}}, if you trace its implementation, you will see that LocatedBlock is created using DatanodeDescriptor (around .. so etc do get called out side of FSEditLog.",design_debt,non-optimal_design,"Fri, 2 May 2008 00:36:44 +0000","Wed, 8 Jul 2009 16:43:04 +0000","Sun, 4 May 2008 19:29:32 +0000",240768,"> DatanodeDescriptor is not used in RPC only DatanodeInfo. Sure it is. Even if it is not, I don't think its a good practice to silently break the contract because we think the contract is not used (yet), (especially for widely used interfaces like Writables) Example use: ClientProtocol.getBlockLocations() returns LocatedBlocks, if you trace its implementation, you will see that LocatedBlock is created using DatanodeDescriptor (around FSNamesystem.java:747) .. so DatanodeDescriptor.read() etc do get called out side of FSEditLog.",-0.388,-0.1293333333,neutral
hadoop,3375,comment_0,"Also, lease related structure should store names and paths as String instead of It will prevent unnecessary converting between String and",design_debt,non-optimal_design,"Tue, 13 May 2008 00:59:27 +0000","Wed, 8 Jul 2009 16:43:06 +0000","Tue, 20 May 2008 23:09:32 +0000",684605,"Also, lease related structure should store names and paths as String instead of StringBytesWritable. It will prevent unnecessary converting between String and StringBytesWritable.",-0.35,-0.35,neutral
hadoop,3654,description,"This is being added as a bugrep so that other people can find it, and the workaround 1. On my machine TestFileSystem will hang, even overnight -even though the build was set with a timeout. 2. halting the build left a JVM running; it was not being killed. 3. Under the IDE, the main thread appears hung in the native library call to get a stack trace, somewhere inside Log4J 4. the IDE could not halt the build, and could not be shut down cleanly either The fix for this problem was to edit and switch to a log4J log pattern that did not print the line of the code %-5p %c %x - %m%n Given that working out a stack trace can be an expensive call, and that it can apparently hang some JVMs, perhaps it should not be the default.",design_debt,non-optimal_design,"Fri, 27 Jun 2008 14:59:25 +0000","Thu, 29 Apr 2010 08:07:15 +0000","Thu, 24 Dec 2009 12:44:28 +0000",47079903,"This is being added as a bugrep so that other people can find it, and the workaround 1. On my machine TestFileSystem will hang, even overnight -even though the build was set with a timeout. 2. halting the build left a JVM running; it was not being killed. 3. Under the IDE, the main thread appears hung in the native library call to get a stack trace, somewhere inside Log4J 4. the IDE could not halt the build, and could not be shut down cleanly either The fix for this problem was to edit conf/log4j.properties and switch to a log4J log pattern that did not print the line of the code log4j.appender.console.layout.ConversionPattern=%-4r %-5p %c %x - %m%n Given that working out a stack trace can be an expensive call, and that it can apparently hang some JVMs, perhaps it should not be the default.",0.1782857143,0.1975833333,neutral
hadoop,3905,comment_1,"In the patch. # I substantially simplified what used to be called by using existing hadoop class instead of a pair of classes import and # is now an abstract class, and is its implementation for storing edits in a file. # Introduced and its implementation called for reading edits from a file. # The rest of the {{FSEditLog}} remained unchanged. The idea here is that one should write implementations of and for a different type of persistent storage and the rest of the logic for the edits log should remain unchanged. Since I don't have other implementations ready yet it is hard to predict what else should be changed or abstracted. FSImage and FSEditLog heavily depend on the storage directory file names so may be that should be changed somehow in the future.",design_debt,non-optimal_design,"Wed, 6 Aug 2008 01:49:11 +0000","Wed, 8 Jul 2009 16:43:16 +0000","Sat, 16 Aug 2008 00:06:40 +0000",857849,"In the patch. I substantially simplified what used to be called EditLogOutputStream by using existing hadoop class org.apache.hadoop.io.DataOutputBuffer instead of a pair of classes import DataOutputStream and ByteArrayOutputStream EditLogOutputStream is now an abstract class, and EditLogFileOutputStream is its implementation for storing edits in a file. Introduced EditLogInputStream and its implementation called EditLogFileInputStream for reading edits from a file. The rest of the FSEditLog remained unchanged. The idea here is that one should write implementations of EditLogOutputStream and EditLogInputStream for a different type of persistent storage and the rest of the logic for the edits log should remain unchanged. Since I don't have other implementations ready yet it is hard to predict what else should be changed or abstracted. FSImage and FSEditLog heavily depend on the storage directory file names so may be that should be changed somehow in the future.",-0.06428571429,-0.04090909091,neutral
hadoop,3925,comment_2,One of our users submitted a job that has a million mappers and million reducers. The JobTracker was runnign with 3GB heap. It went into 100% CPU usage (probably GC). Never came back to life even after 10 minutes. Is there a way (in the current release) to prevent this from happening?,design_debt,non-optimal_design,"Fri, 8 Aug 2008 06:13:55 +0000","Wed, 8 Jul 2009 16:52:57 +0000","Thu, 20 Nov 2008 07:59:21 +0000",8991926,One of our users submitted a job that has a million mappers and million reducers. The JobTracker was runnign with 3GB heap. It went into 100% CPU usage (probably GC). Never came back to life even after 10 minutes. Is there a way (in the current release) to prevent this from happening?,-0.04,-0.04,negative
hadoop,3925,description,The JobTracker can be prone to a denial-of-service attack if a user submits a job that has a very large number of tasks. This has happened once in our cluster. It would be nice to have a configuration setting that limits the maximum tasks that a single job can have.,design_debt,non-optimal_design,"Fri, 8 Aug 2008 06:13:55 +0000","Wed, 8 Jul 2009 16:52:57 +0000","Thu, 20 Nov 2008 07:59:21 +0000",8991926,The JobTracker can be prone to a denial-of-service attack if a user submits a job that has a very large number of tasks. This has happened once in our cluster. It would be nice to have a configuration setting that limits the maximum tasks that a single job can have.,0.03888888889,0.03888888889,negative
hadoop,3999,comment_1,"1. This would be good if it could be easily extended; rather than than a hard coded set of values, clients could add other (key,value) info for schedulers to use. Things like for cycle-scavenging task-trackers, and other extensions that custom schedulers could use. It could also integrate with diagnostics. 2. There's a danger here in trying to do a full grid scheduler. Why Danger? Hard to get right, there are other tools and products that can do a lot of this. Hadoop likes to push work near the data and works best if the work is all Java. 3. Developers are surprisingly bad about estimating workload, especially if you have a few layers between you and the MR jobs. The best metric for how intensive a job will be is ""what was like last time"".",design_debt,non-optimal_design,"Fri, 22 Aug 2008 09:42:15 +0000","Fri, 18 Jul 2014 22:22:39 +0000","Fri, 18 Jul 2014 22:22:39 +0000",186324024,"1. This would be good if it could be easily extended; rather than than a hard coded set of values, clients could add other (key,value) info for schedulers to use. Things like expected-availability for cycle-scavenging task-trackers, and other extensions that custom schedulers could use. It could also integrate with diagnostics. 2. There's a danger here in trying to do a full grid scheduler. Why Danger? Hard to get right, there are other tools and products that can do a lot of this. Hadoop likes to push work near the data and works best if the work is all Java. 3. Developers are surprisingly bad about estimating workload, especially if you have a few layers between you and the MR jobs. The best metric for how long/CPU-intensive/IO intensive a job will be is ""what was like last time"".",0.05619444444,0.05619444444,neutral
hadoop,400,description,"The job tracker tries not to run tasks that have previously failed on a node on that node again, but it doesn't strictly prevent it. I propose to change the rule so that when pollForNewTask is called by a TaskTracker, the JobTracker will only assign it a task that has failed on that TaskTracker, if and only if it has already failed on the entire cluster. Thus, for ""normal"" clusters with more than 4 TaskTrackers, you will be guaranteed that it will run on 4 different TaskTrackers. For small clusters, it will run on every TaskTracker in the cluster at least once. Does that sound reasonable to everyone?",design_debt,non-optimal_design,"Fri, 28 Jul 2006 20:33:32 +0000","Wed, 8 Jul 2009 16:51:51 +0000","Wed, 9 Aug 2006 13:46:03 +0000",1012351,"The job tracker tries not to run tasks that have previously failed on a node on that node again, but it doesn't strictly prevent it. I propose to change the rule so that when pollForNewTask is called by a TaskTracker, the JobTracker will only assign it a task that has failed on that TaskTracker, if and only if it has already failed on the entire cluster. Thus, for ""normal"" clusters with more than 4 TaskTrackers, you will be guaranteed that it will run on 4 different TaskTrackers. For small clusters, it will run on every TaskTracker in the cluster at least once. Does that sound reasonable to everyone?",0.0484,0.0484,neutral
hadoop,4603,comment_9,"This is essentially fixed by using the native libraries in newer releases of Hadoop. The problem is that the native code is completely non-portable and the committer community has shown no real desire to make that code portable. I don't believe we should encourage folks to run without the native code because the performance is likely to be seriously horrendous. (My anecdotal experience says the NN in 0.20.20x is 10-15% slower compared to 0.20.2). At this point, I'd close this as won't fix, just like I did all of my portability JIRAs (including some with patches). I think pretending that we care about portability is sort of silly at this point when it has been demonstrated over and over that we don't.",design_debt,non-optimal_design,"Thu, 6 Nov 2008 21:21:29 +0000","Mon, 21 Jul 2014 16:56:19 +0000","Mon, 21 Jul 2014 16:56:19 +0000",179955290,"This is essentially fixed by using the native libraries in newer releases of Hadoop. The problem is that the native code is completely non-portable and the committer community has shown no real desire to make that code portable. I don't believe we should encourage folks to run without the native code because the performance is likely to be seriously horrendous. (My anecdotal experience says the NN in 0.20.20x is 10-15% slower compared to 0.20.2). At this point, I'd close this as won't fix, just like I did all of my portability JIRAs (including some with patches). I think pretending that we care about portability is sort of silly at this point when it has been demonstrated over and over that we don't.",-0.1506666667,-0.1506666667,negative
hadoop,4997,description,"This is a temporary work around issues discussed in HADOOP-4663. The proposal is to remove all the files under tmp directory, thus bringing the behavior back to 0.17. The main cost is that sync() will not be supported. This is incompatible with 0.18.x, but not with 0.17 because of this reason.",design_debt,non-optimal_design,"Thu, 8 Jan 2009 20:21:26 +0000","Wed, 8 Jul 2009 16:43:29 +0000","Fri, 16 Jan 2009 23:56:50 +0000",704124,"This is a temporary work around issues discussed in HADOOP-4663. The proposal is to remove all the files under tmp directory, thus bringing the behavior back to 0.17. The main cost is that sync() will not be supported. This is incompatible with 0.18.x, but not with 0.17 because of this reason.",0.13,0.13,negative
hadoop,508,description,"Some of my applications using Hadoop DFS receive wrong data after certain random seeks. After some investigation I believe (without looking at source code of that it basically boils down to the fact that the method read(byte[] b, int off, int len), when called with an external buffer larger than the internal buffer, reads into the external buffer directly without using the internal buffer anymore, but without invalidating the internal buffer by setting the variable 'count' to 0 such that a subsequent seek to an offset which is closer to the 'position' of the Positioncache than the internal buffersize will put the current position into the internal buffer containing outdated data from somewhere else.",design_debt,non-optimal_design,"Tue, 5 Sep 2006 20:48:16 +0000","Wed, 8 Jul 2009 16:42:02 +0000","Wed, 27 Sep 2006 22:01:27 +0000",1905191,"Some of my applications using Hadoop DFS receive wrong data after certain random seeks. After some investigation I believe (without looking at source code of java.io.BufferedInputStream) that it basically boils down to the fact that the method read(byte[] b, int off, int len), when called with an external buffer larger than the internal buffer, reads into the external buffer directly without using the internal buffer anymore, but without invalidating the internal buffer by setting the variable 'count' to 0 such that a subsequent seek to an offset which is closer to the 'position' of the Positioncache than the internal buffersize will put the current position into the internal buffer containing outdated data from somewhere else.",-0.1895,-0.16775,neutral
hadoop,5402,summary,TaskTracker ignores most RemoteExceptions from heartbeat processing,design_debt,non-optimal_design,"Wed, 4 Mar 2009 23:40:58 +0000","Mon, 21 Jul 2014 21:28:53 +0000","Mon, 21 Jul 2014 21:28:53 +0000",169768075,TaskTracker ignores most RemoteExceptions from heartbeat processing,-0.2,-0.2,neutral
hadoop,551,description,"In the patch for HADOOP-423, I made the web ui and the job client cli format the percentage done consistently as 0.00 - 100.00%. This has the side effect of making the job client print a lot more lines to the user's console (up to 20k!).",design_debt,non-optimal_design,"Tue, 19 Sep 2006 22:57:35 +0000","Fri, 6 Oct 2006 21:49:09 +0000","Wed, 20 Sep 2006 21:46:31 +0000",82136,"In the patch for HADOOP-423, I made the web ui and the job client cli format the percentage done consistently as 0.00 - 100.00%. This has the side effect of making the job client print a lot more lines to the user's console (up to 20k!).",0.28125,0.28125,neutral
hadoop,551,summary,reduce the number of lines printed to the console during execution,design_debt,non-optimal_design,"Tue, 19 Sep 2006 22:57:35 +0000","Fri, 6 Oct 2006 21:49:09 +0000","Wed, 20 Sep 2006 21:46:31 +0000",82136,reduce the number of lines printed to the console during execution,0.375,0.375,neutral
hadoop,5561,comment_3,"would recommend having an Ant property javadoc.memory that is set to 512m in the build file, but can be overridden by people with problems (or 64 bit JVMs) without having to patch the build file. It could also be used by all build files",design_debt,non-optimal_design,"Mon, 23 Mar 2009 23:49:25 +0000","Tue, 24 Aug 2010 20:36:43 +0000","Thu, 26 Mar 2009 00:25:57 +0000",174992,"would recommend having an Ant property javadoc.memory that is set to 512m in the build file, but can be overridden by people with problems (or 64 bit JVMs) without having to patch the build file. It could also be used by all build files",0,0,neutral
hadoop,5561,description,"The default configuration for the ant task javadoc-dev does not specify a maxmemory and, after churning for a while, fails with an OOM exception:",design_debt,non-optimal_design,"Mon, 23 Mar 2009 23:49:25 +0000","Tue, 24 Aug 2010 20:36:43 +0000","Thu, 26 Mar 2009 00:25:57 +0000",174992,"The default configuration for the ant task javadoc-dev does not specify a maxmemory and, after churning for a while, fails with an OOM exception:",-0.45,-0.45,negative
hadoop,5657,comment_2,Emits only two additional records per map; less expensive key updates. Detects known-bad case when merging a combination of in-memory and on-disk segments.,design_debt,non-optimal_design,"Sat, 11 Apr 2009 00:16:20 +0000","Tue, 24 Aug 2010 20:37:05 +0000","Wed, 29 Apr 2009 04:11:21 +0000",1569301,Emits only two additional records per map; less expensive key updates. Detects known-bad case when merging a combination of in-memory and on-disk segments.,-0.05,-0.05,neutral
hadoop,5657,description,"While TestReduceFetch verifies the reduce semantics for reducing from in-memory segments, it does not validate the data it reads. Data corrupted during the merge will not be detected.",design_debt,non-optimal_design,"Sat, 11 Apr 2009 00:16:20 +0000","Tue, 24 Aug 2010 20:37:05 +0000","Wed, 29 Apr 2009 04:11:21 +0000",1569301,"While TestReduceFetch verifies the reduce semantics for reducing from in-memory segments, it does not validate the data it reads. Data corrupted during the merge will not be detected.",-0.4,-0.4,negative
hadoop,5701,comment_0,"Consider that a cluster have 2000 map slots and jobs submitted in the following sequence: |1:00pm|JobA|1500 maps, each map runs 24 hours| |1:30pm|JobB|1000 maps, each map runs 2 hours| |1:40pm|JobC|3000 maps, each map runs 10 minutes| Then, all 1500 maps in JobA got scheduled and only 500 map slots remained in the cluster at 1pm. 30 minutes later, JobB came and only 500 maps slots got scheduled. At 1:40pm, JobC came but no maps got scheduled until some maps in JobB finished 2 hours later. In this cases, JobA always has 75% of the capacity, JobB and JobC never able to obtain 1/N of the capacity. If JobA has 2000 maps, other jobs have to wait for maps in JobA to finish and have no progress in 24 hours.",design_debt,non-optimal_design,"Fri, 17 Apr 2009 18:49:24 +0000","Wed, 8 Jul 2009 16:41:03 +0000","Fri, 12 Jun 2009 00:07:10 +0000",4771066,"Consider that a cluster have 2000 map slots and jobs submitted in the following sequence: Then, all 1500 maps in JobA got scheduled and only 500 map slots remained in the cluster at 1pm. 30 minutes later, JobB came and only 500 maps slots got scheduled. At 1:40pm, JobC came but no maps got scheduled until some maps in JobB finished 2 hours later. In this cases, JobA always has 75% of the capacity, JobB and JobC never able to obtain 1/N of the capacity. If JobA has 2000 maps, other jobs have to wait for maps in JobA to finish and have no progress in 24 hours.",-0.272,-0.272,neutral
hadoop,5701,summary,"With fair scheduler, long running jobs can easily occurpy a lot of task slots",design_debt,non-optimal_design,"Fri, 17 Apr 2009 18:49:24 +0000","Wed, 8 Jul 2009 16:41:03 +0000","Fri, 12 Jun 2009 00:07:10 +0000",4771066,"With fair scheduler, long running jobs can easily occurpy a lot of task slots",0.4,0.4,positive
hadoop,5891,comment_0,"this whole problem of bootstrapping a cluster where machines don't know who they are is pretty brittle right now. In an ideal world, even the NN would be able to work out its name/address and share it with the rest, but failing that, having everything else work out the details by asking the NN would be handy. It would also be good if everything provided (in the same process and via JMX) a list of (service, address, port) for all the different things that the node runs. I try to reverse engineer that, but it adds more scheduling problems (don't start the downstream nodes until the NN and JT are live), and for some reason jetty comes up bonded to 0:0:0:0:0:1 on one machine, which is particularly irritating. so: +1 to this, I can see the BackupNode having the same problem on scaled up, as it needs to know both the NN and 2N addresses (note addresses, not hostnames. Maybe we should open this up to a general ""nodes to come up better on an under-configured network"" bugrep which those of us who do underconfigure their networks can deal with.",design_debt,non-optimal_design,"Fri, 22 May 2009 00:59:06 +0000","Tue, 24 Aug 2010 20:38:05 +0000","Tue, 2 Jun 2009 06:05:25 +0000",968779,"this whole problem of bootstrapping a cluster where machines don't know who they are is pretty brittle right now. In an ideal world, even the NN would be able to work out its name/address and share it with the rest, but failing that, having everything else work out the details by asking the NN would be handy. It would also be good if everything provided (in the same process and via JMX) a list of (service, address, port) for all the different things that the node runs. I try to reverse engineer that, but it adds more scheduling problems (don't start the downstream nodes until the NN and JT are live), and for some reason jetty comes up bonded to 0:0:0:0:0:1 on one machine, which is particularly irritating. so: +1 to this, I can see the BackupNode having the same problem on scaled up, as it needs to know both the NN and 2N addresses (note addresses, not hostnames. Maybe we should open this up to a general ""nodes to come up better on an under-configured network"" bugrep which those of us who do underconfigure their networks can deal with.",0.09002777778,0.09002777778,negative
hadoop,5891,comment_1,"Steve: I agree that service location can be improved across the board. However, I don't think it's necessarily a good idea to overload the NameNode as a service name daemon. Personally, I'd prefer to use something like ZooKeeper here. Obviously there needs to be at least one host that is in a ""well-known"" location, which could be configured by default as a ""hadoop-zk"" hostname which has multiple A records pointing to all of the ZK nodes. Anyway, I agree that we should work towards the ideal goal, but I'd like to have that discussion in a new JIRA. This one is a very simple fix whereas that one could be pretty significant.",design_debt,non-optimal_design,"Fri, 22 May 2009 00:59:06 +0000","Tue, 24 Aug 2010 20:38:05 +0000","Tue, 2 Jun 2009 06:05:25 +0000",968779,"Steve: I agree that service location can be improved across the board. However, I don't think it's necessarily a good idea to overload the NameNode as a service name daemon. Personally, I'd prefer to use something like ZooKeeper here. Obviously there needs to be at least one host that is in a ""well-known"" location, which could be configured by default as a ""hadoop-zk"" hostname which has multiple A records pointing to all of the ZK nodes. Anyway, I agree that we should work towards the ideal goal, but I'd like to have that discussion in a new JIRA. This one is a very simple fix whereas that one could be pretty significant.",0.1129166667,0.1129166667,neutral
hadoop,6198,comment_1,"Granted, but supporting two filtering interfaces will likely cause maintenance headaches and be more of a burden to a FS implementor; I'd rather pick an API and not support all possible variants. If a FileSystem is more efficient with path-based filtering, it can still work with FileStatus objects, either populating them lazily, filling them with defaults (what many shims do anyway), or even failing if a user queries unsupported data. Since globStatus returns FileStatus objects, any implementation will need to construct them for the set of accepted Paths, anyway. Given that the API seems biased toward FileStatus objects, I'd rather endure a penalty for the hypothetical FS that doesn't return this information, rather than maintain two separate filtering APIs.",design_debt,non-optimal_design,"Tue, 18 Aug 2009 05:46:36 +0000","Thu, 24 Jul 2014 19:57:11 +0000","Thu, 24 Jul 2014 19:57:11 +0000",155657435,"Granted, but supporting two filtering interfaces will likely cause maintenance headaches and be more of a burden to a FS implementor; I'd rather pick an API and not support all possible variants. If a FileSystem is more efficient with path-based filtering, it can still work with FileStatus objects, either populating them lazily, filling them with defaults (what many shims do anyway), or even failing if a user queries unsupported data. Since globStatus returns FileStatus objects, any implementation will need to construct them for the set of accepted Paths, anyway. Given that the API seems biased toward FileStatus objects, I'd rather endure a penalty for the hypothetical FS that doesn't return this information, rather than maintain two separate filtering APIs.",-0.0699375,-0.0699375,negative
hadoop,6198,description,"There's an avoidable overhead in listing/globbing items with some property (e.g. owned by user foo, only files, files larger than _n_ bytes, etc.). Internally, the Path is extracted from a FileStatus object and passed to the PathFilter; simply passing the FileStatus object would allow one to filter on the information in the status object.",design_debt,non-optimal_design,"Tue, 18 Aug 2009 05:46:36 +0000","Thu, 24 Jul 2014 19:57:11 +0000","Thu, 24 Jul 2014 19:57:11 +0000",155657435,"There's an avoidable overhead in listing/globbing items with some property (e.g. owned by user foo, only files, files larger than n bytes, etc.). Internally, the Path is extracted from a FileStatus object and passed to the PathFilter; simply passing the FileStatus object would allow one to filter on the information in the status object.",0.06425,0.06425,neutral
hadoop,6304,comment_2,"I am -1 on this patch. As seen in MAPREDUCE-2238, this pattern is dangerous as it temporarily drops directories into u-x or u-r territory which can screw up other processes working inside. I think HADOOP-7110 is the right solution for performance, and forking as we're doing now is fine for people who don't care about performance.",design_debt,non-optimal_design,"Fri, 9 Oct 2009 06:07:30 +0000","Fri, 21 Jan 2011 08:34:28 +0000","Fri, 21 Jan 2011 08:34:28 +0000",40530418,"I am -1 on this patch. As seen in MAPREDUCE-2238, this pattern is dangerous as it temporarily drops directories into u-x or u-r territory which can screw up other processes working inside. I think HADOOP-7110 is the right solution for performance, and forking as we're doing now is fine for people who don't care about performance.",0.03558333333,0.03558333333,negative
hadoop,6364,description,There was a bit of talk about HDFS-34 last night at Apachecon. One of the points brought up was the difficulty in assuming that the 'hostname' was externally available. Perhaps what needs to happen instead is to follow what httpd and a few other apps do. That is provide a way for the service to know what hostname to advertise itself as and use in all communications. This could also help solve the multi-nic problems (HADOOP-6210) faced when using hadoop in a HA environment.,design_debt,non-optimal_design,"Thu, 5 Nov 2009 17:30:50 +0000","Thu, 22 Mar 2012 06:02:21 +0000","Wed, 2 Nov 2011 18:22:22 +0000",62815892,There was a bit of talk about HDFS-34 last night at Apachecon. One of the points brought up was the difficulty in assuming that the 'hostname' was externally available. Perhaps what needs to happen instead is to follow what httpd and a few other apps do. That is provide a way for the service to know what hostname to advertise itself as and use in all communications. This could also help solve the multi-nic problems (HADOOP-6210) faced when using hadoop in a HA environment.,-0.1279,-0.1279,neutral
hadoop,6589,description,"Currently when authentication fails, RPC server simply closes the connection. Sending certain error messages back to the client may help user debug the problem. Of course, those error messages that are sent back shouldn't compromise system security.",design_debt,non-optimal_design,"Mon, 22 Feb 2010 19:52:34 +0000","Tue, 24 Aug 2010 20:42:15 +0000","Sat, 27 Feb 2010 06:17:38 +0000",383104,"Currently when authentication fails, RPC server simply closes the connection. Sending certain error messages back to the client may help user debug the problem. Of course, those error messages that are sent back shouldn't compromise system security.",-0.2833333333,-0.2833333333,neutral
hadoop,6589,summary,Better error messages for RPC clients when authentication fails,design_debt,non-optimal_design,"Mon, 22 Feb 2010 19:52:34 +0000","Tue, 24 Aug 2010 20:42:15 +0000","Sat, 27 Feb 2010 06:17:38 +0000",383104,Better error messages for RPC clients when authentication fails,-0.1,-0.1,neutral
hadoop,659,comment_4,"I am OK to priotize blocks that have less than 1/3 of their replicas in place. Then I would introduce 3 priority levels. The blocks that have only one replica have the highest priority, followed by blocks having less than 1/3 replicas, and then followed by the rest of the blocks. The data structures that suggested by Konstantin has the performance advantage. But I went through some code design and felt that the data structures and manipulations were quite complicated. It's not a simple solution. I am convinced that it is the right way to go. Besides, adding blocks to the begining or the end of the priority list is not an extensible solution when we have more than 2 priority levels.",design_debt,non-optimal_design,"Tue, 31 Oct 2006 20:29:31 +0000","Wed, 8 Jul 2009 16:42:06 +0000","Thu, 25 Jan 2007 22:59:13 +0000",7439382,"I am OK to priotize blocks that have less than 1/3 of their replicas in place. Then I would introduce 3 priority levels. The blocks that have only one replica have the highest priority, followed by blocks having less than 1/3 replicas, and then followed by the rest of the blocks. The data structures that suggested by Konstantin has the performance advantage. But I went through some code design and felt that the data structures and manipulations were quite complicated. It's not a simple solution. I am convinced that it is the right way to go. Besides, adding blocks to the begining or the end of the priority list is not an extensible solution when we have more than 2 priority levels.",0.09333333333,0.09333333333,neutral
hadoop,659,description,"I see two types of replications that should be accelerated compared to all others. 1. Blocks that have only one remaining copy (but are required to have higher replication). 2. Blocks that have less than 1/3 of their replicas in place. The latter occurs when map/reduce sets replication of certain files to 10, and we want it happen fast to achieve better performance on the tasks. So I think we should distinguish two major groups of under-replicated blocks: first-priority (having only 1 copy or less than 1/3 of required replicas), and the rest. The name-node places first-priority blocks into the beginning of the neededReplication list, and the rest are placed at the end. That way the first-priority blocks will be replicated first and then the others.",design_debt,non-optimal_design,"Tue, 31 Oct 2006 20:29:31 +0000","Wed, 8 Jul 2009 16:42:06 +0000","Thu, 25 Jan 2007 22:59:13 +0000",7439382,"I see two types of replications that should be accelerated compared to all others. 1. Blocks that have only one remaining copy (but are required to have higher replication). 2. Blocks that have less than 1/3 of their replicas in place. The latter occurs when map/reduce sets replication of certain files to 10, and we want it happen fast to achieve better performance on the tasks. So I think we should distinguish two major groups of under-replicated blocks: first-priority (having only 1 copy or less than 1/3 of required replicas), and the rest. The name-node places first-priority blocks into the beginning of the neededReplication list, and the rest are placed at the end. That way the first-priority blocks will be replicated first and then the others.",-0.02222222222,-0.02222222222,neutral
hadoop,6794,comment_2,"The following need to be added to the patch. - configuration property is not present in common-project but is present and needed in mapreduce. - Nine settings for 'Job Summary Appender' are present and needed in mapreduce project but are not there in common project. - Two settings for mapreduce task logs are present and needed in mapreduce project but are not there in common project. -- -- - One more thing: copying this file over from mapreduce will not work because six settings for 'Security appender' are in common but not present in the mapreduce project's log4j.properties file. May be we should also split both the above files per project. But that may be long term, will open a separate ticket if you agree. Other issues - Minor: bin/rcc still refers to hadoop-core at one place. - when i run `ant binary` in hdfs bin directory isn't getting copied into the packaged binary, neither are the hdfs script files copied. - when i run `ant binary` in mapreduce, files aren't getting copied into the packaged bin directory Verified the rest of the changes in all the three patches, they look good. Still couldn't get the cluster up with different installation directories, will be trying in the meanwhile..",design_debt,non-optimal_design,"Mon, 31 May 2010 21:42:07 +0000","Tue, 24 Aug 2010 20:43:17 +0000","Thu, 10 Jun 2010 23:48:08 +0000",871561,"The following need to be added to the patch. conf/hadoop-policy.xml.template: security.admin.operations.protocol.acl configuration property is not present in common-project but is present and needed in mapreduce. conf/log4j.properties Nine settings for 'Job Summary Appender' are present and needed in mapreduce project but are not there in common project. Two settings for mapreduce task logs are present and needed in mapreduce project but are not there in common project. hadoop.tasklog.iscleanup=false log4j.appender.TLA.isCleanup=${hadoop.tasklog.iscleanup} One more thing: copying this file over from mapreduce will not work because six settings for 'Security appender' are in common but not present in the mapreduce project's log4j.properties file. May be we should also split both the above files per project. But that may be long term, will open a separate ticket if you agree. Other issues Minor: bin/rcc still refers to hadoop-core at one place. when i run `ant binary` in hdfs bin directory isn't getting copied into the packaged binary, neither are the hdfs script files copied. when i run `ant binary` in mapreduce, files aren't getting copied into the packaged bin directory Verified the rest of the changes in all the three patches, they look good. Still couldn't get the cluster up with different installation directories, will be trying in the meanwhile..",-0.04626190476,-0.01117307692,neutral
hadoop,6869,comment_4,In continuation of the comment above: since there's {{getFS()}} method which exposes filesystem API from a particular daemon then creating additional 4 wrappers around the standard API doesn't make much sense to me. These wrappers are too obvious and then will simply crowd Herriot API without adding much value.,design_debt,non-optimal_design,"Wed, 21 Jul 2010 17:05:23 +0000","Mon, 27 Sep 2010 04:14:15 +0000","Wed, 28 Jul 2010 03:23:10 +0000",555467,In continuation of the comment above: since there's getFS() method which exposes filesystem API from a particular daemon then creating additional 4 wrappers around the standard API doesn't make much sense to me. These wrappers are too obvious and then will simply crowd Herriot API without adding much value.,-0.1,-0.1,negative
hadoop,6988,comment_9,"I'm sorry, but it is completely short sighted to have a single use env var like this. If we need to modify the tasks environment for something else, are we going to introduce another environment variable? How many are too many?",design_debt,non-optimal_design,"Mon, 4 Oct 2010 23:28:30 +0000","Tue, 24 May 2011 19:12:40 +0000","Tue, 24 May 2011 19:12:40 +0000",20029450,"I'm sorry, but it is completely short sighted to have a single use env var like this. If we need to modify the tasks environment for something else, are we going to introduce another environment variable? How many are too many?",-0.15,-0.15,negative
hadoop,7154,comment_10,"I was very confused by this discussion and dug into it a bit more; here's what I learned. The takeaway is, ARENA_MAX=4 is a win for Java apps. # Java doesn't use {{malloc()}} for object allocations; instead it uses its own directly {{mmap()}}ed arenas. # however, a few things such as direct {{ByteBuffer}}s do end up calling malloc on arbitrary threads. There's not much thread locality in the use of such buffers. As a result, the glibc arena allocator is using a lot of VSS to optimize a codepath that's not very hot. So decreasing the number of arenas is a win, overall, even though it will increase contention (the malloc arena locks are pretty cold so this doesn't matter much) and potentially increase cache churn. But fewer arenas should decrease total cache footprint by increasing reuse.",design_debt,non-optimal_design,"Fri, 25 Feb 2011 00:48:56 +0000","Tue, 21 Apr 2015 21:28:17 +0000","Tue, 8 Mar 2011 23:51:52 +0000",1033376,"I was very confused by this discussion and dug into it a bit more; here's what I learned. The takeaway is, ARENA_MAX=4 is a win for Java apps. Java doesn't use malloc() for object allocations; instead it uses its own directly {{mmap()}}ed arenas. however, a few things such as direct {{ByteBuffer}}s do end up calling malloc on arbitrary threads. There's not much thread locality in the use of such buffers. As a result, the glibc arena allocator is using a lot of VSS to optimize a codepath that's not very hot. So decreasing the number of arenas is a win, overall, even though it will increase contention (the malloc arena locks are pretty cold so this doesn't matter much) and potentially increase cache churn. But fewer arenas should decrease total cache footprint by increasing reuse.",0.131125,0.131125,neutral
hadoop,7358,description,"When a server implementation throws an exception handling an RPC, the Handler thread catches it and logs it before responding with the exception over the IPC channel. This is currently done at INFO level. I'd like to propose that, if the exception is an unchecked exception, it should be logged at WARN level instead. This can help alert operators when they might be hitting some kind of bug.",design_debt,non-optimal_design,"Fri, 3 Jun 2011 21:45:39 +0000","Mon, 16 Mar 2015 19:22:44 +0000","Thu, 24 Nov 2011 01:57:03 +0000",14962284,"When a server implementation throws an exception handling an RPC, the Handler thread catches it and logs it before responding with the exception over the IPC channel. This is currently done at INFO level. I'd like to propose that, if the exception is an unchecked exception, it should be logged at WARN level instead. This can help alert operators when they might be hitting some kind of bug.",-0.08125,-0.08125,neutral
hadoop,7358,summary,Improve log levels when exceptions caught in RPC handler,design_debt,non-optimal_design,"Fri, 3 Jun 2011 21:45:39 +0000","Mon, 16 Mar 2015 19:22:44 +0000","Thu, 24 Nov 2011 01:57:03 +0000",14962284,Improve log levels when exceptions caught in RPC handler,0.4,0.4,neutral
hadoop,7483,comment_1,"Well, as allen said on HADOOP-6605, ""My technical objection is that there is a high likelihood of getting this wrong."" There's a serious risk of the JAVA_HOME autodetector becoming a major maintenance mess. This is the beginning.",design_debt,non-optimal_design,"Mon, 25 Jul 2011 20:05:01 +0000","Thu, 17 Mar 2016 16:41:40 +0000","Thu, 17 Mar 2016 16:41:40 +0000",146608599,"Well, as allen said on HADOOP-6605, ""My technical objection is that there is a high likelihood of getting this wrong."" There's a serious risk of the JAVA_HOME autodetector becoming a major maintenance mess. This is the beginning.",0.06494444444,0.06494444444,negative
hadoop,758,comment_5,"Rather than ignore all those exceptions, wouldn't it be better to at least log them? Also, I'm not sure we need to proceed with all cleanups if any fail. And we shouldn't replicate the cleanup code. The problem is that exceptions in cleanups are masking the exception thrown in the body. Wouldn't something like the following work? IOException ioe = null; try { ... body ... } catch (IOException e) { ioe = e; throw e; } finally { try { ... cleanups... } catch (IOException e) { if (ioe != null) LOG.warn(e) else throw e; } if (ioe != null) throw ioe; }",design_debt,non-optimal_design,"Wed, 29 Nov 2006 05:41:43 +0000","Wed, 8 Jul 2009 16:42:11 +0000","Thu, 25 Jan 2007 21:31:03 +0000",4981760,"Rather than ignore all those exceptions, wouldn't it be better to at least log them? Also, I'm not sure we need to proceed with all cleanups if any fail. And we shouldn't replicate the cleanup code. The problem is that exceptions in cleanups are masking the exception thrown in the body. Wouldn't something like the following work? IOException ioe = null; try { ... body ... } catch (IOException e) { ioe = e; throw e; } finally { try { ... cleanups... } catch (IOException e) { if (ioe != null) LOG.warn(e) else throw e; } if (ioe != null) throw ioe; }",-0.15,-0.15,negative
hadoop,758,comment_8,"I think we should make a good-faith effort to cleanup: user-errors can be common, and this also runs under LocalRunner, not always as a separate process. If there are errors in the cleanups we should log these as warnings, since they should not occur. Which raises the related question: why did the cleanup fail? Closing an open file shouldn't throw an exception. That looks like a bug in DFSClient, no?",design_debt,non-optimal_design,"Wed, 29 Nov 2006 05:41:43 +0000","Wed, 8 Jul 2009 16:42:11 +0000","Thu, 25 Jan 2007 21:31:03 +0000",4981760,"I think we should make a good-faith effort to cleanup: user-errors can be common, and this also runs under LocalRunner, not always as a separate process. If there are errors in the cleanups we should log these as warnings, since they should not occur. Which raises the related question: why did the cleanup fail? Closing an open file shouldn't throw an exception. That looks like a bug in DFSClient, no?",-0.1416,-0.1416,neutral
hadoop,7620,comment_2,"I agree with Alejandro. Build turnaround time is a big pain, and especially on non-SSD, or even worse, NFS, the javadoc build takes quite a while. Lots of iops to create all those .html files. To check that the javadoc changes don't introduce issues, we have test-patch.",design_debt,non-optimal_design,"Fri, 9 Sep 2011 18:01:17 +0000","Sun, 26 Apr 2015 01:26:52 +0000","Sun, 26 Apr 2015 01:26:52 +0000",114420335,"I agree with Alejandro. Build turnaround time is a big pain, and especially on non-SSD, or even worse, NFS, the javadoc build takes quite a while. Lots of iops to create all those .html files. To check that the javadoc changes don't introduce issues, we have test-patch.",-0.03666666667,-0.03666666667,negative
hadoop,7620,description,"Currently, the default profile doesn't generate the javadoc, which gives the developer a false sense of security. Leaving the forrest stuff in the doc profile makes sense.",design_debt,non-optimal_design,"Fri, 9 Sep 2011 18:01:17 +0000","Sun, 26 Apr 2015 01:26:52 +0000","Sun, 26 Apr 2015 01:26:52 +0000",114420335,"Currently, the default profile doesn't generate the javadoc, which gives the developer a false sense of security. Leaving the forrest stuff in the doc profile makes sense.",-0.175,-0.175,negative
hadoop,7985,comment_0,In * Touchz.java should be renamed to Touch.java to avoid compilation everytime. * Record IO generated test files which were compiled everytime. We could modify RccTask.java to doCompile only when sourceFile is newer than the destination file These are just 2 seconds of the 24 seconds needed to build hadoop-common. So obviously not substantial. Will keep looking. I'm beginning to think running from JARs might not be the best idea to have a quick dev cycle. Maybe I should try running from the target/classes directories and skip building the jar altogether.,design_debt,non-optimal_design,"Thu, 19 Jan 2012 21:14:41 +0000","Thu, 15 Aug 2013 19:02:33 +0000","Thu, 15 Aug 2013 19:02:33 +0000",49585672,In hadoop-common-project/hadoop-common: Touchz.java should be renamed to Touch.java to avoid compilation everytime. Record IO generated test files which were compiled everytime. We could modify RccTask.java to doCompile only when sourceFile is newer than the destination file These are just 2 seconds of the 24 seconds needed to build hadoop-common. So obviously not substantial. Will keep looking. I'm beginning to think running from JARs might not be the best idea to have a quick dev cycle. Maybe I should try running from the target/classes directories and skip building the jar altogether.,0.0475,0.0475,neutral
hadoop,7985,comment_1,all protobuf compilation sections in pom.xml can be modified like this I am now trying to optimize mvn -P-cbuild -DskipTests -X compile Even here jsps are being compiled into java files every time unnecessarily,design_debt,non-optimal_design,"Thu, 19 Jan 2012 21:14:41 +0000","Thu, 15 Aug 2013 19:02:33 +0000","Thu, 15 Aug 2013 19:02:33 +0000",49585672,all protobuf compilation sections in pom.xml can be modified like this I am now trying to optimize mvn -P-cbuild -Dmaven.javadoc.skip -DskipTests -X compile Even here jsps are being compiled into java files every time unnecessarily,-0.25,-0.125,neutral
hadoop,7985,comment_3,Just a lil' bit of improvement. Still a long ways to go,design_debt,non-optimal_design,"Thu, 19 Jan 2012 21:14:41 +0000","Thu, 15 Aug 2013 19:02:33 +0000","Thu, 15 Aug 2013 19:02:33 +0000",49585672,Just a lil' bit of improvement. Still a long ways to go,0.46625,0.46625,neutral
hadoop,8209,description,"In 1.x DNs currently refuse to connect to NNs if their build *revision* (ie svn revision) do not match. TTs refuse to connect to JTs if their build *version* (version, revision, user, and source checksum) do not match. This prevents rolling upgrades, which is intentional, see the discussion in HADOOP-5203. The primary motivation in that jira was (1) it's difficult to guarantee every build on a large cluster got deployed correctly, builds don't get rolled back to old versions by accident etc, and (2) mixed versions can lead to execution problems that are hard to debug. However there are also cases when users know they two builds are compatible, eg when deploying a new build which contains the same contents as the previous one, plus a critical security patch that does not affect compatibility. Currently deploying a 1 line patch requires taking down the entire cluster (or trying to work around the issue by lying about the build revision or checksum, yuck). These users would like to be able to perform a rolling upgrade. In order to support this, let's add an option that is off by default, but, when enabled, makes the DN and TT version check just check for an exact version match (eg ""1.0.2"") but ignore the build revision (DN) and the source checksum (TT). Two builds still need to match the major, minor, and point numbers, but nothing else.",design_debt,non-optimal_design,"Sun, 25 Mar 2012 22:17:38 +0000","Wed, 24 Oct 2012 19:12:09 +0000","Thu, 12 Apr 2012 22:06:37 +0000",1554539,"In 1.x DNs currently refuse to connect to NNs if their build revision (ie svn revision) do not match. TTs refuse to connect to JTs if their build version (version, revision, user, and source checksum) do not match. This prevents rolling upgrades, which is intentional, see the discussion in HADOOP-5203. The primary motivation in that jira was (1) it's difficult to guarantee every build on a large cluster got deployed correctly, builds don't get rolled back to old versions by accident etc, and (2) mixed versions can lead to execution problems that are hard to debug. However there are also cases when users know they two builds are compatible, eg when deploying a new build which contains the same contents as the previous one, plus a critical security patch that does not affect compatibility. Currently deploying a 1 line patch requires taking down the entire cluster (or trying to work around the issue by lying about the build revision or checksum, yuck). These users would like to be able to perform a rolling upgrade. In order to support this, let's add an option that is off by default, but, when enabled, makes the DN and TT version check just check for an exact version match (eg ""1.0.2"") but ignore the build revision (DN) and the source checksum (TT). Two builds still need to match the major, minor, and point numbers, but nothing else.",0.09700285714,0.09700285714,neutral
hadoop,829,description,"In the existing implementation, the Namenode has a data structure called DatanodeDescriptor. It is used to serialize contents of the Datanode while writing it to the fsimage. The same serialization method is used to send a Datanode object to the Datanode and/or the Client. This introduces the shortcoming that one cannot introduce non-persistent fields in the DatanodeDescriptor. One solution is to separate out the following two functionality into two separate classes: 1. The fields from a Datanode that are part of the ClientProtocol and/or DatanodeProcotol. 2. The fields from a Datanode that are stored in the fsImage.",design_debt,non-optimal_design,"Fri, 15 Dec 2006 05:10:13 +0000","Wed, 8 Jul 2009 16:42:13 +0000","Mon, 18 Dec 2006 20:59:35 +0000",316162,"In the existing implementation, the Namenode has a data structure called DatanodeDescriptor. It is used to serialize contents of the Datanode while writing it to the fsimage. The same serialization method is used to send a Datanode object to the Datanode and/or the Client. This introduces the shortcoming that one cannot introduce non-persistent fields in the DatanodeDescriptor. One solution is to separate out the following two functionality into two separate classes: 1. The fields from a Datanode that are part of the ClientProtocol and/or DatanodeProcotol. 2. The fields from a Datanode that are stored in the fsImage.",-0.17025,-0.17025,neutral
hadoop,8304,comment_10,"This adds a method to an interface which is an incompatible change, and will break implementers. Is the overhead of wrapping a string in a list causing performance problems?",design_debt,non-optimal_design,"Mon, 23 Apr 2012 10:35:52 +0000","Wed, 23 May 2012 20:15:50 +0000","Tue, 8 May 2012 21:14:32 +0000",1334320,"This adds a method to an interface which is an incompatible change, and will break implementers. Is the overhead of wrapping a string in a list causing performance problems?",-0.2,-0.2,negative
hadoop,8304,description,DNSToSwitchMapping now has only one API to resolve a host list: public List<String> names). But the two major caller: and are taking single host name but have to wrapper it to an single entry ArrayList. This is not necessary especially the host has been cached before.,design_debt,non-optimal_design,"Mon, 23 Apr 2012 10:35:52 +0000","Wed, 23 May 2012 20:15:50 +0000","Tue, 8 May 2012 21:14:32 +0000",1334320,DNSToSwitchMapping now has only one API to resolve a host list: public List<String> resolve(List<String> names). But the two major caller: RackResolver.resolve() and DatanodeManager.resolveNetworkLocation() are taking single host name but have to wrapper it to an single entry ArrayList. This is not necessary especially the host has been cached before.,0.4833333333,0.4,neutral
hadoop,8316,description,"HADOOP-7633 made hdfs, mr and security audit logging on by default (INFO level) in log4j.properties used for the packages, this then got copied over to the non-packaging log4j.properties in HADOOP-8216 (which made them consistent). Seems like we should keep with the v1.x setting which is disabled (WARNING level) by default. There's a performance overhead to audit logging, and HADOOP-7633 provided not rationale (just ""We should add the audit logs as part of default confs"") as to why they were enabled for the packages.",design_debt,non-optimal_design,"Thu, 26 Apr 2012 02:34:24 +0000","Thu, 11 Oct 2012 17:45:05 +0000","Fri, 11 May 2012 19:25:54 +0000",1356690,"HADOOP-7633 made hdfs, mr and security audit logging on by default (INFO level) in log4j.properties used for the packages, this then got copied over to the non-packaging log4j.properties in HADOOP-8216 (which made them consistent). Seems like we should keep with the v1.x setting which is disabled (WARNING level) by default. There's a performance overhead to audit logging, and HADOOP-7633 provided not rationale (just ""We should add the audit logs as part of default confs"") as to why they were enabled for the packages.",-0.2583333333,-0.2583333333,neutral
hadoop,8395,description,"Text from Display set of Shell commands (hadoop fs -text), has a strict subclass check for a loaded key class to be a subclass of WritableComparable. The sequence file writer itself has no such checks (one can create sequence files with just plain writable keys, comparable is needed for sequence file's sorter alone, which not all of them use always), and hence its not reasonable for Text command to carry it either. We should relax the check and simply just check for ""Writable"", not",design_debt,non-optimal_design,"Fri, 11 May 2012 19:58:28 +0000","Thu, 12 May 2016 18:23:00 +0000","Sat, 12 May 2012 06:04:19 +0000",36351,"Text from Display set of Shell commands (hadoop fs -text), has a strict subclass check for a sequence-file-header loaded key class to be a subclass of WritableComparable. The sequence file writer itself has no such checks (one can create sequence files with just plain writable keys, comparable is needed for sequence file's sorter alone, which not all of them use always), and hence its not reasonable for Text command to carry it either. We should relax the check and simply just check for ""Writable"", not ""WritableComparable"".",0.1453333333,0.1453333333,neutral
hadoop,8395,summary,Text shell command unnecessarily demands that a SequenceFile's key class be WritableComparable,design_debt,non-optimal_design,"Fri, 11 May 2012 19:58:28 +0000","Thu, 12 May 2016 18:23:00 +0000","Sat, 12 May 2012 06:04:19 +0000",36351,Text shell command unnecessarily demands that a SequenceFile's key class be WritableComparable,0.15,0.15,negative
hadoop,8819,comment_6,"Note that the change in FTPFileSystem actually is a behavior change, and perhaps an incompatible one. All of the rest of these changes seem harmless, but that one seems a little suspect.",design_debt,non-optimal_design,"Sun, 16 Sep 2012 16:31:48 +0000","Thu, 12 May 2016 18:22:15 +0000","Mon, 17 Sep 2012 17:11:22 +0000",88774,"Note that the change in FTPFileSystem actually is a behavior change, and perhaps an incompatible one. All of the rest of these changes seem harmless, but that one seems a little suspect.",-0.1,-0.1,negative
hadoop,8895,description,TokenRenewer is a fully abstract class. Making it an interface will allow classes extending other classes to implement the interface.,design_debt,non-optimal_design,"Sat, 6 Oct 2012 03:55:55 +0000","Mon, 3 Nov 2014 18:33:59 +0000","Tue, 9 Oct 2012 18:33:46 +0000",311871,TokenRenewer is a fully abstract class. Making it an interface will allow classes extending other classes to implement the interface.,0.1,0.1,neutral
hadoop,8895,summary,"TokenRenewer should be an interface, it is currently a fully abstract class",design_debt,non-optimal_design,"Sat, 6 Oct 2012 03:55:55 +0000","Mon, 3 Nov 2014 18:33:59 +0000","Tue, 9 Oct 2012 18:33:46 +0000",311871,"TokenRenewer should be an interface, it is currently a fully abstract class",0,0,neutral
hadoop,8912,comment_7,"Ah, got it. Thanks for the explanation. It will help for those developers who clone the git repo and `git add ...' their files before generating a patch. In that case, it will help somewhat to commit this, but it won't solve the problem 100% of the time. Raja, do you happen to know if there's a semantically equivalent thing we could do for svn, to ensure that committers don't check in bad line endings?",design_debt,non-optimal_design,"Wed, 10 Oct 2012 18:09:30 +0000","Wed, 3 Sep 2014 23:07:19 +0000","Fri, 12 Oct 2012 05:02:44 +0000",125594,"Ah, got it. Thanks for the explanation. It will help for those developers who clone the git repo and `git add ...' their files before generating a patch. In that case, it will help somewhat to commit this, but it won't solve the problem 100% of the time. Raja, do you happen to know if there's a semantically equivalent thing we could do for svn, to ensure that committers don't check in bad line endings?",0.28,0.28,neutral
hadoop,8912,description,Source code in hadoop-common repo has a bunch of files that have CRLF endings. With more development happening on windows there is a higher chance of more CRLF files getting into the source tree. I would like to avoid that by creating .gitattributes file which prevents sources from having CRLF entries in text files. I am adding a couple of links here to give more primer on what exactly is the issue and how we are trying to fix it. # # This issue for adding .gitattributes file to the tree.,design_debt,non-optimal_design,"Wed, 10 Oct 2012 18:09:30 +0000","Wed, 3 Sep 2014 23:07:19 +0000","Fri, 12 Oct 2012 05:02:44 +0000",125594,Source code in hadoop-common repo has a bunch of files that have CRLF endings. With more development happening on windows there is a higher chance of more CRLF files getting into the source tree. I would like to avoid that by creating .gitattributes file which prevents sources from having CRLF entries in text files. I am adding a couple of links here to give more primer on what exactly is the issue and how we are trying to fix it. http://git-scm.com/docs/gitattributes#_checking_out_and_checking_in http://stackoverflow.com/questions/170961/whats-the-best-crlf-handling-strategy-with-git This issue for adding .gitattributes file to the tree.,-0.07142857143,-0.07142857143,negative
hadoop,922,description,A seek on a DFSInputStream causes causes the next read to re-open the socket connection to the datanode and fetch the remainder of the block all over again. This is not optimal. A small read followed by a small positive seek could re-utilize the data already fetched from the datanode as part of the previous read.,design_debt,non-optimal_design,"Wed, 24 Jan 2007 00:13:09 +0000","Wed, 8 Jul 2009 16:42:15 +0000","Tue, 30 Jan 2007 22:47:37 +0000",599668,A seek on a DFSInputStream causes causes the next read to re-open the socket connection to the datanode and fetch the remainder of the block all over again. This is not optimal. A small read followed by a small positive seek could re-utilize the data already fetched from the datanode as part of the previous read.,-0.2976666667,-0.2976666667,negative
hadoop,9259,comment_0,straightforward hardening of teardown logic,design_debt,non-optimal_design,"Wed, 16 Jan 2013 10:53:53 +0000","Thu, 18 Aug 2016 18:35:36 +0000","Mon, 25 Mar 2013 13:13:56 +0000",5883603,straightforward hardening of teardown logic,0,0,neutral
hadoop,9259,description,the teardown code in assumes that {{fs!=null}} and that it's OK to throw an exception if the delete operation fails. Better to check the {{fs}} value and catch and convert an exception in the {{fs.delete()}} operation to a {{LOG.error()}} instead. This will stop failures in teardown becoming a distraction from the root causes of the problem (that your FileSystem is broken),design_debt,non-optimal_design,"Wed, 16 Jan 2013 10:53:53 +0000","Thu, 18 Aug 2016 18:35:36 +0000","Mon, 25 Mar 2013 13:13:56 +0000",5883603,the teardown code in FileSystemContractBaseTest assumes that fs!=null and that it's OK to throw an exception if the delete operation fails. Better to check the fs value and catch and convert an exception in the fs.delete() operation to a LOG.error() instead. This will stop failures in teardown becoming a distraction from the root causes of the problem (that your FileSystem is broken),0.02397222222,0.02397222222,neutral
hadoop,9259,summary,should be less brittle in teardown,design_debt,non-optimal_design,"Wed, 16 Jan 2013 10:53:53 +0000","Thu, 18 Aug 2016 18:35:36 +0000","Mon, 25 Mar 2013 13:13:56 +0000",5883603,FileSystemContractBaseTest should be less brittle in teardown,0,0,neutral
hadoop,930,comment_8,"Thanks for the review Chris. It's to do with efficiency of listing directories. If you use mime type then you can't tell the difference between files and directories when listing bucket keys. So you have to query each key in a directory which can be prohibitively slow. But if you use the _$folder$ suffix convention (which S3Fox uses too BTW) you can easily distinguish files and directories. The code should be doing this. I agree that it's useful - in fact, the other s3 filesystem needs updating to do this too. Thanks for the tip. The code does detect this condition, but it might be nice to try to workaround as you say (perhaps emitting a warning). Have you done this elsewhere?",design_debt,non-optimal_design,"Thu, 25 Jan 2007 14:48:52 +0000","Thu, 2 May 2013 02:29:14 +0000","Fri, 6 Jun 2008 21:07:05 +0000",43049893,"Thanks for the review Chris. Any reason you didn't use the mime type to denote directory files (as jets3t does)? It's to do with efficiency of listing directories. If you use mime type then you can't tell the difference between files and directories when listing bucket keys. So you have to query each key in a directory which can be prohibitively slow. But if you use the _$folder$ suffix convention (which S3Fox uses too BTW) you can easily distinguish files and directories. I believe MD5 checksum should be set on s3 put (via header), and verified on s3 get. The code should be doing this. I agree that it's useful - in fact, the other s3 filesystem needs updating to do this too. Sometimes 'legacy' buckets have underscores, might consider trying to survive them. Thanks for the tip. The code does detect this condition, but it might be nice to try to workaround as you say (perhaps emitting a warning). Have you done this elsewhere?",0.17625,0.1355769231,neutral
hadoop,9336,description,"Querying is synch'ed and inefficient for short-lived RPC requests. Since the connection already contains the UGI, there should be a means to query it directly and avoid a call to",design_debt,non-optimal_design,"Tue, 26 Feb 2013 20:22:15 +0000","Thu, 12 May 2016 18:21:47 +0000","Thu, 28 Feb 2013 22:07:17 +0000",179102,"Querying UGI.getCurrentUser is synch'ed and inefficient for short-lived RPC requests. Since the connection already contains the UGI, there should be a means to query it directly and avoid a call to UGI.getCurrentUser.",-0.475,-0.2375,negative
hadoop,93,comment_0,"With such big input files the default logic should split things into dfs block-sized splits. Smaller splits should only be used if this would result in fewer than mapred.map.tasks splits. What value do you have for mapred.map.tasks in your mapred-default.xml? Let's make sure that is working before we add a new min.split.size feature. I don't oppose the feature, but it should be generating 356*30G/32M splits, not 356*30G/2K splits as you claim. That's still a lot of splits. If it is too many then we should add the feature you're adding. Note that, as a workaround, it is also easy to implement this w/o patching by defining an InputFormat that subclasses InputFormatBase and specifies a different minSplitSize. But making that a long is a good idea. So, in summary, can you please confirm that the actual number of splits that you object to is 356*30G/32M splits, not 356*30G/2K? Thanks.",design_debt,non-optimal_design,"Sat, 18 Mar 2006 03:59:07 +0000","Wed, 8 Jul 2009 16:51:41 +0000","Wed, 22 Mar 2006 01:38:19 +0000",337152,"With such big input files the default logic should split things into dfs block-sized splits. Smaller splits should only be used if this would result in fewer than mapred.map.tasks splits. What value do you have for mapred.map.tasks in your mapred-default.xml? Let's make sure that is working before we add a new min.split.size feature. I don't oppose the feature, but it should be generating 356*30G/32M splits, not 356*30G/2K splits as you claim. That's still a lot of splits. If it is too many then we should add the feature you're adding. Note that, as a workaround, it is also easy to implement this w/o patching by defining an InputFormat that subclasses InputFormatBase and specifies a different minSplitSize. But making that a long is a good idea. So, in summary, can you please confirm that the actual number of splits that you object to is 356*30G/32M splits, not 356*30G/2K? Thanks.",0.08340740741,0.08340740741,neutral
hadoop,93,comment_1,"From what I've seen, it is always 32M fragments, but that is still 300k input splits/maps, which is a lot. We'd like to be able to drop that by an order of magnitude. (I think in this case that the input splitter never finished, so we don't know.)",design_debt,non-optimal_design,"Sat, 18 Mar 2006 03:59:07 +0000","Wed, 8 Jul 2009 16:51:41 +0000","Wed, 22 Mar 2006 01:38:19 +0000",337152,"From what I've seen, it is always 32M fragments, but that is still 300k input splits/maps, which is a lot. We'd like to be able to drop that by an order of magnitude. (I think in this case that the input splitter never finished, so we don't know.)",0.175,0.175,neutral
hadoop,93,comment_2,"Doug, you are right. The number of splits we got was 356*30G/32M, but still too many.",design_debt,non-optimal_design,"Sat, 18 Mar 2006 03:59:07 +0000","Wed, 8 Jul 2009 16:51:41 +0000","Wed, 22 Mar 2006 01:38:19 +0000",337152,"Doug, you are right. The number of splits we got was 356*30G/32M, but still too many.",0.404,0.404,neutral
hadoop,9419,comment_0,"Never mind. I created a patch, and it is completely useless in fixing this problem. The tasks still OOM because the codec itself is so small and the MergeManager creates new codecs so quickly that on a job with lots of reduces it literally uses up all of the address space with direct byte buffers. Some of the processes get killed by the NM for going over the virtual address space before they OOM. We could try and have the CodecPool detect that the codec is doing the wrong thing and ""correct"" it for the codec, but that is too heavy handed in my opinion.",design_debt,non-optimal_design,"Tue, 19 Mar 2013 18:34:28 +0000","Tue, 19 Mar 2013 21:20:44 +0000","Tue, 19 Mar 2013 21:20:44 +0000",9976,"Never mind. I created a patch, and it is completely useless in fixing this problem. The tasks still OOM because the codec itself is so small and the MergeManager creates new codecs so quickly that on a job with lots of reduces it literally uses up all of the address space with direct byte buffers. Some of the processes get killed by the NM for going over the virtual address space before they OOM. We could try and have the CodecPool detect that the codec is doing the wrong thing and ""correct"" it for the codec, but that is too heavy handed in my opinion.",-0.0813,-0.0813,negative
hadoop,9440,comment_3,Why do we get a protobuf exception in the first place? We shouldn't be modifying the test to capture an IOException as we don't expect that to happen. Please correct my understanding if am wrong.,design_debt,non-optimal_design,"Thu, 28 Mar 2013 07:13:55 +0000","Fri, 17 Jan 2014 09:25:22 +0000","Fri, 12 Jul 2013 06:43:44 +0000",9156589,Why do we get a protobuf exception in the first place? We shouldn't be modifying the test to capture an IOException as we don't expect that to happen. Please correct my understanding if am wrong.,0.09166666667,0.09166666667,negative
hadoop,95,comment_0,"I posted a similar complaint in HADOOP-35, including a patch to make it possible to report on individual file health from the dfs -ls command. I agree that something needs to be added, if only to facilitate debugging of the various block-loss problems I've been seeing.",design_debt,non-optimal_design,"Sat, 18 Mar 2006 09:31:35 +0000","Wed, 8 Jul 2009 16:41:49 +0000","Wed, 18 Oct 2006 18:31:16 +0000",18521981,"I posted a similar complaint in HADOOP-35, including a patch to make it possible to report on individual file health from the dfs -ls command. I agree that something needs to be added, if only to facilitate debugging of the various block-loss problems I've been seeing.",-0.325,-0.325,neutral
hadoop,95,description,"Dfs needs a validation operation similiar to fsck, so that we get to know the files that are corrupted and which data blocks are missing. Dfs namenode also needs to log more specific information such as which block is replication or is deleted. So when something goes wrong, we have a clue what has happened.",design_debt,non-optimal_design,"Sat, 18 Mar 2006 09:31:35 +0000","Wed, 8 Jul 2009 16:41:49 +0000","Wed, 18 Oct 2006 18:31:16 +0000",18521981,"Dfs needs a validation operation similiar to fsck, so that we get to know the files that are corrupted and which data blocks are missing. Dfs namenode also needs to log more specific information such as which block is replication or is deleted. So when something goes wrong, we have a clue what has happened.",-0.2833333333,-0.2833333333,neutral
hadoop,9669,comment_0,Here is a more version that utilizes Java's ByteBuffer. It should be more efficient. The APIs are compatible with the previous version. The APIs might not be ideal towards an efficient implementation of XDR serialization / deserialization. I'm tracking my proposals to the APIs in a separate JIRA.,design_debt,non-optimal_design,"Tue, 25 Jun 2013 19:43:32 +0000","Tue, 24 Sep 2013 23:33:30 +0000","Wed, 18 Sep 2013 01:03:59 +0000",7276827,Here is a more version that utilizes Java's ByteBuffer. It should be more efficient. The APIs are compatible with the previous version. The APIs might not be ideal towards an efficient implementation of XDR serialization / deserialization. I'm tracking my proposals to the APIs in a separate JIRA.,0.125,0.125,neutral
hadoop,9674,description,"This problem was originally mentioned in discussion on HADOOP-8980. When calling initialization of the server's internal {{Listener}} and {{Reader}} threads happens in the background. This initialization is not guaranteed to complete by the time the caller returns from This may be misleading to a caller that expects the server has been fully initialized. This problem sometimes manifests as a test failure in This test looks at the stack frames of all running threads, expecting to find the {{Listener}} and {{Reader}} threads, but sometimes it doesn't find them.",design_debt,non-optimal_design,"Thu, 27 Jun 2013 04:38:37 +0000","Thu, 12 May 2016 18:21:58 +0000","Sun, 30 Jun 2013 17:36:27 +0000",305870,"This problem was originally mentioned in discussion on HADOOP-8980. When calling RPC#Server#start, initialization of the server's internal Listener and Reader threads happens in the background. This initialization is not guaranteed to complete by the time the caller returns from RPC#Server#start. This may be misleading to a caller that expects the server has been fully initialized. This problem sometimes manifests as a test failure in TestRPC#testStopsAllThreads. This test looks at the stack frames of all running threads, expecting to find the Listener and Reader threads, but sometimes it doesn't find them.",-0.0125,-0.2583333333,negative
hadoop,9748,comment_0,Minor change. This helps reduce unnecessary class lock contention between such common methods as and etc.,design_debt,non-optimal_design,"Thu, 18 Jul 2013 17:40:12 +0000","Thu, 12 May 2016 18:27:32 +0000","Fri, 19 Jul 2013 14:08:36 +0000",73704,"Minor change. This helps reduce unnecessary class lock contention between such common methods as UGI#getGroupNames and UGI.getCurrentUser, etc.",0.2,0.1333333333,neutral
hadoop,9748,comment_4,"ensureInitialized() forced many frequently called methods to unconditionally acquire the class lock. This patch certainly reduces the lock contention, but the highly contended getCurrentUser() can still block many threads. I understand this is being addressed in HADOOP-9749. There may be something else we can do to improve this for namenode. According to the ""worst"" jstack of a busy namenode I took, there were 29 BLOCKED handler threads. All of them were blocked on the UGI class lock. Here is the breakdown: - 2 ensureInitialized() - from non static synchronized methods. This Jira will unblock these. - 27 getCurrentUser() Among the 27 threads that were blocked at getCurrentUser(), - 18 - from in most namenode RPC methods - 8 - getBlockLocations() - 1 I think FSPermissionChecker can be modified to be created with a passed in UGI. FSNamesystem can the one already stored in RPC server by calling getRemoteUser(). This will eliminate a bulk of getCurrentUser() calls from namenode RPC handlers. A similar change can be made to mkdirs. Block token generation is not as straightforward. Even without it we can eliminate majority of the calls. We could potentially do the same for other RPC servers. I will file a HDFS jira for this. HADOOP-9749 is still needed since getCurrentUser() is used everywhere beyond namenode RPC server. +1 for this patch.",design_debt,non-optimal_design,"Thu, 18 Jul 2013 17:40:12 +0000","Thu, 12 May 2016 18:27:32 +0000","Fri, 19 Jul 2013 14:08:36 +0000",73704,"ensureInitialized() forced many frequently called methods to unconditionally acquire the class lock. This patch certainly reduces the lock contention, but the highly contended getCurrentUser() can still block many threads. I understand this is being addressed in HADOOP-9749. There may be something else we can do to improve this for namenode. According to the ""worst"" jstack of a busy namenode I took, there were 29 BLOCKED handler threads. All of them were blocked on the UGI class lock. Here is the breakdown: 2 ensureInitialized() - from non static synchronized methods. This Jira will unblock these. 27 getCurrentUser() Among the 27 threads that were blocked at getCurrentUser(), 18 FSPermissionChecker() - from FSNamesystem#getPermissionChecker() in most namenode RPC methods 8 BlockTokenSecretManager#generateToken() - getBlockLocations() 1 NameNodeRpcServer.mkdirs I think FSPermissionChecker can be modified to be created with a passed in UGI. FSNamesystem can the one already stored in RPC server by calling getRemoteUser(). This will eliminate a bulk of getCurrentUser() calls from namenode RPC handlers. A similar change can be made to mkdirs. Block token generation is not as straightforward. Even without it we can eliminate majority of the calls. We could potentially do the same for other RPC servers. I will file a HDFS jira for this. HADOOP-9749 is still needed since getCurrentUser() is used everywhere beyond namenode RPC server. +1 for this patch.",-0.03844117647,-0.03439473684,neutral
hadoop,9748,description,"EnsureInitialized is always sync'ed on the class, when it should only sync if it actually has to initialize.",design_debt,non-optimal_design,"Thu, 18 Jul 2013 17:40:12 +0000","Thu, 12 May 2016 18:27:32 +0000","Fri, 19 Jul 2013 14:08:36 +0000",73704,"EnsureInitialized is always sync'ed on the class, when it should only sync if it actually has to initialize.",0.5,0.5,neutral
hadoop,9763,comment_6,"The patch looks good to me. Some thoughts after discussing with : 1. Maybe we do not need to refresh a cache entry's access time and position in the priority queue when the entry is accessed, since the expected timeout on the client side is based on the time the first request is sent. 2. It would be better if we can add an upper limit for the size of the GSet. This can guarantee NN continues to function even when a large amount of retry requests come within a short period of time.",design_debt,non-optimal_design,"Sun, 21 Jul 2013 10:16:02 +0000","Tue, 27 Aug 2013 22:06:38 +0000","Wed, 24 Jul 2013 06:49:54 +0000",246832,"The patch looks good to me. Some thoughts after discussing with sureshms: 1. Maybe we do not need to refresh a cache entry's access time and position in the priority queue when the entry is accessed, since the expected timeout on the client side is based on the time the first request is sent. 2. It would be better if we can add an upper limit for the size of the GSet. This can guarantee NN continues to function even when a large amount of retry requests come within a short period of time.",0.246,0.246,positive
hadoop,9896,comment_11,"Another run timeout, this time with a bit more log I suspect there is race condition in Client or Server causing this.",design_debt,non-optimal_design,"Wed, 21 Aug 2013 23:21:54 +0000","Thu, 12 May 2016 18:21:44 +0000","Tue, 3 Sep 2013 21:10:43 +0000",1115329,"Another run timeout, this time with a bit more log I suspect there is race condition in Client or Server causing this.",-0.2,-0.2,negative
hadoop,9896,comment_12,", I think the timeout is a separate issue. In our case, if we run testRetryProxy alone, it never fails. However when running all the test cases in the test class together, we will always get the JVM crash error due to testRetryProxy hangs. It could be a race condition. What is your OS and configuration?  suspects your guys never run into our problem because you have faster machines. So it could also be that our Linux VM is slower and we never run into this timeout issue.",design_debt,non-optimal_design,"Wed, 21 Aug 2013 23:21:54 +0000","Thu, 12 May 2016 18:21:44 +0000","Tue, 3 Sep 2013 21:10:43 +0000",1115329,"macrokigol, I think the timeout is a separate issue. In our case, if we run testRetryProxy alone, it never fails. However when running all the test cases in the test class together, we will always get the JVM crash error due to testRetryProxy hangs. It could be a race condition. What is your OS and configuration? shanyu suspects your guys never run into our problem because you have faster machines. So it could also be that our Linux VM is slower and we never run into this timeout issue.",-0.04285714286,-0.04285714286,neutral
hadoop,9896,comment_3,I investigated this problem a little bit. I think there is some socket leak in the test code. Attach a patch that fix the leaking issue in the test by explicitly closing client and server at the end of each test case.,design_debt,non-optimal_design,"Wed, 21 Aug 2013 23:21:54 +0000","Thu, 12 May 2016 18:21:44 +0000","Tue, 3 Sep 2013 21:10:43 +0000",1115329,I investigated this problem a little bit. I think there is some socket leak in the test code. Attach a patch that fix the leaking issue in the test by explicitly closing client and server at the end of each test case.,-0.2,-0.2,negative
hadoop,9896,comment_7,"+1 Thanks Chuan! I tried the patch and it works fine! Not sure when lingering client thread is causing this problem though. Looks like sometimes the Server response was probably consumed by other client thus the testRetryProxy()'s client is waiting forever. But anyway, this patch fixed the TestIPC test case and I think they are good fix.",design_debt,non-optimal_design,"Wed, 21 Aug 2013 23:21:54 +0000","Thu, 12 May 2016 18:21:44 +0000","Tue, 3 Sep 2013 21:10:43 +0000",1115329,"+1 Thanks Chuan! I tried the patch and it works fine! Not sure when lingering client thread is causing this problem though. Looks like sometimes the Server response was probably consumed by other client thus the testRetryProxy()'s client is waiting forever. But anyway, this patch fixed the TestIPC test case and I think they are good fix.",0.5152,0.5152,neutral
hadoop,9896,comment_8,Attach a new patch. A 30 sec timeout is added to each test case. The unit test can pass consistently on my single core Ubuntu box. So I assume the timeout is enough for the test cases.,design_debt,non-optimal_design,"Wed, 21 Aug 2013 23:21:54 +0000","Thu, 12 May 2016 18:21:44 +0000","Tue, 3 Sep 2013 21:10:43 +0000",1115329,Attach a new patch. A 30 sec timeout is added to each test case. The unit test can pass consistently on my single core Ubuntu box. So I assume the timeout is enough for the test cases.,0.28125,0.28125,neutral
hadoop,98,comment_0,"This patch consolidate all of the places where status was put in to or out of the taskTrackers status map, so that the counts of maps and reduces are maintained consistently. I also added the cluster status to the webapp so that you can get a quick view of how busy the cluster is.",design_debt,non-optimal_design,"Thu, 23 Mar 2006 16:13:39 +0000","Wed, 8 Jul 2009 16:51:42 +0000","Fri, 24 Mar 2006 04:08:21 +0000",42882,"This patch consolidate all of the places where status was put in to or out of the taskTrackers status map, so that the counts of maps and reduces are maintained consistently. I also added the cluster status to the webapp so that you can get a quick view of how busy the cluster is.",0.297,0.297,neutral
hadoop,10139,comment_6,"This is a really useful update and should take care of a common complaint on the mailing lists. My comments: # I think the pseudo-distributed instructions result in the job being executed locally instead of being submitted to yarn so its not a complete example. We need minimal yarn configuration to demonstrate job execution on yarn. # should be Same with {{start-yarn.sh}} # {{HADOOP_PREFIX}} seems to be necessary, else {{HADOOP_CONF_DIR}} gets initialized to /etc/hadoop and this breaks the DataNode, for one. We should document setting {{HADOOP_PREFIX}}. # Typo: _quickly perform single operations using Hadoop_; _single_ should be _simple_? # Windows setup is quite different from Linux and the steps given here would be confusing for someone attempting to install on Windows. Can you please add a link to the Windows setup wiki page and clarify that the instructions given here are for Linux only? Users tend to take the Linux instructions and attempt to run them in Cygwin which never works as expected. Nitpick comments, not that important: # Instead of ""Format a new perhaps we can just say Format the filesystem. # It may help readability if steps in the execution section were a numbered list. What do you think? # It would be nice if the bullet points after ""Now you are ready to start your Hadoop cluster in one of the three supported modes were links to subsequent sections, not a big deal.",documentation_debt,low_quality_documentation,"Mon, 2 Dec 2013 18:54:30 +0000","Thu, 4 Sep 2014 01:16:47 +0000","Thu, 30 Jan 2014 19:31:21 +0000",5099811,"This is a really useful update and should take care of a common complaint on the mailing lists. My comments: I think the pseudo-distributed instructions result in the job being executed locally instead of being submitted to yarn so its not a complete example. We need minimal yarn configuration to demonstrate job execution on yarn. bin/start-dfs.sh should be sbin/start-dfs.sh. Same with start-yarn.sh HADOOP_PREFIX seems to be necessary, else HADOOP_CONF_DIR gets initialized to /etc/hadoop and this breaks the DataNode, for one. We should document setting HADOOP_PREFIX. Typo: quickly perform single operations using Hadoop; single should be simple? Windows setup is quite different from Linux and the steps given here would be confusing for someone attempting to install on Windows. Can you please add a link to the Windows setup wiki page https://wiki.apache.org/hadoop/Hadoop2OnWindows and clarify that the instructions given here are for Linux only? Users tend to take the Linux instructions and attempt to run them in Cygwin which never works as expected. Nitpick comments, not that important: Instead of ""Format a new distributed-filesystem perhaps we can just say Format the filesystem. It may help readability if steps in the execution section were a numbered list. What do you think? It would be nice if the bullet points after ""Now you are ready to start your Hadoop cluster in one of the three supported modes were links to subsequent sections, not a big deal.",-0.1005595238,-0.08281372549,positive
hadoop,10139,description,"The document should be understandable to a newcomer because the first place he will go is ""setup a single node"".",documentation_debt,low_quality_documentation,"Mon, 2 Dec 2013 18:54:30 +0000","Thu, 4 Sep 2014 01:16:47 +0000","Thu, 30 Jan 2014 19:31:21 +0000",5099811,"The document should be understandable to a newcomer because the first place he will go is ""setup a single node"".",0.5,0.5,neutral
hadoop,10175,comment_2,"Hi, Chuan. The patch looks good. There is a very minor typo on the test name: I think this patch will be ready to commit after that's corrected.",documentation_debt,low_quality_documentation,"Thu, 19 Dec 2013 02:32:53 +0000","Thu, 12 May 2016 18:21:39 +0000","Mon, 23 Dec 2013 18:43:17 +0000",403824,"Hi, Chuan. The patch looks good. There is a very minor typo on the test name: testMakeQulifiedPath. I think this patch will be ready to commit after that's corrected.",0.3795,0.284625,positive
hadoop,10175,comment_3,"Thanks for reviewing, Chris! Attaching a new patch that corrects the typo.",documentation_debt,low_quality_documentation,"Thu, 19 Dec 2013 02:32:53 +0000","Thu, 12 May 2016 18:21:39 +0000","Mon, 23 Dec 2013 18:43:17 +0000",403824,"Thanks for reviewing, Chris! Attaching a new patch that corrects the typo.",0.2,0.2,positive
hadoop,10175,comment_4,+1 for the patch. Thanks for fixing that typo. I'll commit this.,documentation_debt,low_quality_documentation,"Thu, 19 Dec 2013 02:32:53 +0000","Thu, 12 May 2016 18:21:39 +0000","Mon, 23 Dec 2013 18:43:17 +0000",403824,+1 for the patch. Thanks for fixing that typo. I'll commit this.,0.2,0.2,positive
hadoop,10225,comment_4,"I checked on Maven central, and it looks like the 2.x release pretty much have javadoc and source artifacts: We're missing javadoc for 3.0.0-alpha1 though, so let's update the affects/targets accordingly.",documentation_debt,outdated_documentation,"Sun, 12 Jan 2014 14:05:14 +0000","Thu, 10 Jan 2019 18:37:42 +0000","Thu, 10 Jan 2019 18:37:42 +0000",157609948,"I checked on Maven central, and it looks like the 2.x release pretty much have javadoc and source artifacts: http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22org.apache.hadoop%22%20AND%20a%3A%22hadoop-hdfs%22 We're missing javadoc for 3.0.0-alpha1 though, so let's update the affects/targets accordingly.",-0.05,-0.05,neutral
hadoop,10423,description,"As discussed on the dev mailing lists and MAPREDUCE-4052, we need to update the text of the compatibility policy to discuss a new client combined with an old server.",documentation_debt,outdated_documentation,"Mon, 24 Mar 2014 20:59:54 +0000","Thu, 4 Sep 2014 01:16:46 +0000","Mon, 24 Mar 2014 23:28:26 +0000",8912,"As discussed on the dev mailing lists and MAPREDUCE-4052, we need to update the text of the compatibility policy to discuss a new client combined with an old server.",0,0,neutral
hadoop,10423,summary,Clarify compatibility policy document for combination of new client and old server.,documentation_debt,low_quality_documentation,"Mon, 24 Mar 2014 20:59:54 +0000","Thu, 4 Sep 2014 01:16:46 +0000","Mon, 24 Mar 2014 23:28:26 +0000",8912,Clarify compatibility policy document for combination of new client and old server.,0,0,neutral
hadoop,10526,comment_1,"The patch looks good except a typo: ""Oveerriding"". I changed the target to be 2.5 and trunk. Although this bug is present in other branches, This is where you intend to fix this bug. ""Fix version"" is filled in after check-in.",documentation_debt,low_quality_documentation,"Mon, 5 Dec 2011 15:12:56 +0000","Wed, 3 Sep 2014 20:36:28 +0000","Mon, 21 Apr 2014 19:22:44 +0000",75010188,"The patch looks good except a typo: ""Oveerriding"". I changed the target to be 2.5 and trunk. Although this bug is present in other branches, This is where you intend to fix this bug. ""Fix version"" is filled in after check-in.",0.194,0.194,positive
hadoop,10602,comment_2,"Thanks for taking this, Akira. It looks like you're taking the approach of deleting the links. I think this is fine, because we have all the direct links in the left nav, and the user always has the browser back button too. It streamlines the pages a bit too. Does anyone else out there object to removing the Go Back links? If not, let's go ahead with this approach. To make this change comprehensive, we'll need to update some additional files. Here is what I turned up in a grep of 'Go Back' across the whole repo:",documentation_debt,low_quality_documentation,"Tue, 13 May 2014 18:44:34 +0000","Thu, 12 May 2016 18:27:29 +0000","Thu, 29 May 2014 17:36:21 +0000",1378307,"Thanks for taking this, Akira. It looks like you're taking the approach of deleting the links. I think this is fine, because we have all the direct links in the left nav, and the user always has the browser back button too. It streamlines the pages a bit too. Does anyone else out there object to removing the Go Back links? If not, let's go ahead with this approach. To make this change comprehensive, we'll need to update some additional files. Here is what I turned up in a grep of 'Go Back' across the whole repo: hadoop-common-project/hadoop-auth/src/site/apt/BuildingIt.apt.vm hadoop-common-project/hadoop-auth/src/site/apt/BuildingIt.apt.vm hadoop-common-project/hadoop-auth/src/site/apt/Configuration.apt.vm hadoop-common-project/hadoop-auth/src/site/apt/Configuration.apt.vm hadoop-common-project/hadoop-auth/src/site/apt/Examples.apt.vm hadoop-common-project/hadoop-auth/src/site/apt/Examples.apt.vm hadoop-hdfs-project/hadoop-hdfs/src/site/apt/CentralizedCacheManagement.apt.vm hadoop-hdfs-project/hadoop-hdfs/src/site/apt/HdfsNfsGateway.apt.vm hadoop-hdfs-project/hadoop-hdfs/src/site/apt/ViewFs.apt.vm hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/apt/ServerSetup.apt.vm hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/apt/ServerSetup.apt.vm hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/apt/UsingHttpTools.apt.vm hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/apt/UsingHttpTools.apt.vm hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/apt/DistributedCacheDeploy.apt.vm hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/apt/EncryptedShuffle.apt.vm hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/apt/MapReduce_Compatibility_Hadoop1_Hadoop2.apt.vm hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/apt/PluggableShuffleAndPluggableSort.apt.vm hadoop-tools/hadoop-sls/src/site/apt/SchedulerLoadSimulator.apt.vm hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/CapacityScheduler.apt.vm hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/FairScheduler.apt.vm hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/HistoryServerRest.apt.vm hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/MapredAppMasterRest.apt.vm hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/NodeManager.apt.vm hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/NodeManagerRest.apt.vm hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/ResourceManagerRest.apt.vm hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/TimelineServer.apt.vm hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/WebServicesIntro.apt.vm hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/WritingYarnApplications.apt.vm hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/YarnCommands.apt.vm",0.446125,0.09645945946,positive
hadoop,10602,comment_4,"Thanks  for the comment. The approach of the v1 patch was to delete the only ""dead"" links. Now I agree with you. I'll update the patch to remove all the ""Go Back"" links. By the way, removing all the ""Go Back"" links makes not linked from anywhere. I think we can remove the document because is a superset of it.",documentation_debt,low_quality_documentation,"Tue, 13 May 2014 18:44:34 +0000","Thu, 12 May 2016 18:27:29 +0000","Thu, 29 May 2014 17:36:21 +0000",1378307,"Thanks cnauroth for the comment. The approach of the v1 patch was to delete the only ""dead"" links. we have all the direct links in the left nav, and the user always has the browser back button too. Now I agree with you. I'll update the patch to remove all the ""Go Back"" links. By the way, removing all the ""Go Back"" links makes hadoop-yarn/hadoop-yarn-site/index.html not linked from anywhere. I think we can remove the document because hadoop-yarn/hadoop-yarn-site/YARN.html is a superset of it.",0.1666666667,0.1111111111,neutral
hadoop,10602,description,"Multiple pages of our documentation have ""Go Back"" links that are broken, because they point to an incorrect relative path.",documentation_debt,low_quality_documentation,"Tue, 13 May 2014 18:44:34 +0000","Thu, 12 May 2016 18:27:29 +0000","Thu, 29 May 2014 17:36:21 +0000",1378307,"Multiple pages of our documentation have ""Go Back"" links that are broken, because they point to an incorrect relative path.",0.15,0.15,negative
hadoop,10602,summary,"Documentation has broken ""Go Back"" hyperlinks.",documentation_debt,low_quality_documentation,"Tue, 13 May 2014 18:44:34 +0000","Thu, 12 May 2016 18:27:29 +0000","Thu, 29 May 2014 17:36:21 +0000",1378307,"Documentation has broken ""Go Back"" hyperlinks.",0.15,0.15,negative
hadoop,10904,comment_5,"got it, sounds good. BTW, the method has a typo.",documentation_debt,low_quality_documentation,"Wed, 30 Jul 2014 18:41:06 +0000","Tue, 21 Oct 2014 00:06:05 +0000","Tue, 21 Oct 2014 00:04:53 +0000",7104227,"got it, sounds good. BTW, the Configuration.getPasswordFromCredenitalProviders() method has a typo.",0.388,0.2586666667,positive
hadoop,10915,description,"I've found 4 dead links on the main page of chukwa component. I think a good idea is generate the html with last build , than all links will be updated automatically with packages",documentation_debt,low_quality_documentation,"Thu, 31 Jul 2014 13:55:59 +0000","Thu, 31 Jul 2014 15:44:59 +0000","Thu, 31 Jul 2014 15:44:59 +0000",6540,"I've found 4 dead links on the main page of chukwa component. I think a good idea is generate the html with last build , than all links will be updated automatically with packages",0.088,0.088,neutral
hadoop,10915,summary,chukwa-0.5.0 dead links on the main page,documentation_debt,low_quality_documentation,"Thu, 31 Jul 2014 13:55:59 +0000","Thu, 31 Jul 2014 15:44:59 +0000","Thu, 31 Jul 2014 15:44:59 +0000",6540,chukwa-0.5.0 dead links on the main page,-0.2,-0.2,negative
hadoop,11103,description,"RemoteException has a number of undocumented behaviors * has no javadocs on getClassName. Reading the source, the String returned is the classname of the wrapped remote exception. * String) is equivalent to calling String, null) * Constructors allow null for all arguments * Some of the test code doesn't check for correct error codes to correspond with the wrapped exception type * methods don't document when they might return null",documentation_debt,outdated_documentation,"Wed, 17 Sep 2014 19:51:26 +0000","Tue, 30 Aug 2016 01:31:55 +0000","Tue, 19 May 2015 09:13:47 +0000",21043341,"RemoteException has a number of undocumented behaviors o.a.h.ipc.RemoteException has no javadocs on getClassName. Reading the source, the String returned is the classname of the wrapped remote exception. RemoteException(String, String) is equivalent to calling RemoteException(String, String, null) Constructors allow null for all arguments Some of the test code doesn't check for correct error codes to correspond with the wrapped exception type methods don't document when they might return null",-0.03055555556,-0.02291666667,neutral
hadoop,11289,comment_0,Patch fixes the typo.,documentation_debt,low_quality_documentation,"Mon, 10 Nov 2014 16:53:18 +0000","Fri, 10 Apr 2015 20:04:23 +0000","Mon, 10 Nov 2014 19:06:32 +0000",7994,Patch fixes the typo.,0,0,neutral
hadoop,11289,comment_4,Thank you  for the quick review and commit. I should mention FTR that the patch doesn't need any tests since it is a log message typo fix.,documentation_debt,low_quality_documentation,"Mon, 10 Nov 2014 16:53:18 +0000","Fri, 10 Apr 2015 20:04:23 +0000","Mon, 10 Nov 2014 19:06:32 +0000",7994,Thank you wheat9 for the quick review and commit. I should mention FTR that the patch doesn't need any tests since it is a log message typo fix.,0.15,0.15,positive
hadoop,11289,summary,Fix typo in RpcUtil log message,documentation_debt,low_quality_documentation,"Mon, 10 Nov 2014 16:53:18 +0000","Fri, 10 Apr 2015 20:04:23 +0000","Mon, 10 Nov 2014 19:06:32 +0000",7994,Fix typo in RpcUtil log message,0,0,negative
hadoop,11313,comment_2,"Hi, Tsuyoshi. This is a good idea. Thank you for posting a patch. I have a few recommendations: # Instead of running it through {{hadoop jar}} with the full class name, the shorter way to run this command is {{hadoop checknative}}. I recommend using the shorter form in the documentation. # Related to the above, since it's part of our CLI, I recommend adding documentation to too. Let's include mention of the {{-a}} option. Without {{-a}}, the command only checks for With {{-a}}, the command checks for all the libraries that hadoop-common dynamically links to. # I think it would be more informative for the sample here to show what the output looks like when the native code is found. In particular, it displays the absolute path to each library it finds, which can be very helpful. Here is a sample from one of my dev environments:",documentation_debt,low_quality_documentation,"Tue, 18 Nov 2014 07:50:04 +0000","Fri, 24 Apr 2015 22:49:02 +0000","Sun, 7 Dec 2014 04:14:26 +0000",1628662,"Hi, Tsuyoshi. This is a good idea. Thank you for posting a patch. I have a few recommendations: Instead of running it through hadoop jar with the full class name, the shorter way to run this command is hadoop checknative. I recommend using the shorter form in the documentation. Related to the above, since it's part of our CLI, I recommend adding documentation to CommandsManual.apt.vm too. Let's include mention of the -a option. Without -a, the command only checks for libhadoop.so/hadoop.dll. With -a, the command checks for all the libraries that hadoop-common dynamically links to. I think it would be more informative for the sample here to show what the output looks like when the native code is found. In particular, it displays the absolute path to each library it finds, which can be very helpful. Here is a sample from one of my dev environments:",0.2364545455,0.1734,positive
hadoop,11313,comment_8,"+1 for the patch. I committed this to trunk and branch-2. Tsuyoshi, thank you for contributing this documentation improvement.",documentation_debt,low_quality_documentation,"Tue, 18 Nov 2014 07:50:04 +0000","Fri, 24 Apr 2015 22:49:02 +0000","Sun, 7 Dec 2014 04:14:26 +0000",1628662,"+1 for the patch. I committed this to trunk and branch-2. Tsuyoshi, thank you for contributing this documentation improvement.",0.3236666667,0.3236666667,positive
hadoop,11313,description,"is a good tool to check whether native libraries are loaded correctly. We don't have any docs about this, so we should add it to",documentation_debt,low_quality_documentation,"Tue, 18 Nov 2014 07:50:04 +0000","Fri, 24 Apr 2015 22:49:02 +0000","Sun, 7 Dec 2014 04:14:26 +0000",1628662,"NativeLibraryChecker is a good tool to check whether native libraries are loaded correctly. We don't have any docs about this, so we should add it to NativeLibraries.apt.vm.",0.319,0.2126666667,neutral
hadoop,11437,comment_0,"Hello  I did not find author tag, ""DistCp Version 2"" is pressent ,here Version 2 can be removed..Please correct me if I am wrong..",documentation_debt,low_quality_documentation,"Fri, 19 Dec 2014 17:33:31 +0000","Thu, 12 May 2016 18:27:56 +0000","Wed, 11 Feb 2015 23:48:18 +0000",4688087,"Hello aw I did not find author tag, ""DistCp Version 2"" is pressent ,here Version 2 can be removed..Please correct me if I am wrong..",0.09166666667,0.09166666667,negative
hadoop,11437,comment_1,Look at the very bottom of the readme: That should be removed.,documentation_debt,outdated_documentation,"Fri, 19 Dec 2014 17:33:31 +0000","Thu, 12 May 2016 18:27:56 +0000","Wed, 11 Feb 2015 23:48:18 +0000",4688087,Look at the very bottom of the readme: That should be removed.,0,0,negative
hadoop,11437,comment_3,Hi  Removed from readme and documentation guide..,documentation_debt,outdated_documentation,"Fri, 19 Dec 2014 17:33:31 +0000","Thu, 12 May 2016 18:27:56 +0000","Wed, 11 Feb 2015 23:48:18 +0000",4688087,"Hi aw We should also remove all the ""DistCp V2"" stuff and just make it distcp since there is no distcp v1 anymore Removed from readme and documentation guide..",0,0,negative
hadoop,1147,comment_4,"+1 I was originally liked them, but they mostly end up creating noise. For example, I wrote the sort and word count examples that many people use for the start of their first map/reduce program and my name is in the @author. Someone didn't remove the comments and then handed off their program to someone else and it lead to confusion.",documentation_debt,low_quality_documentation,"Thu, 22 Mar 2007 20:14:04 +0000","Mon, 20 Aug 2007 18:11:50 +0000","Wed, 20 Jun 2007 22:45:50 +0000",7785106,"+1 I was originally liked them, but they mostly end up creating noise. For example, I wrote the sort and word count examples that many people use for the start of their first map/reduce program and my name is in the @author. Someone didn't remove the comments and then handed off their program to someone else and it lead to confusion.",0.2916666667,0.2916666667,negative
hadoop,11544,comment_1,"The patch looks good to me. One comment: looks noop, so could you remove the parameter from core-site.xml and update the document for tracing also?",documentation_debt,outdated_documentation,"Wed, 4 Feb 2015 02:53:38 +0000","Fri, 10 Apr 2015 20:04:43 +0000","Wed, 4 Feb 2015 12:22:06 +0000",34108,"The patch looks good to me. One comment: ""hadoop.htrace.sampler"" looks noop, so could you remove the parameter from core-site.xml and update the document for tracing also?",0.2586666667,0.1552,positive
hadoop,11585,comment_0,"* fixed broken ""Samplers"" section title * added newlines for ease of editing.",documentation_debt,low_quality_documentation,"Wed, 11 Feb 2015 21:50:33 +0000","Thu, 12 May 2016 18:23:45 +0000","Thu, 12 Feb 2015 00:15:20 +0000",8687,"fixed broken ""Samplers"" section title added newlines for ease of editing.",0.025,0.025,neutral
hadoop,11585,summary,Fix formatting in Tracing.md,documentation_debt,low_quality_documentation,"Wed, 11 Feb 2015 21:50:33 +0000","Thu, 12 May 2016 18:23:45 +0000","Thu, 12 Feb 2015 00:15:20 +0000",8687,Fix formatting in Tracing.md,0,0,neutral
hadoop,11720,summary,[JDK8] Fix javadoc errors caused by incorrect or illegal tags in hadoop-tools,documentation_debt,low_quality_documentation,"Mon, 16 Mar 2015 15:38:16 +0000","Fri, 10 Apr 2015 20:04:47 +0000","Tue, 17 Mar 2015 07:11:11 +0000",55975,[JDK8] Fix javadoc errors caused by incorrect or illegal tags in hadoop-tools,-0.5,-0.5,negative
hadoop,11740,comment_6,Thanks Kai for the review! The updated patch addresses the issue in the test code. I also made a pass of the and removed unnecessary Javadoc,documentation_debt,outdated_documentation,"Mon, 23 Mar 2015 20:10:41 +0000","Wed, 30 Sep 2015 19:38:02 +0000","Fri, 3 Apr 2015 22:23:49 +0000",958388,Thanks Kai for the review! The updated patch addresses the issue in the test code. I also made a pass of the TestErasureCoderBase and removed unnecessary Javadoc,0.1333333333,0.1333333333,positive
hadoop,11740,comment_8,"Thanks Kai for the review! I read the test classes again and figured out more about the structure. Let me know if it looks OK now. Regarding Javadoc I believe we shouldn't have empty statements when merging to trunk. If a parameter or return value is self-descriptory, I think it's better not to add a Javadoc than adding an empty one. But let's discuss that separately since it's not in the scope of this JIRA.",documentation_debt,low_quality_documentation,"Mon, 23 Mar 2015 20:10:41 +0000","Wed, 30 Sep 2015 19:38:02 +0000","Fri, 3 Apr 2015 22:23:49 +0000",958388,"Thanks Kai for the review! I read the test classes again and figured out more about the structure. Let me know if it looks OK now. Regarding Javadoc I believe we shouldn't have empty statements when merging to trunk. If a parameter or return value is self-descriptory, I think it's better not to add a Javadoc than adding an empty one. But let's discuss that separately since it's not in the scope of this JIRA.",0.2833333333,0.2833333333,positive
hadoop,11786,summary,Fix Javadoc typos in,documentation_debt,low_quality_documentation,"Wed, 1 Apr 2015 18:10:41 +0000","Thu, 17 Nov 2016 11:55:36 +0000","Wed, 17 Aug 2016 22:05:19 +0000",43559678,Fix Javadoc typos in org.apache.hadoop.fs.FileSystem,0,0,negative
hadoop,11844,description,at least on 2.6.0 points to an invalid link to rumen. Need to verify and potentially fix this link in newer releases.,documentation_debt,low_quality_documentation,"Fri, 17 Apr 2015 14:32:33 +0000","Fri, 8 May 2015 17:00:56 +0000","Fri, 8 May 2015 17:00:56 +0000",1823303,SchedulerLoadSimulator at least on 2.6.0 points to an invalid link to rumen. Need to verify and potentially fix this link in newer releases.,0,0,negative
hadoop,11844,summary,SLS docs point to invalid rumen link,documentation_debt,low_quality_documentation,"Fri, 17 Apr 2015 14:32:33 +0000","Fri, 8 May 2015 17:00:56 +0000","Fri, 8 May 2015 17:00:56 +0000",1823303,SLS docs point to invalid rumen link,0,0,negative
hadoop,12021,comment_3,"Lewis, could you give a little more detail of your Nutch usecase? It's also worth noting that while we provide the description in core-default.xml / hdfs-default.xml / etc for documentation, but probably not in user-provided config files. The -default.xml files are already included in our JARs, so it shouldn't increase dependency size. Loading them in will, however, increase in-memory size, which is probably a concern for some user app.",documentation_debt,low_quality_documentation,"Sat, 23 May 2015 00:36:20 +0000","Mon, 29 Aug 2016 22:19:43 +0000","Sun, 31 May 2015 20:05:45 +0000",761365,"Lewis, could you give a little more detail of your Nutch usecase? It's also worth noting that while we provide the description in core-default.xml / hdfs-default.xml / etc for documentation, but probably not in user-provided config files. The -default.xml files are already included in our JARs, so it shouldn't increase dependency size. Loading them in will, however, increase in-memory size, which is probably a concern for some user app.",-0.15,-0.15,neutral
hadoop,12021,comment_6,Hi Folks. The suggestions here have convinced me that the place to do this is over in Nutch as it is certainly application specific and just causing too much overhead in Hadoop. The text is not required in-memory within the Hadoop Configuration object.,documentation_debt,low_quality_documentation,"Sat, 23 May 2015 00:36:20 +0000","Mon, 29 Aug 2016 22:19:43 +0000","Sun, 31 May 2015 20:05:45 +0000",761365,Hi Folks. The suggestions here have convinced me that the place to do this is over in Nutch as it is certainly application specific and just causing too much overhead in Hadoop. The text is not required in-memory within the Hadoop Configuration object.,0.1456666667,0.1456666667,neutral
hadoop,12452,description,The sample code in Tracing.md have some problems. * compilation error by not importing Tracer. * generic options are not reflected because Tracer is initialized before ToolRunner#run. * it may be confusing to use FsShell in example because it has embedded Tracer now.,documentation_debt,outdated_documentation,"Wed, 30 Sep 2015 13:36:25 +0000","Tue, 30 Aug 2016 01:22:49 +0000","Mon, 5 Oct 2015 18:36:22 +0000",449997,The sample code in Tracing.md have some problems. compilation error by not importing Tracer. generic options are not reflected because Tracer is initialized before ToolRunner#run. it may be confusing to use FsShell in example because it has embedded Tracer now.,-0.2474,-0.2474,negative
hadoop,12452,summary,Fix tracing documention reflecting the update to htrace-4,documentation_debt,outdated_documentation,"Wed, 30 Sep 2015 13:36:25 +0000","Tue, 30 Aug 2016 01:22:49 +0000","Mon, 5 Oct 2015 18:36:22 +0000",449997,Fix tracing documention reflecting the update to htrace-4,0,0,neutral
hadoop,12458,comment_1,"Failed tests aren't related. Thanks for the changes! +1, committing shortly. Quick notes: - Please do not set a Fix Version. Use Target Version field instead. The Fix Version must indicate only the branches where it has *already* been committed to. The former is to indicate requests of branches it must go to, so is more appropriate. - For more typo corrections in future, please also feel free to roll up multiple corrections into the same patch.",documentation_debt,low_quality_documentation,"Sat, 3 Oct 2015 03:15:58 +0000","Tue, 30 Aug 2016 01:22:46 +0000","Sat, 3 Oct 2015 13:10:24 +0000",35666,"Failed tests aren't related. Thanks for the changes! +1, committing shortly. Quick notes: Please do not set a Fix Version. Use Target Version field instead. The Fix Version must indicate only the branches where it has already been committed to. The former is to indicate requests of branches it must go to, so is more appropriate. For more typo corrections in future, please also feel free to roll up multiple corrections into the same patch.",0.19,0.2104166667,neutral
hadoop,12458,description,Spotted this typo in the code while working on a separate YARN issue. E.g Checked in the whole project. Found a few occurrences of the typo in code/comment. The JIRA is meant to help fix those typos.,documentation_debt,low_quality_documentation,"Sat, 3 Oct 2015 03:15:58 +0000","Tue, 30 Aug 2016 01:22:46 +0000","Sat, 3 Oct 2015 13:10:24 +0000",35666,Spotted this typo in the code while working on a separate YARN issue. E.g DEFAULT_RM_NODEMANAGER_CONNECT_RETIRES Checked in the whole project. Found a few occurrences of the typo in code/comment. The JIRA is meant to help fix those typos.,0.25,0.125,negative
hadoop,12458,summary,Retries is typoed to spell Retires in parts of hadoop-yarn and hadoop-common,documentation_debt,low_quality_documentation,"Sat, 3 Oct 2015 03:15:58 +0000","Tue, 30 Aug 2016 01:22:46 +0000","Sat, 3 Oct 2015 13:10:24 +0000",35666,Retries is typoed to spell Retires in parts of hadoop-yarn and hadoop-common,0,0,neutral
hadoop,1245,comment_8,"+1 This looks reasonable to me. Note that this is potentially incompatible, since previously folks could set the number of tasks per node globally at the jobtracker, now it is determined by the configuration of the tasktracker nodes. So we should probably either add a compatibility note (i.e., move this to the INCOMPATIBLE section of CHANGES.txt) or perhaps have a configuration parameter that enables the old behavior. I think a compatibility note is probably sufficient. Thoughts?",documentation_debt,outdated_documentation,"Wed, 11 Apr 2007 00:20:03 +0000","Wed, 8 Jul 2009 16:52:21 +0000","Wed, 24 Oct 2007 22:52:12 +0000",17015529,"+1 This looks reasonable to me. Note that this is potentially incompatible, since previously folks could set the number of tasks per node globally at the jobtracker, now it is determined by the configuration of the tasktracker nodes. So we should probably either add a compatibility note (i.e., move this to the INCOMPATIBLE section of CHANGES.txt) or perhaps have a configuration parameter that enables the old behavior. I think a compatibility note is probably sufficient. Thoughts?",0.2611666667,0.2611666667,neutral
hadoop,12460,comment_4,"Thanks  Few comments, 1. Identation can be same as other lines and \n not required at the end. 2. Document need to be updated for the {{get}} command instead of {{copyToLocal}} 3. {{TestCLI}} failure should be fixed.",documentation_debt,outdated_documentation,"Tue, 6 Oct 2015 14:07:39 +0000","Tue, 30 Aug 2016 01:22:43 +0000","Mon, 19 Oct 2015 06:47:59 +0000",1096820,"Thanks jagadesh.kiran Few comments, 1. Identation can be same as other lines and \n not required at the end. 2. Document need to be updated for the get command instead of copyToLocal 3. TestCLI failure should be fixed.",0,0,neutral
hadoop,13011,comment_10,"line 129 "" This is done by leveraging the Hadoop filesystem abstraction within the provider implementation."" can this be rephrased into something concise without the word ""leveraging""? line 140, backquote line 14, same for `cat 158. Remove the `you`; make it less personal. ""Organisations ? users? "" can you quote ""side files"" not a term I'd encountered before",documentation_debt,low_quality_documentation,"Sat, 9 Apr 2016 13:53:33 +0000","Tue, 30 Aug 2016 01:16:28 +0000","Fri, 22 Apr 2016 05:12:37 +0000",1091944,"line 129 "" This is done by leveraging the Hadoop filesystem abstraction within the provider implementation."" can this be rephrased into something concise without the word ""leveraging""? line 140, backquote hadoop.security.credstore.java-keystore-provider.password-file line 14, same for `cat 158. Remove the `you`; make it less personal. ""Organisations ? users? "" can you quote ""side files"" not a term I'd encountered before",-0.07142857143,-0.04545454545,neutral
hadoop,13039,comment_12,This is for adding missing documentation for config property. No code change was involved.,documentation_debt,outdated_documentation,"Tue, 19 Apr 2016 22:33:30 +0000","Fri, 6 Jan 2017 01:01:38 +0000","Wed, 27 Apr 2016 03:49:39 +0000",623769,The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. This is for adding missing documentation for config property. No code change was involved.,-0.2,0,neutral
hadoop,13039,comment_5,"Per-offline discussion with , the v1 patch refined the documentation to make it clearer.",documentation_debt,low_quality_documentation,"Tue, 19 Apr 2016 22:33:30 +0000","Fri, 6 Jan 2017 01:01:38 +0000","Wed, 27 Apr 2016 03:49:39 +0000",623769,"Per-offline discussion with arpitagarwal, the v1 patch refined the documentation to make it clearer.",0,0,neutral
hadoop,13039,description,The RPC server enforces a maximum length on incoming messages. Messages larger than the maximum are rejected immediately. The maximum length can be tuned by setting configuration property but this is not documented in core-site.xml.,documentation_debt,outdated_documentation,"Tue, 19 Apr 2016 22:33:30 +0000","Fri, 6 Jan 2017 01:01:38 +0000","Wed, 27 Apr 2016 03:49:39 +0000",623769,"The RPC server enforces a maximum length on incoming messages. Messages larger than the maximum are rejected immediately. The maximum length can be tuned by setting configuration property ipc.maximum.data.length, but this is not documented in core-site.xml.",0.365625,0.2982142857,neutral
hadoop,13365,comment_2,"-00: * first pass There's a lot happening here, so let's go through it: * adding some helper routines in hadoop-functions to: ** convert strings to arrays if the array doesn't already exist ** add to arrays based upon a key to dedupe * convert almost all internal users of HADOOP_OPTS and xyz_OPTS to use the array form * update some pre-existing doc references * add several unit tests * rewrite existing unit tests to use the array form To do: * more/better docs * figure out what to do about catalina? * get HADOOP-13341 committed, since this code is several times larger without it * more testing",documentation_debt,low_quality_documentation,"Mon, 11 Jul 2016 15:06:56 +0000","Sat, 1 Sep 2018 20:06:01 +0000","Sat, 1 Sep 2018 20:06:01 +0000",67582745,"-00: first pass There's a lot happening here, so let's go through it: adding some helper routines in hadoop-functions to: convert strings to arrays if the array doesn't already exist add to arrays based upon a key to dedupe convert almost all internal users of HADOOP_OPTS and xyz_OPTS to use the array form update some pre-existing doc references add several unit tests rewrite existing unit tests to use the array form To do: more/better docs figure out what to do about catalina? get HADOOP-13341 committed, since this code is several times larger without it more testing",0.1666666667,0.1666666667,neutral
hadoop,13365,comment_4,"One of the feedbacks from HADOOP-13341 from was that the docs should include an example of _OPTS ordering. We have an opportunity to de-dupe here. If this JIRA does end up de-duping, then that should be documented. If it doesn't de-dupe, then the docs should specifically give an example of ordering.",documentation_debt,low_quality_documentation,"Mon, 11 Jul 2016 15:06:56 +0000","Sat, 1 Sep 2018 20:06:01 +0000","Sat, 1 Sep 2018 20:06:01 +0000",67582745,"One of the feedbacks from HADOOP-13341 from stevel@apache.org was that the docs should include an example of _OPTS ordering. We have an opportunity to de-dupe here. If this JIRA does end up de-duping, then that should be documented. If it doesn't de-dupe, then the docs should specifically give an example of ordering.",0.225,0.225,neutral
hadoop,13732,comment_1,"Hi Mike, if we need to use a more recent version of Maven, then we also need to update BUILDING.txt. Could you comment on the availability of the required Maven version on a few common OSs? e.g. RHEL6, 7, Ubuntu 12/14/16.",documentation_debt,outdated_documentation,"Tue, 18 Oct 2016 18:47:19 +0000","Sat, 22 Oct 2016 00:07:53 +0000","Fri, 21 Oct 2016 23:41:59 +0000",276880,"Hi Mike, if we need to use a more recent version of Maven, then we also need to update BUILDING.txt. Could you comment on the availability of the required Maven version on a few common OSs? e.g. RHEL6, 7, Ubuntu 12/14/16.",0,0,neutral
hadoop,13732,comment_2,"I'd have to make a dependency-check specific note in BUILDING.txt, which seems a little awkard. (The normal build isn't affected, of course.) I'll see what I can do. My only alternative idea is a comment around this plugin in pom.xml. I do agree it needs to be documented somewhere. * I don't even think that maven is _available_ on RHEL 6.6 * My RHEL 7.2 machine looks like it would use version 3.0.5-16 * My Ubuntu 16.04 machine is using 3.3.9 * Looks like Ubuntu 14.04 uses 3.0.5-1 The maven release history page is at",documentation_debt,outdated_documentation,"Tue, 18 Oct 2016 18:47:19 +0000","Sat, 22 Oct 2016 00:07:53 +0000","Fri, 21 Oct 2016 23:41:59 +0000",276880,"I'd have to make a dependency-check specific note in BUILDING.txt, which seems a little awkard. (The normal build isn't affected, of course.) I'll see what I can do. My only alternative idea is a comment around this plugin in pom.xml. I do agree it needs to be documented somewhere. I don't even think that maven is available on RHEL 6.6 My RHEL 7.2 machine looks like it would use version 3.0.5-16 My Ubuntu 16.04 machine is using 3.3.9 Looks like Ubuntu 14.04 uses 3.0.5-1 The maven release history page is at https://maven.apache.org/docs/history.html",0.1125,0.1125,neutral
hadoop,14092,comment_1,Patch 001 * Fix the typo,documentation_debt,low_quality_documentation,"Fri, 17 Feb 2017 06:04:54 +0000","Fri, 21 Apr 2017 21:38:34 +0000","Sat, 18 Feb 2017 18:25:36 +0000",130842,Patch 001 Fix the typo,0,0,neutral
hadoop,14092,comment_3,"+1, committed your attention to detail is appreciated! I know its only a word in the docs, but that could cost a lot of people time. Well spotted too.",documentation_debt,low_quality_documentation,"Fri, 17 Feb 2017 06:04:54 +0000","Fri, 21 Apr 2017 21:38:34 +0000","Sat, 18 Feb 2017 18:25:36 +0000",130842,"+1, committed your attention to detail is appreciated! I know its only a word in the docs, but that could cost a lot of people time. Well spotted too.",0.3603333333,0.3603333333,positive
hadoop,14092,summary,Typo in hadoop-aws index.md,documentation_debt,low_quality_documentation,"Fri, 17 Feb 2017 06:04:54 +0000","Fri, 21 Apr 2017 21:38:34 +0000","Sat, 18 Feb 2017 18:25:36 +0000",130842,Typo in hadoop-aws index.md,0,0,neutral
hadoop,14314,comment_0,"Thanks for filing this JIRA. Since we didn't find official link to replace the old one, suggest to simply remove it.",documentation_debt,outdated_documentation,"Mon, 17 Apr 2017 21:38:32 +0000","Thu, 23 Aug 2018 13:21:29 +0000","Thu, 23 Aug 2018 13:07:10 +0000",42564518,"templedf Thanks for filing this JIRA. Since we didn't find official link to replace the old one, suggest to simply remove it.",0.2,0.2,neutral
hadoop,14314,description,"Unfortunately, Oracle took down opensolaris.org, so the link is dead. The only replacement I could find with a quick search was this PDF:",documentation_debt,outdated_documentation,"Mon, 17 Apr 2017 21:38:32 +0000","Thu, 23 Aug 2018 13:21:29 +0000","Thu, 23 Aug 2018 13:07:10 +0000",42564518,"Unfortunately, Oracle took down opensolaris.org, so the link is dead. The only replacement I could find with a quick search was this PDF: http://cuddletech.com/opensolaris/osdevref.pdf",-0.3736666667,-0.3736666667,negative
hadoop,14314,summary,The OpenSolaris taxonomy link is dead in,documentation_debt,outdated_documentation,"Mon, 17 Apr 2017 21:38:32 +0000","Thu, 23 Aug 2018 13:21:29 +0000","Thu, 23 Aug 2018 13:07:10 +0000",42564518,The OpenSolaris taxonomy link is dead in InterfaceClassification.md,-0.6,-0.3,negative
hadoop,15569,description,"The S3A assumed role doc is now where we document the permissions needed to work with buckets # detail the permissions you need for s3guard user and admin ops # and what you need for SSE-KMS This involves me working them out, so presumably get some new stack traces also: fix any errors noted in the doc",documentation_debt,low_quality_documentation,"Thu, 28 Jun 2018 19:19:44 +0000","Tue, 10 Jul 2018 16:58:49 +0000","Tue, 10 Jul 2018 16:58:49 +0000",1028345,"The S3A assumed role doc is now where we document the permissions needed to work with buckets detail the permissions you need for s3guard user and admin ops and what you need for SSE-KMS This involves me working them out, so presumably get some new stack traces also: fix any errors noted in the doc",0.185,0.185,neutral
hadoop,16291,comment_1,Thanks . Thought it was strange the docs were wrong for this long. Was going to ask for a sanity check on this JIRA but you beat me to it.,documentation_debt,low_quality_documentation,"Thu, 2 May 2019 20:59:59 +0000","Fri, 3 May 2019 16:34:41 +0000","Fri, 3 May 2019 16:28:04 +0000",70085,Thanks daryn. Thought it was strange the docs were wrong for this long. Was going to ask for a sanity check on this JIRA but you beat me to it.,0.2666666667,0.2666666667,negative
hadoop,16291,description,Fix some errors in the HDFS Permissions doc. Noticed this when reviewing HADOOP-16251. The FS Permissions seems to mark a lot of permissions as Not Applicable (N/A) when that is not the case. In particular getFileInfo (getFileStatus) checks READ permission bit as it should.,documentation_debt,low_quality_documentation,"Thu, 2 May 2019 20:59:59 +0000","Fri, 3 May 2019 16:34:41 +0000","Fri, 3 May 2019 16:28:04 +0000",70085,"Fix some errors in the HDFS Permissions doc. Noticed this when reviewing HADOOP-16251. The FS Permissions documentation seems to mark a lot of permissions as Not Applicable (N/A) when that is not the case. In particular getFileInfo (getFileStatus) checks READ permission bit here, as it should.",-0.225,-0.225,negative
hadoop,16291,summary,HDFS Permissions Guide appears incorrect about,documentation_debt,low_quality_documentation,"Thu, 2 May 2019 20:59:59 +0000","Fri, 3 May 2019 16:34:41 +0000","Fri, 3 May 2019 16:28:04 +0000",70085,HDFS Permissions Guide appears incorrect about getFileStatus()/getFileInfo(),0,0,negative
hadoop,1664,comment_0,"I am writing an admin guide for upgrading to Hadoop-0.14. will post it in couple of days. If you have any logs, please add them here. Upgrade and rollback procedure is same as before.",documentation_debt,low_quality_documentation,"Mon, 30 Jul 2007 14:54:35 +0000","Wed, 8 Jul 2009 16:42:33 +0000","Tue, 23 Oct 2007 20:58:27 +0000",7365832,"I am writing an admin guide for upgrading to Hadoop-0.14. will post it in couple of days. If you have any logs, please add them here. Upgrade and rollback procedure is same as before.",0.145,0.145,neutral
hadoop,1926,comment_4,Fixed the javadoc oversight.,documentation_debt,low_quality_documentation,"Thu, 20 Sep 2007 11:51:41 +0000","Wed, 8 Jul 2009 16:52:26 +0000","Tue, 2 Oct 2007 22:16:48 +0000",1074307,Fixed the javadoc oversight.,-0.2,-0.2,neutral
hadoop,2205,comment_0,I'd also like to use this jira to fix 2 minor typos in the website introduced by HADOOP-1917.,documentation_debt,low_quality_documentation,"Wed, 14 Nov 2007 16:06:37 +0000","Wed, 28 Nov 2007 22:32:47 +0000","Thu, 15 Nov 2007 09:09:40 +0000",61383,I'd also like to use this jira to fix 2 minor typos in the website introduced by HADOOP-1917.,0,0,neutral
hadoop,2205,comment_1,"Attached patch fixes some typos and broken links introduced by HADOOP-1917. I'll also ensure that website is updated, here is what my workspace looks like, after this patch:",documentation_debt,low_quality_documentation,"Wed, 14 Nov 2007 16:06:37 +0000","Wed, 28 Nov 2007 22:32:47 +0000","Thu, 15 Nov 2007 09:09:40 +0000",61383,"Attached patch fixes some typos and broken links introduced by HADOOP-1917. I'll also ensure that website is updated, here is what my workspace looks like, after this patch:",0,0,neutral
hadoop,2205,description,HADOOP-1917 changed but did not regenerate and to reflect those changes.,documentation_debt,outdated_documentation,"Wed, 14 Nov 2007 16:06:37 +0000","Wed, 28 Nov 2007 22:32:47 +0000","Thu, 15 Nov 2007 09:09:40 +0000",61383,"HADOOP-1917 changed src/docs/src/documentation/content/xdocs/site.xml, but did not regenerate docs/hdfs_design.html and docs/mailing_lists.html to reflect those changes.",0,0.125,neutral
hadoop,2959,comment_2,I don't think such a feature hold much value for combiners. And I don't think such a contract is ever documented clearly for combiners. Do you have any use cases for that feature for the combiner?,documentation_debt,low_quality_documentation,"Fri, 7 Mar 2008 03:40:02 +0000","Wed, 8 Jul 2009 16:52:39 +0000","Fri, 7 Mar 2008 06:37:13 +0000",10631,I don't think such a feature hold much value for combiners. And I don't think such a contract is ever documented clearly for combiners. Do you have any use cases for that feature for the combiner?,0,0,negative
hadoop,3505,comment_0,"Actually changes in Hadoop 0.18 now allow HOD to take relative paths on the command line. Likewise, if an incorrect conf is present, the errors are reported back to the client. However, we should document that the tarball should not contain a modified conf, so that users know this proactively. In addition to the above, the following changes are identified after reviewing the bugs fixed for HOD in Hadoop 0.18: - HADOOP-3376: Hyperlink Maui in the documentation, crediting Cluster Resources for the software. Also, Torque and Maui should be capitalized in the documentation. - HADOOP-3464: Add 2 new error scenarios in the trouble shooting section. One should describe the error that comes when ringmaster fails, the other when the jobtracker fails. For the latter, we should mention that administrators can review other log messages in the ringmaster log to see which other machines had problems bringing up the jobtracker, apart from the one that is reported on the command line. - HADOOP-3483: Remove any limitations we have specified about creating a cluster directory. Now, we create it automatically. - HADOOP-3184: Document in the Config guide about parameter.",documentation_debt,low_quality_documentation,"Thu, 5 Jun 2008 22:07:49 +0000","Fri, 22 Aug 2008 19:50:40 +0000","Sat, 21 Jun 2008 00:27:40 +0000",1304391,"Actually changes in Hadoop 0.18 now allow HOD to take relative paths on the command line. Likewise, if an incorrect conf is present, the errors are reported back to the client. However, we should document that the tarball should not contain a modified conf, so that users know this proactively. In addition to the above, the following changes are identified after reviewing the bugs fixed for HOD in Hadoop 0.18: HADOOP-3376: Hyperlink Maui in the documentation, crediting Cluster Resources for the software. Also, Torque and Maui should be capitalized in the documentation. HADOOP-3464: Add 2 new error scenarios in the trouble shooting section. One should describe the error that comes when ringmaster fails, the other when the jobtracker fails. For the latter, we should mention that administrators can review other log messages in the ringmaster log to see which other machines had problems bringing up the jobtracker, apart from the one that is reported on the command line. HADOOP-3483: Remove any limitations we have specified about creating a cluster directory. Now, we create it automatically. HADOOP-3184: Document in the Config guide about ringmaster.max-master-failures parameter.",-0.128875,-0.08175,neutral
hadoop,3505,description,"There's a couple HOD limitations that really trip up the unwary. Two I've encountered are that hod can't take relative paths on the command line, and that if you pass hod a tarball with a modified /conf, then the cluster will fail mysteriously to be initialized. I don't see any references to either in the documentation, and it'd be great to write this down.",documentation_debt,low_quality_documentation,"Thu, 5 Jun 2008 22:07:49 +0000","Fri, 22 Aug 2008 19:50:40 +0000","Sat, 21 Jun 2008 00:27:40 +0000",1304391,"There's a couple HOD limitations that really trip up the unwary. Two I've encountered are that hod can't take relative paths on the command line, and that if you pass hod a tarball with a modified /conf, then the cluster will fail mysteriously to be initialized. I don't see any references to either in the documentation, and it'd be great to write this down.",-0.082,-0.082,negative
hadoop,3505,summary,omissions in HOD documentation,documentation_debt,low_quality_documentation,"Thu, 5 Jun 2008 22:07:49 +0000","Fri, 22 Aug 2008 19:50:40 +0000","Sat, 21 Jun 2008 00:27:40 +0000",1304391,omissions in HOD documentation,0,0,neutral
hadoop,3836,comment_2,"I tried the patch. The test still fails. The problem is that there is a typo in line 180 (after the patch). ""reader.close()"" should be It works fine after fixed the typo. Also, the before is not needed.",documentation_debt,low_quality_documentation,"Fri, 25 Jul 2008 20:13:51 +0000","Wed, 8 Jul 2009 16:52:55 +0000","Fri, 1 Aug 2008 22:45:04 +0000",613873,"I tried the patch. The test still fails. The problem is that there is a typo in line 180 (after the patch). ""reader.close()"" should be ""seqReader.close()"". It works fine after fixed the typo. Also, the ""@SuppressWarnings( {""unchecked""} )"" before _testMultipleOutputs(...) is not needed.",0.00675,0.0050625,negative
hadoop,3905,comment_3,I fixed one JavaDoc warning.,documentation_debt,low_quality_documentation,"Wed, 6 Aug 2008 01:49:11 +0000","Wed, 8 Jul 2009 16:43:16 +0000","Sat, 16 Aug 2008 00:06:40 +0000",857849,I fixed one JavaDoc warning.,-0.6,-0.6,neutral
hadoop,4066,comment_1,I think it is better to keep this document in the README file. The reasons being: 1. The README is version-controlled and can change from version to version. 2. All contrib modules are *required* to have a README file.,documentation_debt,low_quality_documentation,"Thu, 4 Sep 2008 00:54:33 +0000","Wed, 8 Jul 2009 17:05:05 +0000","Thu, 4 Sep 2008 22:20:22 +0000",77149,I think it is better to keep this document in the README file. The reasons being: 1. The README is version-controlled and can change from version to version. 2. All contrib modules are required to have a README file.,0.1,0.1,neutral
hadoop,4066,comment_2,"Wiki is *not* an acceptable place for system documentation. In particular, it is not versioned or released.",documentation_debt,low_quality_documentation,"Thu, 4 Sep 2008 00:54:33 +0000","Wed, 8 Jul 2009 17:05:05 +0000","Thu, 4 Sep 2008 22:20:22 +0000",77149,"Wiki is not an acceptable place for system documentation. In particular, it is not versioned or released.",-0.328,-0.328,negative
hadoop,4066,comment_3,I filed to cleanup the README and will delete the version specific stuff from the wiki.,documentation_debt,low_quality_documentation,"Thu, 4 Sep 2008 00:54:33 +0000","Wed, 8 Jul 2009 17:05:05 +0000","Thu, 4 Sep 2008 22:20:22 +0000",77149,I filed https://issues.apache.org/jira/browse/HADOOP-4076 to cleanup the README and will delete the version specific stuff from the wiki.,0,0,neutral
hadoop,4066,description,stop having to duplicate changes in both the readme and wiki This is a change to the README only and as such should not require any QA or anything and obviously no unit tests :),documentation_debt,low_quality_documentation,"Thu, 4 Sep 2008 00:54:33 +0000","Wed, 8 Jul 2009 17:05:05 +0000","Thu, 4 Sep 2008 22:20:22 +0000",77149,stop having to duplicate changes in both the readme and wiki This is a change to the README only and as such should not require any QA or anything and obviously no unit tests,0.1,-0.2,neutral
hadoop,4182,comment_1,"I agree with you that it is a problem at the application / user level. I only wanted to put a simple comment somewhere on the Hadoop Wiki that says that a line must end with an end of line delimiter. If not, user might get different behaviors as I explained earlier. This simple comment can keep a user from accidental un-expected results.",documentation_debt,low_quality_documentation,"Tue, 16 Sep 2008 05:36:36 +0000","Wed, 8 Jul 2009 17:05:24 +0000","Tue, 23 Sep 2008 16:58:13 +0000",645697,"I agree with you that it is a problem at the application / user level. I only wanted to put a simple comment somewhere on the Hadoop Wiki that says that a line must end with an end of line delimiter. If not, user might get different behaviors as I explained earlier. This simple comment can keep a user from accidental un-expected results.",0,0,neutral
hadoop,4182,description,"When Text input data is used with streaming, every line is expected to end with a newline. Hadoop results are undefined if input files do not end in a newline. (The results will depend on how files are assigned to mappers.) Example: In streaming if mapper = xargs cat reducer = cat and the input is a two line, where each line is symbolic link in HDFS link1\n link2\n EOF link1 points to a file which contains This is line1EOF link2 points to a file which contains This is line2EOF Now running a streaming job such that, there is only one split, will produce results: This is line1This is line2\t\n But if there were two splits, the result will be This is line1\t\n This is line2\t\n So in summary, the output depends on the factor that how many mappers were invoked. As a caution, it should be recorded in Streaming wiki that users always put a new line at the end of each line to get away with such problems.",documentation_debt,low_quality_documentation,"Tue, 16 Sep 2008 05:36:36 +0000","Wed, 8 Jul 2009 17:05:24 +0000","Tue, 23 Sep 2008 16:58:13 +0000",645697,"When Text input data is used with streaming, every line is expected to end with a newline. Hadoop results are undefined if input files do not end in a newline. (The results will depend on how files are assigned to mappers.) Example: In streaming if mapper = xargs cat reducer = cat and the input is a two line, where each line is symbolic link in HDFS link1\n link2\n EOF link1 points to a file which contains This is line1EOF link2 points to a file which contains This is line2EOF Now running a streaming job such that, there is only one split, will produce results: This is line1This is line2\t\n But if there were two splits, the result will be This is line1\t\n This is line2\t\n So in summary, the output depends on the factor that how many mappers were invoked. As a caution, it should be recorded in Streaming wiki that users always put a new line at the end of each line to get away with such problems.",-0.18,-0.18,neutral
hadoop,4603,description,"A default installation as outlined in the docs won't start on Solaris 10 x86. The ""whoami"" utility is in path ""/usr/ucb"" on Solaris 10, which isn't in the standard PATH environment variable unless the user has added that specifically. The documentation should reflect this. Solaris 10 also seemed to throw NPEs if you didn't explicitly set the IP address to bind the servers to. Simply overriding the IP address fixes the problem.",documentation_debt,outdated_documentation,"Thu, 6 Nov 2008 21:21:29 +0000","Mon, 21 Jul 2014 16:56:19 +0000","Mon, 21 Jul 2014 16:56:19 +0000",179955290,"A default installation as outlined in the docs won't start on Solaris 10 x86. The ""whoami"" utility is in path ""/usr/ucb"" on Solaris 10, which isn't in the standard PATH environment variable unless the user has added that specifically. The documentation should reflect this. Solaris 10 also seemed to throw NPEs if you didn't explicitly set the IP address to bind the servers to. Simply overriding the IP address fixes the problem.",0.1025,0.1025,neutral
hadoop,4611,description,"The documentation for the Tool interface will not work out of the box. It seems to have taken the Sort() implementation in examples, but has ripped out some important information. 1) args[1] and args[2] should probably be args[0] and args[1], as most MapReduce tasks don't take the first argument that examples.jar takes 2) int run() needs to actually return an int 3) and are deprecated. 4) the call to ToolRunner.run() in main() should take ""new MyApp()"" instead of ""Sort()"" as an argument More generally, a working implementation of Tool in the docs would be handy.",documentation_debt,low_quality_documentation,"Fri, 7 Nov 2008 06:37:30 +0000","Sat, 19 Jul 2014 00:04:56 +0000","Sat, 19 Jul 2014 00:04:56 +0000",179688446,"The documentation for the Tool interface will not work out of the box. It seems to have taken the Sort() implementation in examples, but has ripped out some important information. 1) args[1] and args[2] should probably be args[0] and args[1], as most MapReduce tasks don't take the first argument that examples.jar takes 2) int run() needs to actually return an int 3) JobConf.setInputPath() and JobConf.setOutputPath() are deprecated. 4) the call to ToolRunner.run() in main() should take ""new MyApp()"" instead of ""Sort()"" as an argument More generally, a working implementation of Tool in the docs would be handy.",0.07916666667,0.059375,negative
hadoop,4611,summary,Documentation for Tool interface is a bit busted,documentation_debt,low_quality_documentation,"Fri, 7 Nov 2008 06:37:30 +0000","Sat, 19 Jul 2014 00:04:56 +0000","Sat, 19 Jul 2014 00:04:56 +0000",179688446,Documentation for Tool interface is a bit busted,0,0,negative
hadoop,4719,comment_1,"""&amp;lt;number of replicas&amp;gt;"" should be changed to since the ls output does not contain '<' and '>' anymore.",documentation_debt,outdated_documentation,"Tue, 25 Nov 2008 01:00:50 +0000","Mon, 3 Aug 2009 12:59:03 +0000","Tue, 24 Mar 2009 23:44:12 +0000",10363402,"""&lt;number of replicas&gt;"" should be changed to ""number_of_replicas"" since the ls output does not contain '<' and '>' anymore.",0,0,neutral
hadoop,4719,summary,The ls shell command documentation is out-dated,documentation_debt,outdated_documentation,"Tue, 25 Nov 2008 01:00:50 +0000","Mon, 3 Aug 2009 12:59:03 +0000","Tue, 24 Mar 2009 23:44:12 +0000",10363402,The ls shell command documentation is out-dated,0,0,negative
hadoop,4985,comment_2,"- FSDirectory empty line change. - Javadoc for should not remove {{@throws IOException}} but rather replace it with {{@throws - file: "" ...)}} should be does not exist: "" ...)}} I once tried to unify this in the code, but it's now back with all different messages. - Additionally we can remove IOException in FSEditLog.close() and then and then",documentation_debt,low_quality_documentation,"Wed, 7 Jan 2009 02:10:41 +0000","Wed, 12 Aug 2020 23:02:16 +0000","Mon, 12 Jan 2009 21:18:36 +0000",500875,"FSDirectory empty line change. Javadoc for FSDirectory.setReplication() should not remove @throws IOException but rather replace it with @throws QuotaExceededException FileNotFoundException(""Unknown file: "" ...) should be FileNotFoundException(""File does not exist: "" ...) I once tried to unify this in the code, but it's now back with all different messages. Additionally we can remove IOException in FSEditLog.close() FSEditLog.existsNew() FSEditLog.logOpenFile() FSEditLog.getFsEditName() and then FSDirectory.persistBlocks() FSDirectory.closeFile() FSDirectory.setTimes() and then FSNamesystem.getBlockLocationsInternal()",-0.1,-0.01666666667,negative
hadoop,524,description,Documentation for contrib modules like streaming and the small jobs benchmark does not appear in the Javadoc.,documentation_debt,low_quality_documentation,"Tue, 12 Sep 2006 22:40:35 +0000","Sat, 3 Feb 2007 03:26:58 +0000","Thu, 4 Jan 2007 19:05:10 +0000",9836675,Documentation for contrib modules like streaming and the small jobs benchmark does not appear in the Javadoc.,0,0,neutral
hadoop,524,summary,Contrib documentation does not appear in Javadoc,documentation_debt,low_quality_documentation,"Tue, 12 Sep 2006 22:40:35 +0000","Sat, 3 Feb 2007 03:26:58 +0000","Thu, 4 Jan 2007 19:05:10 +0000",9836675,Contrib documentation does not appear in Javadoc,0,0,neutral
hadoop,5771,comment_2,"Attaching the new patch incorporating Vinod's offline comment. * Removed unused imports. * Added new assertion to check if the passed user actually ran the job. * Modified, {{TrApp}} to stop check if the local job runner and map output start, so that {{TrApp}} can be run on a {{MiniMRCluster}} * Changed documentation, added it to javadoc and release note of JIRA issue.",documentation_debt,outdated_documentation,"Tue, 5 May 2009 06:11:08 +0000","Tue, 24 Aug 2010 20:37:25 +0000","Mon, 11 May 2009 16:37:16 +0000",555968,"Attaching the new patch incorporating Vinod's offline comment. Removed unused imports. Added new assertion to check if the passed user actually ran the job. Modified, TrApp to stop check if the local job runner and map output start, so that TrApp can be run on a MiniMRCluster Changed documentation, added it to javadoc and release note of JIRA issue.",-0.1,-0.1,neutral
hadoop,6134,description,"New Hadoop Common Site Set up site, initial pass. May need to add more content. May need to update some links.",documentation_debt,low_quality_documentation,"Wed, 8 Jul 2009 23:29:45 +0000","Fri, 17 Jul 2009 15:54:54 +0000","Fri, 17 Jul 2009 15:54:54 +0000",750309,"New Hadoop Common Site Set up site, initial pass. May need to add more content. May need to update some links.",0.1666666667,0.1666666667,neutral
hadoop,6145,comment_5,"also, the trunk patch has a quick fix to bring the rm documentation in line with what was committed in HADOOP-6139.",documentation_debt,outdated_documentation,"Thu, 9 Jul 2009 08:02:37 +0000","Fri, 2 Jul 2010 04:49:07 +0000","Tue, 14 Jul 2009 17:42:05 +0000",466768,"also, the trunk patch has a quick fix to bring the rm documentation in line with what was committed in HADOOP-6139.",0.5,0.5,positive
hadoop,6151,comment_6,Messed up the JavaDoc. Now fixed.,documentation_debt,low_quality_documentation,"Wed, 15 Jul 2009 16:24:17 +0000","Tue, 24 Aug 2010 20:39:04 +0000","Fri, 18 Sep 2009 16:33:30 +0000",5616553,Messed up the JavaDoc. Now fixed.,-0.2,-0.2,negative
hadoop,6229,comment_12,I've just committed this. Thanks Boris! Could you update the release note with the new behaviour please?,documentation_debt,outdated_documentation,"Wed, 2 Sep 2009 00:40:39 +0000","Wed, 26 Apr 2017 08:44:34 +0000","Mon, 7 Sep 2009 13:27:32 +0000",478013,I've just committed this. Thanks Boris! Could you update the release note with the new behaviour please?,0.3666666667,0.3666666667,neutral
hadoop,6364,comment_1,"This sorta-kinda exists in the undocumented configuration slave.host.name, right?",documentation_debt,low_quality_documentation,"Thu, 5 Nov 2009 17:30:50 +0000","Thu, 22 Mar 2012 06:02:21 +0000","Wed, 2 Nov 2011 18:22:22 +0000",62815892,"This sorta-kinda exists in the undocumented configuration slave.host.name, right?",-0.02433333333,-0.02433333333,neutral
hadoop,6364,comment_2,I don't know. I've never tried it since it is undocumented. :),documentation_debt,low_quality_documentation,"Thu, 5 Nov 2009 17:30:50 +0000","Thu, 22 Mar 2012 06:02:21 +0000","Wed, 2 Nov 2011 18:22:22 +0000",62815892,I don't know. I've never tried it since it is undocumented.,0.1333333333,0,neutral
hadoop,6536,description,"deletes contents of sym-linked directory when we pass a symlink. If this is the behavior, it should be documented as so. Or it should be changed not to delete the contents of the sym-linked directory.",documentation_debt,low_quality_documentation,"Wed, 3 Feb 2010 05:54:15 +0000","Mon, 12 Dec 2011 06:18:39 +0000","Tue, 20 Jul 2010 05:23:08 +0000",14426933,"FileUtil.fullyDelete(dir) deletes contents of sym-linked directory when we pass a symlink. If this is the behavior, it should be documented as so. Or it should be changed not to delete the contents of the sym-linked directory.",0,0,neutral
hadoop,6538,comment_2,"The documentation in the XML looks wrong - it's copied from the setting above. Also, the code in should be updated to reflect this default (right now it defaults to kerberos)",documentation_debt,low_quality_documentation,"Thu, 4 Feb 2010 00:53:07 +0000","Tue, 24 Aug 2010 20:41:47 +0000","Thu, 4 Feb 2010 07:46:34 +0000",24807,"The documentation in the XML looks wrong - it's copied from the setting above. Also, the code in UserGroupInformation should be updated to reflect this default (right now it defaults to kerberos)",-0.11825,-0.11825,negative
hadoop,6538,comment_4,"The description in the XML still reads ""Is service-level authorization enabled?"" which is copied from the conf item above. It should instead describe the options ""simple"" vs ""kerberos""",documentation_debt,low_quality_documentation,"Thu, 4 Feb 2010 00:53:07 +0000","Tue, 24 Aug 2010 20:41:47 +0000","Thu, 4 Feb 2010 07:46:34 +0000",24807,"The description in the XML still reads ""Is service-level authorization enabled?"" which is copied from the conf item above. It should instead describe the options ""simple"" vs ""kerberos""",0.09366666667,0.09366666667,neutral
hadoop,6658,comment_5,New patch which excludes annotations that are marked Private or LimitedPrivate. Also removes Javadoc and RAT warnings.,documentation_debt,low_quality_documentation,"Wed, 24 Mar 2010 04:14:18 +0000","Wed, 28 Mar 2018 20:17:32 +0000","Thu, 22 Apr 2010 20:48:54 +0000",2565276,New patch which excludes annotations that are marked Private or LimitedPrivate. Also removes Javadoc and RAT warnings.,-0.3,-0.3,neutral
hadoop,6730,comment_9,Updated patch. Removed static from fc initialization. Removed invalid comments.,documentation_debt,low_quality_documentation,"Wed, 28 Apr 2010 18:50:09 +0000","Mon, 12 Dec 2011 06:19:03 +0000","Sat, 1 May 2010 21:09:36 +0000",267567,Updated patch. Removed static from fc initialization. Removed invalid comments.,-0.2916666667,-0.2916666667,neutral
hadoop,6839,comment_9,"I can see there are 6 javadoc warnings and all of them are related to warning: So, I don't think the patch could raise the number of java doc warning considering the scope of the patch.",documentation_debt,low_quality_documentation,"Fri, 25 Jun 2010 08:24:21 +0000","Thu, 2 May 2013 02:29:31 +0000","Wed, 14 Jul 2010 18:47:11 +0000",1678970,"I can see there are 6 javadoc warnings and all of them are related to warning: sun.security.krb5.Config. So, I don't think the patch could raise the number of java doc warning considering the scope of the patch.",-0.2,0,negative
hadoop,6885,comment_4,Updated the patch to fix the IPC javadoc warnings. The earlier warnings are seen when using javadoc-dev. Both targets build cleanly on trunk with this patch. Thanks Jakob.,documentation_debt,low_quality_documentation,"Wed, 28 Jul 2010 01:46:25 +0000","Mon, 12 Dec 2011 06:19:06 +0000","Wed, 18 Aug 2010 21:51:40 +0000",1886715,Updated the patch to fix the IPC javadoc warnings. The earlier warnings are seen when using javadoc-dev. Both targets build cleanly on trunk with this patch. Thanks Jakob.,-0.2,-0.2,neutral
hadoop,6885,description,There are a couple java docs warnings in Groups and,documentation_debt,low_quality_documentation,"Wed, 28 Jul 2010 01:46:25 +0000","Mon, 12 Dec 2011 06:19:06 +0000","Wed, 18 Aug 2010 21:51:40 +0000",1886715,There are a couple java docs warnings in Groups and RefreshUserMappingsProtocol.,-0.6,-0.6,neutral
hadoop,6885,summary,Fix java doc warnings in Groups and,documentation_debt,low_quality_documentation,"Wed, 28 Jul 2010 01:46:25 +0000","Mon, 12 Dec 2011 06:19:06 +0000","Wed, 18 Aug 2010 21:51:40 +0000",1886715,Fix java doc warnings in Groups and RefreshUserMappingsProtocol,-0.6,-0.6,neutral
hadoop,6925,comment_5,"Forgot to mention that I verified TestCodec passed on trunk and the merge to branch-0.21. Previous comment has a typo, meant *branch-0.21* not branch-0.20.",documentation_debt,low_quality_documentation,"Tue, 24 Aug 2010 22:09:57 +0000","Mon, 12 Dec 2011 06:18:39 +0000","Tue, 24 Aug 2010 22:48:49 +0000",2332,"Forgot to mention that I verified TestCodec passed on trunk and the merge to branch-0.21. Previous comment has a typo, meant branch-0.21 not branch-0.20.",0.03,0.03,neutral
hadoop,6936,comment_5,"Fixed the FAQ links. The HowToConfigure page is highly outdated. I do not know if it still serves a purpose beyond the tutorial (which explains how to too). It could be possibly set up to redirect to a current documentation. But as Nicholas notes, this is all doable by anyone signed into the Wiki (free account signups, just gotta enter captchas). Filing a JIRA for this is not required.",documentation_debt,outdated_documentation,"Wed, 1 Sep 2010 19:38:57 +0000","Sun, 12 Jun 2011 18:55:25 +0000","Sun, 12 Jun 2011 18:52:36 +0000",24534819,"Fixed the FAQ links. The HowToConfigure page is highly outdated. I do not know if it still serves a purpose beyond the tutorial (which explains how to too). It could be possibly set up to redirect to a current documentation. But as Nicholas notes, this is all doable by anyone signed into the Wiki (free account signups, just gotta enter captchas). Filing a JIRA for this is not required.",0.1051666667,0.1051666667,negative
hadoop,6936,description,has links to : both of which are 404 as of time of filing this issue.,documentation_debt,low_quality_documentation,"Wed, 1 Sep 2010 19:38:57 +0000","Sun, 12 Jun 2011 18:55:25 +0000","Sun, 12 Jun 2011 18:52:36 +0000",24534819,http://wiki.apache.org/hadoop/FAQ#A12//s has links to : http://hadoop.apache.org/core/docs/current/hadoop-default.html#dfs.replication.min http://hadoop.apache.org/common/docs/current/hadoop-default.html#dfs.safemode.threshold.pct both of which are 404 as of time of filing this issue.,0,0,negative
hadoop,6936,summary,broken links in,documentation_debt,low_quality_documentation,"Wed, 1 Sep 2010 19:38:57 +0000","Sun, 12 Jun 2011 18:55:25 +0000","Sun, 12 Jun 2011 18:52:36 +0000",24534819,broken links in http://wiki.apache.org/hadoop/FAQ#A12//s,-0.2,-0.2,negative
hadoop,7057,comment_1,For apparent reasons - typo correction - patch doesn't provide any tests. I'm going to commit this for it is trivial.,documentation_debt,low_quality_documentation,"Wed, 1 Dec 2010 00:59:34 +0000","Tue, 15 Nov 2011 00:50:50 +0000","Wed, 1 Dec 2010 01:44:24 +0000",2690,For apparent reasons - typo correction - patch doesn't provide any tests. I'm going to commit this for it is trivial.,0.1,0.1,negative
hadoop,7057,summary,IOUtils.readFully and IOUtils.skipFully have typo in exception creation's message,documentation_debt,low_quality_documentation,"Wed, 1 Dec 2010 00:59:34 +0000","Tue, 15 Nov 2011 00:50:50 +0000","Wed, 1 Dec 2010 01:44:24 +0000",2690,IOUtils.readFully and IOUtils.skipFully have typo in exception creation's message,0,0,negative
hadoop,7323,comment_1,This looks good. The only nit I noticed is that the javadoc on codecsByName in is incorrect.,documentation_debt,low_quality_documentation,"Mon, 23 May 2011 22:55:09 +0000","Wed, 8 Jun 2011 11:15:35 +0000","Tue, 7 Jun 2011 18:32:27 +0000",1280238,This looks good. The only nit I noticed is that the javadoc on codecsByName in CompressionCodecFactory is incorrect.,0.388,0.388,positive
hadoop,7634,description,The cluster setup docs indicate task-controller.cfg must be owned by the user running TaskTracker but the code checks for root. We should update the docs to reflect the real requirement.,documentation_debt,outdated_documentation,"Wed, 14 Sep 2011 02:47:30 +0000","Wed, 17 Oct 2012 18:27:26 +0000","Mon, 19 Sep 2011 16:18:46 +0000",480676,The cluster setup docs indicate task-controller.cfg must be owned by the user running TaskTracker but the code checks for root. We should update the docs to reflect the real requirement.,0.1666666667,0.1666666667,neutral
hadoop,7634,summary,Cluster setup docs specify wrong owner for task-controller.cfg,documentation_debt,low_quality_documentation,"Wed, 14 Sep 2011 02:47:30 +0000","Wed, 17 Oct 2012 18:27:26 +0000","Mon, 19 Sep 2011 16:18:46 +0000",480676,Cluster setup docs specify wrong owner for task-controller.cfg,-0.125,-0.125,negative
hadoop,7919,description,"The following only resides in core-default.xml and doesn't look like its used anywhere at all. At least a grep of the prop name and parts of it does not give me back anything at all. These settings are now configurable via generic Log4J opts, via the shipped log4j.properties file in the distributions.",documentation_debt,low_quality_documentation,"Tue, 13 Dec 2011 20:34:20 +0000","Mon, 5 Mar 2012 02:49:05 +0000","Mon, 2 Jan 2012 06:28:51 +0000",1677271,"The following only resides in core-default.xml and doesn't look like its used anywhere at all. At least a grep of the prop name and parts of it does not give me back anything at all. These settings are now configurable via generic Log4J opts, via the shipped log4j.properties file in the distributions.",-0.1,-0.1,negative
hadoop,7924,comment_3,"Thanks for the good feedback. Updated patch attached, addresses your comments, also adds an ""haadmin"" hook into bin/hadoop. #1 Lemme know if my comment wrt naming made sense. #2 Agree. Was originally thinking in this case the admin would just make the standby active directly, but we need to run through the whole failover in this case for fencing. Updated code and tests. For sanity, I ran bin/hadoop haadmin failover X Y and verified that it works (and prints something reasonable) when X isn't running, and fails when Y isn't yet running (and A remains active of course). Note that this means if we failover from X to Y and there's a typo in X then you might unexpectedely end up with two actives (added Another jira for warning if X is not specified as one of the options in dfs.ha.namenodes or we add a flag that's required to failover from a service we can't connect to? #3 Done. #4 Done. Also fixed up the other HA exceptions to be consistent.",documentation_debt,low_quality_documentation,"Wed, 14 Dec 2011 09:40:08 +0000","Fri, 6 Jan 2012 14:38:48 +0000","Thu, 5 Jan 2012 21:13:44 +0000",1942416,"Thanks for the good feedback. Updated patch attached, addresses your comments, also adds an ""haadmin"" hook into bin/hadoop. #1 Lemme know if my comment wrt naming made sense. #2 Agree. Was originally thinking in this case the admin would just make the standby active directly, but we need to run through the whole failover in this case for fencing. Updated code and tests. For sanity, I ran bin/hadoop haadmin failover X Y and verified that it works (and prints something reasonable) when X isn't running, and fails when Y isn't yet running (and A remains active of course). Note that this means if we failover from X to Y and there's a typo in X then you might unexpectedely end up with two actives (added testManualFailoverCanResultInTwoActives). Another jira for warning if X is not specified as one of the options in dfs.ha.namenodes or we add a flag that's required to failover from a service we can't connect to? #3 Done. #4 Done. Also fixed up the other HA exceptions to be consistent.",0.106875,0.09865384615,positive
hadoop,7924,comment_4,"in I think it's clearer to structure the code like: rather than trying to collapse both exceptions into one throw. Also, should log the exception thrown by getServiceState. - In {{failover()}}, I think you probably want to catch all Throwables in another catch clause - eg what if it's in some bad state and your failover attempt caused it to crash, which would give your IPC a - indentation - I think better to not abbreviate ""first"" and ""second"" - - Can you add some javadoc to -- it's strange that this is a test case... it's more like you're showing that a particular user error can cause a problem, rather than showing something about the bug, right? Or else it should be a test case that fails, with an @Ignore explaining why it fails, maybe? - Just to confirm, the manual test you mentioned was done with two NNs in a running HA cluster?",documentation_debt,outdated_documentation,"Wed, 14 Dec 2011 09:40:08 +0000","Fri, 6 Jan 2012 14:38:48 +0000","Thu, 5 Jan 2012 21:13:44 +0000",1942416,"in preFailoverChecks, I think it's clearer to structure the code like: rather than trying to collapse both exceptions into one throw. Also, should log the exception thrown by getServiceState. In failover(), I think you probably want to catch all Throwables in another catch clause - eg what if it's in some bad state and your failover attempt caused it to crash, which would give your IPC a SocketTimeoutException. indentation I think better to not abbreviate ""first"" and ""second"" Can you add some javadoc to testManualFailoverCanResultInTwoActives  it's strange that this is a test case... it's more like you're showing that a particular user error can cause a problem, rather than showing something about the bug, right? Or else it should be a test case that fails, with an @Ignore explaining why it fails, maybe? Just to confirm, the manual test you mentioned was done with two NNs in a running HA cluster?",-0.07415625,-0.05187777778,neutral
hadoop,8209,comment_6,"Thanks ATM. Will fix the spelling misstake, running the full suite now.",documentation_debt,low_quality_documentation,"Sun, 25 Mar 2012 22:17:38 +0000","Wed, 24 Oct 2012 19:12:09 +0000","Thu, 12 Apr 2012 22:06:37 +0000",1554539,"Thanks ATM. Will fix the spelling misstake, running the full suite now.",0.2,0.2,neutral
hadoop,8341,comment_3,"Addressing the missing license statement in findbugs exclude file. The test passes for me locally. I think it is caused by running with an older version of an HDFS jar, so I am going to upload the new patch and see if that fixes it.",documentation_debt,low_quality_documentation,"Tue, 1 May 2012 20:09:22 +0000","Thu, 12 May 2016 18:25:56 +0000","Tue, 8 May 2012 13:24:35 +0000",580513,"Addressing the missing license statement in findbugs exclude file. The test passes for me locally. I think it is caused by running with an older version of an HDFS jar, so I am going to upload the new patch and see if that fixes it.",-0.1,-0.1,negative
hadoop,8367,comment_1,"I was wrong here. Since the connection is reused, the is needed. Will better document this in the code; will change the jira title to do this.",documentation_debt,outdated_documentation,"Mon, 7 May 2012 18:46:44 +0000","Thu, 11 Oct 2012 17:45:12 +0000","Wed, 23 May 2012 21:19:46 +0000",1391582,"I was wrong here. Since the connection is reused, the declaringClassProtocolName is needed. Will better document this in the code; will change the jira title to do this.",0.08333333333,0.08333333333,neutral
hadoop,8367,comment_3,Minor comments: # #* Typo differnt #* The newly added comment is not very clear. Can you please add more information about what you mean by metaProtocols. Also the sentense does not read right. It might make sense to capture the same comments from hadoop_rpc.proto in here. # hadoop_rpc.proto some lines are going beyond 80 characters. Also the last sentence in the newly added comment does not read right.,documentation_debt,low_quality_documentation,"Mon, 7 May 2012 18:46:44 +0000","Thu, 11 Oct 2012 17:45:12 +0000","Wed, 23 May 2012 21:19:46 +0000",1391582,Minor comments: ProtobufRpcEngine.java Typo differnt The newly added comment is not very clear. Can you please add more information about what you mean by metaProtocols. Also the sentense does not read right. It might make sense to capture the same comments from hadoop_rpc.proto in here. hadoop_rpc.proto some lines are going beyond 80 characters. Also the last sentence in the newly added comment does not read right.,-0.10675,-0.09488888889,negative
hadoop,8367,summary,Improve documentation of in rpc headers,documentation_debt,low_quality_documentation,"Mon, 7 May 2012 18:46:44 +0000","Thu, 11 Oct 2012 17:45:12 +0000","Wed, 23 May 2012 21:19:46 +0000",1391582,Improve documentation of declaringClassProtocolName in rpc headers,0.4,0.4,neutral
hadoop,8741,description,"Hi, The links from the cluster setup pages to the configuration files are broken. Read-only default configuration should be The same holds for the three configuration : core, hdfs and mapred.",documentation_debt,low_quality_documentation,"Tue, 28 Aug 2012 14:27:13 +0000","Wed, 24 May 2017 09:57:48 +0000","Wed, 24 May 2017 09:57:47 +0000",149455834,"Hi, The links from the cluster setup pages to the configuration files are broken. http://hadoop.apache.org/common/docs/stable/cluster_setup.html Read-only default configuration http://hadoop.apache.org/common/docs/current/core-default.html should be http://hadoop.apache.org/common/docs/r1.0.3/core-default.html The same holds for the three configuration : core, hdfs and mapred.",-0.35,-0.35,negative
hadoop,8741,summary,"Broken links from ""Cluster setup"" to *-default.html",documentation_debt,low_quality_documentation,"Tue, 28 Aug 2012 14:27:13 +0000","Wed, 24 May 2017 09:57:48 +0000","Wed, 24 May 2017 09:57:47 +0000",149455834,"Broken links from ""Cluster setup"" to *-default.html",-0.175,-0.175,negative
hadoop,8929,comment_6,Summary and the bug description needs update based on this [comment |,documentation_debt,outdated_documentation,"Mon, 15 Oct 2012 19:46:19 +0000","Thu, 12 May 2016 18:22:35 +0000","Tue, 16 Oct 2012 06:07:18 +0000",37259,Summary and the bug description needs update based on this comment,0,0,neutral
hadoop,9165,comment_1,"Junping, I'd suggest splitting the YARN and MAPREDUCE docs. I am in favor of YARN-291 instead of TT changes mainly because it flows so much smoothly in YARN and also how complicated the TT changes could turn out to be. Please close this as invalid as you already have YARN and MAPREDUCE specific tickets. Tx.",documentation_debt,low_quality_documentation,"Sun, 23 Dec 2012 08:01:38 +0000","Mon, 1 Apr 2013 21:22:11 +0000","Thu, 27 Dec 2012 09:15:56 +0000",350058,"Junping, I'd suggest splitting the YARN and MAPREDUCE docs. I am in favor of YARN-291 instead of TT changes mainly because it flows so much smoothly in YARN and also how complicated the TT changes could turn out to be. Please close this as invalid as you already have YARN and MAPREDUCE specific tickets. Tx.",0.13125,0.13125,neutral
hadoop,9218,comment_0,Several methods on ipc.Server and RPC use type Writable *internally* as a wrapper so that they work across multiple RpcEngine kinds. The request on the wire is in Protobuf for the Protocbuf Rpc Engine. Also the basic rpc headers are also in protobuf. Patch adds javadoc and also changes the names of wrappers.,documentation_debt,low_quality_documentation,"Wed, 16 Jan 2013 00:44:10 +0000","Tue, 27 Aug 2013 22:06:41 +0000","Fri, 15 Feb 2013 02:27:46 +0000",2598216,Several methods on ipc.Server and RPC use type Writable internally as a wrapper so that they work across multiple RpcEngine kinds. The request on the wire is in Protobuf for the Protocbuf Rpc Engine. Also the basic rpc headers are also in protobuf. Patch adds javadoc and also changes the names of wrappers.,0.02,0.02,neutral
hadoop,9267,description,It's not friendly for new users when the command line scripts don't show usage instructions when passed the defacto Unix usage flags. Imagine this sequence of commands: Same applies for the `hdfs` script.,documentation_debt,low_quality_documentation,"Thu, 31 Jan 2013 03:29:16 +0000","Thu, 12 May 2016 18:21:53 +0000","Fri, 22 Feb 2013 18:38:00 +0000",1955324,It's not friendly for new users when the command line scripts don't show usage instructions when passed the defacto Unix usage flags. Imagine this sequence of commands: Same applies for the `hdfs` script.,-0.172,-0.172,negative
hadoop,9336,comment_7,"+1 on the code change, but more detailed comment on getRemoteUser() will be nice, in order to avoid potential misuse.",documentation_debt,low_quality_documentation,"Tue, 26 Feb 2013 20:22:15 +0000","Thu, 12 May 2016 18:21:47 +0000","Thu, 28 Feb 2013 22:07:17 +0000",179102,"+1 on the code change, but more detailed comment on getRemoteUser() will be nice, in order to avoid potential misuse.",0.175,0.175,neutral
hadoop,9763,comment_12,"The patch looks very good to me. Only some minors: 1. it would be better if more javadoc/comments can be added to such as the general steps for each check process, and what is tested for each step. 2. Maybe we can assign an initial random value for",documentation_debt,low_quality_documentation,"Sun, 21 Jul 2013 10:16:02 +0000","Tue, 27 Aug 2013 22:06:38 +0000","Wed, 24 Jul 2013 06:49:54 +0000",246832,"The patch looks very good to me. Only some minors: 1. TestLightWeightCache.java, it would be better if more javadoc/comments can be added to TestLightWeightCache, such as the general steps for each check process, and what is tested for each step. 2. Maybe we can assign an initial random value for LightWeightCacheTestCase#currentTestTime?",0.2615,0.2179166667,positive
hadoop,10169,comment_4,"As Jing pointed out, the function needs to be synchronized as written. It's true that once initialized the code will no longer modify the map, but there's a race during the initialization itself. If two or more threads call this function before it's initialized then they can both attempt to put() at the same time. Alternatively, a thread could be late to the party and be calling get() just as the initializing thread is calling put() which is also a thread safety violation. One possible way to avoid the synchronization bottleneck is to use a volatile boolean to indicate whether it's initialized so the vast majority of callers don't have to grab a lock. Something like this pseudo-code:",requirement_debt,non-functional_requirements_not_fully_satisfied,"Tue, 17 Dec 2013 05:37:44 +0000","Thu, 12 May 2016 18:27:56 +0000","Fri, 20 Dec 2013 22:09:01 +0000",318677,"As Jing pointed out, the function needs to be synchronized as written. It's true that once initialized the code will no longer modify the map, but there's a race during the initialization itself. If two or more threads call this function before it's initialized then they can both attempt to put() at the same time. Alternatively, a thread could be late to the party and be calling get() just as the initializing thread is calling put() which is also a thread safety violation. One possible way to avoid the synchronization bottleneck is to use a volatile boolean to indicate whether it's initialized so the vast majority of callers don't have to grab a lock. Something like this pseudo-code:",0.06427777778,0.06427777778,neutral
hadoop,10353,summary,is not thread safe,requirement_debt,non-functional_requirements_not_fully_satisfied,"Thu, 20 Feb 2014 18:41:33 +0000","Thu, 4 Sep 2014 01:16:46 +0000","Thu, 27 Feb 2014 19:17:50 +0000",606977,FsUrlStreamHandlerFactory is not thread safe,-0.25,-0.25,negative
hadoop,10427,comment_0,Patch that states in the {{KeyProvider}} javadocs that implementations must be thread-safe and makes and {{UserProvider}} (the {{KeyProvider}} impls in Hadoop) thread-safe. uses a read-write lock while UserProvider synchronizes its methods.,requirement_debt,non-functional_requirements_not_fully_satisfied,"Tue, 25 Mar 2014 05:25:48 +0000","Thu, 12 May 2016 18:26:58 +0000","Wed, 9 Apr 2014 19:43:52 +0000",1347484,Patch that states in the KeyProvider javadocs that implementations must be thread-safe and makes JavaKeyStoreProvider and UserProvider (the KeyProvider impls in Hadoop) thread-safe. JavaKeyStoreProvider uses a read-write lock while UserProvider synchronizes its methods.,0.1666666667,0.1666666667,neutral
hadoop,10427,comment_6,I start getting a little concerned when we are talking about thread safety of these. Mainly because the should not be used as a database. Initial implementation of the KeyProvider API assumes a rather basic and controlled access to key management. We do need to ensure thread safety for these implementations and equally as important for protection against corrupted keystores. See: HADOOP-10224 I will add that jira as related. I would like to consider a more appropriate provider type for access from a KMS as well.,requirement_debt,non-functional_requirements_not_fully_satisfied,"Tue, 25 Mar 2014 05:25:48 +0000","Thu, 12 May 2016 18:26:58 +0000","Wed, 9 Apr 2014 19:43:52 +0000",1347484,I start getting a little concerned when we are talking about thread safety of these. Mainly because the JavaKeystoreProvider should not be used as a database. Initial implementation of the KeyProvider API assumes a rather basic and controlled access to key management. We do need to ensure thread safety for these implementations and equally as important for protection against corrupted keystores. See: HADOOP-10224 I will add that jira as related. I would like to consider a more appropriate provider type for access from a KMS as well.,0.1900833333,0.1900833333,negative
hadoop,10427,comment_7,"Thread safety is needed regardless of KMS server, in case a client is doing multithreaded stuff. Agree that the is not the ideal backend for the KMS server. Currently is the only thing we have in Hadoop. The KMS server simply uses the KeyProvider API, so you could plugin any impl there, potentially one talking with a KMIP server. The KMS server does not attempt to provide a reliable store, just a client/server implementation deferring to another implementation. Sounds reasonable?",requirement_debt,non-functional_requirements_not_fully_satisfied,"Tue, 25 Mar 2014 05:25:48 +0000","Thu, 12 May 2016 18:26:58 +0000","Wed, 9 Apr 2014 19:43:52 +0000",1347484,"Thread safety is needed regardless of KMS server, in case a client is doing multithreaded stuff. Agree that the JavaKeyStoreProvider is not the ideal backend for the KMS server. Currently is the only thing we have in Hadoop. The KMS server simply uses the KeyProvider API, so you could plugin any impl there, potentially one talking with a KMIP server. The KMS server does not attempt to provide a reliable store, just a client/server implementation deferring to another implementation. Sounds reasonable?",0.08133333333,0.08133333333,neutral
hadoop,10427,description,The {{KeyProvider}} API should be thread-safe so it can be used safely in server apps.,requirement_debt,non-functional_requirements_not_fully_satisfied,"Tue, 25 Mar 2014 05:25:48 +0000","Thu, 12 May 2016 18:26:58 +0000","Wed, 9 Apr 2014 19:43:52 +0000",1347484,The KeyProvider API should be thread-safe so it can be used safely in server apps.,0.225,0.225,neutral
hadoop,10427,summary,KeyProvider implementations should be thread safe,requirement_debt,non-functional_requirements_not_fully_satisfied,"Tue, 25 Mar 2014 05:25:48 +0000","Thu, 12 May 2016 18:26:58 +0000","Wed, 9 Apr 2014 19:43:52 +0000",1347484,KeyProvider implementations should be thread safe,0.25,0.25,neutral
hadoop,10681,comment_10,"The synchronized blocks would've made a lot of sense if setInput() or was atomic. Since it only reads part of the data (64kb or so) in for an invocation, the user has never been able to use this with multiple threads safely. To make sure this was never used with threading in something like HBase, I cross-checked & HBase has an unsynchronized improved version of gzip which writes its own header/trailer chunks without synchronization.",requirement_debt,non-functional_requirements_not_fully_satisfied,"Wed, 11 Jun 2014 17:41:42 +0000","Thu, 8 Sep 2016 06:49:18 +0000","Sun, 5 Oct 2014 14:49:19 +0000",10012057,"The synchronized blocks would've made a lot of sense if setInput() or decompress/compress() was atomic. Since it only reads part of the data (64kb or so) in for an invocation, the user has never been able to use this with multiple threads safely. To make sure this was never used with threading in something like HBase, I cross-checked & HBase has an unsynchronized improved version of gzip which writes its own header/trailer chunks without synchronization. https://github.com/apache/hbase/blob/c61cb7fb55124547a36a6ef56afaec43676039f8/hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/ReusableStreamGzipCodec.java#L100",-0.08133333333,-0.061,neutral
hadoop,2424,comment_0,"With the right header, the existing LzoCodec should be compatible with lzop. It would probably be better implemented as an anyway.",requirement_debt,requirement_partially_implemented,"Fri, 14 Dec 2007 00:16:50 +0000","Fri, 4 Jan 2008 18:33:59 +0000","Fri, 14 Dec 2007 00:23:54 +0000",424,"With the right header, the existing LzoCodec should be compatible with lzop. It would probably be better implemented as an InputFormat/OutputFormat, anyway.",0.538,0.538,positive
hadoop,3394,description,"Here is the current prototype implementation of a distributed free text index using Hadoop based on Doug Cutting's design: There has also been some discussion about this on the Hadoop Wiki: This work is not finished, so it is not intended for inclusion yet. For a description of the contribution and its current status see the report in doc/index.html in the attached archive that gives some details of the implementation. This work was designed as a contrib contribution. However, as there are at least two other projects (Bailey and Katta) with similar goals it seemed a good idea to make this code available for discussion.",requirement_debt,requirement_partially_implemented,"Thu, 15 May 2008 16:13:13 +0000","Thu, 9 Jun 2011 11:26:41 +0000","Thu, 9 Jun 2011 11:26:41 +0000",96750808,"Here is the current prototype implementation of a distributed free text index using Hadoop based on Doug Cutting's design: http://www.mail-archive.com/general@lucene.apache.org/msg00338.html There has also been some discussion about this on the Hadoop Wiki: http://wiki.apache.org/hadoop/DistributedLucene This work is not finished, so it is not intended for inclusion yet. For a description of the contribution and its current status see the report in doc/index.html in the attached archive that gives some details of the implementation. This work was designed as a contrib contribution. However, as there are at least two other projects (Bailey and Katta) with similar goals it seemed a good idea to make this code available for discussion.",0.2802,0.2802,neutral
hadoop,8236,comment_2,"Todd, These timeouts look reasonable to me. Worth noting that new-active is also the timeout for the active pre-check, ie the check that the new active is alive and well before we ask the current active to go standby. This is important because we don't want to impatiently wait 5s before fencing then wait a minute to make the new active active. In practice since we already contacted the new active we probably won't have to wait 60s to transition it to active unless something happened in between the pre-check and the transition to active, which is why 60s timeout here is reasonable. Nit: can remove the ""TODO"" before transitionToActive since this is now configurable. Otherwise patch looks great.",requirement_debt,requirement_partially_implemented,"Fri, 9 Mar 2012 02:33:31 +0000","Wed, 23 May 2012 20:15:53 +0000","Mon, 2 Apr 2012 03:49:27 +0000",2078156,"Todd, These timeouts look reasonable to me. Worth noting that new-active is also the timeout for the active pre-check, ie the check that the new active is alive and well before we ask the current active to go standby. This is important because we don't want to impatiently wait 5s before fencing then wait a minute to make the new active active. In practice since we already contacted the new active we probably won't have to wait 60s to transition it to active unless something happened in between the pre-check and the transition to active, which is why 60s timeout here is reasonable. Nit: can remove the ""TODO"" before transitionToActive since this is now configurable. Otherwise patch looks great.",0.2656458333,0.2656458333,positive
hadoop,8236,comment_3,New patch removes the TODO. I'll commit this version momentarily.,requirement_debt,requirement_partially_implemented,"Fri, 9 Mar 2012 02:33:31 +0000","Wed, 23 May 2012 20:15:53 +0000","Mon, 2 Apr 2012 03:49:27 +0000",2078156,New patch removes the TODO. I'll commit this version momentarily.,0.1,0.1,neutral
hadoop,9309,description,"Checking for Snappy support calls native method This method has not been implemented for Windows in hadoop.dll, so it throws",requirement_debt,requirement_partially_implemented,"Thu, 14 Feb 2013 21:18:45 +0000","Thu, 21 Feb 2013 18:58:39 +0000","Thu, 21 Feb 2013 18:45:01 +0000",595576,"Checking for Snappy support calls native method NativeCodeLoader#buildSupportsSnappy. This method has not been implemented for Windows in hadoop.dll, so it throws UnsatisfiedLinkError.",0.2,0.1333333333,neutral
hadoop,930,comment_0,"Here's a patch for a native S3 filesystem. * Writes are supported. * The scheme is s3n making it completely independent of the existing block-based S3 filesystem. It might be possible to make a general (read-only) S3 filesystem that can read both types, but I haven't attempted that here (it can go in another Jira if needed). * Empty directories are written using the naming convention of appending ""_$folder$"" to the key. This is the approach taken by S3Fox, and - crucially for efficiency - it makes it possible to tell if a key represents a file or a directory from a list bucket operation. * There's a new unit test for the contract of FileSystem to ensure that different implementations are consistent. Both S3 filesystems and HDFS are tested using this test. It would be good to add other filesystems later. * Renames are not supported as S3 doesn't support them natively (yet). It would be possible to support renames by getting the client to copy the data out of S3 then back again. * The Jets3t library has been upgraded to the latest version (0.6.0)",requirement_debt,requirement_partially_implemented,"Thu, 25 Jan 2007 14:48:52 +0000","Thu, 2 May 2013 02:29:14 +0000","Fri, 6 Jun 2008 21:07:05 +0000",43049893,"Here's a patch for a native S3 filesystem. Writes are supported. The scheme is s3n making it completely independent of the existing block-based S3 filesystem. It might be possible to make a general (read-only) S3 filesystem that can read both types, but I haven't attempted that here (it can go in another Jira if needed). Empty directories are written using the naming convention of appending ""_$folder$"" to the key. This is the approach taken by S3Fox, and - crucially for efficiency - it makes it possible to tell if a key represents a file or a directory from a list bucket operation. There's a new unit test (FileSystemContractBaseTest) for the contract of FileSystem to ensure that different implementations are consistent. Both S3 filesystems and HDFS are tested using this test. It would be good to add other filesystems later. Renames are not supported as S3 doesn't support them natively (yet). It would be possible to support renames by getting the client to copy the data out of S3 then back again. The Jets3t library has been upgraded to the latest version (0.6.0)",0.1574166667,0.1574166667,neutral
hadoop,10295,comment_0,"Initial patch for review. The patch adds a option to distcp. If the option is set, it simply checks the checksum type for each source file and uses it for creating the corresponding temp file in the target FS. Still need to add unit tests and do some system tests.",test_debt,lack_of_tests,"Mon, 27 Jan 2014 22:43:52 +0000","Thu, 4 Sep 2014 01:16:46 +0000","Fri, 31 Jan 2014 00:01:18 +0000",263846,"Initial patch for review. The patch adds a ""-preservechecksumtype"" option to distcp. If the option is set, it simply checks the checksum type for each source file and uses it for creating the corresponding temp file in the target FS. Still need to add unit tests and do some system tests.",0.07025,0.07025,neutral
hadoop,10374,description,"There are valid use cases for accessing the InterfaceAudience annotations programatically. In HBase, we are writing a unit test to check whether every class in the client packages are annotated with one of the annotations. Plus, we are also thinking about a golden file to containing all public method sigs, so that we can ensure public facing API-compatibility from a unit test. Related: HBASE-8546, HBASE-10462, HBASE-8275",test_debt,lack_of_tests,"Thu, 27 Feb 2014 21:45:33 +0000","Thu, 4 Sep 2014 01:16:46 +0000","Thu, 27 Feb 2014 23:34:08 +0000",6515,"There are valid use cases for accessing the InterfaceAudience annotations programatically. In HBase, we are writing a unit test to check whether every class in the client packages are annotated with one of the annotations. Plus, we are also thinking about a golden file to containing all public method sigs, so that we can ensure public facing API-compatibility from a unit test. Related: HBASE-8546, HBASE-10462, HBASE-8275",0.1231875,0.1231875,neutral
hadoop,10427,comment_2,"patch adds documentation, synchronized keywords and synchronization via a read write lock. it is obvious and not that easy to write a testcase",test_debt,lack_of_tests,"Tue, 25 Mar 2014 05:25:48 +0000","Thu, 12 May 2016 18:26:58 +0000","Wed, 9 Apr 2014 19:43:52 +0000",1347484,"patch adds documentation, synchronized keywords and synchronization via a read write lock. it is obvious and not that easy to write a testcase",-0.146,-0.146,negative
hadoop,10508,comment_2,The patch looks good. Can you please provide a unit test for this functionality?,test_debt,lack_of_tests,"Tue, 15 Apr 2014 20:36:20 +0000","Fri, 15 Aug 2014 05:39:34 +0000","Tue, 29 Apr 2014 06:08:40 +0000",1157540,The patch looks good. Can you please provide a unit test for this functionality?,0.488,0.488,positive
hadoop,10673,comment_0,The patch updates the existing rpc metrics in the case of exception. HDFS unit tests will be updated to cover this.,test_debt,lack_of_tests,"Mon, 9 Jun 2014 18:01:13 +0000","Mon, 1 Dec 2014 03:07:25 +0000","Tue, 15 Jul 2014 23:15:40 +0000",3129267,The patch updates the existing rpc metrics in the case of exception. HDFS unit tests will be updated to cover this.,0,0,neutral
hadoop,10729,description,"We have ProtocolInfo specified in protocol interface with version info, but we don't have unit test to verify if/how it works. We should have tests to track this annotation work as expectation.",test_debt,lack_of_tests,"Fri, 20 Jun 2014 01:56:56 +0000","Fri, 25 Oct 2019 20:25:47 +0000","Tue, 8 Dec 2015 21:45:09 +0000",46381693,"We have ProtocolInfo specified in protocol interface with version info, but we don't have unit test to verify if/how it works. We should have tests to track this annotation work as expectation.",0,0,neutral
hadoop,11117,comment_7,I agree ... but this is something minimal which can go in with little/no work though it looks like a couple of tests are not picking up the changed text. What we could do long term is replicate some of the stuff we did for the networking errors: point to wiki pages. But how is anyone going to make sense of a ? I don't want to write the wiki page for that.,test_debt,low_coverage,"Mon, 22 Sep 2014 20:42:40 +0000","Wed, 8 Oct 2014 16:11:45 +0000","Wed, 8 Oct 2014 16:11:45 +0000",1366145,I agree ... but this is something minimal which can go in with little/no work though it looks like a couple of tests are not picking up the changed text. What we could do long term is replicate some of the stuff we did for the networking errors: point to wiki pages. But how is anyone going to make sense of a NoMatchException ? I don't want to write the wiki page for that.,-0.1416666667,-0.1416666667,neutral
hadoop,11219,comment_1,"In Tez, we provide a Tez Shuffle Handler that is similar to MR Shuffle Handler with some added feature support. There may be an interaction here that needs to be tested and verified. I'd be happy to verify once there is a patch available.",test_debt,lack_of_tests,"Thu, 23 Oct 2014 00:23:40 +0000","Sun, 11 Feb 2024 10:57:54 +0000","Thu, 8 Jun 2023 19:23:44 +0000",272228404,"In Tez, we provide a Tez Shuffle Handler that is similar to MR Shuffle Handler with some added feature support. There may be an interaction here that needs to be tested and verified. I'd be happy to verify once there is a patch available.",0.5605,0.5605,neutral
hadoop,11409,comment_4,"Thanks for the patch, Gera! When we throw the exception due to lacking a scheme, it would be very helpful to users to mention the property where the bad URI originated to show them what needs to be changed. The unit test should have an Assert.fail call or something similar after the getFileContext call, otherwise the lack of throwing an exception for a bad fs URI will still pass the test. ""Excpected"" s/b ""Expected""",test_debt,low_coverage,"Mon, 15 Dec 2014 23:03:47 +0000","Fri, 10 Apr 2015 20:04:49 +0000","Thu, 18 Dec 2014 21:30:27 +0000",253600,"Thanks for the patch, Gera! When we throw the exception due to lacking a scheme, it would be very helpful to users to mention the property where the bad URI originated to show them what needs to be changed. The unit test should have an Assert.fail call or something similar after the getFileContext call, otherwise the lack of throwing an exception for a bad fs URI will still pass the test. ""Excpected"" s/b ""Expected""",0.02655,0.02655,positive
hadoop,11677,comment_3,Added missing import. Could not find any test class for HTTPServer2. I will try to add tests for this class,test_debt,lack_of_tests,"Thu, 5 Mar 2015 09:57:13 +0000","Tue, 30 Aug 2016 01:30:40 +0000","Mon, 23 Nov 2015 03:58:15 +0000",22701662,Added missing import. Could not find any test class for HTTPServer2. I will try to add tests for this class,-0.1333333333,-0.1333333333,negative
hadoop,11730,comment_4,"patch applied; tested against s3 EU. Given the nature of these problems, it may be good to start thinking about whether we can do things with better simulate failures; the test here is a good start, though we may want more complex policies...mockito might be the tool to reach for.",test_debt,low_coverage,"Thu, 19 Mar 2015 23:22:57 +0000","Sat, 31 Oct 2020 00:26:01 +0000","Thu, 23 Apr 2015 20:45:10 +0000",3014533,"patch applied; tested against s3 EU. Given the nature of these problems, it may be good to start thinking about whether we can do things with better simulate failures; the test here is a good start, though we may want more complex policies...mockito might be the tool to reach for.",0.3565,0.3565,neutral
hadoop,11730,description,"s3n attempts to read again when it encounters IOException during read. But the current logic does not reopen the connection, thus, it ends up with no-op, and committing the wrong(truncated) output. Here's a stack trace as an example. It seems this is a regression, which was introduced by the following optimizations. Also, test cases should be reviewed so that it covers this scenario.",test_debt,low_coverage,"Thu, 19 Mar 2015 23:22:57 +0000","Sat, 31 Oct 2020 00:26:01 +0000","Thu, 23 Apr 2015 20:45:10 +0000",3014533,"s3n attempts to read again when it encounters IOException during read. But the current logic does not reopen the connection, thus, it ends up with no-op, and committing the wrong(truncated) output. Here's a stack trace as an example. 2015-03-13 20:17:24,835 [TezChild] INFO org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor - Starting output org.apache.tez.mapreduce.output.MROutput@52008dbd to vertex scope-12 2015-03-13 20:17:24,866 [TezChild] DEBUG org.jets3t.service.impl.rest.httpclient.HttpMethodReleaseInputStream - Released HttpMethod as its response data stream threw an exception org.apache.http.ConnectionClosedException: Premature end of Content-Length delimited message body (expected: 296587138; received: 155648 at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:184) at org.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:138) at org.jets3t.service.io.InterruptableInputStream.read(InterruptableInputStream.java:78) at org.jets3t.service.impl.rest.httpclient.HttpMethodReleaseInputStream.read(HttpMethodReleaseInputStream.java:146) at org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsInputStream.read(NativeS3FileSystem.java:145) at java.io.BufferedInputStream.read1(BufferedInputStream.java:273) at java.io.BufferedInputStream.read(BufferedInputStream.java:334) at java.io.DataInputStream.read(DataInputStream.java:100) at org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:180) at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216) at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174) at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185) at org.apache.pig.builtin.PigStorage.getNext(PigStorage.java:259) at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:204) at org.apache.tez.mapreduce.lib.MRReaderMapReduce.next(MRReaderMapReduce.java:116) at org.apache.pig.backend.hadoop.executionengine.tez.plan.operator.POSimpleTezLoad.getNextTuple(POSimpleTezLoad.java:106) at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:307) at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNextTuple(POForEach.java:246) at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:307) at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter.getNextTuple(POFilter.java:91) at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:307) at org.apache.pig.backend.hadoop.executionengine.tez.plan.operator.POStoreTez.getNextTuple(POStoreTez.java:117) at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.runPipeline(PigProcessor.java:313) at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.run(PigProcessor.java:192) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) 2015-03-13 20:17:24,867 [TezChild] INFO org.apache.hadoop.fs.s3native.NativeS3FileSystem - Received IOException while reading 'user/hadoop/tsato/readlarge/input/cloudian-s3.log.20141119', attempting to reopen. 2015-03-13 20:17:24,867 [TezChild] DEBUG org.jets3t.service.impl.rest.httpclient.HttpMethodReleaseInputStream - Released HttpMethod as its response data stream is fully consumed 2015-03-13 20:17:24,868 [TezChild] INFO org.apache.tez.dag.app.TaskAttemptListenerImpTezDag - Commit go/no-go request from attempt_1426245338920_0001_1_00_000004_0 2015-03-13 20:17:24,868 [TezChild] INFO org.apache.tez.dag.app.dag.impl.TaskImpl - attempt_1426245338920_0001_1_00_000004_0 given a go for committing the task output. It seems this is a regression, which was introduced by the following optimizations. https://issues.apache.org/jira/browse/HADOOP-10589 https://issues.apache.org/jira/browse/HADOOP-10457 Also, test cases should be reviewed so that it covers this scenario.",-0.005,0.00404290429,negative
hadoop,11880,comment_4,"None of the above. I thought you might have an opinion on what the next step is. Frankly, I'm half-leaning towards setting a different hard-coded path and disabling parallel tests in HDFS again since it's pretty clear that the unit tests are pretty screwed up based upon investigation. :( We still need to fix it, though.",test_debt,low_coverage,"Mon, 27 Apr 2015 14:52:15 +0000","Fri, 1 Sep 2017 15:22:29 +0000","Fri, 1 Sep 2017 15:22:29 +0000",74133014,"None of the above. I thought you might have an opinion on what the next step is. Frankly, I'm half-leaning towards setting a different hard-coded path and disabling parallel tests in HDFS again since it's pretty clear that the unit tests are pretty screwed up based upon stevel@apache.org's investigation. We still need to fix it, though.",-0.034875,0.05025,negative
hadoop,11880,comment_5,"Got it. Thanks! I just commented in HDFS-9263 that the Maven build, when running tests in parallel mode, automatically creates a separate test directory for each concurrent test process to use. Thus, the isolation is achieved at the Maven build level, and there is no requirement for {{MiniDFSCluster}} itself to generate a unique workspace. Of course, that only holds true if all tests are playing nice and using the properties passed down by Maven to determine their test path. I'd like to explore fixing this by backtracing from the bad paths shown in the RAT report to try to find the offending tests. Given that I think is correct already, I suspect we'll find the problems are in tests that don't use {{MiniDFSCluster}}, or in tests that explicitly configure their own data directories instead of relying on I'm reluctant to revert the test parallelization. If it's more urgent to complete this cutover first though, then I understand.",test_debt,expensive_tests,"Mon, 27 Apr 2015 14:52:15 +0000","Fri, 1 Sep 2017 15:22:29 +0000","Fri, 1 Sep 2017 15:22:29 +0000",74133014,"Got it. Thanks! Either revert with a hard-coded path and expect parallel tests to fail, or extend the dfs builder to add an operation to set the subdir, which would be retained over a test case/test suite. I just commented in HDFS-9263 that the Maven build, when running tests in parallel mode, automatically creates a separate test directory for each concurrent test process to use. Thus, the isolation is achieved at the Maven build level, and there is no requirement for MiniDFSCluster itself to generate a unique workspace. Of course, that only holds true if all tests are playing nice and using the properties passed down by Maven to determine their test path. I'd like to explore fixing this by backtracing from the bad paths shown in the RAT report to try to find the offending tests. Given that I think MiniDFSCluster#getBaseDirectory is correct already, I suspect we'll find the problems are in tests that don't use MiniDFSCluster, or in tests that explicitly configure their own data directories instead of relying on MiniDFSCluster#getBaseDirectory. I'm reluctant to revert the test parallelization. If it's more urgent to complete this cutover first though, then I understand.",0.02004166667,0.01853333333,neutral
hadoop,11880,comment_7,"The naughty test is The problem is specific to the HDFS-9263 patch, so I'll add more details over there.",test_debt,flaky_test,"Mon, 27 Apr 2015 14:52:15 +0000","Fri, 1 Sep 2017 15:22:29 +0000","Fri, 1 Sep 2017 15:22:29 +0000",74133014,"The naughty test is TestMiniDFSCluster. The problem is specific to the HDFS-9263 patch, so I'll add more details over there.",-0.4,-0.2,neutral
hadoop,1245,comment_1,"I've also run into this, and came up with a patch that involved: - adding a 'maxTasks' value to the TaskTrackerStatus (set by the local - modifying JobTracker to track totalTaskCapacity instead of per-node maxTasks, and to use the particular task tracker's maxTasks value when deciding whether to assign it another task If this seems like a reasonable approach, I can do more testing and provide a patch.",test_debt,lack_of_tests,"Wed, 11 Apr 2007 00:20:03 +0000","Wed, 8 Jul 2009 16:52:21 +0000","Wed, 24 Oct 2007 22:52:12 +0000",17015529,"I've also run into this, and came up with a patch that involved: adding a 'maxTasks' value to the TaskTrackerStatus (set by the local mapred.tasktracker.tasks.maximum) modifying JobTracker to track totalTaskCapacity instead of per-node maxTasks, and to use the particular task tracker's maxTasks value when deciding whether to assign it another task If this seems like a reasonable approach, I can do more testing and provide a patch.",0.521,0.1389166667,neutral
hadoop,12485,description,Don't prefer ipv4 stack and instead use ipv6 if it's there. This should mean that tests actually bind on ipv6 if it's there. It should mean that tests running against this branch are more likely to be representative of what would be running in production.,test_debt,lack_of_tests,"Fri, 16 Oct 2015 00:51:26 +0000","Tue, 29 Dec 2015 21:55:21 +0000","Tue, 29 Dec 2015 21:55:21 +0000",6469435,Don't prefer ipv4 stack and instead use ipv6 if it's there. This should mean that tests actually bind on ipv6 if it's there. It should mean that tests running against this branch are more likely to be representative of what would be running in production.,0,0,neutral
hadoop,1254,summary,TestCheckpoint fails intermittently,test_debt,flaky_test,"Thu, 12 Apr 2007 17:50:45 +0000","Wed, 8 Jul 2009 16:42:25 +0000","Fri, 13 Apr 2007 17:43:25 +0000",85960,TestCheckpoint fails intermittently,-0.4,-0.4,negative
hadoop,12829,comment_0,"Attached a patch. Doesn't include any tests -- not sure exactly what to test. I internally tested by changing STATS_DATA_CLEANER to package-private and writing the following test: which passes with the change and hangs without it. I'm unclear on if hadoop even wants something like this, since I'm not up to speed on how hadoop handles JVM reuse for unit tests.",test_debt,lack_of_tests,"Sat, 20 Feb 2016 01:19:42 +0000","Mon, 16 Dec 2019 10:04:45 +0000","Tue, 23 Feb 2016 19:33:42 +0000",324840,"Attached a patch. Doesn't include any tests  not sure exactly what to test. I internally tested by changing STATS_DATA_CLEANER to package-private and writing the following test: which passes with the change and hangs without it. I'm unclear on if hadoop even wants something like this, since I'm not up to speed on how hadoop handles JVM reuse for unit tests.",0.070375,0.070375,negative
hadoop,12888,comment_5,"attach the patch to this JIRA, use a name of the form hit the the ""submit patch"" button and it'll be trigger an automatic review. Yetus will complain about the lack of tests, but we'll have to go with that. Did it work for you in any manual tests Moving the issue from HDFS to Hadoop as its in the common module",test_debt,lack_of_tests,"Mon, 29 Feb 2016 22:53:19 +0000","Tue, 30 Aug 2016 01:17:40 +0000","Wed, 16 Mar 2016 14:32:53 +0000",1352374,"attach the patch to this JIRA, use a name of the form HADOOP-12888-001.patch, hit the the ""submit patch"" button and it'll be trigger an automatic review. Yetus will complain about the lack of tests, but we'll have to go with that. Did it work for you in any manual tests Moving the issue from HDFS to Hadoop as its in the common module",0.1666666667,0.125,neutral
hadoop,13051,description,"On {{branch-2}}, the below is the (incorrect) behaviour today, where paths with special characters get dropped during globStatus calls: Whereas trunk has the right behaviour, subtly fixed via the pattern library change of HADOOP-12436: (I've placed a ^M explicitly to indicate presence of the intentional hidden character) We should still add a simple test-case to cover this situation for future regressions.",test_debt,lack_of_tests,"Fri, 22 Apr 2016 12:55:57 +0000","Thu, 12 May 2016 18:27:42 +0000","Thu, 5 May 2016 21:23:12 +0000",1153635,"On branch-2, the below is the (incorrect) behaviour today, where paths with special characters get dropped during globStatus calls: Whereas trunk has the right behaviour, subtly fixed via the pattern library change of HADOOP-12436: (I've placed a ^M explicitly to indicate presence of the intentional hidden character) We should still add a simple test-case to cover this situation for future regressions.",0.2026666667,0.2026666667,neutral
hadoop,13353,comment_1,The fix looks good to me. Thanks for the contribution. Would you also like to add a regression test?,test_debt,lack_of_tests,"Fri, 8 Jul 2016 02:29:15 +0000","Wed, 2 Oct 2019 17:13:30 +0000","Fri, 5 Aug 2016 23:38:48 +0000",2495373,The fix looks good to me. Thanks liangvls@gmail.com for the contribution. Would you also like to add a regression test?,0.392,0.392,positive
hadoop,13386,comment_0,"successor to HADOOP-12527; given the discourse there I'm going to change the title of that one and close this as a duplicate. see also: for some coverage of the problem. We aren't scared of Avro, but do need to take care of its transitive dependencies. Help there showing what they are and testing all down the stack is very much appreciated",test_debt,low_coverage,"Mon, 18 Jul 2016 23:23:06 +0000","Sat, 27 Jan 2024 06:37:07 +0000","Sat, 26 Mar 2022 11:32:04 +0000",179410138,"successor to HADOOP-12527; given the discourse there I'm going to change the title of that one and close this as a duplicate. see also: http://steveloughran.blogspot.co.uk/2016/05/fear-of-dependencies.html for some coverage of the problem. We aren't scared of Avro, but do need to take care of its transitive dependencies. Help there showing what they are and testing all down the stack is very much appreciated",0.03816666667,0.03816666667,neutral
hadoop,14479,comment_0,"Thanks for the report . I tried this locally too and ran into the same problem. We're supposed to be running the ISA-L tests as part of precommit, so I'm not sure what happened here. Seems like there's a test gap for ISA-L vs. the Java coder (time to revisit HDFS-11066?)  /  could you assist with debugging this?",test_debt,low_coverage,"Fri, 2 Jun 2017 14:27:32 +0000","Sat, 19 Aug 2017 02:50:34 +0000","Thu, 29 Jun 2017 03:28:35 +0000",2293263,"Thanks for the report Ayappan. I tried this locally too and ran into the same problem. We're supposed to be running the ISA-L tests as part of precommit, so I'm not sure what happened here. Seems like there's a test gap for ISA-L vs. the Java coder (time to revisit HDFS-11066?) Sammi / drankye could you assist with debugging this?",0,0,neutral
hadoop,14479,comment_14,"did we ever file that JIRA to get ISA-L re-enabled? ISA-L is required for production usage of EC, so having testing is really important.",test_debt,low_coverage,"Fri, 2 Jun 2017 14:27:32 +0000","Sat, 19 Aug 2017 02:50:34 +0000","Thu, 29 Jun 2017 03:28:35 +0000",2293263,"drankye did we ever file that JIRA to get ISA-L re-enabled? ISA-L is required for production usage of EC, so having testing is really important.",0.2,0.2,neutral
hadoop,14479,comment_3,"Hi , I agree, it would be good to fill the test gap. Let's revisit HDFS-11066. Thanks!",test_debt,low_coverage,"Fri, 2 Jun 2017 14:27:32 +0000","Sat, 19 Aug 2017 02:50:34 +0000","Thu, 29 Jun 2017 03:28:35 +0000",2293263,"Hi andrew.wang, We're supposed to be running the ISA-L tests as part of precommit, so I'm not sure what happened here. Seems like there's a test gap for ISA-L vs. the Java coder (time to revisit HDFS-11066?) I agree, it would be good to fill the test gap. Let's revisit HDFS-11066. Thanks!",0.296,0.148,positive
hadoop,14479,comment_4,"Hmm, I thought the test gap was majorly caused by the precommit missing ISA-L building. Quite some time ago it was done in HADOOP-12626 but removed in HADOOP-13342. I thought we should bring it back and re-enable the ISA-L building & tests in precommit. Andrew, do you think so? If sounds good, we can fire a new issue to do it. Thanks!",test_debt,low_coverage,"Fri, 2 Jun 2017 14:27:32 +0000","Sat, 19 Aug 2017 02:50:34 +0000","Thu, 29 Jun 2017 03:28:35 +0000",2293263,"Hmm, I thought the test gap was majorly caused by the precommit missing ISA-L building. Quite some time ago it was done in HADOOP-12626 but removed in HADOOP-13342. I thought we should bring it back and re-enable the ISA-L building & tests in precommit. Andrew, do you think so? If sounds good, we can fire a new issue to do it. Thanks!",0.03133333333,0.03133333333,neutral
hadoop,14479,comment_8,"First of all, these three failed unit tests are not related with ISA-L version change. There are actually hidden issues. The implementation of native XOR encoder/decoder has array out of index issue which cause the JVM crash. One failed unit test is to test underlying encoder reuse by execution testCoding twice, while the underlying encoder is released explicitly by By review the context, I think is not the right place to release the encoder. 3. One failed unit test is because it doesn't consider the native RS coder situation. I have tried the patch locally. , thanks for reporting this. You can have a try with the new patch. Besides, I will close HADOOP-14593 later since I have merged fixes for 3 unit cases into one patch.",test_debt,expensive_tests,"Fri, 2 Jun 2017 14:27:32 +0000","Sat, 19 Aug 2017 02:50:34 +0000","Thu, 29 Jun 2017 03:28:35 +0000",2293263,"First of all, these three failed unit tests are not related with ISA-L version change. There are actually hidden issues. 1.TestHHXORErasureCoder The implementation of native XOR encoder/decoder has array out of index issue which cause the JVM crash. 2.TestRSErasureCoder One failed unit test is to test underlying encoder reuse by execution testCoding twice, while the underlying encoder is released explicitly by ErasureEncodingStep.finish. By review the context, I think ErasureEncodingStep.finish is not the right place to release the encoder. 3. TestCodecRawCoderMapping One failed unit test is because it doesn't consider the native RS coder situation. I have tried the patch locally. Ayappan, thanks for reporting this. You can have a try with the new patch. Besides, I will close HADOOP-14593 later since I have merged fixes for 3 unit cases into one patch.",-0.1403888889,-0.1151333333,negative
hadoop,15577,comment_0,"We don't need the 0-rename stuff, because distcp, except in the --atomic mode, isn't trying to do atomic operations. What we do need, is for distcp to not upload to a temp file and rename each one into place: remove that and for non-atomic uploads you eliminate the O(data) delay after each upload. Closing as a duplicate of that. *as that JIRA has no code/tests, I would support anyone who sat down to do implement the feature* There's also lots of work going on with HDFS to have an explicit multipart upload mechanism for filesystems, which can be used for a block-by-block upload to S3, this would improve distcp upload perf on files in HDFS > 1 block, as the blocks could be uploaded in parallel with locality. Keep an eye on that",test_debt,lack_of_tests,"Mon, 2 Jul 2018 22:14:48 +0000","Tue, 3 Jul 2018 14:20:20 +0000","Tue, 3 Jul 2018 14:20:20 +0000",57932,"We don't need the 0-rename stuff, because distcp, except in the --atomic mode, isn't trying to do atomic operations. What we do need, is for distcp to not upload to a temp file and rename each one into place: remove that and for non-atomic uploads you eliminate the O(data) delay after each upload. Closing as a duplicate of that. as that JIRA has no code/tests, I would support anyone who sat down to do implement the feature There's also lots of work going on with HDFS to have an explicit multipart upload mechanism for filesystems, which can be used for a block-by-block upload to S3, this would improve distcp upload perf on files in HDFS > 1 block, as the blocks could be uploaded in parallel with locality. Keep an eye on that",0.07795555556,0.07795555556,neutral
hadoop,16044,comment_6,"Good questions, and the answers are complicated by JVM implementations which may not honor DNS TTL for negative caching. Testing with fault injection could shed light here, but not sure how easy/difficult this is? Could we commit this change while we dig into this more?",test_debt,lack_of_tests,"Fri, 11 Jan 2019 18:02:56 +0000","Mon, 14 Jan 2019 20:03:40 +0000","Mon, 14 Jan 2019 19:47:06 +0000",265450,"Good questions, and the answers are complicated by JVM implementations which may nothonorDNS TTL for negative caching. Testing with fault injection could shed light here, but not sure how easy/difficult this is? Could we commit this change while we dig into this more?",0.151,0.151,neutral
hadoop,16160,comment_0,"If AdlFS contract test is not enabled, {{fs}} is null in When calling it calls and then it calls {{fs.delete()}}, finally NPE occurs. This issue is fixed by HADOOP-14170 in 2.9+, so I'll backport HADOOP-14170 to fix this issue.",test_debt,lack_of_tests,"Mon, 4 Mar 2019 09:57:24 +0000","Mon, 4 Mar 2019 11:05:53 +0000","Mon, 4 Mar 2019 11:05:53 +0000",4109,"If AdlFS contract test is not enabled, fs is null in TestAdlFileSystemContractLive#setUp. When calling TestAdlFileSystemContractLive#tearDown(), it calls FileSystemContractBaseTest#tearDown and then it calls fs.delete(), finally NPE occurs. This issue is fixed by HADOOP-14170 in 2.9+, so I'll backport HADOOP-14170 to fix this issue.",0,0,negative
hadoop,16207,comment_8,"Also, to run the tests in parallel - the jobs need to start using a different directory name. Currently, all of them use testMRJob (The method name in the common class that all tests inherit from). The issue with the local dir conflict is a MR configuration afaik (Likely the MR tmp dir config property). YARN clusters should already be able to run in parallel (different ports, random dir names, etc) I'd also be careful trying to run too many of these in parallel, given the amount of memory they consume. Maybe a different parallelism flag for any tests running on a cluster? How about simplifying the code and letting the tests reside in the same class, which makes the code easier to read and allows sharing a cluster more easily. Haven't seen the WIP patch - but sharing a cluster across different tests, which may or may not trigger at the same time seems like it may cause problems. The tests also use a 1 s sleep for the InconsistentFS to get into a consistent state. That can lead to flakiness in the tests. A higher sleep, unless InconsistentFS can be set up with an actual waitForConsistency method which is not time based.",test_debt,flaky_test,"Tue, 26 Mar 2019 13:07:38 +0000","Fri, 4 Oct 2019 13:27:40 +0000","Fri, 4 Oct 2019 13:17:44 +0000",16589406,"Also, to run the tests in parallel - the jobs need to start using a different directory name. Currently, all of them use testMRJob (The method name in the common class that all tests inherit from). The issue with the local dir conflict is a MR configuration afaik (Likely the MR tmp dir config property). YARN clusters should already be able to run in parallel (different ports, random dir names, etc) I'd also be careful trying to run too many of these in parallel, given the amount of memory they consume. Maybe a different parallelism flag for any tests running on a cluster? How about simplifying the code and letting the tests reside in the same class, which makes the code easier to read and allows sharing a cluster more easily. Haven't seen the WIP patch - but sharing a cluster across different tests, which may or may not trigger at the same time seems like it may cause problems. The tests also use a 1 s sleep for the InconsistentFS to get into a consistent state. That can lead to flakiness in the tests. A higher sleep, unless InconsistentFS can be set up with an actual waitForConsistency method which is not time based.",-0.09324,-0.09324,neutral
hadoop,16359,comment_5,yes I tried the patch on 2.9 and it worked - before the patch the native libs are not bundled under Not sure how to test this - does other codecs have unit tests for the bundling part?,test_debt,lack_of_tests,"Tue, 11 Jun 2019 03:15:20 +0000","Wed, 2 Oct 2019 17:14:48 +0000","Fri, 14 Jun 2019 22:24:05 +0000",328125,weichiu yes I tried the patch on 2.9 and it worked - before the patch the native libs are not bundled under $HADOOP_HOME/lib/native. Not sure how to test this - does other codecs have unit tests for the bundling part?,0.2,0.1,neutral
hadoop,1961,comment_11,I was dealing with multiple patches and totally missed running the unit tests for this. TestDFSShell faile because of the following change : There are no change the fix. Only TestDFSShell.java is changed. Also updated the patch to copy multiple directories.,test_debt,lack_of_tests,"Thu, 27 Sep 2007 23:33:00 +0000","Wed, 8 Jul 2009 16:42:11 +0000","Fri, 5 Oct 2007 00:09:01 +0000",606961,I was dealing with multiple patches and totally missed running the unit tests for this. TestDFSShell faile because of the following change : bin/hadoop fs -get dir1 localdir # and localdir exists 0.13 and 0.14.1 copy contents of dir1 into localdir 0.14.2 copies dir1 into localdir There are no change the fix. Only TestDFSShell.java is changed. Also updated the patch to copy multiple directories.,-0.08,-0.08,negative
hadoop,2077,comment_0,Simple fix. I haven't gotten around to testing it much since svn.apache.org is _super_ slow...,test_debt,lack_of_tests,"Thu, 18 Oct 2007 18:51:57 +0000","Wed, 8 Jul 2009 16:52:29 +0000","Mon, 14 Jan 2008 18:28:01 +0000",7601764,Simple fix. I haven't gotten around to testing it much since svn.apache.org is super slow...,0.1355,0.1355,negative
hadoop,2148,comment_7,I verified both test failures. They are not related to the patch imo. In the first case took too long. It finished only 47 cases out of 71 in 13 minutes when the junit framework terminated it. In the second case the cluster fell into a infinite loop trying to replicate a block. Filed HADOOP-3050 to investigate it.,test_debt,expensive_tests,"Sat, 3 Nov 2007 00:12:52 +0000","Wed, 8 Jul 2009 16:42:46 +0000","Wed, 19 Mar 2008 22:48:25 +0000",11918133,I verified both test failures. They are not related to the patch imo. In the first case TestDFSStorageStateRecovery took too long. It finished only 47 cases out of 71 in 13 minutes when the junit framework terminated it. In the second case the cluster fell into a infinite loop trying to replicate a block. Filed HADOOP-3050 to investigate it.,-0.05833333333,-0.05833333333,negative
hadoop,2796,comment_1,"There are no test cases in this patch, because the commit of HADOOP-2848 missed committing the testHod.py file. This will cause a conflict now as the test cases should really be added to that file. Will submit test cases as part of a separate patch.",test_debt,lack_of_tests,"Thu, 7 Feb 2008 13:12:36 +0000","Wed, 21 May 2008 20:05:50 +0000","Fri, 21 Mar 2008 14:26:42 +0000",3719646,"There are no test cases in this patch, because the commit of HADOOP-2848 missed committing the testHod.py file. This will cause a conflict now as the test cases should really be added to that file. Will submit test cases as part of a separate patch.",-0.1,-0.1,negative
hadoop,3077,description,"HADOOP-2899 and HADOOP-2936 introduced minor problems in their unit tests that should be fixed. As per HADOOP-2899: There's an incorrect hardcoded error message. Also, a test data uses a user name which should be changed. As per HADOOP-2936: Temporary testing directories are being left behind after some tests run.",test_debt,low_coverage,"Mon, 24 Mar 2008 05:22:30 +0000","Wed, 18 May 2011 22:03:39 +0000","Wed, 18 May 2011 22:03:39 +0000",99420069,"HADOOP-2899 and HADOOP-2936 introduced minor problems in their unit tests that should be fixed. As per HADOOP-2899: There's an incorrect hardcoded error message. Also, a test data uses a user name which should be changed. As per HADOOP-2936: Temporary testing directories are being left behind after some tests run.",-0.325,-0.325,negative
hadoop,3375,comment_5,+1 code looks good. Is there a way to enhance an existing unit test to catch this situation?,test_debt,lack_of_tests,"Tue, 13 May 2008 00:59:27 +0000","Wed, 8 Jul 2009 16:43:06 +0000","Tue, 20 May 2008 23:09:32 +0000",684605,+1 code looks good. Is there a way to enhance an existing unit test to catch this situation?,0.388,0.388,positive
hadoop,3560,comment_2,i have not added tests since the test require creation of large input files.,test_debt,lack_of_tests,"Fri, 13 Jun 2008 20:20:56 +0000","Wed, 8 Jul 2009 16:41:31 +0000","Tue, 17 Jun 2008 05:15:34 +0000",291278,i have not added tests since the test require creation of large input files.,0,0,negative
hadoop,3649,comment_2,# This does not work if you have 2 out of 3 replicas corrupt. When the first replication happens you will unconditionally remove the block from the corrupt replica map not waiting for the second replication to complete. We should make a test case for that. # needs JavaDoc explaining what it tests. # Could you please remove redundant import of ByteBuffer in # You should not corruptReplica() twice (the same one) in,test_debt,lack_of_tests,"Thu, 26 Jun 2008 17:28:58 +0000","Wed, 8 Jul 2009 16:43:12 +0000","Tue, 1 Jul 2008 02:37:01 +0000",378483,This does not work if you have 2 out of 3 replicas corrupt. When the first replication happens you will unconditionally remove the block from the corrupt replica map not waiting for the second replication to complete. We should make a test case for that. testBlockCorruptionRecoveryPolicy() needs JavaDoc explaining what it tests. Could you please remove redundant import of ByteBuffer in TestDatanodeBlockScanner. You should not corruptReplica() twice (the same one) in testBlockCorruptionRecoveryPolicy(),-0.04666666667,-0.03888888889,negative
hadoop,4300,comment_1,"I haven't tested on 0.19, but we've been using the Trash feature on our clusters for some time . Could you check if Namenode config also has the non-zero fs.trash.interval? - Client uses 'fs.trash.interval' to determine whether to delete files or move them to Trash. - Namenode uses 'fs.trash.interval' for running the expunge thread to clean up the users' Trash directories. So your files should be deleted between namenode's 'fs.trash.interval to 'fs.trash.interval * 2' period.",test_debt,lack_of_tests,"Mon, 29 Sep 2008 06:32:19 +0000","Wed, 8 Jul 2009 16:43:20 +0000","Fri, 17 Oct 2008 21:27:01 +0000",1608882,"I haven't tested on 0.19, but we've been using the Trash feature on our clusters for some time . Could you check if Namenode config also has the non-zero fs.trash.interval? Client uses 'fs.trash.interval' to determine whether to delete files or move them to Trash. Namenode uses 'fs.trash.interval' for running the expunge thread to clean up the users' Trash directories. So your files should be deleted between namenode's 'fs.trash.interval to 'fs.trash.interval * 2' period.",-0.01164285714,-0.01086666667,neutral
hadoop,4719,comment_0,Did not run tests . Change in documentation .,test_debt,lack_of_tests,"Tue, 25 Nov 2008 01:00:50 +0000","Mon, 3 Aug 2009 12:59:03 +0000","Tue, 24 Mar 2009 23:44:12 +0000",10363402,Did not run tests . Change in documentation .,0,0,negative
hadoop,4997,comment_1,"Dhruba, we should also remove the unit tests that test the tmp directory does not get removed after restarting the cluster.",test_debt,expensive_tests,"Thu, 8 Jan 2009 20:21:26 +0000","Wed, 8 Jul 2009 16:43:29 +0000","Fri, 16 Jan 2009 23:56:50 +0000",704124,"Dhruba, we should also remove the unit tests that test the tmp directory does not get removed after restarting the cluster.",0,0,neutral
hadoop,6145,comment_3,"Attaching patch for trunk. Problem was the trash method was throwing FileNotFound, which was not being handled. There are three different places where the non-existence of the file being deleted could conceivably be handled. Fixed by just checking for the file's existence at the beginning and exiting if not found. No unit test because the affected unit test is TestHDFSCLI, which is in HDFS project. Manually tested and it works. The reason this wasn't detected previously is that it only manifests itself when the trash feature is enabled, which apparently it isn't on the minidfscluster that powers TestCLI. We should probably looking at running Test*CLI with both trash on and trash off.",test_debt,lack_of_tests,"Thu, 9 Jul 2009 08:02:37 +0000","Fri, 2 Jul 2010 04:49:07 +0000","Tue, 14 Jul 2009 17:42:05 +0000",466768,"Attaching patch for trunk. Problem was the trash method was throwing FileNotFound, which was not being handled. There are three different places where the non-existence of the file being deleted could conceivably be handled. Fixed by just checking for the file's existence at the beginning and exiting if not found. No unit test because the affected unit test is TestHDFSCLI, which is in HDFS project. Manually tested and it works. The reason this wasn't detected previously is that it only manifests itself when the trash feature is enabled, which apparently it isn't on the minidfscluster that powers TestCLI. We should probably looking at running Test*CLI with both trash on and trash off.",-0.1273125,-0.1273125,negative
hadoop,6220,comment_0,This turns an interrupt into an includes the original exception as the cause. No test. This is a tricky one to set up a test for as you need to block the startup and then interrupt the starting thread.,test_debt,lack_of_tests,"Thu, 27 Aug 2009 12:01:14 +0000","Thu, 12 May 2016 18:22:56 +0000","Wed, 28 Sep 2011 20:38:10 +0000",65867816,This turns an interrupt into an InterruptedIOException; includes the original exception as the cause. No test. This is a tricky one to set up a test for as you need to block the startup and then interrupt the starting thread.,-0.2333333333,-0.2333333333,negative
hadoop,6220,comment_11,No tests as it's so hard to recreate this situation in a test; you need one thread sleeping and another interrupting.,test_debt,lack_of_tests,"Thu, 27 Aug 2009 12:01:14 +0000","Thu, 12 May 2016 18:22:56 +0000","Wed, 28 Sep 2011 20:38:10 +0000",65867816,No tests as it's so hard to recreate this situation in a test; you need one thread sleeping and another interrupting.,0.05,0.05,negative
hadoop,6220,comment_14,+1 voting by self no tests as there is no easy way to generate the race condition.,test_debt,lack_of_tests,"Thu, 27 Aug 2009 12:01:14 +0000","Thu, 12 May 2016 18:22:56 +0000","Wed, 28 Sep 2011 20:38:10 +0000",65867816,+1 voting by self no tests as there is no easy way to generate the race condition.,-0.292,-0.292,negative
hadoop,6279,comment_0,"Does not include unit tests - this is an existing open issue HADOOP-3634. This patch is a one-liner, though. Manual verification steps: 1) Launch daemon with configured in 2) Go to /metrics on web UI 3) Verify totalMemoryM is present",test_debt,lack_of_tests,"Tue, 22 Sep 2009 23:01:36 +0000","Tue, 24 Aug 2010 20:39:55 +0000","Thu, 8 Oct 2009 18:54:24 +0000",1367568,"Does not include unit tests - this is an existing open issue HADOOP-3634. This patch is a one-liner, though. Manual verification steps: 1) Launch daemon with NoEmitMetricsContext configured in hadoop-metrics.properties 2) Go to /metrics on web UI 3) Verify totalMemoryM is present",0.1666666667,0.125,neutral
hadoop,6313,comment_0,This patch 1. defines Syncable interface 2. makes FSDataOutputStream to implement Syncable interface 3. makes of RawLocalFileSystem to implement the Syncable interface and also makes it a 4. implement a unit test to test 2 and 3.,test_debt,lack_of_tests,"Wed, 14 Oct 2009 17:19:27 +0000","Tue, 24 Aug 2010 20:40:35 +0000","Fri, 30 Oct 2009 19:53:35 +0000",1391648,This patch 1. defines Syncable interface 2. makes FSDataOutputStream to implement Syncable interface 3. makes LocalFSFileOutputStream of RawLocalFileSystem to implement the Syncable interface and also makes it a BufferedOutputStream 4. implement a unit test to test 2 and 3.,0,0,neutral
hadoop,6492,comment_8,"I just committed this, but without the AvroFsInput class. Trivial as it is, it should probably be submitted as a separate issue, with some unit tests. Thanks, Aaron!",test_debt,lack_of_tests,"Thu, 14 Jan 2010 04:40:07 +0000","Tue, 24 Aug 2010 20:41:30 +0000","Sat, 16 Jan 2010 01:08:23 +0000",160096,"I just committed this, but without the AvroFsInput class. Trivial as it is, it should probably be submitted as a separate issue, with some unit tests. Thanks, Aaron!",0.3,0.3,neutral
hadoop,6536,comment_11,Changes look good. Can you add a test for deleting dangling links also?,test_debt,lack_of_tests,"Wed, 3 Feb 2010 05:54:15 +0000","Mon, 12 Dec 2011 06:18:39 +0000","Tue, 20 Jul 2010 05:23:08 +0000",14426933,Changes look good. I would also add that fullyDelete should delete dangling links (it currently does but we should add a test). Can you add a test for deleting dangling links also?,0.388,0.2586666667,positive
hadoop,6536,comment_2,+1 Ravi's proposal. I would also add that fullyDelete should delete dangling links (it currently does but we should add a test). The current behavior is due to java making it difficult to identify symlinks.,test_debt,lack_of_tests,"Wed, 3 Feb 2010 05:54:15 +0000","Mon, 12 Dec 2011 06:18:39 +0000","Tue, 20 Jul 2010 05:23:08 +0000",14426933,+1 Ravi's proposal. I would also add that fullyDelete should delete dangling links (it currently does but we should add a test). The current behavior is due to java making it difficult to identify symlinks.,-0.06666666667,-0.06666666667,neutral
hadoop,6730,comment_3,"Hey Ravi, per the previous comment I left the getFileStatus outside the try block so the is now swallowed silently and is thrown up to copy (whose API wants This deserves a comment. The patch just has util classes - perhaps you forgot to add the test class in your diff?",test_debt,lack_of_tests,"Wed, 28 Apr 2010 18:50:09 +0000","Mon, 12 Dec 2011 06:19:03 +0000","Sat, 1 May 2010 21:09:36 +0000",267567,"Hey Ravi, per the previous comment I left the getFileStatus outside the try block so the FileNotFoundException is now swallowed silently and is thrown up to copy (whose API wants FileNotFoundThrown). This deserves a comment. The patch just has util classes - perhaps you forgot to add the test class in your diff?",-0.2,-0.1333333333,negative
hadoop,6730,description,"Thanks to Eli, He noticed that there is no test for FileContext#Copy operation. On further investigation with the help of Sanjay we found that there is bug in *FileStatus dstFs = should be in try...catch block.",test_debt,low_coverage,"Wed, 28 Apr 2010 18:50:09 +0000","Mon, 12 Dec 2011 06:19:03 +0000","Sat, 1 May 2010 21:09:36 +0000",267567,"Thanks to Eli, He noticed that there is no test for FileContext#Copy operation. On further investigation with the help of Sanjay we found that there is bug in FileContext#checkDest. FileStatus dstFs = getFileStatus(dst); should be in try...catch block.",0.2791666667,0.209375,negative
hadoop,6879,comment_8,The patch isn't final yet because there's not way to run framework validation tests from Ant,test_debt,lack_of_tests,"Sat, 24 Jul 2010 07:27:29 +0000","Mon, 12 Dec 2011 06:20:05 +0000","Mon, 4 Oct 2010 05:10:42 +0000",6212593,The patch isn't final yet because there's not way to run framework validation tests from Ant,0,0,negative
hadoop,6925,comment_0,Patch fixes the read() implementation to correctly mask with 0xff before upcasting to int. Also augments the unit test to check the single-byte read() function - the new test fails before this patch.,test_debt,low_coverage,"Tue, 24 Aug 2010 22:09:57 +0000","Mon, 12 Dec 2011 06:18:39 +0000","Tue, 24 Aug 2010 22:48:49 +0000",2332,Patch fixes the read() implementation to correctly mask with 0xff before upcasting to int. Also augments the unit test to check the single-byte read() function - the new test fails before this patch.,0.05,0.05,neutral
hadoop,7098,comment_0,"+1 The lack of a definition looks like an oversight (these variables were introduced in HADOOP-2551). is already honoured by bin/mapred, so no changes are needed there. Bernd, have you tested this manually? There's currently no easy way to write an automated test for this change.",test_debt,lack_of_tests,"Tue, 11 Jan 2011 17:24:40 +0000","Tue, 15 Nov 2011 00:50:20 +0000","Fri, 4 Mar 2011 05:37:08 +0000",4450348,"+1 The lack of a HADOOP_TASKTRACKER_OPTS definition looks like an oversight (these variables were introduced in HADOOP-2551). HADOOP_TASKTRACKER_OPTS is already honoured by bin/mapred, so no changes are needed there. Bernd, have you tested this manually? There's currently no easy way to write an automated test for this change.",0.2115,0.2115,neutral
hadoop,7098,comment_1,"I did test this manually indeed. The patch is in active use in my env. There are things like which might help setting up a test, but just for this small improvement it seem like overkill.",test_debt,lack_of_tests,"Tue, 11 Jan 2011 17:24:40 +0000","Tue, 15 Nov 2011 00:50:20 +0000","Fri, 4 Mar 2011 05:37:08 +0000",4450348,"I did test this manually indeed. The patch is in active use in my env. There are things like http://code.google.com/p/jbash/ which might help setting up a test, but just for this small improvement it seem like overkill.",0.2236666667,0.2236666667,neutral
hadoop,7118,comment_3,"+1 LGTM. I would also convert the test to JUnitv4 but one can't have everything, I guess ;)",test_debt,expensive_tests,"Tue, 25 Jan 2011 22:51:30 +0000","Mon, 12 Dec 2011 06:19:06 +0000","Wed, 26 Jan 2011 07:08:28 +0000",29818,"+1 LGTM. I would also convert the test to JUnitv4 but one can't have everything, I guess",0.2,0,neutral
hadoop,7118,description,"In HADOOP-7082 I stupidly introduced a regression whereby will throw an NPE if it is called before any .get() call is made, since the properties member is not initialized. This is causing a failure in on my box, but doesn't appear to trigger any failures in the non-contrib tests since .get() is usually called first. This JIRA is to fix the bug and add a unit test for writeXml in common (apparently it never had a unit test)",test_debt,lack_of_tests,"Tue, 25 Jan 2011 22:51:30 +0000","Mon, 12 Dec 2011 06:19:06 +0000","Wed, 26 Jan 2011 07:08:28 +0000",29818,"In HADOOP-7082 I stupidly introduced a regression whereby Configuration.writeXml will throw an NPE if it is called before any .get() call is made, since the properties member is not initialized. This is causing a failure in TestCapacitySchedulerWithJobTracker on my box, but doesn't appear to trigger any failures in the non-contrib tests since .get() is usually called first. This JIRA is to fix the bug and add a unit test for writeXml in common (apparently it never had a unit test)",-0.08,-0.06666666667,negative
hadoop,7946,description,"Hi friends, I am writing the map-reduce using scripting languages(Ruby).I am planning to write unit test cases in ruby.In ruby i can test the input and expected output by using local file system but i am feeling it is not right way.Is there any way to test real map-reduce running on hdfs . Can you suggest me any testing framework is there for scripting languages.",test_debt,lack_of_tests,"Mon, 2 Jan 2012 10:07:58 +0000","Mon, 2 Jan 2012 12:30:51 +0000","Mon, 2 Jan 2012 11:48:18 +0000",6020,"Hi friends, I am writing the map-reduce using scripting languages(Ruby).I am planning to write unit test cases in ruby.In ruby i can test the input and expected output by using local file system but i am feeling it is not right way.Is there any way to test real map-reduce running on hdfs . Can you suggest me any testing framework is there for scripting languages.",-0.0327,-0.0327,neutral
hadoop,8100,comment_0,This JIRA applies to trunk as well. Would you please provide a PATCH for trunk? On the patch: For the key names I'd remove 'authentication' as these properties can be used for different things than authentication. Testcase is missing.,test_debt,lack_of_tests,"Wed, 22 Feb 2012 21:56:04 +0000","Thu, 17 Jul 2014 15:04:14 +0000","Thu, 17 Jul 2014 15:04:14 +0000",75661690,This JIRA applies to trunk as well. Would you please provide a PATCH for trunk? On the patch: For the key names I'd remove 'authentication' as these properties can be used for different things than authentication. Testcase is missing.,0.10775,0.10775,neutral
hadoop,8245,comment_1,Attached patch fixes the flaky behavior for me. I looped for 30+ minutes and didn't see failures after applying this.,test_debt,flaky_test,"Wed, 4 Apr 2012 03:04:16 +0000","Wed, 4 Apr 2012 19:21:28 +0000","Wed, 4 Apr 2012 19:21:28 +0000",58632,Attached patch fixes the flaky behavior for me. I looped TestZKFailoverController for 30+ minutes and didn't see failures after applying this.,0.2,0.2,neutral
hadoop,8245,summary,Fix flakiness in,test_debt,flaky_test,"Wed, 4 Apr 2012 03:04:16 +0000","Wed, 4 Apr 2012 19:21:28 +0000","Wed, 4 Apr 2012 19:21:28 +0000",58632,Fix flakiness in TestZKFailoverController,0,0,negative
hadoop,8358,comment_10,"Failing test is unrelated to this change. Findbugs succeeded this time, so the previous issue was something else on trunk at the time or was a flaky result.",test_debt,flaky_test,"Fri, 4 May 2012 07:44:56 +0000","Thu, 11 Oct 2012 17:45:09 +0000","Mon, 28 May 2012 15:42:38 +0000",2102262,"Failing test org.apache.hadoop.fs.viewfs.TestViewFsTrash is unrelated to this change. Findbugs succeeded this time, so the previous issue was something else on trunk at the time or was a flaky result.",0.1,0.02857142857,negative
hadoop,8597,comment_1,This looks like a useful addition. Can you please add a unit test for it?,test_debt,lack_of_tests,"Sat, 14 Jul 2012 07:37:02 +0000","Fri, 15 Feb 2013 13:12:35 +0000","Tue, 11 Sep 2012 20:48:25 +0000",5145083,This looks like a useful addition. Can you please add a unit test for it?,0.35,0.35,positive
hadoop,9267,comment_0,"Patch for {{hadoop}} and {{hdfs}}. Ran TestHDFSCLI and TestCLI successfully. No unit tests since this is kind of trivial, but I could wrangle it if desired.",test_debt,lack_of_tests,"Thu, 31 Jan 2013 03:29:16 +0000","Thu, 12 May 2016 18:21:53 +0000","Fri, 22 Feb 2013 18:38:00 +0000",1955324,"Patch for hadoop and hdfs. Ran TestHDFSCLI and TestCLI successfully. No unit tests since this is kind of trivial, but I could wrangle it if desired.",0.1813333333,0.1813333333,positive
hadoop,9369,comment_4,"Hi Karthik, the patch seems fine to me, and I agree that the patch is so simple and writing a test sufficiently difficult that it seems unnecessary to write a test for this. One question - can you comment on the ramifications of this issue? How does it manifest itself? And what triggers it?",test_debt,lack_of_tests,"Wed, 6 Mar 2013 03:18:43 +0000","Mon, 3 Nov 2014 18:33:57 +0000","Thu, 7 Mar 2013 23:57:01 +0000",160698,"Hi Karthik, the patch seems fine to me, and I agree that the patch is so simple and writing a test sufficiently difficult that it seems unnecessary to write a test for this. One question - can you comment on the ramifications of this issue? How does it manifest itself? And what triggers it?",0.03333333333,0.03333333333,positive
hadoop,9544,comment_6,"Thanks very much for the review and commit. Just to document it, my full test run came back successful. There was just 1 failure in which is known to be a flaky test.",test_debt,flaky_test,"Fri, 3 May 2013 17:22:11 +0000","Wed, 15 May 2013 05:16:05 +0000","Fri, 3 May 2013 22:47:32 +0000",19521,"Thanks very much for the review and commit. Just to document it, my full test run came back successful. There was just 1 failure in TestBalancerWithNodeGroup, which is known to be a flaky test.",0.2166666667,0.2166666667,positive
hadoop,9763,comment_0,adds LightWeightCache. Still need to test it.,test_debt,lack_of_tests,"Sun, 21 Jul 2013 10:16:02 +0000","Tue, 27 Aug 2013 22:06:38 +0000","Wed, 24 Jul 2013 06:49:54 +0000",246832,h5017_20130721.patch: adds LightWeightCache. Still need to test it.,0.1405,0.09366666667,neutral
hadoop,9763,comment_10,adds more tests.,test_debt,lack_of_tests,"Sun, 21 Jul 2013 10:16:02 +0000","Tue, 27 Aug 2013 22:06:38 +0000","Wed, 24 Jul 2013 06:49:54 +0000",246832,c9763_20130723b.patch: adds more tests.,0,0,neutral
hadoop,9791,description,We've seen historically that paths longer than 260 chars can cause things not to work on Windows if not properly handled. Filing a tracking Jira to add a native io test case with long paths for new FileUtil access check methods added with HADOOP-9413.,test_debt,low_coverage,"Tue, 30 Jul 2013 02:20:21 +0000","Thu, 12 May 2016 18:27:41 +0000","Thu, 19 Sep 2013 07:10:36 +0000",4423815,We've seen historically that paths longer than 260 chars can cause things not to work on Windows if not properly handled. Filing a tracking Jira to add a native io test case with long paths for new FileUtil access check methods added with HADOOP-9413.,-0.3,-0.3,negative
hbase,12749,comment_7,"check out HBASE-12332. We don't have a FileStatus to attach to in that particular case initially. In the snapshot case we have the pattern file and get a fileStatus, and in the replicas case we have a file status as well[1] The fs was needed for (and and Probably can just as easily as of a FS in Didn't tackle those pieces yet (they also seem more tightly coupled than ideal) [1]",architecture_debt,violation_of_modularity,"Tue, 23 Dec 2014 03:01:48 +0000","Wed, 3 Jun 2015 15:29:07 +0000","Wed, 24 Dec 2014 13:09:30 +0000",122862,"the only other concern is the removed FileStatus from the StoreInfo and the addition of the FileSystem, but I still have to figure out why is that necessary. check out HBASE-12332. We don't have a FileStatus to attach to in that particular case initially. In the snapshot case we have the pattern file and get a fileStatus, and in the replicas case we have a file status as well[1] The fs was needed for StoreFileInfo#getFileStatus (and getModificaitonTime) and StoreFile.Reader#open. Probably can just as easily as of a FS in StoreFileInfo#getFileStatus. Didn't tackle those pieces yet (they also seem more tightly coupled than ideal) [1] https://github.com/apache/hbase/blob/master/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java#L186",0.2103333333,0.0745,neutral
hbase,13871,comment_3,"Do we have to have a class named FirstOnRowFakeCell at top level of our class hierarchy? Is it only used in CellUtil? Could it be an inner class of it? Any reason for this change? 5252 this.isScan = scan.isGetScan() ? -1 : 0; 5252 this.isScan = scan.isGetScan() ? 1 : 0; Otherwise, seems good.",architecture_debt,violation_of_modularity,"Tue, 9 Jun 2015 06:25:08 +0000","Fri, 17 Jun 2022 05:25:11 +0000","Wed, 10 Jun 2015 13:12:06 +0000",110818,"Do we have to have a class named FirstOnRowFakeCell at top level of our class hierarchy? Is it only used in CellUtil? Could it be an inner class of it? Any reason for this change? 5252 this.isScan = scan.isGetScan() ? -1 : 0; 5252 this.isScan = scan.isGetScan() ? 1 : 0; Otherwise, seems good.",0.1069090909,0.1069090909,neutral
hbase,13871,comment_4,Yes this class need not be top level.. Let me see how we can make it inner. Yes this change is intended. Pls see the compare in isStopROw is changed. So we need this change.,architecture_debt,violation_of_modularity,"Tue, 9 Jun 2015 06:25:08 +0000","Fri, 17 Jun 2022 05:25:11 +0000","Wed, 10 Jun 2015 13:12:06 +0000",110818,Yes this class need not be top level.. Let me see how we can make it inner. 5252 this.isScan = scan.isGetScan() ? -1 : 0; 5252 this.isScan = scan.isGetScan() ? 1 : 0; Yes this change is intended. Pls see the compare in isStopROw is changed. So we need this change.,0.1125,0.045,neutral
hbase,13871,comment_6,"I will change it in next patch. BTW , we might add some more similar classes in future. This is byte[] backed fake Cell impl and we will have a BB backed one also. Also LastOnRowFakeCell will come in.. JFYI. I will keep this class(es) private audience only even if CellUtil is public exposed. Thanks Stack.",architecture_debt,violation_of_modularity,"Tue, 9 Jun 2015 06:25:08 +0000","Fri, 17 Jun 2022 05:25:11 +0000","Wed, 10 Jun 2015 13:12:06 +0000",110818,"Do we have to have a class named FirstOnRowFakeCell at top level of our class hierarchy? Is it only used in CellUtil? Could it be an inner class of it? I will change it in next patch. BTW , we might add some more similar classes in future. This is byte[] backed fake Cell impl and we will have a BB backed one also. Also LastOnRowFakeCell will come in.. JFYI. I will keep this class(es) private audience only even if CellUtil is public exposed. Thanks Stack.",0.02544444444,0.06140740741,neutral
hbase,14956,comment_0,We cannot bump JLine to a newer version since the version of JRuby we ship is an old one (see HBASE-13338). Also this shouldn't cause any issue to use zkcli since it only disables the auto complete functionality. Another option is to use a version of ZK that doesn't have ZOOKEEPER-1718.,architecture_debt,using_obsolete_technology,"Wed, 9 Dec 2015 07:30:32 +0000","Sat, 19 Dec 2015 00:18:27 +0000","Sat, 19 Dec 2015 00:18:27 +0000",838075,We cannot bump JLine to a newer version since the version of JRuby we ship is an old one (see HBASE-13338). Also this shouldn't cause any issue to use zkcli since it only disables the auto complete functionality. Another option is to use a version of ZK that doesn't have ZOOKEEPER-1718.,0,0,neutral
hbase,16789,comment_0,"Removed directory layout reference from CompactionTool and moved it to The new tool named CompactionTool is added with changed interface. The new CT takes table, regions, column families as an input command line arguments. Both the legacy and new CT use APIs provided by MasterStorage/ RegionStorage classes. Map Reduce functionality of old CT is not yet implemented in the new CT as there are on-going discussions about it. Manually tested old CT and the new CT.",architecture_debt,violation_of_modularity,"Fri, 7 Oct 2016 00:44:02 +0000","Thu, 23 Jun 2022 20:30:29 +0000","Thu, 20 Oct 2016 20:50:08 +0000",1195566,"Removed directory layout reference from CompactionTool and moved it to LagacyCompactionTool. The new tool named CompactionTool is added with changed interface. The new CT takes table, regions, column families as an input command line arguments. Both the legacy and new CT use APIs provided by MasterStorage/ RegionStorage classes. Map Reduce functionality of old CT is not yet implemented in the new CT as there are on-going discussions about it. Manually tested old CT and the new CT.",0.1126,0.09383333333,neutral
hbase,17726,summary,[C++] Move implementation from header to cc for request retry,architecture_debt,violation_of_modularity,"Fri, 3 Mar 2017 20:48:23 +0000","Thu, 23 Jun 2022 20:19:46 +0000","Tue, 11 Apr 2017 13:15:21 +0000",3342418,[C++] Move implementation from header to cc for request retry,0,0,neutral
hbase,19969,comment_21,Josh: See HBASE-20123. Looks like we would need hadoop 3.1.0+ (or 3.0.2+) to make backup tests fully working.,architecture_debt,using_obsolete_technology,"Fri, 9 Feb 2018 22:57:42 +0000","Wed, 28 Mar 2018 02:50:38 +0000","Sat, 10 Mar 2018 17:49:55 +0000",2487133,Josh: See HBASE-20123. Looks like we would need hadoop 3.1.0+ (or 3.0.2+) to make backup tests fully working.,0.3,0.3,neutral
hbase,21,summary,hbase jar has hbase-default.xml at top-level rather than under a conf dir,architecture_debt,violation_of_modularity,"Fri, 1 Feb 2008 04:44:28 +0000","Fri, 22 Aug 2008 21:13:04 +0000","Thu, 15 May 2008 19:29:22 +0000",9038694,hbase jar has hbase-default.xml at top-level rather than under a conf dir,-0.05,-0.05,neutral
hbase,22837,description,"Currently, explanation about *Custom WAL Directory* configuration is a sub-topic of *Bulk Loading,* chapter, yet this subject has not much relation with bulk loading at all. It should rather be moved to a sub-section of the *Write Ahead Log (WAL)* chapter.",architecture_debt,violation_of_modularity,"Mon, 12 Aug 2019 18:42:08 +0000","Thu, 15 Aug 2019 08:12:57 +0000","Tue, 13 Aug 2019 09:34:02 +0000",53514,"Currently, explanation aboutCustom WAL Directoryconfiguration is a sub-topic ofBulk Loading,chapter, yet this subject has not much relation with bulk loading at all. It should rather be moved to a sub-section of theWrite Ahead Log (WAL)chapter.",0,0,neutral
hbase,3368,comment_1,"Agreed that we should move all messaging up into zk and remove heartbeating carrying messages but I was thinking that for 0.90, delete of region clears it from HMaster in-memory too. The worst that could happen is balancer ran meantime. If so, it'll fail close of a region not opened anywhere but the in-memory PENDING_CLOSE would be cleared by the delete-of-region cleanup. What you think?",architecture_debt,violation_of_modularity,"Thu, 16 Dec 2010 06:37:55 +0000","Fri, 20 Nov 2015 12:41:50 +0000","Mon, 21 Mar 2011 21:04:17 +0000",8259982,"Agreed that we should move all messaging up into zk and remove heartbeating carrying messages but I was thinking that for 0.90, delete of region clears it from HMaster in-memory too. The worst that could happen is balancer ran meantime. If so, it'll fail close of a region not opened anywhere but the in-memory PENDING_CLOSE would be cleared by the delete-of-region cleanup. What you think?",-0.1625,-0.1625,neutral
hbase,414,description,Let's move all the client classes into the o.a.h.h.client package. Files to move: * HTable * HBaseAdmin * HConnection * HConnectionManager Is there anything else I am missing? Obviously a lot of the tests will get moved too.,architecture_debt,violation_of_modularity,"Wed, 6 Feb 2008 06:50:37 +0000","Fri, 22 Aug 2008 21:13:07 +0000","Fri, 15 Feb 2008 00:29:31 +0000",754734,Let's move all the client classes into the o.a.h.h.client package. Files to move: HTable HBaseAdmin HConnection HConnectionManager Is there anything else I am missing? Obviously a lot of the tests will get moved too.,-0.1333333333,-0.1333333333,neutral
hbase,414,summary,Move client classes into client package,architecture_debt,violation_of_modularity,"Wed, 6 Feb 2008 06:50:37 +0000","Fri, 22 Aug 2008 21:13:07 +0000","Fri, 15 Feb 2008 00:29:31 +0000",754734,Move client classes into client package,0,0,neutral
hbase,467,comment_4,I'm fine w/ all being under o.a.h.h.r rather than under subpackages under o.a.h.h.r -- especially if its loads of work. Introducing o.a.h.h.r package is sufficient improvement over what we had previous. Good stuff.,architecture_debt,violation_of_modularity,"Sun, 24 Feb 2008 05:57:43 +0000","Fri, 22 Aug 2008 21:35:04 +0000","Sun, 24 Feb 2008 22:10:11 +0000",58348,I'm fine w/ all being under o.a.h.h.r rather than under subpackages under o.a.h.h.r  especially if its loads of work. Introducing o.a.h.h.r package is sufficient improvement over what we had previous. Good stuff.,0.6198333333,0.6198333333,positive
hbase,8056,comment_6,"Moved code, added tests",architecture_debt,violation_of_modularity,"Sat, 9 Mar 2013 00:40:12 +0000","Thu, 16 Jun 2022 05:49:14 +0000","Wed, 20 Mar 2013 01:13:44 +0000",952412,"Moved code, added tests",0,0,neutral
hbase,8103,description,Site and info plugins are too old. The site plugin is in the wrong place. Can't generate reports w/o update and move of location.,architecture_debt,using_obsolete_technology,"Thu, 14 Mar 2013 05:52:38 +0000","Fri, 5 Apr 2013 01:00:48 +0000","Thu, 14 Mar 2013 05:54:55 +0000",137,Site and info plugins are too old. The site plugin is in the wrong place. Can't generate reports w/o update and move of location.,-0.08333333333,-0.08333333333,negative
hbase,10892,comment_6,Yeah we need to fix the source of those zombies in precommit builds. It's not this patch. Will commit shortly unless objection. I will fix the long lines at commit time.,build_debt,build_others,"Tue, 1 Apr 2014 23:32:09 +0000","Sat, 21 Feb 2015 23:30:24 +0000","Fri, 25 Apr 2014 22:35:01 +0000",2070172,Yeah we need to fix the source of those zombies in precommit builds. It's not this patch. Will commit shortly unless objection. I will fix the long lines at commit time.,0.15,0.15,negative
hbase,14517,comment_2,Because RPC.proto has depended on HBase.proto and there will be a cycle dependency if the VersionInfo is put in RPC.proto.,build_debt,under-declared_dependencies,"Wed, 30 Sep 2015 05:30:06 +0000","Fri, 1 Jul 2022 21:38:21 +0000","Fri, 9 Oct 2015 22:15:05 +0000",837899,stack Why you move the VersionInfo from RPC to HBase protos? Because RPC.proto has depended on HBase.proto and there will be a cycle dependency if the VersionInfo is put in RPC.proto.,0,0,neutral
hbase,15732,description,"{{hbase-rsgroup}} is a new module that does not appear in the assembly. The binary tarball still contains the jars through dependencies, but we need the test-jar as well for running the  can you take a quick look.",build_debt,build_others,"Thu, 28 Apr 2016 18:54:15 +0000","Fri, 29 Apr 2016 03:53:41 +0000","Fri, 29 Apr 2016 00:15:36 +0000",19281,"hbase-rsgroup is a new module that does not appear in the assembly. The binary tarball still contains the jars through dependencies, but we need the test-jar as well for running the IntegrationTestRSGroup. toffer can you take a quick look.",0.228,0.152,neutral
hbase,22038,description,"When building the hbase c++ client with Dockerfile, it fails, because of the url resources not found. But this patch just solve the problem in temporary, cos when some dependent libraries are removed someday, the failure will appear again. Maybe a base docker image which contains all these dependencies maintained by us is required in the long run.",build_debt,build_others,"Tue, 12 Mar 2019 04:16:03 +0000","Thu, 23 Jun 2022 20:38:12 +0000","Wed, 20 Mar 2019 03:35:10 +0000",688747,"When building the hbase c++ client with Dockerfile, it fails, because of the url resources not found. But this patch just solve the problem in temporary, cos when some dependent libraries are removed someday, the failure will appear again. Maybe a base docker image which contains all these dependencies maintained by us is required in the long run.",-0.01233333333,-0.01233333333,negative
hbase,22981,comment_1,It is a bit complicated since not all the plugins are active in all stages. JDK specific ones do not have @author checks so the option is one example that causes build failure there. Whitespace ignore list is a similar option. I'm thinking of removing the unused flags that are not used at all (--jenkins and and also add the flag to bypass the stage specific issues.,build_debt,build_others,"Fri, 6 Sep 2019 11:24:47 +0000","Sat, 7 Sep 2019 02:29:23 +0000","Fri, 6 Sep 2019 13:03:41 +0000",5934,It is a bit complicated since not all the plugins are active in all stages. JDK specific ones do not have @author checks so the--author-ignore-list option is one example that causes build failure there. Whitespace ignore list is a similar option. I'm thinking of removing the unused flags that are not used at all (-jenkins and --build-url-patchdir) and also add the-ignore-unknown-options=true flag to bypass the stage specific issues.,-0.04375,-0.1673125,negative
hbase,22981,summary,Remove unused flags for Yetus,build_debt,build_others,"Fri, 6 Sep 2019 11:24:47 +0000","Sat, 7 Sep 2019 02:29:23 +0000","Fri, 6 Sep 2019 13:03:41 +0000",5934,Remove unused flags for Yetus,0,0,neutral
hbase,8089,comment_17,"Matt - the only other thing is a dependency on HBase's Bytes in a couple places. I think this can easily be removed. Part of the point of this effort is to have HBase ship a standard implementation that other Hadoop ecosystem components can rely on. If I just wanted something for my own application, I'd use Orderly and be done with it.",build_debt,over-declared_dependencies,"Wed, 13 Mar 2013 16:18:34 +0000","Thu, 16 Jun 2022 05:49:38 +0000","Sat, 11 Jun 2022 18:11:53 +0000",291779599,"Matt - the only other thing is a dependency on HBase's Bytes in a couple places. I think this can easily be removed. Part of the point of this effort is to have HBase ship a standard implementation that other Hadoop ecosystem components can rely on. If I just wanted something for my own application, I'd use Orderly and be done with it.",0,0,neutral
hbase,9479,comment_13,"Sorry about the pain. In 0.94, client and server were all bundled up in the one ball w/ client dependencies those of the servers so yeah, it is ugly. We've been trying to improve our story in 0.96. Some pruning/edit has been done to 'shield' downstreamers in 0.96 from the lorry-load of dependencies pulled in by our dependencies but for sure we could do better.",build_debt,build_others,"Mon, 9 Sep 2013 20:35:23 +0000","Thu, 16 Jun 2022 18:04:00 +0000","Mon, 9 Sep 2013 21:22:21 +0000",2818,"mobiusinversion Sorry about the pain. In 0.94, client and server were all bundled up in the one ball w/ client dependencies those of the servers so yeah, it is ugly. We've been trying to improve our story in 0.96. Some pruning/edit has been done to 'shield' downstreamers in 0.96 from the lorry-load of dependencies pulled in by our dependencies but for sure we could do better.",-0.284375,-0.284375,negative
hbase,9479,description,"Right now, HBase contains so many dependencies, that using the most basic HBase functionality such as HConnection in a larger application is unreasonably hard. For example, trying to include HBase connectivity in a Spring web app leads to hundreds of JarClassLoader errors such as: Why is this all bundled together? Why not have an ""hbase-client"" or ""hbase-client-dev"" package which is friendly for creating applications? I have spent 2+ days attempting to run a web service which is backed by HBase with no luck. I have created several stack overflow questions: The use of BeanUtils is also known to have a very bad issue: ""The three jars contain wrong classes"" Why is this so difficult? How do I include what I need to make an HBase app. So far I have tried using Maven, but this approach is draconian, and I have not succeeded. Am I Pwned?",build_debt,build_others,"Mon, 9 Sep 2013 20:35:23 +0000","Thu, 16 Jun 2022 18:04:00 +0000","Mon, 9 Sep 2013 21:22:21 +0000",2818,"Right now, HBase contains so many dependencies, that using the most basic HBase functionality such as HConnection in a larger application is unreasonably hard. For example, trying to include HBase connectivity in a Spring web app leads to hundreds of JarClassLoader errors such as: Why is this all bundled together? Why not have an ""hbase-client"" or ""hbase-client-dev"" package which is friendly for creating applications? I have spent 2+ days attempting to run a web service which is backed by HBase with no luck. I have created several stack overflow questions: http://stackoverflow.com/questions/18703903/java-massive-class-collision http://stackoverflow.com/questions/18690582/how-to-create-jetty-spring-app-with-hbase-connection The use of BeanUtils is also known to have a very bad issue: ""The three jars contain wrong classes"" https://issues.apache.org/jira/browse/BEANUTILS-398 Why is this so difficult? How do I include what I need to make an HBase app. So far I have tried using Maven, but this approach is draconian, and I have not succeeded. Am I Pwned?",-0.1365625,-0.1365625,negative
hbase,10001,description,"We have a mockup to test only the client. If we want to include the network, without beeing limited by the i/o, we don't have much tools. This coprocessor helps to test this. I put it in the main code as to make it usable without adding a jar... I don't think it's possible avoid the WAL writes in the coprocessors. It would be great to have it to simplify the test with any kind of client (i.e. w/o changing the durability).",code_debt,complex_code,"Tue, 19 Nov 2013 15:13:42 +0000","Wed, 18 Dec 2013 22:53:27 +0000","Wed, 20 Nov 2013 11:40:51 +0000",73629,"We have a mockup to test only the client. If we want to include the network, without beeing limited by the i/o, we don't have much tools. This coprocessor helps to test this. I put it in the main code as to make it usable without adding a jar... I don't think it's possible avoid the WAL writes in the coprocessors. It would be great to have it to simplify the test with any kind of client (i.e. w/o changing the durability).",0.28,0.28,neutral
hbase,10562,description,"See parent. This test was removed from 0.95.2. While investigating while it fails in 0.94 with Hadoop2 I found it is extremely slow and uses up a *lot* of threads. , any input?",code_debt,multi-thread_correctness,"Tue, 18 Feb 2014 02:05:41 +0000","Wed, 26 Feb 2014 04:45:39 +0000","Tue, 18 Feb 2014 02:24:17 +0000",1116,"See parent. This test was removed from 0.95.2. While investigating while it fails in 0.94 with Hadoop2 I found it is extremely slow and uses up a lot of threads. stack, any input?",-0.1333333333,-0.1,negative
hbase,10631,description,"There is an extra seek(0) on FileLink open, that we can skip",code_debt,low_quality_code,"Fri, 28 Feb 2014 01:21:25 +0000","Thu, 14 Aug 2014 20:20:07 +0000","Fri, 28 Feb 2014 09:21:20 +0000",28795,"There is an extra seek(0) on FileLink open, that we can skip",0,0,neutral
hbase,10631,summary,Avoid extra seek on FileLink open,code_debt,low_quality_code,"Fri, 28 Feb 2014 01:21:25 +0000","Thu, 14 Aug 2014 20:20:07 +0000","Fri, 28 Feb 2014 09:21:20 +0000",28795,Avoid extra seek on FileLink open,-0.2,-0.2,neutral
hbase,10968,comment_1,Thanks Matteo. Patch also removes an unused local variable.,code_debt,dead_code,"Sat, 12 Apr 2014 01:10:49 +0000","Sat, 21 Feb 2015 23:30:17 +0000","Sat, 12 Apr 2014 01:56:36 +0000",2747,Thanks Matteo. Patch also removes an unused local variable.,0.2,0.2,positive
hbase,11011,comment_3,"In this case is not bad, is an ""expected"" situation, where the RS died before moving the compacted file. Anyway, I think that all the loop can be removed, since is trying to lookup files in and if they are there are already loaded for sure.",code_debt,low_quality_code,"Thu, 17 Apr 2014 02:06:05 +0000","Sat, 21 Feb 2015 23:35:09 +0000","Fri, 18 Apr 2014 17:19:58 +0000",141233,"The fact that a file is missing seems pretty bad, yet it's at DEBUG. In this case is not bad, is an ""expected"" situation, where the RS died before moving the compacted file. Anyway, I think that all the ""for(compactionOutputs)"" loop can be removed, since is trying to lookup files in /table/region/family/ and if they are there are already loaded for sure.",-0.28125,-0.2763888889,neutral
hbase,11011,comment_4,"This might be, but the log message doesn't convey any of that. Taken out of context (so most users reading our logs), it just says a file is missing. I'm trusting you on this one :)",code_debt,low_quality_code,"Thu, 17 Apr 2014 02:06:05 +0000","Sat, 21 Feb 2015 23:35:09 +0000","Fri, 18 Apr 2014 17:19:58 +0000",141233,"In this case is not bad, is an ""expected"" situation, where the RS died before moving the compacted file. This might be, but the log message doesn't convey any of that. Taken out of context (so most users reading our logs), it just says a file is missing. since is trying to lookup files in /table/region/family/ and if they are there are already loaded for sure. I'm trusting you on this one",0.01666666667,-0.0925,neutral
hbase,11011,comment_6,"none of the above, the removed code was doing nothing (see the comment inside the function). The output files in the entry are simply not useful",code_debt,low_quality_code,"Thu, 17 Apr 2014 02:06:05 +0000","Sat, 21 Feb 2015 23:35:09 +0000","Fri, 18 Apr 2014 17:19:58 +0000",141233,"Does the changed code in completeCompactionMarker require a unit test? Or is there already one? none of the above, the removed code was doing nothing (see the comment inside the function). The output files in the entry are simply not useful",-0.25,-0.125,negative
hbase,11352,comment_1,Expiration would be measured in days. Maybe use different unit for the above config ? There're long lines in the patch - line length should be 100 or shorter.,code_debt,low_quality_code,"Fri, 13 Jun 2014 22:35:40 +0000","Fri, 17 Jun 2022 05:58:29 +0000","Tue, 23 Feb 2016 17:38:39 +0000",53550179,Expiration would be measured in days. Maybe use different unit for the above config ? There're long lines in the patch - line length should be 100 or shorter.,0,0,neutral
hbase,11352,comment_2,Fixed the long log lines and updated the expiration time to hours.,code_debt,low_quality_code,"Fri, 13 Jun 2014 22:35:40 +0000","Fri, 17 Jun 2022 05:58:29 +0000","Tue, 23 Feb 2016 17:38:39 +0000",53550179,Fixed the long log lines and updated the expiration time to hours.,0,0,neutral
hbase,11621,comment_4,"Here was 0.98 test suite with patch - on the same host as the previous run: With patch, hbase-server module went from 66 min to 51 min.",code_debt,slow_algorithm,"Wed, 30 Jul 2014 22:36:24 +0000","Fri, 6 Apr 2018 17:51:13 +0000","Thu, 31 Jul 2014 16:46:45 +0000",65421,"Here was 0.98 test suite with patch - on the same host as the previous run: With patch, hbase-server module went from 66 min to 51 min.",0,0,neutral
hbase,11621,description,Daryn proposed the following change in HDFS-6773: With this change in runtime for TestAdmin went from 8:35 min to 7 min,code_debt,slow_algorithm,"Wed, 30 Jul 2014 22:36:24 +0000","Fri, 6 Apr 2018 17:51:13 +0000","Thu, 31 Jul 2014 16:46:45 +0000",65421,"Daryn proposed the following change in HDFS-6773: With this change in HBaseTestingUtility#startMiniDFSCluster(), runtime for TestAdmin went from 8:35 min to 7 min",0,0,neutral
hbase,11621,summary,Make MiniDFSCluster run faster,code_debt,slow_algorithm,"Wed, 30 Jul 2014 22:36:24 +0000","Fri, 6 Apr 2018 17:51:13 +0000","Thu, 31 Jul 2014 16:46:45 +0000",65421,Make MiniDFSCluster run faster,0,0,neutral
hbase,11935,comment_12,Turns out that this is not the full story. We're still leaking ZK Connection somewhere when the slave cluster's ZK ensemble is not up. Debugging...,code_debt,low_quality_code,"Wed, 10 Sep 2014 18:47:37 +0000","Fri, 17 Jun 2022 17:13:44 +0000","Fri, 13 Feb 2015 01:59:31 +0000",13417914,Turns out that this is not the full story. We're still leaking ZK Connection somewhere when the slave cluster's ZK ensemble is not up. Debugging...,-0.05316666667,-0.05316666667,negative
hbase,11935,comment_3,Let's add some logging too... The queue failover if very quiet.,code_debt,low_quality_code,"Wed, 10 Sep 2014 18:47:37 +0000","Fri, 17 Jun 2022 17:13:44 +0000","Fri, 13 Feb 2015 01:59:31 +0000",13417914,Let's add some logging too... The queue failover if very quiet.,0.438,0.438,neutral
hbase,12106,description,Test annotation interfaces used to be under then moved to We should move them to,code_debt,low_quality_code,"Fri, 26 Sep 2014 21:11:49 +0000","Fri, 6 Apr 2018 17:54:44 +0000","Tue, 7 Oct 2014 07:42:21 +0000",901832,Test annotation interfaces used to be under hbase-common/src/test then moved to hbase-annotations/src/main. We should move them to hbase-annotations/src/test.,0,0,neutral
hbase,12115,comment_0,Removed unnecessary changes from the commit.,code_debt,dead_code,"Mon, 29 Sep 2014 22:36:24 +0000","Fri, 6 Apr 2018 17:54:29 +0000","Tue, 30 Sep 2014 20:35:08 +0000",79124,Removed unnecessary changes from the commit.,0.2,0.2,neutral
hbase,12266,comment_10,"Thanks Guys, frankly it looks to me such retry(including HBASE-7070) just makes code more complicated to read and easy to create new bugs in complex system. and it is hard to be covered by test. As mentioned in that jira: since it is caused by client side timeout, why not just throw exception so that user(or app layer code) knows it and set a bigger value. the timeout value is case by case, that is why we make it configurable, right?",code_debt,complex_code,"Wed, 15 Oct 2014 09:50:34 +0000","Fri, 17 Jun 2022 17:37:03 +0000","Thu, 30 Apr 2015 17:29:26 +0000",17048332,"Thanks Guys, frankly it looks to me such retry(including HBASE-7070) just makes code more complicated to read and easy to create new bugs in complex system. and it is hard to be covered by test. As mentioned in that jira: 1.A next request is very large, so first time it is failed because of timeout since it is caused by client side timeout, why not just throw exception so that user(or app layer code) knows it and set a bigger value. the timeout value is case by case, that is why we make it configurable, right?",0.164875,0.0519,negative
hbase,12266,comment_5,Is this endless loop really?,code_debt,low_quality_code,"Wed, 15 Oct 2014 09:50:34 +0000","Fri, 17 Jun 2022 17:37:03 +0000","Thu, 30 Apr 2015 17:29:26 +0000",17048332,Is this endless loop really?,0,0,neutral
hbase,12266,summary,Slow Scan can cause dead loop in ClientScanner,code_debt,low_quality_code,"Wed, 15 Oct 2014 09:50:34 +0000","Fri, 17 Jun 2022 17:37:03 +0000","Thu, 30 Apr 2015 17:29:26 +0000",17048332,Slow Scan can cause dead loop in ClientScanner,-0.6,-0.6,negative
hbase,12428,comment_12,Isn't this (-maybe) a cleaner fix? Keep logic local to the method where it is needed. Sorry to be pedantic. If you prefer your version here's a +1 for that too :),code_debt,low_quality_code,"Tue, 4 Nov 2014 20:56:08 +0000","Fri, 6 Apr 2018 17:54:55 +0000","Thu, 6 Nov 2014 21:04:28 +0000",173300,Isn't this (-maybe) a cleaner fix? Keep logic local to the method where it is needed. Sorry to be pedantic. If you prefer your version here's a +1 for that too,0.075,-0.025,positive
hbase,12464,comment_4,"This looks good, except for Jimmy's comments above. Nit. should be renamed to Can you rebase the patch on top of current master code. FAILED_CLOSE should be fine I think. Not much to do there. We default to 10 attempts to assign. Maybe we should bump it up to 30 or so. I did not check how long we are sleeping overall, but for client -> server operations we are retrying 35 times for a total of 10 min before giving up. We can do the similar for region assignment.",code_debt,low_quality_code,"Wed, 12 Nov 2014 18:12:22 +0000","Fri, 6 Apr 2018 17:55:10 +0000","Thu, 20 Nov 2014 22:02:39 +0000",705017,"This looks good, except for Jimmy's comments above. Nit. waitingForRetryingMetaAssignment should be renamed to waitForRetryingMetaAssignment(). Can you rebase the patch on top of current master code. FAILED_CLOSE should be fine I think. Not much to do there. is this essentially the same as setting maximumAttempts to a huge number? In many cases, a region may not be able to heal automatically without a pill. We default to 10 attempts to assign. Maybe we should bump it up to 30 or so. I did not check how long we are sleeping overall, but for client -> server operations we are retrying 35 times for a total of 10 min before giving up. We can do the similar for region assignment.",0.1917777778,0.2428333333,positive
hbase,12562,comment_10,v3 should fix findbugs warnings.,code_debt,low_quality_code,"Sat, 22 Nov 2014 07:04:33 +0000","Wed, 3 Jun 2015 15:29:33 +0000","Fri, 6 Mar 2015 22:54:18 +0000",9042585,v3 should fix findbugs warnings.,-0.6,-0.6,negative
hbase,12562,comment_2,+1. Looks good to me with some some minor comments: 1) You can break the loop after set canDrop to false 2) Just to check acquiring lock on writestate and memstore are always in this order 3) There maybe no need for the following condition 4. Rename to may be better,code_debt,low_quality_code,"Sat, 22 Nov 2014 07:04:33 +0000","Wed, 3 Jun 2015 15:29:33 +0000","Fri, 6 Mar 2015 22:54:18 +0000",9042585,+1. Looks good to me with some some minor comments: 1) You can break the loop after set canDrop to false 2) Just to check acquiring lock on writestate and memstore are always in this order 3) There maybe no need for the following condition 4. Rename getBiggestMemstoreOfSecondaryRegion to getBiggestMemstoreOfRegionReplica may be better,0.2626666667,0.2626666667,positive
hbase,1271,comment_3,"* LocalHBaseCluster - is not so general thing... as I understand it is used for testing.. never for real cluster... so my change will simplify this testing by providing automatic another port binding. * HRS InfoServer is really general thing... but port of InfoServer is not so critical... we can document it like a future... ""Automatic InfoServer port binding"". And if you want explicit port setting we can add console param for HRS start without this changes we cant star't several HRS on one node with info servers * thx... will fix my spelling",code_debt,complex_code,"Fri, 20 Mar 2009 05:00:09 +0000","Sun, 13 Sep 2009 22:24:29 +0000","Wed, 29 Apr 2009 23:15:43 +0000",3521734,"LocalHBaseCluster - is not so general thing... as I understand it is used for testing.. never for real cluster... so my change will simplify this testing by providing automatic another port binding. HRS InfoServer is really general thing... but port of InfoServer is not so critical... we can document it like a future... ""Automatic InfoServer port binding"". And if you want explicit port setting we can add console param for HRS start without this changes we cant star't several HRS on one node with info servers thx... will fix my spelling",0.05625,0.05625,neutral
hbase,12729,comment_1,"Hey , in the trunk patch, in RsRpcServices, this hunk: This is applying the mutations twice. I think you meant to remove the second invocation of mutateRows() right?",code_debt,duplicated_code,"Fri, 19 Dec 2014 19:49:36 +0000","Fri, 20 Nov 2015 11:54:27 +0000","Tue, 20 Jan 2015 01:07:09 +0000",2697453,"Hey jesse_yates, in the trunk patch, in RsRpcServices, this hunk: This is applying the mutations twice. I think you meant to remove the second invocation of mutateRows() right?",0.2635,0.2635,neutral
hbase,12749,comment_12,"v3 tackles the javadoc, fingbugs, checkstyle and line length issues.",code_debt,low_quality_code,"Tue, 23 Dec 2014 03:01:48 +0000","Wed, 3 Jun 2015 15:29:07 +0000","Wed, 24 Dec 2014 13:09:30 +0000",122862,"v3 tackles the javadoc, fingbugs, checkstyle and line length issues.",0,0,neutral
hbase,12833,comment_13,"On closer inspection, it looks like SecurityAdmin and need updated with this new style as well. ReplicationAdmin appears to manage it's own connection by design, though the rational eludes me.",code_debt,low_quality_code,"Fri, 9 Jan 2015 20:40:05 +0000","Fri, 6 Apr 2018 17:55:48 +0000","Fri, 16 Jan 2015 19:55:32 +0000",602127,"On closer inspection, it looks like SecurityAdmin and VisibilityLabelsAdmin need updated with this new style as well. ReplicationAdmin appears to manage it's own connection by design, though the rational eludes me.",0.6125,0.6125,neutral
hbase,12833,description,TestShell is erring out (timeout) consistently for me. Culprit is OOM cannot create native thread. It looks to me like test_table.rb and hbase/table.rb are made for leaking connections. table calls for every table but provides no close() method to clean it up. test_table creates a new table with every test.,code_debt,low_quality_code,"Fri, 9 Jan 2015 20:40:05 +0000","Fri, 6 Apr 2018 17:55:48 +0000","Fri, 16 Jan 2015 19:55:32 +0000",602127,TestShell is erring out (timeout) consistently for me. Culprit is OOM cannot create native thread. It looks to me like test_table.rb and hbase/table.rb are made for leaking connections. table calls ConnectionFactory.createConnection() for every table but provides no close() method to clean it up. test_table creates a new table with every test.,0.1464285714,0.128125,negative
hbase,12833,summary,[shell] table.rb leaks connections,code_debt,low_quality_code,"Fri, 9 Jan 2015 20:40:05 +0000","Fri, 6 Apr 2018 17:55:48 +0000","Fri, 16 Jan 2015 19:55:32 +0000",602127,[shell] table.rb leaks connections,0,0,neutral
hbase,12888,comment_2,"For consistency sake, it should also print this number of regions for meta. Right now that's fixed at 1.",code_debt,low_quality_code,"Tue, 20 Jan 2015 21:25:58 +0000","Fri, 17 Jun 2022 18:57:04 +0000","Fri, 2 Mar 2018 02:26:23 +0000",98168425,"For consistency sake, it should also print this number of regions for meta. Right now that's fixed at 1.",0.2635,0.2635,neutral
hbase,12905,description,"while working on HBASE-12898, hadoop-annotations showed up as an undeclared dependency. Looks like a hand full of hadoop InterfaceAudience annotations made it in through backports.",code_debt,low_quality_code,"Thu, 22 Jan 2015 17:14:03 +0000","Fri, 20 Nov 2015 11:56:05 +0000","Thu, 22 Jan 2015 18:49:23 +0000",5720,"while working on HBASE-12898, hadoop-annotations showed up as an undeclared dependency. Looks like a hand full of hadoop InterfaceAudience annotations made it in through backports.",0.3,0.3,neutral
hbase,12905,summary,Clean up presence of hadoop annotations in 0.98,code_debt,low_quality_code,"Thu, 22 Jan 2015 17:14:03 +0000","Fri, 20 Nov 2015 11:56:05 +0000","Thu, 22 Jan 2015 18:49:23 +0000",5720,Clean up presence of hadoop annotations in 0.98,0.4,0.4,neutral
hbase,1298,description,"is a key in my ""userdata"" table which happens to be the start key for a region named lists a link to which is incorrect because the "" + "" character is not properly URI Encoded. This results in a misleading user error message: ""This region is no longer available. It may be due to a split, a merge or the name changed. "" manually escaping the "" + "" character as ""%2B"" produces the correct output. A quick skim of master.jsp suggests it has a similar problem: it doesn't URI Encode table names when constructing links to table.jsp",code_debt,low_quality_code,"Mon, 30 Mar 2009 17:43:35 +0000","Sun, 13 Sep 2009 22:24:31 +0000","Fri, 3 Apr 2009 14:18:40 +0000",333305,"""UAZAAAAAZNaGnEKI+gC"" is a key in my ""userdata"" table which happens to be the start key for a region named ""userdata,UAZAAAAAZNaGnEKI+gC,1238170268268"" ""/table.jsp?name=userdata"" lists a link to ""/regionhistorian.jsp?regionname=userdata,UAZAAAAAZNaGnEKI+gC,1238170268268"" which is incorrect because the "" + "" character is not properly URI Encoded. This results in a misleading user error message: ""This region is no longer available. It may be due to a split, a merge or the name changed. "" manually escaping the "" + "" character as ""%2B"" produces the correct output. A quick skim of master.jsp suggests it has a similar problem: it doesn't URI Encode table names when constructing links to table.jsp",0.1160714286,0.07386363636,negative
hbase,13341,comment_4,"Oops, disregard the above. (Wrong JIRA). Only two things are nits: 1. Put a period at the end of the usage message addition to be consistent with the other lines. 2. Change the dereference from {{""$ALL"" != ""true""}} to {{""$\{ALL}"" != ""true}}, again, just to be consistent with the rest of the script. +1!",code_debt,low_quality_code,"Thu, 26 Mar 2015 17:06:45 +0000","Fri, 24 Jun 2022 17:41:45 +0000","Thu, 2 Apr 2015 04:59:37 +0000",561172,"Oops, disregard the above. (Wrong JIRA). Only two things are nits: 1. Put a period at the end of the usage message addition to be consistent with the other lines. 2. Change the dereference from ""$ALL"" != ""true"" to ""${ALL}"" != ""true, again, just to be consistent with the rest of the script. +1!",0.03222222222,0.03222222222,neutral
hbase,1337,description,When comparing timestamps in the KeyValues tests have shown that it is faster to do a byte[] compare on the timestamps rather than converting to longs and then do the long compare. In the case where you have one byte[] and one long that you need to compare it is better to convert both to long and do the long compare. Some numbers: Compare byte[] to byte[] timer 1042 ns Compare bytes2long to bytes2long timer 3143 ns Compare long to long timer 281 ns bytes2long timer 1328 ns long2bytes timer 1349 ns,code_debt,slow_algorithm,"Wed, 22 Apr 2009 20:57:30 +0000","Sat, 11 Jun 2022 20:35:55 +0000","Wed, 17 Jun 2009 16:09:38 +0000",4821128,When comparing timestamps in the KeyValues tests have shown that it is faster to do a byte[] compare on the timestamps rather than converting to longs and then do the long compare. In the case where you have one byte[] and one long that you need to compare it is better to convert both to long and do the long compare. Some numbers: Compare byte[] to byte[] timer 1042 ns Compare bytes2long to bytes2long timer 3143 ns Compare long to long timer 281 ns bytes2long timer 1328 ns long2bytes timer 1349 ns,0.1666666667,0.1666666667,neutral
hbase,13395,comment_5,It is deprecated in 1.0 I think. So we can remove it.,code_debt,dead_code,"Fri, 3 Apr 2015 09:37:09 +0000","Thu, 16 Jun 2022 18:18:32 +0000","Sat, 25 Mar 2017 16:09:04 +0000",62404315,It is deprecated in 1.0 I think. So we can remove it.,0,0,neutral
hbase,13395,description,"This class is marked as deprecated, probably can remove it, and if any methods from this specific class are in active use, need to decide what to do on callers' side. Should be able to replace with just Table interface usage.",code_debt,dead_code,"Fri, 3 Apr 2015 09:37:09 +0000","Thu, 16 Jun 2022 18:18:32 +0000","Sat, 25 Mar 2017 16:09:04 +0000",62404315,"This class is marked as deprecated, probably can remove it, and if any methods from this specific class are in active use, need to decide what to do on callers' side. Should be able to replace with just Table interface usage.",0.444,0.444,neutral
hbase,13528,comment_0,"I think this line is also redundant? The if selectNow is false, then we will not execute throttleCompaction then 'size' is useless Thanks.",code_debt,low_quality_code,"Wed, 22 Apr 2015 01:19:47 +0000","Sat, 4 Jul 2015 12:30:36 +0000","Fri, 24 Apr 2015 09:24:37 +0000",201890,"I think this line is also redundant? The if selectNow is false, then we will not execute throttleCompaction then 'size' is useless Thanks.",-0.15,-0.03333333333,negative
hbase,13528,comment_1,"Yes, it's redundant, just like this is OK?",code_debt,low_quality_code,"Wed, 22 Apr 2015 01:19:47 +0000","Sat, 4 Jul 2015 12:30:36 +0000","Fri, 24 Apr 2015 09:24:37 +0000",201890,"Yes, it's redundant, just like this is OK?",0.475,0.475,negative
hbase,1359,description,"If you see I removed and ip or something for security reasons Once I truncate the table, hbase freaks out for about 10 seconds and all the thrift servers die. Thrift server log: 2009-04-02 12:09:08,971 INFO Retrying connect to server: /:60020. Already tried 0 time(s). You see this a bunch of times and then it times out The hbase shell 13:01:08 INFO Quorum servers: Truncating t2; it may take a while Disabling table... 09/04/30 13:01:19 INFO client.HBaseAdmin: Disabled t2 0 row(s) in 10.3417 seconds Dropping table... 09/04/30 13:01:19 INFO client.HBaseAdmin: Deleted t2 0 row(s) in 0.1592 seconds Creating table... 0 row(s) in 14.7567 seconds undefined local variable or method `lsit' for #<Object:0x3bbe9a50 from (hbase):3 undefined local variable or method `lsit' for #<Object:0x3bbe9a50 from (hbase):4 null from `proc essRow' from `metaScan' from `metaScan' from `list Tables' from `listTables' from `invoke0' from `invoke' from `invoke' from `invoke' from ndling' from `invoke' from `call' from `cacheAndCal l' from `call' from `interpret' from `interpret' ... 113 levels... from `call' from `call ' from `call' from `cacheAndCal l' from `call' from `__file_ _' from `__file__ ' from `load' from `runScript' from `runNormally' from `runFromMain' from `run' from `run' from `main' from `list' from",code_debt,slow_algorithm,"Thu, 30 Apr 2009 17:39:21 +0000","Sun, 13 Sep 2009 22:24:34 +0000","Mon, 20 Jul 2009 22:03:21 +0000",7014240,"If you see **** I removed and ip or something for security reasons Once I truncate the table, hbase freaks out for about 10 seconds and all the thrift servers die. Thrift server log: 2009-04-02 12:09:08,971 INFO org.apache.hadoop.ipc.HBaseClass: Retrying connect to server: /*****:60020. Already tried 0 time(s). You see this a bunch of times and then it times out The hbase shell nhbase(main):001:0> truncate 't2' 09/04/30 13:01:08 INFO zookeeper.ZooKeeperWrapper: Quorum servers: **** Truncating t2; it may take a while Disabling table... 09/04/30 13:01:19 INFO client.HBaseAdmin: Disabled t2 0 row(s) in 10.3417 seconds Dropping table... 09/04/30 13:01:19 INFO client.HBaseAdmin: Deleted t2 0 row(s) in 0.1592 seconds Creating table... 0 row(s) in 14.7567 seconds hbase(main):002:0> lsit NameError: undefined local variable or method `lsit' for #<Object:0x3bbe9a50> from (hbase):3 hbase(main):003:0> lsit NameError: undefined local variable or method `lsit' for #<Object:0x3bbe9a50> from (hbase):4 hbase(main):004:0> list NativeException: java.lang.NullPointerException: null from org/apache/hadoop/hbase/client/HConnectionManager.java:344:in `proc essRow' from org/apache/hadoop/hbase/client/MetaScanner.java:64:in `metaScan' from org/apache/hadoop/hbase/client/MetaScanner.java:29:in `metaScan' from org/apache/hadoop/hbase/client/HConnectionManager.java:351:in `list Tables' from org/apache/hadoop/hbase/client/HBaseAdmin.java:121:in `listTables' from sun/reflect/NativeMethodAccessorImpl.java:-2:in `invoke0' from sun/reflect/NativeMethodAccessorImpl.java:39:in `invoke' from sun/reflect/DelegatingMethodAccessorImpl.java:25:in `invoke' from java/lang/reflect/Method.java:597:in `invoke' from org/jruby/javasupport/JavaMethod.java:298:in `invokeWithExceptionHa ndling' from org/jruby/javasupport/JavaMethod.java:259:in `invoke' from org/jruby/java/invokers/InstanceMethodInvoker.java:36:in `call' from org/jruby/runtime/callsite/CachingCallSite.java:260:in `cacheAndCal l' from org/jruby/runtime/callsite/CachingCallSite.java:75:in `call' from org/jruby/ast/CallNoArgNode.java:61:in `interpret' from org/jruby/ast/ForNode.java:101:in `interpret' ... 113 levels... from org/jruby/internal/runtime/methods/DynamicMethod.java:226:in `call' from org/jruby/internal/runtime/methods/CompiledMethod.java:216:in `call ' from org/jruby/internal/runtime/methods/CompiledMethod.java:71:in `call' from org/jruby/runtime/callsite/CachingCallSite.java:260:in `cacheAndCal l' from org/jruby/runtime/callsite/CachingCallSite.java:75:in `call' from home/fds/ts/hadoop/hbase/bin/$dot_dot/bin/hirb.rb:441:in `_file _' from home/fds/ts/hadoop/hbase/bin/$dot_dot/bin/hirb.rb:-1:in `_file_ ' from home/fds/ts/hadoop/hbase/bin/$dot_dot/bin/hirb.rb:-1:in `load' from org/jruby/Ruby.java:564:in `runScript' from org/jruby/Ruby.java:467:in `runNormally' from org/jruby/Ruby.java:340:in `runFromMain' from org/jruby/Main.java:214:in `run' from org/jruby/Main.java:100:in `run' from org/jruby/Main.java:84:in `main' from /home/fds/ts/hadoop/hbase/bin/../bin/hirb.rb:300:in `list' from (hbase):5hbase(main):005:0> hbase(main):006:0*",-0.2135,-0.04650649351,negative
hbase,13629,summary,Add to 1.0: HBASE-12957 may be extremely slow on region with lots of expired data,code_debt,slow_algorithm,"Wed, 6 May 2015 05:19:16 +0000","Fri, 24 Jun 2022 18:45:02 +0000","Wed, 6 May 2015 05:33:09 +0000",833,Add to 1.0: HBASE-12957 region_mover#isSuccessfulScan may be extremely slow on region with lots of expired data,0,0,negative
hbase,13928,comment_1,"is right, the added key is wrong, and never used. It is missing the {{.bucket.}} as mentioned and should be reading Only then an operator can set the sizes. The wrong property is misleading at most, but needs fixing anyways.",code_debt,low_quality_code,"Wed, 17 Jun 2015 23:08:35 +0000","Fri, 24 Jun 2022 19:24:13 +0000","Mon, 11 Jul 2016 15:08:08 +0000",33667173,"gsbiju is right, the added hbase-default.xml key is wrong, and never used. It is missing the .bucket. as mentioned and should be reading hbase.bucketcache.bucket.sizes. Only then an operator can set the sizes. The wrong property is misleading at most, but needs fixing anyways.",-0.1282,-0.07615,negative
hbase,1396,summary,Remove unused sequencefile and mapfile config. from hbase-default.xml,code_debt,dead_code,"Sat, 9 May 2009 02:05:36 +0000","Sun, 13 Sep 2009 22:24:37 +0000","Sat, 9 May 2009 02:05:50 +0000",14,Remove unused sequencefile and mapfile config. from hbase-default.xml,-0.1666666667,-0.1666666667,neutral
hbase,14162,comment_5,Using the variable thrift.version instead of hard coding version 0.9.2 in v3.,code_debt,low_quality_code,"Tue, 28 Jul 2015 17:39:19 +0000","Fri, 1 Jul 2022 20:45:51 +0000","Fri, 31 Jul 2015 22:06:36 +0000",275237,Using the variable thrift.version instead of hard coding version 0.9.2 in v3.,-0.1,-0.1,neutral
hbase,14162,comment_7,-1 on v3. the point of the check is to make sure the thrift.version variable doesn't get updated without a review of compatibility. also it's a regular expression so using the variable will be unnecessarily permissive.,code_debt,low_quality_code,"Tue, 28 Jul 2015 17:39:19 +0000","Fri, 1 Jul 2022 20:45:51 +0000","Fri, 31 Jul 2015 22:06:36 +0000",275237,-1 on v3. the point of the check is to make sure the thrift.version variable doesn't get updated without a review of compatibility. also it's a regular expression so using the variable will be unnecessarily permissive.,0,0,neutral
hbase,14494,comment_0,Simple patch which adds some commas to the help message for the follow shell commands: * * grant.rb * * revoke.rb,code_debt,low_quality_code,"Fri, 25 Sep 2015 21:44:37 +0000","Tue, 16 Jan 2018 12:31:51 +0000","Thu, 1 Oct 2015 19:07:03 +0000",508946,Simple patch which adds some commas to the help message for the follow shell commands: delete_table_snapshots.rb grant.rb list_table_snapshots.rb revoke.rb,-0.1083333333,-0.005,neutral
hbase,14494,description,"noticed that the grant shell command was missing commas in-between the method arguments in the usage/help messages. After taking a look at some of the others, it's not alone. I'll make a pass over the shell commands and try to find some more help messages which tell the user to run a bad command.",code_debt,low_quality_code,"Fri, 25 Sep 2015 21:44:37 +0000","Tue, 16 Jan 2018 12:31:51 +0000","Thu, 1 Oct 2015 19:07:03 +0000",508946,"enis noticed that the grant shell command was missing commas in-between the method arguments in the usage/help messages. After taking a look at some of the others, it's not alone. I'll make a pass over the shell commands and try to find some more help messages which tell the user to run a bad command.",0.2222222222,0.2222222222,negative
hbase,14494,summary,Wrong usage messages on shell commands,code_debt,low_quality_code,"Fri, 25 Sep 2015 21:44:37 +0000","Tue, 16 Jan 2018 12:31:51 +0000","Thu, 1 Oct 2015 19:07:03 +0000",508946,Wrong usage messages on shell commands,-0.25,-0.25,negative
hbase,14517,comment_5,Fix checkstyle errors,code_debt,low_quality_code,"Wed, 30 Sep 2015 05:30:06 +0000","Fri, 1 Jul 2022 21:38:21 +0000","Fri, 9 Oct 2015 22:15:05 +0000",837899,Fix checkstyle errors,-0.4,-0.4,negative
hbase,14604,comment_4,The classes are in the same package. Please make the new methods package private. Patch for 0.98 branch should contain 0.98 in the filename. Extension should be either .txt or .patch. Please attach patch for master branch.,code_debt,low_quality_code,"Wed, 14 Oct 2015 09:54:05 +0000","Tue, 20 Oct 2015 15:20:36 +0000","Tue, 20 Oct 2015 09:40:16 +0000",517571,The classes are in the same package. Please make the new methods package private. Patch for 0.98 branch should contain 0.98 in the filename. Extension should be either .txt or .patch. Please attach patch for master branch.,0.07857142857,0.07857142857,neutral
hbase,14677,description,"BucketCache implementation of freeBlock has a bad performance / concurrency and affects bulk block evictions from cache on a file close (after split, compactions or region moves).",code_debt,slow_algorithm,"Thu, 22 Oct 2015 17:46:57 +0000","Thu, 22 Oct 2015 20:33:03 +0000","Thu, 22 Oct 2015 17:49:09 +0000",132,"BucketCache implementation of freeBlock has a bad performance / concurrency and affects bulk block evictions from cache on a file close (after split, compactions or region moves).",-0.4,-0.4,negative
hbase,15207,comment_2,Fixed summary and description. It is about the stuck balancer. Should also fix the crazy logging. Can do that in subtask. Thanks,code_debt,low_quality_code,"Tue, 2 Feb 2016 19:36:52 +0000","Fri, 5 Feb 2016 22:38:47 +0000","Fri, 5 Feb 2016 22:38:47 +0000",270115,Fixed summary and description. It is about the stuck balancer. Should also fix the crazy logging. Can do that in subtask. Thanks busbey,-0.025,-0.025,neutral
hbase,15207,comment_4,"Happened again in new loading. Its hard to make sense of because the logging overwhelms. Let me address that in a patch first. Can then look at hang. In this current case we filled 10 log files of 256MB each... but it looks like we 'recovered'. LB reports: 2016-02-02 17:15:05,123 DEBUG Finished computing new load balance plan. Computation took 3761ms to try 217600 different iterations. Found a solution that moves 31 regions; Going from a computed cost of 341.5035724035313 to a new cost of 47.8518130393211 And going back over logs, yeah, I get sessions of log spewing.... maybe the load balancer is ok... its just this crazy logging phenomenon",code_debt,low_quality_code,"Tue, 2 Feb 2016 19:36:52 +0000","Fri, 5 Feb 2016 22:38:47 +0000","Fri, 5 Feb 2016 22:38:47 +0000",270115,"Happened again in new loading. Its hard to make sense of because the logging overwhelms. Let me address that in a patch first. Can then look at hang. In this current case we filled 10 log files of 256MB each... but it looks like we 'recovered'. LB reports: 2016-02-02 17:15:05,123 DEBUG [ve0524.halxg.cloudera.com,16000,1454461776827_ChoreService_1] balancer.StochasticLoadBalancer: Finished computing new load balance plan. Computation took 3761ms to try 217600 different iterations. Found a solution that moves 31 regions; Going from a computed cost of 341.5035724035313 to a new cost of 47.8518130393211 And going back over logs, yeah, I get sessions of log spewing.... maybe the load balancer is ok... its just this crazy logging phenomenon",-0.00859375,-0.005729166667,negative
hbase,15293,comment_9,"Please insert space around 'catch' There're 7 tabs in the patch. Please remove them. Once these are addressed, it should be good.",code_debt,low_quality_code,"Fri, 19 Feb 2016 13:41:34 +0000","Thu, 15 Sep 2016 19:31:40 +0000","Fri, 1 Apr 2016 14:01:45 +0000",3630011,"Please insert space around 'catch' There're 7 tabs in the patch. Please remove them. Once these are addressed, it should be good.",0.346,0.346,neutral
hbase,15397,comment_3,It is not used any more.,code_debt,dead_code,"Fri, 4 Mar 2016 10:39:15 +0000","Fri, 17 Jun 2022 18:42:53 +0000","Fri, 4 Mar 2016 21:50:19 +0000",40264,Why is the Boolean removed ? It is not used any more.,0,0,neutral
hbase,15490,description,"Currently there're two in our branch-1 code base (one in package, the other in and both are in use. This is a regression of HBASE-14969 and only exists in branch-1. We should remove the one in and change the default compaction throughput controller back to to keep compatible with previous branch-1 version Thanks  for pointing out the issue.",code_debt,duplicated_code,"Sat, 19 Mar 2016 18:11:32 +0000","Mon, 21 Mar 2016 10:51:03 +0000","Sun, 20 Mar 2016 08:20:29 +0000",50937,"Currently there're two CompactionThroughputControllerFactory in our branch-1 code base (one in o.a.h.h.regionserver.compactions package, the other in o.a.h.h.regionserver.throttle) and both are in use. This is a regression of HBASE-14969 and only exists in branch-1. We should remove the one in o.a.h.h.regionserver.compactions, and change the default compaction throughput controller back to NoLimitThroughputController to keep compatible with previous branch-1 version Thanks ghelmling for pointing out the issue.",0.05833333333,0.02916666667,neutral
hbase,15490,summary,Remove duplicated in branch-1,code_debt,duplicated_code,"Sat, 19 Mar 2016 18:11:32 +0000","Mon, 21 Mar 2016 10:51:03 +0000","Sun, 20 Mar 2016 08:20:29 +0000",50937,Remove duplicated CompactionThroughputControllerFactory in branch-1,0,0,neutral
hbase,15704,comment_6,"Thanks . Let's remove the code in master then. It is not good to keep un-used code around. Vladimir, can we remove the backup.example classes in master, and do the rename only in HFileArchiver. In branch-1, we can keep the example classes and do the HFileArchiver rename only.",code_debt,dead_code,"Mon, 25 Apr 2016 19:09:20 +0000","Fri, 1 Jul 2022 20:14:53 +0000","Thu, 24 Aug 2017 20:32:02 +0000",41995362,"I don't know if we need to keep it around. Thanks jesse_yates. Let's remove the code in master then. It is not good to keep un-used code around. Vladimir, can we remove the backup.example classes in master, and do the rename only in HFileArchiver. In branch-1, we can keep the example classes and do the HFileArchiver rename only.",0.104,0.08914285714,negative
hbase,15704,description,This class is in backup package (as well as backup/examples classes) but is not backup - related. Remove examples classes from a codebase,code_debt,low_quality_code,"Mon, 25 Apr 2016 19:09:20 +0000","Fri, 1 Jul 2022 20:14:53 +0000","Thu, 24 Aug 2017 20:32:02 +0000",41995362,This class is in backup package (as well as backup/examples classes) but is not backup - related. Remove examples classes from a codebase,0.3155,0.3155,negative
hbase,15707,comment_3,Please fix checkstyle warning.,code_debt,low_quality_code,"Mon, 25 Apr 2016 23:26:14 +0000","Thu, 9 Nov 2017 00:48:29 +0000","Wed, 27 Apr 2016 20:51:13 +0000",163499,Please fix checkstyle warning.,-0.2,-0.2,negative
hbase,15707,comment_5,"Understood the checkstyle warning, will provide a new patch, thanks !",code_debt,low_quality_code,"Mon, 25 Apr 2016 23:26:14 +0000","Thu, 9 Nov 2017 00:48:29 +0000","Wed, 27 Apr 2016 20:51:13 +0000",163499,"Understood the checkstyle warning, will provide a new patch, thanks tedyu!",-0.1,-0.1,positive
hbase,15835,comment_7,"Hey , can you remove instances of setting the port to -1 in existing tests? I did a quick grep and there looks to be only a handful. Can you add some class-level javadoc about the minicluster ports going random instead of default. Do you think it might be worth a debug-level logging message too? I could image someone setting a value and in one special case having it fail.",code_debt,low_quality_code,"Mon, 16 May 2016 08:54:16 +0000","Fri, 1 Jul 2022 20:27:59 +0000","Wed, 9 May 2018 14:58:37 +0000",62489061,"Hey daniel_vimont, can you remove instances of setting the port to -1 in existing tests? I did a quick grep and there looks to be only a handful. Can you add some class-level javadoc about the minicluster ports going random instead of default. Do you think it might be worth a debug-level logging message too? I could image someone setting a value and in one special case having it fail.",-0.1,-0.1,neutral
hbase,1655,comment_1,"Patch looks pretty good. Small nitpicky issues with some needless reorderings of stuff and also some tabbing oddities. Can be fixed on commit. I remember why we're using TreeMap now instead of HashMap. HashMap with byte[] as a key does not work with default comparator, and you cannot pass one in. There is no real downside to using TreeMap unless you have hundreds or thousands of tables, and even then logarithmic on the client-side on the order of 100-1000 is negligible. More efficient in memory but a little dirtier on the GC... however this is almost exclusively client-side (and there's ever only one) so really makes no difference IMO. So, back to TreeMap. The API seems to have grown quite a bit. Do we need all these permutations of getTable() ? Could we drop the String taking ones besides the simplest one? (This is why we don't support String in most of the HBase API now, leads to very long and ugly APIs) The default getTable(byte [] tableName) also requires instantiating a new each time internally, even if we are reusing an existing HTable... Whether there is a significant overhead or not to that, we should avoid it when unnecessary. Typical usage is probably to not supply your own HBC, so if someone wants that level of control, give them the ability to build the Pool themselves rather than expose the friendlier, higher-level methods with all the different ways to instantiate the pool. Blah, this is better said in a new patch....",code_debt,low_quality_code,"Tue, 14 Jul 2009 19:52:59 +0000","Wed, 12 Aug 2020 23:02:36 +0000","Fri, 17 Jul 2009 19:26:30 +0000",257611,"Patch looks pretty good. Small nitpicky issues with some needless reorderings of stuff and also some tabbing oddities. Can be fixed on commit. I remember why we're using TreeMap now instead of HashMap. HashMap with byte[] as a key does not work with default comparator, and you cannot pass one in. There is no real downside to using TreeMap unless you have hundreds or thousands of tables, and even then logarithmic on the client-side on the order of 100-1000 is negligible. More efficient in memory but a little dirtier on the GC... however this is almost exclusively client-side (and there's ever only one) so really makes no difference IMO. So, back to TreeMap. The API seems to have grown quite a bit. Do we need all these permutations of getTable() ? Could we drop the String taking ones besides the simplest one? (This is why we don't support String in most of the HBase API now, leads to very long and ugly APIs) The default getTable(byte [] tableName) also requires instantiating a new HBaseConfiguration() each time internally, even if we are reusing an existing HTable... Whether there is a significant overhead or not to that, we should avoid it when unnecessary. Typical usage is probably to not supply your own HBC, so if someone wants that level of control, give them the ability to build the Pool themselves rather than expose the friendlier, higher-level methods with all the different ways to instantiate the pool. Blah, this is better said in a new patch....",0.08128571429,0.08128571429,neutral
hbase,1655,comment_2,"This doesn't include any stargate stuff, only issues there are the tab issues, reordering, and whether to expose the pools or not. I'm not sure we want to remove the ability to work with HTablePool directly or not. This patch re-publics a bunch of stuff, cuts down on the API quite a bit, and reduces re-instantiation of HBC when getting pools that already exist and will always reuse the original when getting tables. We should definitely keep that. Otherwise the changes are open to discussion, I'm not particularly sold on my own patch, just providing it so we have a point of reference for discussion. The biggest thing to figure out is whether we even expose HTablePool non-statically to the user. If not, we go down the path of a much longer easy-to-use API... /me sleeping on it",code_debt,low_quality_code,"Tue, 14 Jul 2009 19:52:59 +0000","Wed, 12 Aug 2020 23:02:36 +0000","Fri, 17 Jul 2009 19:26:30 +0000",257611,"This doesn't include any stargate stuff, only issues there are the tab issues, reordering, and whether to expose the pools or not. I'm not sure we want to remove the ability to work with HTablePool directly or not. This patch re-publics a bunch of stuff, cuts down on the API quite a bit, and reduces re-instantiation of HBC when getting pools that already exist and will always reuse the original when getting tables. We should definitely keep that. Otherwise the changes are open to discussion, I'm not particularly sold on my own patch, just providing it so we have a point of reference for discussion. The biggest thing to figure out is whether we even expose HTablePool non-statically to the user. If not, we go down the path of a much longer easy-to-use API... /me sleeping on it",-0.197047619,-0.197047619,neutral
hbase,1655,comment_4,"A few questions/comments on the comments: - Why does the key to a HashMap need a comparator? - I much prefer the type of String vs byte[] for the tableName throughout all the method signatures and as a key to the internal map, but I tried to keep it as-is because it seemed to fit more with other HBase code. Can we just change it to String throughout and go back to the HashMap? - I am not a fan of the current API either. I'd much prefer that the HTablePool be instantiated by a client and we get rid of the static methods and static Map altogether. This fits in much more nicely for people like me who are using IoC containers like Spring. It also allows the ability to have multiple HTablePool instances, maybe each with their own configuration (which is currently just the max size). - Sorry about the tab/spaces issue. I didn't clean it up carefully enough. - Sorry about the reordering of imports. I use Eclipse which does this automatically and I was too lazy to try to restore the order back to the way it was. I'll try harder next time. So what is the next step? Should I make a new patch with an HTablePool that is meant to be instantiated and w/o the static methods?",code_debt,low_quality_code,"Tue, 14 Jul 2009 19:52:59 +0000","Wed, 12 Aug 2020 23:02:36 +0000","Fri, 17 Jul 2009 19:26:30 +0000",257611,"A few questions/comments on the comments: Why does the key to a HashMap need a comparator? I much prefer the type of String vs byte[] for the tableName throughout all the method signatures and as a key to the internal map, but I tried to keep it as-is because it seemed to fit more with other HBase code. Can we just change it to String throughout and go back to the HashMap? I am not a fan of the current API either. I'd much prefer that the HTablePool be instantiated by a client and we get rid of the static methods and static Map altogether. This fits in much more nicely for people like me who are using IoC containers like Spring. It also allows the ability to have multiple HTablePool instances, maybe each with their own configuration (which is currently just the max size). Sorry about the tab/spaces issue. I didn't clean it up carefully enough. Sorry about the reordering of imports. I use Eclipse which does this automatically and I was too lazy to try to restore the order back to the way it was. I'll try harder next time. So what is the next step? Should I make a new patch with an HTablePool that is meant to be instantiated and w/o the static methods?",-0.1090416667,-0.1315595238,neutral
hbase,1655,description,"A discussion on the HBase user mailing list led to some suggested improvements for the class. I will be submitting a patch that contains the following changes to HTablePool: * Remove constructors that were not used. * Change access to remaining contstructor from public to private to enforce use of the static factory method getPool. * Change internal map from TreeMap to HashMap because I couldn't see any reason it needed to be sorted. * Remove HBaseConfiguration and tableName member variables since they aren't really properties of the pool itself. They are associated with the HTable that should get instantiated when one is requested from the pool, but not already there.",code_debt,dead_code,"Tue, 14 Jul 2009 19:52:59 +0000","Wed, 12 Aug 2020 23:02:36 +0000","Fri, 17 Jul 2009 19:26:30 +0000",257611,"A discussion on the HBase user mailing list (http://markmail.org/thread/7leeha56ny5mwecg) led to some suggested improvements for the org.apache.hadoop.hbase.client.HTablePool class. I will be submitting a patch that contains the following changes to HTablePool: Remove constructors that were not used. Change access to remaining contstructor from public to private to enforce use of the static factory method getPool. Change internal map from TreeMap to HashMap because I couldn't see any reason it needed to be sorted. Remove HBaseConfiguration and tableName member variables since they aren't really properties of the pool itself. They are associated with the HTable that should get instantiated when one is requested from the pool, but not already there.",-0.1458333333,-0.07954545455,neutral
hbase,16998,comment_6,".004 latest from rb. Missing InterfaceAudience annotations, constructor cleanup in QuotaObserverChore.",code_debt,low_quality_code,"Wed, 2 Nov 2016 19:26:32 +0000","Fri, 6 Apr 2018 04:00:26 +0000","Mon, 30 Jan 2017 17:12:40 +0000",7681568,".004 latest from rb. Missing InterfaceAudience annotations, constructor cleanup in QuotaObserverChore.",-0.1333333333,-0.1333333333,negative
hbase,17101,comment_0,"including a rough patch, will cleanup and upload.",code_debt,low_quality_code,"Tue, 15 Nov 2016 18:35:53 +0000","Thu, 23 Jun 2022 19:12:57 +0000","Tue, 31 Jan 2017 18:59:41 +0000",6654228,"including a rough patch, will cleanup and upload.",0,0,neutral
hbase,17184,description,"Initially I only wanted to fix a badly worded Log message but found a bunch of code convention violations etc. so I decided to clean up the class. It is only whitespace changes with the exception of the one log message (""too many"").",code_debt,low_quality_code,"Mon, 28 Nov 2016 10:00:06 +0000","Sat, 3 Dec 2016 00:49:12 +0000","Wed, 30 Nov 2016 20:58:05 +0000",212279,"Initially I only wanted to fix a badly worded Log message but found a bunch of code convention violations etc. so I decided to clean up the class. It is only whitespace changes with the exception of the one log message (""too many"").",0.01875,0.01875,negative
hbase,17184,summary,Code cleanup of LruBlockCache,code_debt,low_quality_code,"Mon, 28 Nov 2016 10:00:06 +0000","Sat, 3 Dec 2016 00:49:12 +0000","Wed, 30 Nov 2016 20:58:05 +0000",212279,Code cleanup of LruBlockCache,0,0,neutral
hbase,17192,comment_6,+1. Thanks for debug steps (ugly).,code_debt,low_quality_code,"Tue, 29 Nov 2016 09:50:31 +0000","Fri, 24 Jun 2022 19:34:43 +0000","Tue, 29 Nov 2016 14:52:25 +0000",18114,+1. Thanks for debug steps (ugly).,-0.15,-0.15,positive
hbase,1723,comment_0,"Use get.setTimeStamp instead of get.setTimeRange This patch rearranges getRowWithColumnsTs a bit, because the original version was unneccessary repetitive.",code_debt,duplicated_code,"Thu, 30 Jul 2009 08:30:33 +0000","Sat, 11 Jun 2022 21:00:56 +0000","Tue, 19 Jan 2010 18:12:34 +0000",14982121,"Use get.setTimeStamp instead of get.setTimeRange This patch rearranges getRowWithColumnsTs a bit, because the original version was unneccessary repetitive.",0,0,negative
hbase,17338,comment_19,+1 on commit. Lets get up on this new basis. Nice cleanup  (Thanks too for doc edit).,code_debt,low_quality_code,"Mon, 19 Dec 2016 13:18:49 +0000","Fri, 1 Jul 2022 21:33:56 +0000","Fri, 10 Mar 2017 05:43:36 +0000",6971087,+1 on commit. Lets get up on this new basis. Nice cleanup anoop.hbase (Thanks too for doc edit).,0.4375,0.48125,positive
hbase,17338,comment_8,+1 Add more comment on commit to MemstoreSize about diff between heap and data size.... (can be a version of comment that is later in class...) Glad of the simplification. Good stuff @anoop sam john,code_debt,complex_code,"Mon, 19 Dec 2016 13:18:49 +0000","Fri, 1 Jul 2022 21:33:56 +0000","Fri, 10 Mar 2017 05:43:36 +0000",6971087,+1 Add more comment on commit to MemstoreSize about diff between heap and data size.... (can be a version of comment that is later in RegionServerAccounting class...) Glad of the simplification. Good stuff @anoop sam john,0.5753333333,0.5753333333,positive
hbase,17383,comment_3,No longer this case as some other cleanup corrected the log. Just closing as can not reproduce,code_debt,low_quality_code,"Wed, 28 Dec 2016 10:14:44 +0000","Thu, 22 Mar 2018 07:28:21 +0000","Thu, 22 Mar 2018 07:28:21 +0000",38783617,No longer this case as some other cleanup corrected the log. Just closing as can not reproduce,0,0,negative
hbase,17383,description,Currently we get this log Here the global offheap memstore size is greater than the blocking size. The memstore heap overhead need not be included in this log unless the higher water mark breach is only due to the heap overhead.,code_debt,low_quality_code,"Wed, 28 Dec 2016 10:14:44 +0000","Thu, 22 Mar 2018 07:28:21 +0000","Thu, 22 Mar 2018 07:28:21 +0000",38783617,Currently we get this log Here the global offheap memstore size is greater than the blocking size. The memstore heap overhead need not be included in this log unless the higher water mark breach is only due to the heap overhead.,-0.125,-0.125,neutral
hbase,17383,summary,Improve log msg when offheap memstore breaches higher water mark,code_debt,low_quality_code,"Wed, 28 Dec 2016 10:14:44 +0000","Thu, 22 Mar 2018 07:28:21 +0000","Thu, 22 Mar 2018 07:28:21 +0000",38783617,Improve log msg when offheap memstore breaches higher water mark,0,0,neutral
hbase,17394,comment_2,"The logic is correct, the definition is [min, max], I added comments through HBASE-12148 to make it clear and hopefully, it will help code reading a bit easier.",code_debt,low_quality_code,"Fri, 30 Dec 2016 01:04:58 +0000","Mon, 9 Jan 2017 19:51:04 +0000","Mon, 9 Jan 2017 19:51:03 +0000",931565,"The logic is correct, the definition is [min, max], I added comments through HBASE-12148 to make it clear and hopefully, it will help code reading a bit easier.",0.5583333333,0.5583333333,positive
hbase,17480,comment_2,+1 That is a nice lump of code removed. You remove Do we have an explicit test of the splitting path any more post this removal?,code_debt,dead_code,"Tue, 17 Jan 2017 23:08:57 +0000","Fri, 17 Jun 2022 18:05:45 +0000","Thu, 19 Jan 2017 17:05:56 +0000",151019,+1 That is a nice lump of code removed. You remove TestSplitTransaction. Do we have an explicit test of the splitting path any more post this removal? syuanjiang,0.6625,0.33125,positive
hbase,17480,description,"HBASE-14551 moves the split region logic to the master-side. With the code in HBASE-14551, the split transaction code from region server side became un-used. There is no need to keep region_server-side split region code. We should remove them to avoid code duplication.",code_debt,dead_code,"Tue, 17 Jan 2017 23:08:57 +0000","Fri, 17 Jun 2022 18:05:45 +0000","Thu, 19 Jan 2017 17:05:56 +0000",151019,"HBASE-14551 moves the split region logic to the master-side. With the code in HBASE-14551, the split transaction code from region server side became un-used. There is no need to keep region_server-side split region code. We should remove them to avoid code duplication.",0.075,0.075,neutral
hbase,17808,comment_1,"I have done a little performance test. I used YSCB's workloadc with 1 client 100 threads against one regionserver. But surprisedly I noticed performance regression with 'fastpath', no matter with I implemented or the original Is it something wrong with fastpath or am I doing something wrong? Ping , since Stack is the original author of fastpath. YSCB workloadc with 100 threads |32982| issue)|32287| fastpath)|34563|",code_debt,slow_algorithm,"Mon, 20 Mar 2017 09:13:13 +0000","Wed, 15 Dec 2021 02:48:27 +0000","Wed, 15 Dec 2021 02:48:27 +0000",149535314,"I have done a little performance test. I used YSCB's workloadc with 1 client 100 threads against one regionserver. But surprisedly I noticed performance regression with 'fastpath', no matter with FastPathRWQueueRpcExecutor I implemented or the original FastPathBalancedQueueRpcExecutor. Is it something wrong with fastpath or am I doing something wrong? Ping stack, since Stack is the original author of fastpath. YSCB workloadc with 100 threads",0.02,-0.075,neutral
hbase,18085,comment_13,"Sorry, don't quite catch you sir, mind clarify? While writing the JMH case, I felt that using AtomicBoolean or volatile boolean we're actually re-implementing the tryLock logic. And I won't be surprised if JIT has any optimization for its native implementation (smile).",code_debt,duplicated_code,"Fri, 19 May 2017 19:04:08 +0000","Wed, 1 Aug 2018 06:24:22 +0000","Wed, 24 May 2017 07:57:01 +0000",391973,"Can we use the return value of purgeLock.tryLock() passed to BlackHole or so? Sorry, don't quite catch you sir, mind clarify? While writing the JMH case, I felt that using AtomicBoolean or volatile boolean we're actually re-implementing the tryLock logic. And I won't be surprised if JIT has any optimization for its native implementation (smile).",-0.4873333333,-0.2924,neutral
hbase,18085,comment_14,"In ur test method for tryLock, there is no logic other than just try lock and release. If that is been removed as dead code by compiler? One way to avoid that is using the return value. See BlackHole in JMH and its usage.. The diff in numbers that reported by JMH benchmark is so huge! The impl of tryLock is having a volatile read and all. So this huge diff in numbers looks strange no? That was my doubt.",code_debt,dead_code,"Fri, 19 May 2017 19:04:08 +0000","Wed, 1 Aug 2018 06:24:22 +0000","Wed, 24 May 2017 07:57:01 +0000",391973,"In ur test method for tryLock, there is no logic other than just try lock and release. If that is been removed as dead code by compiler? One way to avoid that is using the return value. See BlackHole in JMH and its usage.. The diff in numbers that reported by JMH benchmark is so huge! The impl of tryLock is having a volatile read and all. So this huge diff in numbers looks strange no? That was my doubt.",-0.1302857143,-0.1302857143,negative
hbase,18085,comment_9,"Reading the code in I can see it uses a state variable up in the layer in which is declared volatile. So we ourselves using a boolean volatile will be better any way right? Pls check once whether I misread some code path,",code_debt,low_quality_code,"Fri, 19 May 2017 19:04:08 +0000","Wed, 1 Aug 2018 06:24:22 +0000","Wed, 24 May 2017 07:57:01 +0000",391973,"Reading the code in ReentrantLock#tryLock(), I can see it uses a state variable up in the layer in AbstractQueuedSynchronizer which is declared volatile. So we ourselves using a boolean volatile will be better any way right? Pls check once whether I misread some code path,",-0.1356666667,-0.1356666667,neutral
hbase,18085,description,"Parallel purge in ObjectPool is meaningless and will cause contention issue since has synchronization (source code shown below) We observed threads blocking on the purge method while using offheap bucket cache, and we could easily reproduce this by testing the 100% cache hit case in bucket cache with enough reading threads. We propose to add a purgeLock and use tryLock to avoid parallel purge.",code_debt,multi-thread_correctness,"Fri, 19 May 2017 19:04:08 +0000","Wed, 1 Aug 2018 06:24:22 +0000","Wed, 24 May 2017 07:57:01 +0000",391973,"Parallel purge in ObjectPool is meaningless and will cause contention issue since ReferenceQueue#poll has synchronization (source code shown below) We observed threads blocking on the purge method while using offheap bucket cache, and we could easily reproduce this by testing the 100% cache hit case in bucket cache with enough reading threads. We propose to add a purgeLock and use tryLock to avoid parallel purge.",-0.1166666667,-0.1166666667,negative
hbase,18092,comment_13,"Just a nitpick, I think you merged the incorrect patch for master. I was looking at my latest master checkout and see that patch for master that went in is dated May 25, not yesterday. It's not a big deal in this case because the two patches have no functional difference, only that the newer patch got rid of a redundant piece of code.",code_debt,dead_code,"Mon, 22 May 2017 18:47:28 +0000","Wed, 1 Aug 2018 06:22:22 +0000","Fri, 9 Jun 2017 18:15:45 +0000",1553297,"yuzhihong@gmail.com Just a nitpick, I think you merged the incorrect patch for master. I was looking at my latest master checkout and see that patch for master that went in is dated May 25, not yesterday. It's not a big deal in this case because the two patches have no functional difference, only that the newer patch got rid of a redundant piece of code.",0.2666666667,0.2666666667,neutral
hbase,18092,comment_4,"Attaching another patch for master. I found a redundant piece of code related to the patch. The only difference between the previous and this patch is {Code} @@ -528,9 +542,7 @@ public class implements ReplicationListener { */ public void src) { LOG.info(""Done with the recovered queue "" + - if (src instanceof ReplicationSource) { - - } + {Code} Also attaching patches for branch-1 and branch-1.3 as they both have diverged a little bit.",code_debt,low_quality_code,"Mon, 22 May 2017 18:47:28 +0000","Wed, 1 Aug 2018 06:22:22 +0000","Fri, 9 Jun 2017 18:15:45 +0000",1553297,Attaching another patch for master. I found a redundant piece of code related to the patch. The only difference between the previous and this patch is Also attaching patches for branch-1 and branch-1.3 as they both have diverged a little bit.,0.1,0.125,neutral
hbase,18092,description,Removing a peer does not clean up the associated metrics and state from walsById map in the,code_debt,low_quality_code,"Mon, 22 May 2017 18:47:28 +0000","Wed, 1 Aug 2018 06:22:22 +0000","Fri, 9 Jun 2017 18:15:45 +0000",1553297,Removing a peer does not clean up the associated metrics and state from walsById map in the ReplicationSourceManager.,-0.4,-0.4,negative
hbase,18092,summary,Removing a peer does not properly clean up the state and metrics,code_debt,low_quality_code,"Mon, 22 May 2017 18:47:28 +0000","Wed, 1 Aug 2018 06:22:22 +0000","Fri, 9 Jun 2017 18:15:45 +0000",1553297,Removing a peer does not properly clean up the ReplicationSourceManager state and metrics,-0.4,-0.4,negative
hbase,18180,comment_1,Have a look at TableOutputFormat inside Connection leak problem has already been addressed in this package. Code sample for reference.,code_debt,low_quality_code,"Wed, 7 Jun 2017 02:54:21 +0000","Wed, 1 Aug 2018 06:21:19 +0000","Mon, 19 Jun 2017 13:40:24 +0000",1075563,pankaj2461 Have a look at TableOutputFormat inside org.apache.hadoop.hbase.mapreduce. Connection leak problem has already been addressed in this package. Code sample for reference.,-0.01666666667,-0.004761904762,neutral
hbase,18180,summary,Possible connection leak while closing BufferedMutator in TableOutputFormat,code_debt,low_quality_code,"Wed, 7 Jun 2017 02:54:21 +0000","Wed, 1 Aug 2018 06:21:19 +0000","Mon, 19 Jun 2017 13:40:24 +0000",1075563,Possible connection leak while closing BufferedMutator in TableOutputFormat,-0.2,-0.2,negative
hbase,18501,description,"We need to do some cleanup for the *public* class as much as possible. Otherwise, the HTD and HCD may linger in the code base for a long time.",code_debt,low_quality_code,"Wed, 2 Aug 2017 09:59:59 +0000","Wed, 21 Mar 2018 22:12:12 +0000","Sat, 26 Aug 2017 12:09:25 +0000",2081366,"We need to do some cleanup for the public class as much as possible. Otherwise, the HTD and HCD may linger in the code base for a long time.",0,0,neutral
hbase,18646,comment_1,"If the config is specific to log roll, please reflect this in the name of config.",code_debt,low_quality_code,"Mon, 21 Aug 2017 22:32:06 +0000","Fri, 1 Jul 2022 20:12:05 +0000","Fri, 1 Sep 2017 17:18:52 +0000",931606,"hbase.backup.master.timeout.millis - log roll procedure timeout (default - 60000 ) If the config is specific to log roll, please reflect this in the name of config.",0.2,0.07,neutral
hbase,18646,comment_4,Isn't the above default too short for a large cluster ? Please remove commented out code from patch.,code_debt,dead_code,"Mon, 21 Aug 2017 22:32:06 +0000","Fri, 1 Jul 2022 20:12:05 +0000","Fri, 1 Sep 2017 17:18:52 +0000",931606,Isn't the above default too short for a large cluster ? Please remove commented out code from patch.,0.35,0.35,negative
hbase,18909,comment_14,Does this still reference to deprecated API?,code_debt,low_quality_code,"Sat, 30 Sep 2017 03:40:48 +0000","Wed, 21 Mar 2018 22:21:06 +0000","Sat, 7 Oct 2017 13:30:27 +0000",640179,Does this still reference to deprecated API?,0.281,0.281,neutral
hbase,18909,comment_6,"Attach a patch to fix the too long line report. But why ruby-lint report ""undefined method java_import""?  Any ideas?",code_debt,low_quality_code,"Sat, 30 Sep 2017 03:40:48 +0000","Wed, 21 Mar 2018 22:21:06 +0000","Sat, 7 Oct 2017 13:30:27 +0000",640179,"Attach a patch to fix the too long line report. But why ruby-lint report ""undefined method java_import""? mdrob Any ideas?",0,0,negative
hbase,19031,comment_6,The deprecation in RemoteHTable was added by this which shipped in version 0.99 tree parent author Enis Soztutar Enis Soztutar <enis@apache.org HBASE-11797 Create Table interface to replace HTableInterface (Carter) ... so the removal is fine. +1 from me on patch.,code_debt,dead_code,"Tue, 17 Oct 2017 20:28:32 +0000","Wed, 21 Mar 2018 22:20:10 +0000","Sat, 28 Oct 2017 23:24:46 +0000",960974,The deprecation in RemoteHTable was added by this which shipped in version 0.99 tree 9ea2b68183049c62f4216ebd09fee17868ab4983 parent 310ac4f71d0c8f28aa8dd8aa2b144fcf206dc83f author Enis Soztutar <enis@apache.org> Tue Sep 2 13:07:02 2014 -0700 committer Enis Soztutar <enis@apache.org> Tue Sep 2 13:07:02 2014 -0700 HBASE-11797 Create Table interface to replace HTableInterface (Carter) ... so the removal is fine. +1 from me on patch.,-0.025,-0.025,neutral
hbase,19073,summary,Cleanup,code_debt,low_quality_code,"Mon, 23 Oct 2017 22:47:02 +0000","Wed, 1 Aug 2018 06:22:22 +0000","Wed, 25 Oct 2017 03:05:27 +0000",101905,Cleanup CoordinatedStateManager,0,0,neutral
hbase,19183,description,Currently the modules hbase-checkstyle and hbase-error-prone define the groupId redundantly. Remove the groupId from these POMs.,code_debt,low_quality_code,"Sat, 4 Nov 2017 21:59:42 +0000","Tue, 7 Nov 2017 12:10:59 +0000","Tue, 7 Nov 2017 07:22:19 +0000",206557,Currently the modules hbase-checkstyle and hbase-error-prone define the groupId redundantly. Remove the groupId from these POMs.,-0.2,-0.2,negative
hbase,19183,summary,Removed redundant groupId from Maven modules,code_debt,low_quality_code,"Sat, 4 Nov 2017 21:59:42 +0000","Tue, 7 Nov 2017 12:10:59 +0000","Tue, 7 Nov 2017 07:22:19 +0000",206557,Removed redundant groupId from Maven modules,0,0,neutral
hbase,19187,comment_8,Correction of test failures. Removed the on heap BC tests. Fixed one javadoc warn.,code_debt,low_quality_code,"Mon, 6 Nov 2017 12:23:41 +0000","Tue, 29 Dec 2020 06:18:44 +0000","Sat, 11 Nov 2017 07:11:53 +0000",413292,Correction of test failures. Removed the on heap BC tests. Fixed one javadoc warn.,-0.2666666667,-0.2666666667,neutral
hbase,19241,comment_5,"+1 after fix the checkstyle warnings. Meanwhile, please add one line change change to trigger hbase-server test. Thanks.",code_debt,low_quality_code,"Sat, 11 Nov 2017 12:43:01 +0000","Wed, 21 Mar 2018 22:21:08 +0000","Mon, 13 Nov 2017 09:02:52 +0000",159591,"+1 after fix the checkstyle warnings. Meanwhile, please add one line change change to trigger hbase-server test. Thanks.",1.85E-17,1.85E-17,neutral
hbase,19241,comment_6,Fix checkstyle and javadoc warnings. Add one line change in hbase-server module to trigger the UTs.,code_debt,low_quality_code,"Sat, 11 Nov 2017 12:43:01 +0000","Wed, 21 Mar 2018 22:21:08 +0000","Mon, 13 Nov 2017 09:02:52 +0000",159591,Fix checkstyle and javadoc warnings. Add one line change in hbase-server module to trigger the UTs.,-0.3,-0.3,neutral
hbase,19241,summary,Improve javadoc for AsyncAdmin and cleanup warnings for the implementation classes,code_debt,low_quality_code,"Sat, 11 Nov 2017 12:43:01 +0000","Wed, 21 Mar 2018 22:21:08 +0000","Mon, 13 Nov 2017 09:02:52 +0000",159591,Improve javadoc for AsyncAdmin and cleanup warnings for the implementation classes,-0.1,-0.1,neutral
hbase,19300,comment_2,"As far as I can tell, synchronizing on outer (the context) is correct: I don't know why error-prone flagged {{synchronized (outer)}}",code_debt,low_quality_code,"Sun, 19 Nov 2017 03:33:56 +0000","Wed, 1 Aug 2018 06:22:30 +0000","Mon, 27 Nov 2017 19:27:38 +0000",748422,"As far as I can tell, synchronizing on outer (the context) is correct: I don't know why error-prone flagged synchronized (outer)",0.6375,0.6375,neutral
hbase,19373,description,Fix the remaining Checkstyle error regarding line length in the *hbase-annotations* module.,code_debt,low_quality_code,"Wed, 29 Nov 2017 11:11:15 +0000","Fri, 17 Jun 2022 18:29:54 +0000","Fri, 8 Dec 2017 22:26:20 +0000",818105,Fix the remaining Checkstyle error regarding line length in the hbase-annotations module.,-0.4,-0.4,neutral
hbase,19373,summary,Fix Checkstyle error in hbase-annotations,code_debt,low_quality_code,"Wed, 29 Nov 2017 11:11:15 +0000","Fri, 17 Jun 2022 18:29:54 +0000","Fri, 8 Dec 2017 22:26:20 +0000",818105,Fix Checkstyle error in hbase-annotations,-0.4,-0.4,neutral
hbase,19478,description,Currently issues one Get per WAL file: This is rather inefficient considering the number of WAL files in production can get quite large. We should use multi-get to reduce the number of calls to backup table (which normally resides on another server).,code_debt,slow_algorithm,"Sun, 10 Dec 2017 17:10:15 +0000","Mon, 1 Jan 2018 17:45:29 +0000","Mon, 1 Jan 2018 14:55:59 +0000",1892744,Currently BackupLogCleaner#getDeletableFiles() issues one Get per WAL file: This is rather inefficient considering the number of WAL files in production can get quite large. We should use multi-get to reduce the number of calls to backup table (which normally resides on another server).,-0.375,-0.375,negative
hbase,19531,summary,Remove needless volatile declaration,code_debt,low_quality_code,"Sat, 16 Dec 2017 11:02:40 +0000","Mon, 18 Dec 2017 19:07:40 +0000","Mon, 18 Dec 2017 16:47:49 +0000",193509,Remove needless volatile declaration,-0.531,-0.531,negative
hbase,19862,comment_2,Will cleanup checkstyles on commit. Ping  since you reviewed the related change too.,code_debt,low_quality_code,"Thu, 25 Jan 2018 22:01:27 +0000","Wed, 21 Mar 2018 22:23:45 +0000","Fri, 26 Jan 2018 08:27:33 +0000",37566,Will cleanup checkstyles on commit. Ping zghaobac since you reviewed the related change too.,0.1,0.1,neutral
hbase,1990,description,"Consider the following client code... byte b[] = result.getValue( ); put.add( Bytes.toBytes( ""value"") ); ... the requirement to supply family and qualifiers as bytes causes code to get cluttered and verbose. At worst, it scares peoples un-necessarily about HBase development, and at best, developers inevitably will get tired of doing all this casting and then add their own wrapper classes around the HBase client to make their code more readable. I would like to see something like this in the API... byte b[] = result.getValue( ""family""), ""qualifier"" ); put.add( ""family"", ""qualifer"", Bytes.toBytes( ""value"") ); ... where the Hbase client can perform the required Bytes.toBytes() conversion behind the scenes.",code_debt,complex_code,"Wed, 18 Nov 2009 21:48:59 +0000","Sat, 11 Jun 2022 23:15:15 +0000","Mon, 12 May 2014 00:40:37 +0000",141274298,"Consider the following client code... byte b[] = result.getValue( Bytes.toBytes(""family""), Bytes.toBytes(""qualifier"") ); put.add( Bytes.toBytes(""family""), Bytes.toBytes(""qualifer""), Bytes.toBytes( ""value"") ); ... the requirement to supply family and qualifiers as bytes causes code to get cluttered and verbose. At worst, it scares peoples un-necessarily about HBase development, and at best, developers inevitably will get tired of doing all this casting and then add their own wrapper classes around the HBase client to make their code more readable. I would like to see something like this in the API... byte b[] = result.getValue( ""family""), ""qualifier"" ); put.add( ""family"", ""qualifer"", Bytes.toBytes( ""value"") ); ... where the Hbase client can perform the required Bytes.toBytes() conversion behind the scenes.",-0.0575,-0.04107142857,negative
hbase,19969,comment_14,Vlad: Can you address checkstyle warnings ?,code_debt,low_quality_code,"Fri, 9 Feb 2018 22:57:42 +0000","Wed, 28 Mar 2018 02:50:38 +0000","Sat, 10 Mar 2018 17:49:55 +0000",2487133,Vlad: Can you address checkstyle warnings ?,-0.6,-0.6,neutral
hbase,19969,comment_2,"Add backup after 'Get' I don't think the above passes checkstyle Do you want to implement in this JIRA ? I don't think the above is right - we use org.slf4j Is the change to public for testing ? Drop commented out code. If we get into the if block, the rename() call below would fail, right ? Drop commented out code. Please address checkstyle, findbugs warnings. Will continue reviewing.",code_debt,low_quality_code,"Fri, 9 Feb 2018 22:57:42 +0000","Wed, 28 Mar 2018 02:50:38 +0000","Sat, 10 Mar 2018 17:49:55 +0000",2487133,"Add backup after 'Get' I don't think the above passes checkstyle Do you want to implement in this JIRA ? I don't think the above is right - we use org.slf4j Is the change to public for testing ? Drop commented out code. If we get into the if block, the rename() call below would fail, right ? Drop commented out code. Please address checkstyle, findbugs warnings. Will continue reviewing.",-0.1689166667,-0.1689166667,negative
hbase,19991,comment_1,"careful debugging revealed a few more places where the jersey deps were leaking in. I also need to add a comment somewhere that upgrading to jersey 2.26 (from our current 2.25.1) will likely need an upgrade to jetty 9.4, so should be done with great care, as that was one of the things i tried here and it didn't work as well as I thought it would",code_debt,low_quality_code,"Tue, 13 Feb 2018 18:22:17 +0000","Wed, 1 Aug 2018 06:23:06 +0000","Wed, 21 Feb 2018 16:52:43 +0000",685826,"careful debugging revealed a few more places where the jersey deps were leaking in. I also need to add a comment somewhere that upgrading to jersey 2.26 (from our current 2.25.1) will likely need an upgrade to jetty 9.4, so should be done with great care, as that was one of the things i tried here and it didn't work as well as I thought it would",0.3556,0.3556,neutral
hbase,20595,comment_7,Updated master patch removes the unused imports that checkstyle complained about.,code_debt,low_quality_code,"Wed, 16 May 2018 21:37:08 +0000","Mon, 22 Apr 2019 16:10:14 +0000","Wed, 23 May 2018 18:58:44 +0000",595296,Updated master patch removes the unused imports that checkstyle complained about.,0.05,0.05,neutral
hbase,2068,comment_13,"+1 Works great! There are few now obsolete imports, like in the changed Statistics classes. Could remove on commit. Otherwise please commit.",code_debt,low_quality_code,"Tue, 22 Dec 2009 13:46:58 +0000","Fri, 12 Oct 2012 06:14:13 +0000","Mon, 4 Jan 2010 21:02:15 +0000",1149317,"+1 Works great! There are few now obsolete imports, like in the changed Statistics classes. Could remove on commit. Otherwise please commit.",0.15,0.15,positive
hbase,2068,comment_14,Committed. Thanks for the patches lads. I removed the unused import. Added a couple of licenses too. Excellent.,code_debt,low_quality_code,"Tue, 22 Dec 2009 13:46:58 +0000","Fri, 12 Oct 2012 06:14:13 +0000","Mon, 4 Jan 2010 21:02:15 +0000",1149317,Committed. Thanks for the patches lads. I removed the unused import. Added a couple of licenses too. Excellent.,0.33,0.33,positive
hbase,2068,comment_2,Another inconsistency is that MasterMetrics uses a MetricsIntValue and the uses a MetricsRate class I suggest after fixing MetricsRate to replace the MasterMetrics one with it as well. I am also wondering if it would have been better to call it MetricsIntRate to indicate the size of the internal counter.,code_debt,low_quality_code,"Tue, 22 Dec 2009 13:46:58 +0000","Fri, 12 Oct 2012 06:14:13 +0000","Mon, 4 Jan 2010 21:02:15 +0000",1149317,Another inconsistency is that MasterMetrics uses a MetricsIntValue and the RegionsServerMetrics uses a MetricsRate class I suggest after fixing MetricsRate to replace the MasterMetrics one with it as well. I am also wondering if it would have been better to call it MetricsIntRate to indicate the size of the internal counter.,0.5655,0.5655,neutral
hbase,20696,summary,Shell list_peers print useless string,code_debt,low_quality_code,"Thu, 7 Jun 2018 08:06:09 +0000","Wed, 1 Aug 2018 06:21:59 +0000","Thu, 7 Jun 2018 12:44:19 +0000",16690,Shell list_peers print useless string,-0.4,-0.4,negative
hbase,20787,comment_0,I will also remove the various commits/reverts of the initial patch to simplify things.,code_debt,low_quality_code,"Tue, 26 Jun 2018 00:11:57 +0000","Tue, 10 Jul 2018 04:55:21 +0000","Tue, 10 Jul 2018 04:55:21 +0000",1226604,I will also remove the various commits/reverts of the initial patch to simplify things.,0.2,0.2,neutral
hbase,20975,comment_0,"+1 on removing it for now. We have optimized too much before getting things correct... Let's keep the logic simple first. Also, there are some bad style issues. At least let's remove the space between 'stackTail' and '--', it looks like '-- And do not do assignment in the condition block of if. Let's change to",code_debt,low_quality_code,"Mon, 30 Jul 2018 07:56:06 +0000","Tue, 14 Aug 2018 11:44:47 +0000","Mon, 13 Aug 2018 12:24:30 +0000",1225704,"+1 on removing it for now. We have optimized too much before getting things correct... Let's keep the logic simple first. Also, there are some bad style issues. At least let's remove the space between 'stackTail' and '-', it looks like '->' is the operator... And do not do assignment in the condition block of if. Let's change to",-0.0425,-0.0425,negative
hbase,20975,description,"Find this one when investigating HBASE-20921, too. Here is some code from executeRollback in You can see my comments in the code above, reuseLock can cause the procedure executing(rollback) without a lock. Though I haven't found any bugs introduced by this issue, it is indeed a potential bug need to fix. I think we can just remove the reuseLock logic. Acquire and release lock every time. Find another case that during rolling back, the procedure's lock may not be released properly: see comment:",code_debt,multi-thread_correctness,"Mon, 30 Jul 2018 07:56:06 +0000","Tue, 14 Aug 2018 11:44:47 +0000","Mon, 13 Aug 2018 12:24:30 +0000",1225704,"Find this one when investigating HBASE-20921, too. Here is some code from executeRollback in ProcedureExecutor.java. You can see my comments in the code above, reuseLock can cause the procedure executing(rollback) without a lock. Though I haven't found any bugs introduced by this issue, it is indeed a potential bug need to fix. I think we can just remove the reuseLock logic. Acquire and release lock every time. Find another case that during rolling back, the procedure's lock may not be released properly: see comment: https://issues.apache.org/jira/browse/HBASE-20975?focusedCommentId=16565123&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16565123",-0.08333333333,-0.0625,neutral
hbase,21173,comment_1,I think the intention of HBASE-21138 is to let do the cleanup. Can you remove the duplicate region.close() call in these subtests ? Thanks,code_debt,duplicated_code,"Sat, 8 Sep 2018 14:28:39 +0000","Fri, 1 Feb 2019 20:12:31 +0000","Tue, 11 Sep 2018 09:52:38 +0000",242639,I think the intention of HBASE-21138 is to let HBaseTestingUtility.closeRegionAndWAL do the cleanup. Can you remove the duplicate region.close() call in these subtests ? Thanks,0.1,0.08,neutral
hbase,21173,comment_4,"Attach 002 patch as  suggestions.Thanks {{testSequenceId}} - In line 269, Replace region.close() with - In line 285, we need to verify that the value of is consistent before and after region.close(), so we keep region.close() and replace it with - In line 315, replace region.close() with - In line 317, remove duplicate and set this.region to null - In line 578, replace region.close() with and set this.region to null - In line 951, replace region.close() with - In line 1083, replace region.close() with - In line 1281, set this.region to null - In line 1281, set this.region to null - In line 4175, keep region.close() and set region to null as said by Mingliang Liu - In line 6234, remove region.close() Other places where set this.region null value after will be fine as said by .",code_debt,duplicated_code,"Sat, 8 Sep 2018 14:28:39 +0000","Fri, 1 Feb 2019 20:12:31 +0000","Tue, 11 Sep 2018 09:52:38 +0000",242639,"Attach 002 patch as yuzhihong@gmail.com liuml07 suggestions.Thanks testSequenceId In line 269, Replace region.close() with HBaseTestingUtility.closeRegionAndWAL(region) In line 285, we need to verify that the value of region.getMaxFlushedSeqId() is consistent before and after region.close(), so we keep region.close() and replace it with HBaseTestingUtility.closeRegionAndWAL(region). testCloseCarryingSnapshot In line 315, replace region.close() with HBaseTestingUtility.closeRegionAndWAL(region) In line 317, remove duplicate HBaseTestingUtility.closeRegionAndWAL(region) and set this.region to null testCloseWithFailingFlush In line 578, replace region.close() with HBaseTestingUtility.closeRegionAndWAL(region) and set this.region to null testRecoveredEditsReplayCompaction In line 951, replace region.close() with HBaseTestingUtility.closeRegionAndWAL(region) testFlushMarkers In line 1083, replace region.close() with HBaseTestingUtility.closeRegionAndWAL(region) testGetWhileRegionClose In line 1281, set this.region to null testBatchPut_whileMultipleRowLocksHeld In line 1281, set this.region to null testRegionInfoFileCreation In line 4175, keep region.close() and set region to null as said by Mingliang Liu testBulkLoadReplicationEnabled In line 6234, remove region.close() Other places to set this.region null value after HTU.closeRegionAndWAL() is good to explicitly make the HTU.closeRegionAndWAL() in tearDown a no-op. Other places where set this.region null value after HTU.closeRegionAndWAL(), will be fine as said by liuml07.",0.05,0.05253333333,neutral
hbase,21173,description,"After HBASE-21138, some test methods still have the duplicate HRegion#close.So open this issue to remove the duplicate close",code_debt,duplicated_code,"Sat, 8 Sep 2018 14:28:39 +0000","Fri, 1 Feb 2019 20:12:31 +0000","Tue, 11 Sep 2018 09:52:38 +0000",242639,"After HBASE-21138, some test methods still have the duplicate HRegion#close.So open this issue to remove the duplicate close",0.1405,0.1405,negative
hbase,21173,summary,Remove the duplicate HRegion#close in TestHRegion,code_debt,duplicated_code,"Sat, 8 Sep 2018 14:28:39 +0000","Fri, 1 Feb 2019 20:12:31 +0000","Tue, 11 Sep 2018 09:52:38 +0000",242639,Remove the duplicate HRegion#close in TestHRegion,0,0,neutral
hbase,21599,comment_3,Thanks Ankit! Let me rebase and fix the checkstyle warnings.,code_debt,low_quality_code,"Thu, 13 Dec 2018 22:05:35 +0000","Thu, 3 Jan 2019 18:09:05 +0000","Thu, 3 Jan 2019 18:09:05 +0000",1800210,Thanks Ankit! Let me rebase and fix the checkstyle warnings.,-0.1,-0.1,positive
hbase,21599,description,and Pretty trivial stuff to clean up now.,code_debt,low_quality_code,"Thu, 13 Dec 2018 22:05:35 +0000","Thu, 3 Jan 2019 18:09:05 +0000","Thu, 3 Jan 2019 18:09:05 +0000",1800210,and Pretty trivial stuff to clean up now.,0.3,0.3,negative
hbase,21599,summary,Fix findbugs and javadoc warnings from HBASE-21246,code_debt,low_quality_code,"Thu, 13 Dec 2018 22:05:35 +0000","Thu, 3 Jan 2019 18:09:05 +0000","Thu, 3 Jan 2019 18:09:05 +0000",1800210,Fix findbugs and javadoc warnings from HBASE-21246,-0.6,-0.6,neutral
hbase,21678,comment_1,Test failures are unrelated. The checkstyle and whitespace issues reported are related. I didn't change formatting during the backport but can do so to make these tools happier. Let me do that,code_debt,low_quality_code,"Fri, 4 Jan 2019 21:19:11 +0000","Tue, 22 Sep 2020 07:36:25 +0000","Thu, 24 Jan 2019 17:32:06 +0000",1714375,Test failures are unrelated. The checkstyle and whitespace issues reported are related. I didn't change formatting during the backport but can do so to make these tools happier. Let me do that,0.025,0.025,negative
hbase,21678,comment_2,Adjusted some formatting in StoreFile. Let's see if that is better,code_debt,low_quality_code,"Fri, 4 Jan 2019 21:19:11 +0000","Tue, 22 Sep 2020 07:36:25 +0000","Thu, 24 Jan 2019 17:32:06 +0000",1714375,Adjusted some formatting in StoreFile. Let's see if that is better,0.25,0.25,neutral
hbase,21678,comment_5,Updated patch address checkstyle warnings and unit test failure,code_debt,low_quality_code,"Fri, 4 Jan 2019 21:19:11 +0000","Tue, 22 Sep 2020 07:36:25 +0000","Thu, 24 Jan 2019 17:32:06 +0000",1714375,Updated patch address checkstyle warnings and unit test failure,-0.5,-0.5,negative
hbase,21678,comment_7,"Sigh, new checkstyle nits now that previous attempt is touching more files just to clean up checkstyle nits",code_debt,low_quality_code,"Fri, 4 Jan 2019 21:19:11 +0000","Tue, 22 Sep 2020 07:36:25 +0000","Thu, 24 Jan 2019 17:32:06 +0000",1714375,"Sigh, new checkstyle nits now that previous attempt is touching more files just to clean up checkstyle nits",0,0,negative
hbase,21678,comment_8,Otherwise change looks good. I can fix the remaining checkstyle warns on commit. Would be good to get a +1 if you have any spare time to make a quick check   or original author,code_debt,low_quality_code,"Fri, 4 Jan 2019 21:19:11 +0000","Tue, 22 Sep 2020 07:36:25 +0000","Thu, 24 Jan 2019 17:32:06 +0000",1714375,Otherwise change looks good. I can fix the remaining checkstyle warns on commit. Would be good to get a +1 if you have any spare time to make a quick check lhofhansl abhishek.chouhan or original author andrewcheng,0.484,0.363,positive
hbase,22017,comment_7,Removing the whitespace,code_debt,low_quality_code,"Fri, 8 Mar 2019 12:41:07 +0000","Wed, 2 Oct 2019 17:13:51 +0000","Wed, 13 Mar 2019 07:04:24 +0000",411797,Removing thewhitespace,0,0,neutral
hbase,22034,comment_3,"Great, I munged some files so now I own their checkstyle and javadoc problems. :-/",code_debt,low_quality_code,"Mon, 11 Mar 2019 19:15:33 +0000","Fri, 6 Sep 2019 00:32:11 +0000","Fri, 22 Mar 2019 00:28:40 +0000",882787,"Great, I munged some files so now I own their checkstyle and javadoc problems. :-/",-0.15,-0.15,negative
hbase,22034,comment_4,Updated patch for checkstyle and javadoc warnings,code_debt,low_quality_code,"Mon, 11 Mar 2019 19:15:33 +0000","Fri, 6 Sep 2019 00:32:11 +0000","Fri, 22 Mar 2019 00:28:40 +0000",882787,Updated patch for checkstyle and javadoc warnings,-0.6,-0.6,neutral
hbase,22193,description,"Now the default config is Integer.MAX_VALUE. The ITBLL failed to open the region as HBASE-22163 and retry 170813 to reopen. After I fixed the problem and restart master, I found it need take a long time to init the old procedure logs because there are too many old logs... Code in",code_debt,low_quality_code,"Tue, 9 Apr 2019 03:21:48 +0000","Tue, 30 Apr 2019 13:15:19 +0000","Sat, 13 Apr 2019 03:17:46 +0000",345358,"Now the default config isInteger.MAX_VALUE.  The ITBLL failed to open the region asHBASE-22163 and retry 170813 to reopen. After I fixed the problem and restart master, I found it need take a long time to init the old procedure logs because there are too many old logs... Code inWALProcedureStore,java.",-0.2125,-0.2125,negative
hbase,22203,description,The DemoClient.java currently uses a not consistent formatting and should be reformatted.,code_debt,low_quality_code,"Wed, 10 Apr 2019 15:09:06 +0000","Sun, 28 Apr 2019 00:52:16 +0000","Fri, 12 Apr 2019 07:49:08 +0000",146402,The DemoClient.java currently uses a not consistent formatting and should be reformatted.,0,0,negative
hbase,22228,description,ThrottlingException was deprecated in 2.0.0 and should be removed in 3.0.0.,code_debt,dead_code,"Fri, 12 Apr 2019 20:55:47 +0000","Sat, 13 Apr 2019 13:25:47 +0000","Sat, 13 Apr 2019 08:50:40 +0000",42893,ThrottlingException was deprecated in 2.0.0 and should be removed in 3.0.0.,0,0,negative
hbase,2241,description,"This is a quick workaround until we do a better balancer. Taking a region off line when cluster is under load is bad news. Latency goes up as we wait on regions to come up in new locations. The load balancer should only cut in if the cluster is way out of alignment. I'd argue that 10% deviance from the avg. is not good enough reason moving regions around when cluster is under load. Balancer already has a knack for cutting in at most inopportune moments: during cluster startup, when new node is added to a small cluster, or moving a region just after its been opened on a node. We'll need to do a better balancer but meantime lets just allow that region loading can be sloppier, say 20% or 30% off the average before balancer cuts in.",code_debt,slow_algorithm,"Fri, 19 Feb 2010 19:53:51 +0000","Fri, 12 Oct 2012 06:14:57 +0000","Fri, 19 Feb 2010 20:06:24 +0000",753,"This is a quick workaround until we do a better balancer. Taking a region off line when cluster is under load is bad news. Latency goes up as we wait on regions to come up in new locations. The load balancer should only cut in if the cluster is way out of alignment. I'd argue that 10% deviance from the avg. is not good enough reason moving regions around when cluster is under load. Balancer already has a knack for cutting in at most inopportune moments: during cluster startup, when new node is added to a small cluster, or moving a region just after its been opened on a node. We'll need to do a better balancer but meantime lets just allow that region loading can be sloppier, say 20% or 30% off the average before balancer cuts in.",-0.1010416667,-0.1010416667,negative
hbase,2247,comment_1,-1 on adding a new API that duplicates another but with one distinction. Make this a Scan option instead.,code_debt,duplicated_code,"Mon, 22 Feb 2010 23:02:02 +0000","Sat, 11 Jun 2022 23:07:17 +0000","Sun, 26 Jan 2014 20:04:33 +0000",123886951,-1 on adding a new API that duplicates another but with one distinction. Make this a Scan option instead.,0,0,neutral
hbase,2247,description,"hbase-2180 added pread whenever we do a get -- random-read -- and kept the old sync+position+read for when scanning. In between is the case of small scans. Small scans of 0-100 or so rows where the cells are small will likely fit a single hfile blocksize, especially if its the default 64k. We should recognize small scans and flip to random-read to satisfy (somehow). It'll up the performance a bit.",code_debt,slow_algorithm,"Mon, 22 Feb 2010 23:02:02 +0000","Sat, 11 Jun 2022 23:07:17 +0000","Sun, 26 Jan 2014 20:04:33 +0000",123886951,"hbase-2180 added pread whenever we do a get  random-read  and kept the old sync+position+read for when scanning. In between is the case of small scans. Small scans of 0-100 or so rows where the cells are small will likely fit a single hfile blocksize, especially if its the default 64k. We should recognize small scans and flip to random-read to satisfy (somehow). It'll up the performance a bit.",0.2048333333,0.2048333333,neutral
hbase,22656,comment_1,+1 (non-binding) Nice catch. The two method and are never used.,code_debt,dead_code,"Thu, 4 Jul 2019 03:44:46 +0000","Wed, 14 Aug 2019 02:28:55 +0000","Sun, 7 Jul 2019 15:35:11 +0000",301825,+1 (non-binding) Nice catch. The two method RegionServerTableMetrics::updatePutBatch() and RegionServerTableMetrics::updateDeleteBatch() are never used.,0.4125,0.4125,positive
hbase,22832,description,The method was deprecated in and should be removed for 3.0.0.,code_debt,dead_code,"Sat, 10 Aug 2019 21:43:03 +0000","Thu, 15 Aug 2019 08:12:53 +0000","Sun, 11 Aug 2019 19:43:14 +0000",79211,The method getHTableDescriptor was deprecated in HFileOutputFormat2 and should be removed for 3.0.0.,0,0,negative
hbase,22832,summary,Remove deprecated method in HFileOutputFormat2,code_debt,dead_code,"Sat, 10 Aug 2019 21:43:03 +0000","Thu, 15 Aug 2019 08:12:53 +0000","Sun, 11 Aug 2019 19:43:14 +0000",79211,Remove deprecated method in HFileOutputFormat2,0,0,neutral
hbase,22936,summary,Close memStoreScanners in else memory leak,code_debt,low_quality_code,"Wed, 28 Aug 2019 03:13:34 +0000","Tue, 7 Apr 2020 23:27:13 +0000","Wed, 2 Oct 2019 11:47:40 +0000",3054846,Close memStoreScanners in StoreScanner#updateReaders else memory leak,-0.2,-0.2,negative
hbase,2295,comment_10,I agree isEmpty is better than size... I'll just put fix in under this issue. Thanks Todd. I changed it to do this instead on trunk and branch:,code_debt,low_quality_code,"Fri, 5 Mar 2010 23:31:34 +0000","Fri, 20 Nov 2015 12:43:37 +0000","Tue, 9 Mar 2010 17:11:23 +0000",322789,I agree isEmpty is better than size... I'll just put fix in under this issue. Thanks Todd. I changed it to do this instead on trunk and branch:,0.25,0.25,positive
hbase,22981,comment_0,It is better to remove the unused flags instead of just ignoring those.,code_debt,low_quality_code,"Fri, 6 Sep 2019 11:24:47 +0000","Sat, 7 Sep 2019 02:29:23 +0000","Fri, 6 Sep 2019 13:03:41 +0000",5934,It is better to remove the unused flags instead of just ignoring those.,0.5,0.5,neutral
hbase,23087,description,The class is IA.Private and it has not been released yet so let's just remove this method to keep the class clean.,code_debt,dead_code,"Sat, 28 Sep 2019 11:44:32 +0000","Fri, 17 Jun 2022 18:42:49 +0000","Sun, 29 Sep 2019 01:30:29 +0000",49557,The class is IA.Private and it has not been released yet so let's just remove this method to keep the class clean.,0.2,0.2,neutral
hbase,23087,summary,Remove the deprecated bulkload method in,code_debt,dead_code,"Sat, 28 Sep 2019 11:44:32 +0000","Fri, 17 Jun 2022 18:42:49 +0000","Sun, 29 Sep 2019 01:30:29 +0000",49557,Remove the deprecated bulkload method in AsyncClusterConnection,0,0,neutral
hbase,23646,description,In {{hbase-rest}} Checkstyle reports a lot of violations. The remaining violations in the tests should be fixed.,code_debt,low_quality_code,"Sun, 5 Jan 2020 23:31:38 +0000","Wed, 11 Mar 2020 03:34:11 +0000","Mon, 20 Jan 2020 21:22:13 +0000",1288235,In hbase-rest Checkstyle reports a lot of violations. The remaining violations in the tests should be fixed.,-0.4,-0.4,negative
hbase,23646,summary,Fix remaining Checkstyle violations in tests of hbase-rest,code_debt,low_quality_code,"Sun, 5 Jan 2020 23:31:38 +0000","Wed, 11 Mar 2020 03:34:11 +0000","Mon, 20 Jan 2020 21:22:13 +0000",1288235,Fix remaining Checkstyle violations in tests of hbase-rest,-0.4,-0.4,neutral
hbase,239,comment_3,Committed w/ below message. Resolving. M bin/hbase Had a hard-coded name for the hbase jar. Fix so allows for version in jar name.,code_debt,low_quality_code,"Tue, 28 Aug 2007 21:56:48 +0000","Mon, 4 Feb 2008 18:41:16 +0000","Wed, 29 Aug 2007 04:31:51 +0000",23703,Committed w/ below message. Resolving. M bin/hbase Had a hard-coded name for the hbase jar. Fix so allows for version in jar name.,0.175,0.175,neutral
hbase,23,comment_16,"FYI, leave out the CHANGES.txt changes. Your patch will fail if someone has made a commit ahead of yours. I tried the patch. Looks really good. I love that we're now showing server names and thanks for renaming catalog tables section. Would suggest that no reason catalog tables shouldn't be clickable as user tables are. Usually will be one region only but even so, the new Table page shows where that region is located and gives a never-before available quick-path to the hosting region server. The Is Split column in the table will probably never be true especially when we are doing this: Would suggest you remove it (Sorry - -my fault for suggesting it in the first place) Would you mind fixing the requests calcuation? Its kinda weird at the moment. Its requests per (default 3 seconds). It should be showing requests per second. Thats what people expect. On the code, FYI, the hadoop convention is two-spaces for tabs. Regards your TODO, that you've duplicated code until we add MetaTable, thats fine. Was wondering if the .META. table was made of multiple regions and your scanner had to cross from one to the other, do you handle that case? Otherwise, the patch is great.",code_debt,duplicated_code,"Wed, 21 Nov 2007 23:00:46 +0000","Fri, 22 Aug 2008 21:13:04 +0000","Wed, 14 May 2008 19:08:23 +0000",15106057,"FYI, leave out the CHANGES.txt changes. Your patch will fail if someone has made a commit ahead of yours. I tried the patch. Looks really good. I love that we're now showing server names and thanks for renaming catalog tables section. Would suggest that no reason catalog tables shouldn't be clickable as user tables are. Usually will be one region only but even so, the new Table page shows where that region is located and gives a never-before available quick-path to the hosting region server. The Is Split column in the table will probably never be true especially when we are doing this: Would suggest you remove it (Sorry - -my fault for suggesting it in the first place) Would you mind fixing the requests calcuation? Its kinda weird at the moment. Its requests per hbase.regionsserver.msginterval (default 3 seconds). It should be showing requests per second. Thats what people expect. On the code, FYI, the hadoop convention is two-spaces for tabs. Regards your TODO, that you've duplicated code until we add MetaTable, thats fine. Was wondering if the .META. table was made of multiple regions and your scanner had to cross from one to the other, do you handle that case? Otherwise, the patch is great.",0.03313157895,0.02997619048,positive
hbase,23,comment_17,"It think so. The first loop scans over each region of .META. Regards the calculations, I searched in the whole source and it doesn't seem to be used, only referred to in the text so I think I will just remove it. Correct me if I'm wrong. So that means that they are clickable? I'm confused.",code_debt,dead_code,"Wed, 21 Nov 2007 23:00:46 +0000","Fri, 22 Aug 2008 21:13:04 +0000","Wed, 14 May 2008 19:08:23 +0000",15106057,"Was wondering if the .META. table was made of multiple regions and your scanner had to cross from one to the other, do you handle that case? It think so. The first loop scans over each region of .META. Regards the calculations, I searched ""hbase.regionserver.msginterval"" in the whole source and it doesn't seem to be used, only referred to in the text so I think I will just remove it. Correct me if I'm wrong. Would suggest that no reason catalog tables shouldn't be clickable as user tables are. So that means that they are clickable? I'm confused.",-0.0125,-0.006730769231,neutral
hbase,23,comment_18,"Review please. Things that changed : + Request load is now expressed in req/sec. + ROOT and META tables are clickable. The fact that these tables are not handled like user tables internally impacts the way I could show information. Suggestions appreciated. + Small indentation change. - Removed ""is split"" information on regions.",code_debt,low_quality_code,"Wed, 21 Nov 2007 23:00:46 +0000","Fri, 22 Aug 2008 21:13:04 +0000","Wed, 14 May 2008 19:08:23 +0000",15106057,"Review please. Things that changed : + Request load is now expressed in req/sec. + ROOT and META tables are clickable. The fact that these tables are not handled like user tables internally impacts the way I could show information. Suggestions appreciated. + Small indentation change. Removed ""is split"" information on regions.",0.1,0.08571428571,neutral
hbase,23,description,"Currently, looking at a cluster under load, you'll often trip over the disorientating listing of more than one region with a null end key (Was seen by Billy yesterday and psaab today). UI should list out all of its attributes. Also sort region listings by server address so easier finding servers.",code_debt,low_quality_code,"Wed, 21 Nov 2007 23:00:46 +0000","Fri, 22 Aug 2008 21:13:04 +0000","Wed, 14 May 2008 19:08:23 +0000",15106057,"Currently, looking at a cluster under load, you'll often trip over the disorientating listing of more than one region with a null end key (Was seen by Billy yesterday and psaab today). UI should list out all of its attributes. Also sort region listings by server address so easier finding servers.",0.1666666667,0.1666666667,negative
hbase,2555,comment_0,Patch removes constant and two unused HCD methods which use constant. HBase does not support anything but HFiles right now.,code_debt,dead_code,"Sun, 16 May 2010 21:36:30 +0000","Fri, 20 Nov 2015 12:42:41 +0000","Sun, 16 May 2010 22:47:17 +0000",4247,Patch removes constant and two unused HCD methods which use constant. HBase does not support anything but HFiles right now.,0.03175,0.03175,negative
hbase,2555,description,"Now that we have HFile, it seems that the constant is no longer useful. I've grepped around the codebase and couldn't find it used elsewhere. Perhaps kept around in case folks decide to store their HBase data into a MapFile?",code_debt,dead_code,"Sun, 16 May 2010 21:36:30 +0000","Fri, 20 Nov 2015 12:42:41 +0000","Sun, 16 May 2010 22:47:17 +0000",4247,"Now that we have HFile, it seems that the constant http://hadoop.apache.org/hbase/docs/r0.20.4/api/org/apache/hadoop/hbase/HColumnDescriptor.html#MAPFILE_INDEX_INTERVAL is no longer useful. I've grepped around the codebase and couldn't find it used elsewhere. Perhaps kept around in case folks decide to store their HBase data into a MapFile?",-0.1666666667,-0.1666666667,negative
hbase,2585,comment_2,"At a minimum, lets fix the dumb NPE and throw a better exception. Looking at code, it looks like the RS will just retry later to report the split (See ~#544 in HRegionServer).",code_debt,low_quality_code,"Thu, 20 May 2010 15:20:38 +0000","Sat, 11 Jun 2022 23:34:32 +0000","Wed, 16 Jul 2014 20:50:42 +0000",131175004,"At a minimum, lets fix the dumb NPE and throw a better exception. Looking at code, it looks like the RS will just retry later to report the split (See ~#544 in HRegionServer).",-0.025,-0.025,negative
hbase,2694,comment_6,"Final patch for commit. Includes changes from Todd's comments, added a few licenses, and also fixed mixed newlines on final patch file.",code_debt,low_quality_code,"Tue, 8 Jun 2010 18:39:57 +0000","Fri, 20 Nov 2015 12:43:18 +0000","Sat, 12 Jun 2010 01:20:41 +0000",283244,"Final patch for commit. Includes changes from Todd's comments, added a few licenses, and also fixed mixed newlines on final patch file.",0.1,0.1,neutral
hbase,274,comment_12,+1 This should make my HBase cache patch much more efficient.,code_debt,slow_algorithm,"Sat, 5 Jan 2008 01:51:53 +0000","Thu, 2 May 2013 02:29:12 +0000","Tue, 15 Jan 2008 10:16:24 +0000",894271,+1 This should make my HBase cache patch much more efficient.,0,0,positive
hbase,2925,comment_10,You may have left out a couple of print statements in TestHCM. Not sure if that was intentional. Otherwise it's good to go.,code_debt,low_quality_code,"Tue, 17 Aug 2010 21:27:52 +0000","Fri, 20 Nov 2015 12:42:13 +0000","Mon, 6 Sep 2010 15:52:27 +0000",1707875,You may have left out a couple of print statements in TestHCM. Not sure if that was intentional. Otherwise it's good to go.,0,0,neutral
hbase,3057,description,"In we spin up cluster, create three tables, shut it down, start it back up, and ensure we still have three regions. A subtle race condition during the first shutdown makes it so the flush of META doesn't finish so when we start back up there are no user regions. I'm not sure if there are reasons the ordering is as such, but the is the section of code in CloseRegionHandler around line 118: We remove from the online map of regions before actually closing. But what the main run() loop in the RS is waiting on to determine when it can shut down is that the online region map is empty. Any reason not to swap these two and do the close before removing from online regions?",code_debt,multi-thread_correctness,"Thu, 30 Sep 2010 18:44:50 +0000","Fri, 20 Nov 2015 12:43:07 +0000","Thu, 30 Sep 2010 18:54:09 +0000",559,"In TestRestartCluster.testClusterRestart() we spin up cluster, create three tables, shut it down, start it back up, and ensure we still have three regions. A subtle race condition during the first shutdown makes it so the flush of META doesn't finish so when we start back up there are no user regions. I'm not sure if there are reasons the ordering is as such, but the is the section of code in CloseRegionHandler around line 118: We remove from the online map of regions before actually closing. But what the main run() loop in the RS is waiting on to determine when it can shut down is that the online region map is empty. Any reason not to swap these two and do the close before removing from online regions?",0.01313333333,0.01094444444,neutral
hbase,3257,comment_1,"Patch was put to review board, but it's not forwarded to jira. Here is the review request: Summary: Coprocessors: Extend server side integration API to include HLog operations Coprocessor based extensions should be able to: - Observe, rewrite, or skip WALEdits as they are being written to the WAL - Write arbitrary content into WALEdits - Act on contents of WALEdits in the regionserver context during reconstruction Code changes: - a new coprocessor interface WALCPObserver is added which provides preWALWrite() and postWALWrite() upcalls to HLog, before and after at doWrite(). - added 2 new upcalls for RegionObserver for WAL replay, preWALRestore() and postWALRestore(). - a sample implementation -- -- was create which can add, remove, modify WALEdit before writing it to WAL. - test cases which use to test WAL write and replay. - added coprocessor loading at TestHLog to make sure it doesn't affect HLog. I need feedback for: - The new cp interface name -- WALCPObserver -- is not perfect. The ideal name is WALObserver but it's been used already. Other options include HLogObserver(H is not preferred), - No support for monitor master log splitting. I don't have a use case to support the requirement right now.",code_debt,low_quality_code,"Sun, 21 Nov 2010 19:43:23 +0000","Fri, 20 Nov 2015 12:42:27 +0000","Mon, 7 Feb 2011 23:01:50 +0000",6751107,"Patch was put to review board, but it's not forwarded to jira. Here is the review request: https://review.cloudera.org/r/1515/ Summary: Coprocessors: Extend server side integration API to include HLog operations Coprocessor based extensions should be able to: Observe, rewrite, or skip WALEdits as they are being written to the WAL Write arbitrary content into WALEdits Act on contents of WALEdits in the regionserver context during reconstruction Code changes: a new coprocessor interface WALCPObserver is added which provides preWALWrite() and postWALWrite() upcalls to HLog, before and after HLog.write.append(), at doWrite(). added 2 new upcalls for RegionObserver for WAL replay, preWALRestore() and postWALRestore(). a sample implementation  SampleRegionWALObserver  was create which can add, remove, modify WALEdit before writing it to WAL. test cases which use SampleRegionWALObserver to test WAL write and replay. added coprocessor loading at TestHLog to make sure it doesn't affect HLog. I need feedback for: The new cp interface name  WALCPObserver  is not perfect. The ideal name is WALObserver but it's been used already. Other options include HLogObserver(H is not preferred), LogObserver(confusion). No support for monitor master log splitting. I don't have a use case to support the requirement right now.",-0.1596944444,-0.1108846154,neutral
hbase,325,summary,[hbase] Compaction cleanup; less deleting + prevent possible file leaks,code_debt,low_quality_code,"Wed, 5 Dec 2007 16:02:33 +0000","Fri, 22 Aug 2008 21:34:59 +0000","Wed, 5 Dec 2007 16:06:46 +0000",253,[hbase] Compaction cleanup; less deleting + prevent possible file leaks,-0.2,-0.2,neutral
hbase,3571,description,"This lock in CompactSplitThread doesn't do anything best as I can tell: If two instances of CompactSplitThread, it would not prevent the two threads contending since its local to the instance. Remove it.",code_debt,dead_code,"Fri, 25 Feb 2011 20:25:53 +0000","Sun, 12 Jun 2022 18:12:04 +0000","Mon, 20 Jun 2011 16:39:06 +0000",9922393,"This lock in CompactSplitThread doesn't do anything best as I can tell: If two instances of CompactSplitThread, it would not prevent the two threads contending since its local to the instance. Remove it.",-0.1458333333,-0.1458333333,negative
hbase,3597,comment_4,Looks fine. Aren't all those expensive?,code_debt,slow_algorithm,"Thu, 3 Mar 2011 22:03:11 +0000","Fri, 20 Nov 2015 12:43:45 +0000","Wed, 4 May 2011 00:21:30 +0000",5278699,Looks fine. Aren't all those System.currentTimeMillis() expensive?,0.2,0.1333333333,positive
hbase,3597,comment_5,"That would be called on average once or twice per second on a normal cluster, I'm pretty sure is a few order of magnitudes more expensive than what those metrics are doing.",code_debt,slow_algorithm,"Thu, 3 Mar 2011 22:03:11 +0000","Fri, 20 Nov 2015 12:43:45 +0000","Wed, 4 May 2011 00:21:30 +0000",5278699,"That would be called on average once or twice per second on a normal cluster, I'm pretty sure DefaultEnvironmentEdge is a few order of magnitudes more expensive than what those metrics are doing.",0.2,0.2,negative
hbase,3673,comment_1,"The patch did seem to make a difference in our particular use case, in terms of the average time it took to get a htable from the pool. For the sake of a more formal evaluation, I put together a benchmark (see attached test case), which did show noticeable difference. Specifically, when you've a htable pool of size 150, and a worker pool of 100 threads, where each thread does a get followed by a put a million times, the total time spent was 7775614 ms with the patch, as opposed to 12654820 ms without. That's turns out to be a 40% improvement. That said, using different htable pools for different use cases (e.g., different htables) might be the way to go, as that will tend to reduce the level of concurrency on any given htable pool.",code_debt,slow_algorithm,"Fri, 18 Mar 2011 22:20:04 +0000","Fri, 20 Nov 2015 12:40:39 +0000","Thu, 24 Mar 2011 22:28:49 +0000",518925,"The patch did seem to make a difference in our particular use case, in terms of the average time it took to get a htable from the pool. For the sake of a more formal evaluation, I put together a benchmark (see attached test case), which did show noticeable difference. Specifically, when you've a htable pool of size 150, and a worker pool of 100 threads, where each thread does a get followed by a put a million times, the total time spent was 7775614 ms with the patch, as opposed to 12654820 ms without. That's turns out to be a 40% improvement. That said, using different htable pools for different use cases (e.g., different htables) might be the way to go, as that will tend to reduce the level of concurrency on any given htable pool.",0.4084,0.4084,neutral
hbase,369,description,"The client api (as defined by HTable and HBaseAdmin are not insignificantly different from HRegionInterface (wire protocol). They could be made much more similar by implementing a couple of new Writable classes (e.g., SortedMapWritable).",code_debt,low_quality_code,"Sat, 11 Aug 2007 23:19:34 +0000","Mon, 4 Feb 2008 18:41:56 +0000","Thu, 16 Aug 2007 22:53:30 +0000",430436,"The client api (as defined by HTable and HBaseAdmin are not insignificantly different from HRegionInterface (wire protocol). They could be made much more similar by implementing a couple of new Writable classes (e.g., SortedMapWritable).",0.25,0.25,neutral
hbase,3807,comment_5,Removing the unused code for determining time elapsed in RegionServerMetrics : int seconds = - if (seconds == 0) { seconds = 1; } Will upload the patch ASAP,code_debt,dead_code,"Thu, 21 Apr 2011 06:28:51 +0000","Fri, 20 Nov 2015 12:43:21 +0000","Wed, 10 Aug 2011 19:43:28 +0000",9638077,Removing the unused code for determining time elapsed in RegionServerMetrics : int seconds = (int)((System.currentTimeMillis() - this.lastUpdate)/1000); if (seconds == 0) { seconds = 1; } Will upload the patch ASAP,0,0,neutral
hbase,4088,comment_1,Small fix to logging message -- check for null before getting list size.,code_debt,low_quality_code,"Tue, 12 Jul 2011 17:41:15 +0000","Fri, 20 Nov 2015 11:53:16 +0000","Wed, 13 Jul 2011 04:20:05 +0000",38330,Small fix to logging message  check for null before getting list size.,0,0,neutral
hbase,4247,comment_1,"On 1., agree. I agree on 2. too. On your 'Secondly', yes. That seems dirty. Agree on calling stop from abort (Doesn't it do this in a few places? IIRC).",code_debt,low_quality_code,"Wed, 24 Aug 2011 02:28:51 +0000","Fri, 20 Nov 2015 11:53:21 +0000","Sat, 17 Sep 2011 18:26:27 +0000",2131056,"On 1., agree. I agree on 2. too. On your 'Secondly', yes. That seems dirty. Agree on calling stop from abort (Doesn't it do this in a few places? IIRC).",0.02857142857,0.02857142857,neutral
hbase,4247,comment_5,Plz excuse me about the formatting. I didn't know the Markup tags the codes should be placed under,code_debt,low_quality_code,"Wed, 24 Aug 2011 02:28:51 +0000","Fri, 20 Nov 2015 11:53:21 +0000","Sat, 17 Sep 2011 18:26:27 +0000",2131056,Plz excuse me about the formatting. I didn't know the Markup tags the codes should be placed under,0,0,negative
hbase,4250,description,"* Did some restructuring of sections * Added section for common patch feedback (based on my own experiences in a recent patch). ** I ran these past Todd & JD at this week's hackathon. * mentioning ReviewBoard in patch submission process ** this didn't appear anywhere before. I still need to add more on tips on using it, but this is a start.",code_debt,low_quality_code,"Wed, 24 Aug 2011 21:09:06 +0000","Sun, 12 Jun 2022 19:18:23 +0000","Wed, 24 Aug 2011 21:11:14 +0000",128,"Did some restructuring of sections Added section for common patch feedback (based on my own experiences in a recent patch). I ran these past Todd & JD at this week's hackathon. mentioning ReviewBoard in patch submission process this didn't appear anywhere before. I still need to add more on tips on using it, but this is a start.",0.07025,0.07025,neutral
hbase,4250,summary,"developer.xml - restructuring, plus adding common patch feedback",code_debt,low_quality_code,"Wed, 24 Aug 2011 21:09:06 +0000","Sun, 12 Jun 2022 19:18:23 +0000","Wed, 24 Aug 2011 21:11:14 +0000",128,"developer.xml - restructuring, plus adding common patch feedback",0.344,0.344,neutral
hbase,4303,description,"Currently it's outputting: REGION = Notice the missing quotes around tableName, etc.",code_debt,low_quality_code,"Tue, 30 Aug 2011 19:59:19 +0000","Fri, 20 Nov 2015 11:52:24 +0000","Tue, 30 Aug 2011 21:57:41 +0000",7102,"Currently it's outputting: REGION => {NAME => '.META.,,1 TableName => ', STARTKEY => '', ENDKEY => '', ENCODED => 1028785192,} Notice the missing quotes around tableName, etc.",-0.4,-0.2,neutral
hbase,4303,summary,has bad quoting,code_debt,low_quality_code,"Tue, 30 Aug 2011 19:59:19 +0000","Fri, 20 Nov 2015 11:52:24 +0000","Tue, 30 Aug 2011 21:57:41 +0000",7102,HRegionInfo.toString has bad quoting,-0.6,-0.3,negative
hbase,4311,comment_2,"Thanks for the pointer, I'll update table creation in HBASE-4313's refactor of hbck tests. The existing version and the current dev version of the hbck tests does a somewhat hacky cleanup to make the table clean before messing it up again.",code_debt,low_quality_code,"Wed, 31 Aug 2011 16:31:47 +0000","Sun, 12 Jun 2022 19:20:18 +0000","Thu, 12 Apr 2012 16:49:37 +0000",19441070,"Thanks for the pointer, I'll update table creation in HBASE-4313's refactor of hbck tests. The existing version and the current dev version of the hbck tests does a somewhat hacky cleanup to make the table clean before messing it up again.",0.4,0.4,neutral
hbase,4459,comment_7,- Why is Queue added within the scope of this JIRA? Seems unrelated. - Can you remove the unnecessary import re-org at the top? - Can we have a unit test which shows the backwards compatibility of this? Thanks for working on this Ram.,code_debt,low_quality_code,"Thu, 22 Sep 2011 18:46:58 +0000","Fri, 20 Nov 2015 11:55:50 +0000","Thu, 20 Oct 2011 18:23:23 +0000",2417785,Why is Queue added within the scope of this JIRA? Seems unrelated. Can you remove the unnecessary import re-org at the top? Can we have a unit test which shows the backwards compatibility of this? Thanks for working on this Ram.,0.225,0.18,neutral
hbase,467,comment_3,"On second thought, I am going to put HBASE-467 on the back burner until HBASE-469 is completed. Since the original patch was based on unrefactored code, the changes will be easier to apply now than later.",code_debt,low_quality_code,"Sun, 24 Feb 2008 05:57:43 +0000","Fri, 22 Aug 2008 21:35:04 +0000","Sun, 24 Feb 2008 22:10:11 +0000",58348,"On second thought, I am going to put HBASE-467 on the back burner until HBASE-469 is completed. Since the original patch was based on unrefactored code, the changes will be easier to apply now than later.",0,0,neutral
hbase,467,comment_6,There is just too much intermodule coupling to do this properly without breaking encapsulation.,code_debt,low_quality_code,"Sun, 24 Feb 2008 05:57:43 +0000","Fri, 22 Aug 2008 21:35:04 +0000","Sun, 24 Feb 2008 22:10:11 +0000",58348,There is just too much intermodule coupling to do this properly without breaking encapsulation.,-0.5,-0.5,negative
hbase,4740,comment_6,"@Stack Yeah, 0 is actually the original behavior in the pre-HBASE-4552 version it I think would just eat exceptions and bail out without completing. It is more complicated because of bulk atomicity. Will update boolean if it works -- there is some template checking in another place so assumed it needed boxed type. The difference is that the version uses a different instance. I'll refactor to exclude that portion and require it in the test. I tried the previous version with a small data set on psuedo-dist cluster and live cluster. For this particular patch I tried this one by looping the relevant unit tests 100 times and seeing that they passed all the time. I haven't tested this exact version on real cluster.",code_debt,low_quality_code,"Thu, 3 Nov 2011 17:58:14 +0000","Fri, 20 Nov 2015 11:53:11 +0000","Tue, 8 Nov 2011 14:40:38 +0000",420144,"@Stack Yeah, 0 is actually the original behavior in the pre-HBASE-4552 version it I think would just eat exceptions and bail out without completing. It is more complicated because of bulk atomicity. Will update boolean if it works  there is some template checking in another place so assumed it needed boxed type. The difference is that the version uses a different LoadIncrementalHandlers instance. I'll refactor to exclude that portion and require it in the test. I tried the previous version with a small data set on psuedo-dist cluster and live cluster. For this particular patch I tried this one by looping the relevant unit tests 100 times and seeing that they passed all the time. I haven't tested this exact version on real cluster.",-0.1235,-0.1235,neutral
hbase,4778,comment_3,"@Ted: In this particular case, I think we ended up finding that HDFS wasn't reading truncated HFiles properly. Overall though, the idea is to prioritize data integrity & consistency over availability. We shouldn't be silently opening regions with missing data. We should instead understand why the data is missing. If someone wants to add a flag and allow this to happen, then that's fine.",code_debt,low_quality_code,"Sat, 12 Nov 2011 02:57:05 +0000","Fri, 12 Oct 2012 05:34:56 +0000","Mon, 14 Nov 2011 22:29:43 +0000",243158,"@Ted: In this particular case, I think we ended up finding that HDFS wasn't reading truncated HFiles properly. Overall though, the idea is to prioritize data integrity & consistency over availability. We shouldn't be silently opening regions with missing data. We should instead understand why the data is missing. If someone wants to add a 'data.loss.acceptable' flag and allow this to happen, then that's fine.",0.14,0.03123809524,negative
hbase,4859,description,"See description at HBASE-3553. We had a patch ready for this in HBASE-3620 but never applied it publicly. Testing showed massive speedup in HBCK, especially when RegionServers were down or had long response times.",code_debt,slow_algorithm,"Wed, 23 Nov 2011 23:37:21 +0000","Fri, 20 Nov 2015 11:52:31 +0000","Sat, 10 Dec 2011 00:40:24 +0000",1386183,"See description at HBASE-3553. We had a patch ready for this in HBASE-3620 but never applied it publicly. Testing showed massive speedup in HBCK, especially when RegionServers were down or had long response times.",0.07766666667,0.07766666667,neutral
hbase,4911,summary,Clean shutdown,code_debt,low_quality_code,"Wed, 30 Nov 2011 22:51:21 +0000","Fri, 12 Oct 2012 05:35:09 +0000","Thu, 1 Dec 2011 00:09:24 +0000",4683,Clean shutdown,0.4,0.4,neutral
hbase,5039,description,book.xml * Arch chapter/Regions. clearing up a little more in region assignment * FAQ. Adding an architecture section. * MapReduce chapter. Fixed nit that Ian Varley brought to my attention on RDBMS summary.,code_debt,low_quality_code,"Thu, 15 Dec 2011 15:14:55 +0000","Sun, 12 Jun 2022 19:57:55 +0000","Thu, 15 Dec 2011 15:17:26 +0000",151,book.xml Arch chapter/Regions. clearing up a little more in region assignment FAQ. Adding an architecture section. MapReduce chapter. Fixed nit that Ian Varley brought to my attention on RDBMS summary.,0,0,neutral
hbase,5039,summary,"[book] book.xml - more cleanup of Arch chapter on regions, adding a FAQ entry",code_debt,low_quality_code,"Thu, 15 Dec 2011 15:14:55 +0000","Sun, 12 Jun 2022 19:57:55 +0000","Thu, 15 Dec 2011 15:17:26 +0000",151,"[book] book.xml - more cleanup of Arch chapter on regions, adding a FAQ entry",0,0,neutral
hbase,5110,comment_2,"Ah, I missed that thread... I just wanted to clarify if this is for readability or performance... do you see this function getting called a lot in a write workload? Your comments on the mailing list thread indicate that it's performance sensitive, but I don't see how that would be the case.",code_debt,slow_algorithm,"Fri, 30 Dec 2011 19:01:01 +0000","Sun, 12 Jun 2022 20:02:21 +0000","Tue, 21 Apr 2015 00:50:03 +0000",104305742,"Ah, I missed that thread... I just wanted to clarify if this is for readability or performance... do you see this function getting called a lot in a write workload? Your comments on the mailing list thread indicate that it's performance sensitive, but I don't see how that would be the case.",-0.2,-0.2,neutral
hbase,5110,comment_3,"I see it a lot in the heavy write scenario when major compaction occurs in the background, but to be realistic even when i see this method called 2000 times during a test of 5 hours, across a cluster of 10 RS (each RS log contains +/-200 calls of this method), i don't think this method present a performance problem. So for my point of view this is more readability issue before it becomes a performance problem. It is strange to me to see code asking each time in an iteration for object existence especially if creating the object is not heavy task.",code_debt,low_quality_code,"Fri, 30 Dec 2011 19:01:01 +0000","Sun, 12 Jun 2022 20:02:21 +0000","Tue, 21 Apr 2015 00:50:03 +0000",104305742,"I see it a lot in the heavy write scenario when major compaction occurs in the background, but to be realistic even when i see this method called 2000 times during a test of 5 hours, across a cluster of 10 RS (each RS log contains +/-200 calls of this method), i don't think this method present a performance problem. So for my point of view this is more readability issue before it becomes a performance problem. It is strange to me to see code asking each time in an iteration for object existence especially if creating the object is not heavy task.",-0.1471111111,-0.1471111111,neutral
hbase,5110,description,"The HLog class (method has unnecessary if check in a loop. static byte [][] long oldestWALseqid, final Map<byte [], Long // This method is static so it can be unit tested the easier. List<byte [] for (Map.Entry<byte [], Long if <= oldestWALseqid) { if (regions == null) regions = new ArrayList<byte [] } } return regions == null? null: regions.toArray(new byte [][] } The following change is suggested static byte [][] long oldestWALseqid, final Map<byte [], Long // This method is static so it can be unit tested the easier. List<byte [] for (Map.Entry<byte [], Long if <= oldestWALseqid) { } } return regions.size() == 0? null: regions.toArray(new byte [][] }",code_debt,low_quality_code,"Fri, 30 Dec 2011 19:01:01 +0000","Sun, 12 Jun 2022 20:02:21 +0000","Tue, 21 Apr 2015 00:50:03 +0000",104305742,"The HLog class (method findMemstoresWithEditsEqualOrOlderThan) has unnecessary if check in a loop. static byte [][] findMemstoresWithEditsEqualOrOlderThan(final long oldestWALseqid, final Map<byte [], Long> regionsToSeqids) { // This method is static so it can be unit tested the easier. List<byte []> regions = null; for (Map.Entry<byte [], Long> e: regionsToSeqids.entrySet()) { if (e.getValue().longValue() <= oldestWALseqid) { if (regions == null) regions = new ArrayList<byte []>(); regions.add(e.getKey()); } } return regions == null? null: regions.toArray(new byte [][] {HConstants.EMPTY_BYTE_ARRAY}); } The following change is suggested static byte [][] findMemstoresWithEditsEqualOrOlderThan(final long oldestWALseqid, final Map<byte [], Long> regionsToSeqids) { // This method is static so it can be unit tested the easier. List<byte []> regions = new ArrayList<byte []>(); for (Map.Entry<byte [], Long> e: regionsToSeqids.entrySet()) { if (e.getValue().longValue() <= oldestWALseqid) { regions.add(e.getKey()); } } return regions.size() == 0? null: regions.toArray(new byte [][] {HConstants.EMPTY_BYTE_ARRAY} ); }",-0.07193939394,-0.04960087719,neutral
hbase,5110,summary,code enhancement - remove unnecessary if-checks in every loop in HLog class,code_debt,complex_code,"Fri, 30 Dec 2011 19:01:01 +0000","Sun, 12 Jun 2022 20:02:21 +0000","Tue, 21 Apr 2015 00:50:03 +0000",104305742,code enhancement - remove unnecessary if-checks in every loop in HLog class,0.5,0.5,neutral
hbase,5210,comment_3,"Any fix in getRandomFilename will just reduce the chance of file name collision. Since this a rare case, I think it may be better to just fail the task if failed to commit the files in the moveTaskOutputs(), without overwriting the existing files. In HDFS 0.23, rename() takes an option not to overwrite. With HADOOP 0.20, we can just do our best to check any conflicts before committing the files.",code_debt,low_quality_code,"Mon, 16 Jan 2012 19:03:42 +0000","Sun, 12 Jun 2022 20:10:10 +0000","Sat, 18 Jul 2015 10:15:59 +0000",110473937,"Any fix in getRandomFilename will just reduce the chance of file name collision. Since this a rare case, I think it may be better to just fail the task if failed to commit the files in the moveTaskOutputs(), without overwriting the existing files. In HDFS 0.23, rename() takes an option not to overwrite. With HADOOP 0.20, we can just do our best to check any conflicts before committing the files.",0.025,0.025,neutral
hbase,5217,comment_12,@Alex Is this dependent on another issue being committed first? In doTestGetRegionInfo the tabs are wrong. Should be two spaces like the rest of the file. With your changes are we going to start a cluster each time (Was the testAll method trying to avoid our making a cluster each time)?,code_debt,low_quality_code,"Tue, 17 Jan 2012 19:17:36 +0000","Sun, 12 Jun 2022 20:10:28 +0000","Wed, 22 Apr 2015 00:33:57 +0000",102834981,@Alex Is this dependent on another issue being committed first? In doTestGetRegionInfo the tabs are wrong. Should be two spaces like the rest of the file. With your changes are we going to start a cluster each time (Was the testAll method trying to avoid our making a cluster each time)?,-0.025,-0.025,negative
hbase,5282,comment_0,"When debugging, open region file was attempting to open either a truncated or 0 size hlogfile (which is throws IOException at out from getReader), and leaking a handle on every open attempt. Patch applies on 0.92 and trunk.",code_debt,low_quality_code,"Thu, 26 Jan 2012 13:05:35 +0000","Fri, 12 Oct 2012 05:35:00 +0000","Thu, 26 Jan 2012 23:50:18 +0000",38683,"When debugging, open region file was attempting to open either a truncated or 0 size hlogfile (which is throws IOException at out from getReader), and leaking a handle on every open attempt. Patch applies on 0.92 and trunk.",0,0,neutral
hbase,5282,description,"When debugging hbck, found that the code responsible for this exception can leak open file handles.",code_debt,low_quality_code,"Thu, 26 Jan 2012 13:05:35 +0000","Fri, 12 Oct 2012 05:35:00 +0000","Thu, 26 Jan 2012 23:50:18 +0000",38683,"When debugging hbck, found that the code responsible for this exception can leak open file handles.",0.1,0.1,negative
hbase,5282,summary,Possible file handle leak with truncated HLog file.,code_debt,low_quality_code,"Thu, 26 Jan 2012 13:05:35 +0000","Fri, 12 Oct 2012 05:35:00 +0000","Thu, 26 Jan 2012 23:50:18 +0000",38683,Possible file handle leak with truncated HLog file.,-0.2,-0.2,negative
hbase,5329,comment_8,Thanks for activating this JIRA. Indentation is off for the above block. I prefer the old way of keeping long lockId so that the above parsing can be omitted.,code_debt,low_quality_code,"Fri, 3 Feb 2012 07:49:47 +0000","Mon, 23 Sep 2013 18:31:39 +0000","Fri, 28 Sep 2012 03:45:25 +0000",20548538,Thanks for activating this JIRA. Indentation is off for the above block. I prefer the old way of keeping long lockId so that the above parsing can be omitted.,0.06666666667,0.06666666667,neutral
hbase,5466,comment_1,Thanks for the finding. We use two spaces for indentation. Can you regenerate patch ? Refer to HBASE-3678.,code_debt,low_quality_code,"Thu, 23 Feb 2012 21:03:09 +0000","Fri, 12 Oct 2012 05:35:03 +0000","Fri, 24 Feb 2012 01:00:23 +0000",14234,Thanks for the finding. We use two spaces for indentation. Can you regenerate patch ? Refer to HBASE-3678.,0.1,0.1,neutral
hbase,5466,comment_5,+1 on patch (except for the spacing that is not like the rest of the file),code_debt,low_quality_code,"Thu, 23 Feb 2012 21:03:09 +0000","Fri, 12 Oct 2012 05:35:03 +0000","Fri, 24 Feb 2012 01:00:23 +0000",14234,+1 on patch (except for the spacing that is not like the rest of the file),-0.4,-0.4,neutral
hbase,5466,comment_6,"TestZooKeeper passed locally with patch v2. There should be a space between } and finally, finally and {, if and (, ) and { Overall, +1 on patch v2. Please fix formatting in v3.",code_debt,low_quality_code,"Thu, 23 Feb 2012 21:03:09 +0000","Fri, 12 Oct 2012 05:35:03 +0000","Fri, 24 Feb 2012 01:00:23 +0000",14234,"TestZooKeeper passed locally with patch v2. There should be a space between } and finally, finally and {, if and (, ) and { Overall, +1 on patch v2. Please fix formatting in v3.",0.06666666667,0.06666666667,negative
hbase,5466,description,"Having upgraded to CDH3U3 version of hbase we found we had a zookeeper connection leak, tracking it down we found that closing the connection will only close the zookeeper connection if all calls to get the connection have been closed, there is incCount and decCount in the HConnection class, When a table is opened it makes a call to the metascanner class which opens a connection to the meta table, this table never gets closed. This caused the count in the HConnection class to never return to zero meaning that the zookeeper connection will not close when we close all the tables or calling true);",code_debt,low_quality_code,"Thu, 23 Feb 2012 21:03:09 +0000","Fri, 12 Oct 2012 05:35:03 +0000","Fri, 24 Feb 2012 01:00:23 +0000",14234,"Having upgraded to CDH3U3 version of hbase we found we had a zookeeper connection leak, tracking it down we found that closing the connection will only close the zookeeper connection if all calls to get the connection have been closed, there is incCount and decCount in the HConnection class, When a table is opened it makes a call to the metascanner class which opens a connection to the meta table, this table never gets closed. This caused the count in the HConnection class to never return to zero meaning that the zookeeper connection will not close when we close all the tables or calling HConnectionManager.deleteConnection(config, true);",-0.4225,-0.09933333333,neutral
hbase,5591,comment_0,"sc requested code review of ""HBASE-5591 [jira] is identical to Bytes.getBytes()"". Reviewers: tedyu, dhruba, JIRA is identical to Bytes.getBytes() Remove the redundant method. Task ID: # Blame Rev: TEST PLAN Revert Plan: Tags: REVISION DETAIL AFFECTED FILES MANAGE HERALD DIFFERENTIAL RULES WHY DID I GET THIS EMAIL? Tip: use the X-Herald-Rules header to filter Herald messages in your client.",code_debt,duplicated_code,"Fri, 16 Mar 2012 02:11:50 +0000","Mon, 23 Sep 2013 18:44:58 +0000","Mon, 1 Oct 2012 18:44:12 +0000",17253142,"sc requested code review of ""HBASE-5591 [jira] ThiftServerRunner.HBaseHandler.toBytes() is identical to Bytes.getBytes()"". Reviewers: tedyu, dhruba, JIRA ThiftServerRunner.HBaseHandler.toBytes() is identical to Bytes.getBytes() Remove the redundant method. Task ID: # Blame Rev: TEST PLAN Revert Plan: Tags: REVISION DETAIL https://reviews.facebook.net/D2355 AFFECTED FILES src/main/java/org/apache/hadoop/hbase/regionserver/HRegionThriftServer.java src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java MANAGE HERALD DIFFERENTIAL RULES https://reviews.facebook.net/herald/view/differential/ WHY DID I GET THIS EMAIL? https://reviews.facebook.net/herald/transcript/5229/ Tip: use the X-Herald-Rules header to filter Herald messages in your client.",-0.0635,-0.03175,neutral
hbase,5591,comment_5,"sc has commented on the revision ""HBASE-5591 [jira] is identical to Bytes.getBytes()"". INLINE COMMENTS It was added by me actually. Because I checked bb) and found that it's different from this one. But this one is the same as bb). These names are really confusing. REVISION DETAIL BRANCH getbytes",code_debt,low_quality_code,"Fri, 16 Mar 2012 02:11:50 +0000","Mon, 23 Sep 2013 18:44:58 +0000","Mon, 1 Oct 2012 18:44:12 +0000",17253142,"sc has commented on the revision ""HBASE-5591 [jira] ThiftServerRunner.HBaseHandler.toBytes() is identical to Bytes.getBytes()"". INLINE COMMENTS src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java:611 It was added by me actually. Because I checked Bytes.toBytes(ByteBuffer bb) and found that it's different from this one. But this one is the same as Bytes.getBytes(ByteBuffer bb). These names are really confusing. REVISION DETAIL https://reviews.facebook.net/D2355 BRANCH getbytes",-0.06692857143,-0.03904166667,neutral
hbase,5595,description,Fix this ugly exception that shows when running 0.92.1 when on local filesystem:,code_debt,low_quality_code,"Fri, 16 Mar 2012 17:59:37 +0000","Fri, 20 Nov 2015 11:53:56 +0000","Sun, 18 Mar 2012 19:45:56 +0000",179179,Fix this ugly exception that shows when running 0.92.1 when on local filesystem:,-1,-1,negative
hbase,5635,comment_11,As per the patch the below variable is of no use now,code_debt,dead_code,"Mon, 26 Mar 2012 09:56:31 +0000","Tue, 26 Feb 2013 17:02:58 +0000","Mon, 23 Apr 2012 16:50:31 +0000",2444040,As per the patch the below variable is of no use now,0,0,negative
hbase,5635,comment_2,Please change the wording in the following log: In this case the worker thread is not exiting.,code_debt,low_quality_code,"Mon, 26 Mar 2012 09:56:31 +0000","Tue, 26 Feb 2013 17:02:58 +0000","Mon, 23 Apr 2012 16:50:31 +0000",2444040,Please change the wording in the following log: In this case the worker thread is not exiting.,0.2,0.2,negative
hbase,5958,comment_10,Makes me nervous to reach in and use the private constructor... do you have some benchmarks that show that there's a noticeable speedup by doing so?,code_debt,slow_algorithm,"Tue, 8 May 2012 17:34:38 +0000","Mon, 13 Jun 2022 16:35:55 +0000","Fri, 8 Jun 2012 16:28:02 +0000",2674404,Makes me nervous to reach in and use the private constructor... do you have some benchmarks that show that there's a noticeable speedup by doing so?,0.1,0.1,negative
hbase,5958,comment_7,Protostuff is usually a little bit faster as well. I haven't personally run those benchmarks in a while and it looks like the most recent are not up yet. But still something to consider.,code_debt,slow_algorithm,"Tue, 8 May 2012 17:34:38 +0000","Mon, 13 Jun 2022 16:35:55 +0000","Fri, 8 Jun 2012 16:28:02 +0000",2674404,Protostuff is usually a little bit faster as well. https://github.com/eishay/jvm-serializers/wiki/Home/25fd014e66738268670adaf44ff5408ba2244d37 I haven't personally run those benchmarks in a while and it looks like the most recent are not up yet. But still something to consider.,0.304,0.304,neutral
hbase,5958,comment_9,"By the way, is it an option to use reflection to access the private constructor? If so, I can have a wrap method to use the private constructor, or the original copyFrom if the private constructor is not accessible. Reflection has overhead of course.",code_debt,low_quality_code,"Tue, 8 May 2012 17:34:38 +0000","Mon, 13 Jun 2022 16:35:55 +0000","Fri, 8 Jun 2012 16:28:02 +0000",2674404,"By the way, is it an option to use reflection to access the private constructor? If so, I can have a wrap method to use the private constructor, or the original copyFrom if the private constructor is not accessible. Reflection has overhead of course.",-0.1876666667,-0.1876666667,neutral
hbase,5958,description,"ByteString.copyFrom makes a copy of a byte array in case it is changed in other thread. In most case, we don't need to worry about that. We should avoid copying the bytes for performance issue.",code_debt,slow_algorithm,"Tue, 8 May 2012 17:34:38 +0000","Mon, 13 Jun 2022 16:35:55 +0000","Fri, 8 Jun 2012 16:28:02 +0000",2674404,"ByteString.copyFrom makes a copy of a byte array in case it is changed in other thread. In most case, we don't need to worry about that. We should avoid copying the bytes for performance issue.",0.02825,0.02825,neutral
hbase,6050,comment_8,Patch looks good. Minor: Please insert spaces around regionDir:,code_debt,low_quality_code,"Fri, 18 May 2012 15:41:10 +0000","Tue, 26 Feb 2013 08:16:18 +0000","Sun, 27 May 2012 16:38:55 +0000",781065,Patch looks good. Minor: Please insert spaces around regionDir:,0.488,0.488,positive
hbase,6201,comment_6,"Sorry for chiming in late, but here's how I see it after quite a bit of internal discussions with some of the HBase and HDFS devs. First of all, lets not got caught up in terminology of what is a unit test, what is a functional test, etc. If we assume this stance than all the tests, essentially, fall under the 3 main categories: # tests that muck about with the internals of the particular single project (HDFS, HBase, etc) using things like private APIs (or sometimes even things like reflections, etc to really get into the guts of the system) # tests that concern themselves with a single project (HDFS, HBase, etc) but use only public APIs AND don't use # tests that concern themselves with multiple projects at the same time (imagine a test that submits an Oozie workflow that has some Pig and Hive actions actively manipulating data in HBase) but only using public APIs It is pretty clear that #3 definitely belongs to Bigtop while #1 definitely belongs to individual projects. For quite some time I was thinking that #2 belongs to Bigtop testbase as well, but I've changed my mind. I now believe that such tests should reside in individual projects and: # be clearly marked as such not to be confused with class #1 (test suites, test lists, naming convention work, etc) # be written/refactored in such a way that doesn't tie them to a particular deployment strategy. IOW they should assume the subsystem to be deployed. # be hooked up to the project build's system in such a way that takes care of deploying the least amount of a system to make them run (e.g. MiniDFS, MiniMR, etc.) Thus if HBase can follow these rules and have a subset of tests that can be executed in different envs. both HBase core devs and bigger Bigtop dev community win, since we can leverage each other's work. Makes sense? If it does I can help with re-factoring.",code_debt,low_quality_code,"Tue, 12 Jun 2012 02:33:19 +0000","Fri, 20 Nov 2015 11:53:53 +0000","Tue, 24 Sep 2013 21:21:05 +0000",40589266,"Sorry for chiming in late, but here's how I see it after quite a bit of internal discussions with some of the HBase and HDFS devs. First of all, lets not got caught up in terminology of what is a unit test, what is a functional test, etc. If we assume this stance than all the tests, essentially, fall under the 3 main categories: tests that muck about with the internals of the particular single project (HDFS, HBase, etc) using things like private APIs (or sometimes even things like reflections, etc to really get into the guts of the system) tests that concern themselves with a single project (HDFS, HBase, etc) but use only public APIs AND don't use tests that concern themselves with multiple projects at the same time (imagine a test that submits an Oozie workflow that has some Pig and Hive actions actively manipulating data in HBase) but only using public APIs It is pretty clear that #3 definitely belongs to Bigtop while #1 definitely belongs to individual projects. For quite some time I was thinking that #2 belongs to Bigtop testbase as well, but I've changed my mind. I now believe that such tests should reside in individual projects and: be clearly marked as such not to be confused with class #1 (test suites, test lists, naming convention work, etc) be written/refactored in such a way that doesn't tie them to a particular deployment strategy. IOW they should assume the subsystem to be deployed. be hooked up to the project build's system in such a way that takes care of deploying the least amount of a system to make them run (e.g. MiniDFS, MiniMR, etc.) Thus if HBase can follow these rules and have a subset of tests that can be executed in different envs. both HBase core devs and bigger Bigtop dev community win, since we can leverage each other's work. Makes sense? If it does I can help with re-factoring.",0.1753,0.1753,neutral
hbase,6282,comment_8,"Lets apply this and close this issue. W/ this applied, if you enable TRACE, you see this kinda of stuff: It is ridiculous detail but could be life saver debugging. Could later work on pb toStringing so it doesn't dump it all... just start of String and a length instead but this should be good for now. Remove Objects class since no longer used.",code_debt,dead_code,"Wed, 27 Jun 2012 19:07:57 +0000","Mon, 23 Sep 2013 18:30:39 +0000","Thu, 15 Nov 2012 03:56:24 +0000",12127707,"Lets apply this and close this issue. W/ this applied, if you enable TRACE, you see this kinda of stuff: It is ridiculous detail but could be life saver debugging. Could later work on pb toStringing so it doesn't dump it all... just start of String and a length instead but this should be good for now. Remove Objects class since no longer used.",0.1188333333,0.1188333333,neutral
hbase,6675,summary,takes too much time: 652.393s,code_debt,slow_algorithm,"Tue, 28 Aug 2012 13:57:44 +0000","Mon, 13 Jun 2022 19:10:57 +0000","Wed, 16 Nov 2016 22:10:15 +0000",133171951,TestTableInputFormatScan takes too much time: 652.393s,-0.5,-0.5,neutral
hbase,6697,comment_0,"master & region server ports where set to ""random"" in the LocalHBaseCluster class; but not in the test hbase-site. So the pattern used in this test class was not working. Fixed, let's see if it breaks anything on hadoop qa...",code_debt,low_quality_code,"Thu, 30 Aug 2012 08:07:04 +0000","Mon, 23 Sep 2013 18:45:02 +0000","Sat, 1 Sep 2012 12:05:20 +0000",187096,"master & region server ports where set to ""random"" in the LocalHBaseCluster class; but not in the test hbase-site. So the pattern used in this test class was not working. Fixed, let's see if it breaks anything on hadoop qa...",-0.03333333333,-0.03333333333,negative
hbase,7000,comment_4,patch looks good. nit: insert space before 1:,code_debt,low_quality_code,"Wed, 17 Oct 2012 08:36:32 +0000","Mon, 23 Sep 2013 18:31:23 +0000","Thu, 18 Oct 2012 17:19:08 +0000",117756,patch looks good. nit: insert space before 1:,0.388,0.388,positive
hbase,7604,description,"the static method getReferencedPath() contains almost the same code done internally by the FileLink, and is only used by ExportSnapshot. Remove that code and use the class directly",code_debt,duplicated_code,"Thu, 17 Jan 2013 16:39:29 +0000","Mon, 23 Sep 2013 18:31:08 +0000","Thu, 17 Jan 2013 19:51:44 +0000",11535,"the static method getReferencedPath() contains almost the same code done internally by the FileLink, and is only used by ExportSnapshot. Remove that code and use the class directly",-0.4375,-0.4375,negative
hbase,7604,summary,Remove duplicated code from HFileLink,code_debt,duplicated_code,"Thu, 17 Jan 2013 16:39:29 +0000","Mon, 23 Sep 2013 18:31:08 +0000","Thu, 17 Jan 2013 19:51:44 +0000",11535,Remove duplicated code from HFileLink,0,0,negative
hbase,76,comment_7,"Agreed on separation. The comparison of String and Text was just something I wanted to settle in my mind since I had heard that String was far less efficient than Text. Turns out, not so much.",code_debt,slow_algorithm,"Mon, 7 Jan 2008 22:16:02 +0000","Fri, 22 Aug 2008 21:13:05 +0000","Thu, 15 May 2008 22:03:00 +0000",11144818,"Agreed on separation. The comparison of String and Text was just something I wanted to settle in my mind since I had heard that String was far less efficient than Text. Turns out, not so much.",0.06666666667,0.06666666667,negative
hbase,7933,comment_1,"I think the bug is a race condition for the parent znode for the table. deletes parent znode, so that we do not leak znodes for deleted tables.",code_debt,low_quality_code,"Mon, 25 Feb 2013 21:51:35 +0000","Mon, 23 Sep 2013 18:31:40 +0000","Wed, 27 Feb 2013 00:40:54 +0000",96559,"I think the bug is a race condition for the parent znode for the table. TableLockManager#tableDeleted() deletes parent znode, so that we do not leak znodes for deleted tables.",0.1,0.1,neutral
hbase,7940,comment_1,Thanks for diggingin on this Ted. I changed the top-level pom but did not realize we had hard-coded versions in all submodules. How about this patch. It has us set version once in one place.,code_debt,low_quality_code,"Tue, 26 Feb 2013 17:41:38 +0000","Mon, 23 Sep 2013 18:31:32 +0000","Tue, 26 Feb 2013 18:29:20 +0000",2862,Thanks for diggingin on this Ted. I changed the top-level pom but did not realize we had hard-coded versions in all submodules. How about this patch. It has us set version once in one place.,0.175,0.175,neutral
hbase,798,comment_11,"Jim, I may have misspoken in my explanation. There are only single HRS methods that take all arguments. Well, there are two of deleteAll but there's different HR implementations. public void deleteAll(final byte [] regionName, final byte [] row, final byte [] column, final long timestamp, final long lockId) public void deleteAll(final byte [] regionName, final byte [] row, final long timestamp, final long lockId) public void deleteFamily(byte [] regionName, byte [] row, byte [] family, long timestamp, final long lockId) public void batchUpdate(final byte [] regionName, BatchUpdate b, final long lockId)",code_debt,duplicated_code,"Wed, 6 Aug 2008 04:29:41 +0000","Sun, 13 Sep 2009 22:33:29 +0000","Wed, 13 Aug 2008 02:35:03 +0000",597922,"Jim, I may have misspoken in my explanation. There are only single HRS methods that take all arguments. Well, there are two of deleteAll but there's different HR implementations. public void deleteAll(final byte [] regionName, final byte [] row, final byte [] column, final long timestamp, final long lockId) public void deleteAll(final byte [] regionName, final byte [] row, final long timestamp, final long lockId) public void deleteFamily(byte [] regionName, byte [] row, byte [] family, long timestamp, final long lockId) public void batchUpdate(final byte [] regionName, BatchUpdate b, final long lockId)",0.15775,0.15775,neutral
hbase,798,comment_16,"Reviewed patch. -1 because: - should renew the lease if it is passed an outstanding lock id - should use as it is much more efficient than - In HRegion, factor out the multiple occurrances of: into a private or protected method, such as: - isRowLocked should be protected or private",code_debt,slow_algorithm,"Wed, 6 Aug 2008 04:29:41 +0000","Sun, 13 Sep 2009 22:33:29 +0000","Wed, 13 Aug 2008 02:35:03 +0000",597922,"Reviewed patch. -1 because: HRegionServer.getLockFromId should renew the lease if it is passed an outstanding lock id HRegionServer.rowlocks should use java.util.concurrent.ConcurrentHashMap as it is much more efficient than Collections.synchronizedMap(HashMap) In HRegion, factor out the multiple occurrances of: into a private or protected method, such as: isRowLocked should be protected or private",0.2333333333,0.15,neutral
hbase,798,comment_3,"Here's my first go. This adds lockRow and unlockRow methods to the client/HTable. The return type of lockRow is a new client object RowLock (contains long lockid and byte[] row). unlockRow takes a RowLock as argument. Also adds new versions of commit, deleteAll, and deleteFamily that also can take RowLock's. Within HRS I created new versions of the same functions which take the actual lock ids: long in HRS (randomized id as with scannerids, used for leases, etc), Integer in HR (existing type used for row locks). Existing functions, which do not explicitly take locks, now call the same HR functions but with nulls as row locks. Internally HR checks if it was passed a valid row lock or not. If it's a null, it obtains a row lock and releases it at the end of the call. If it's valid, it makes use of the lock, and does nothing at the end. Otherwise an exception is thrown. Code has not been completely cleaned up, but this demonstrates how the client row locks are being implemented.",code_debt,low_quality_code,"Wed, 6 Aug 2008 04:29:41 +0000","Sun, 13 Sep 2009 22:33:29 +0000","Wed, 13 Aug 2008 02:35:03 +0000",597922,"Here's my first go. This adds lockRow and unlockRow methods to the client/HTable. The return type of lockRow is a new client object RowLock (contains long lockid and byte[] row). unlockRow takes a RowLock as argument. Also adds new versions of commit, deleteAll, and deleteFamily that also can take RowLock's. Within HRS I created new versions of the same functions which take the actual lock ids: long in HRS (randomized id as with scannerids, used for leases, etc), Integer in HR (existing type used for row locks). Existing functions, which do not explicitly take locks, now call the same HR functions but with nulls as row locks. Internally HR checks if it was passed a valid row lock or not. If it's a null, it obtains a row lock and releases it at the end of the call. If it's valid, it makes use of the lock, and does nothing at the end. Otherwise an exception is thrown. Code has not been completely cleaned up, but this demonstrates how the client row locks are being implemented.",0.0115,0.0115,neutral
hbase,8193,comment_0,"A round-trip to ZK is expensive... Should we put this call as optionnal with a parameter? For long running clusters, thise case might never happend. But I also agree that this call might not be done so often so perfs impacts might be small...",code_debt,slow_algorithm,"Mon, 25 Mar 2013 10:25:48 +0000","Thu, 16 Jun 2022 05:54:51 +0000","Tue, 30 Dec 2014 04:36:14 +0000",55707026,"A round-trip to ZK is expensive... Should we put this call as optionnal with a parameter? For long running clusters, thise case might never happend. But I also agree that this call might not be done so often so perfs impacts might be small...",0.06666666667,0.06666666667,negative
hbase,8256,comment_6,"The patch touches the same code as HBASE-4955. Not a huge deal, I can redo it. - I fear your may end up with something that will work only with our version of surefire, and we will get into new surefire regressions when we will use the official version. - The last version of surefire may contains fixes that you could use (I've seen some stuff around categories). A quick word of what I had in mind myself (that's thtake agese content of the patch v2 in HBASE-4955) - migrate to surefire 2.14.1 or 2.15 if the regressions we get with these versions are acceptable. - remove the profiles no one use such as runSmallTests - for small tests, use the new feature 'reuseFork' of surefire: this is the only way (in the official release) to have separate logs per test without paying the cost of a fork per single test. - for medium and large, as today: multiple fork in parallel. If what you do does not break this, I will be happy :-) The ideal solution would be to migrate to the official version before doing this, but I must agree but it can take longer than expected.",code_debt,dead_code,"Wed, 3 Apr 2013 18:07:36 +0000","Thu, 16 Jun 2022 05:57:12 +0000","Mon, 27 Jan 2014 18:03:21 +0000",25833345,"The patch touches the same code as HBASE-4955. Not a huge deal, I can redo it. I fear your may end up with something that will work only with our version of surefire, and we will get into new surefire regressions when we will use the official version. The last version of surefire may contains fixes that you could use (I've seen some stuff around categories). A quick word of what I had in mind myself (that's thtake agese content of the patch v2 in HBASE-4955) migrate to surefire 2.14.1 or 2.15 if the regressions we get with these versions are acceptable. remove the profiles no one use such as runSmallTests for small tests, use the new feature 'reuseFork' of surefire: this is the only way (in the official release) to have separate logs per test without paying the cost of a fork per single test. for medium and large, as today: multiple fork in parallel. If what you do does not break this, I will be happy The ideal solution would be to migrate to the official version before doing this, but I must agree but it can take longer than expected.",0.1405,0.0175625,neutral
hbase,8527,comment_5,lgtm. Minor nit: Should there be a comment on when the point of no return is?,code_debt,low_quality_code,"Fri, 10 May 2013 21:19:51 +0000","Thu, 16 Jun 2022 17:04:53 +0000","Thu, 23 May 2013 20:38:33 +0000",1120722,lgtm. Minor nit: Should there be a comment on when the point of no return is?,0,0,neutral
hbase,8527,summary,clean up code around compaction completion in HStore,code_debt,low_quality_code,"Fri, 10 May 2013 21:19:51 +0000","Thu, 16 Jun 2022 17:04:53 +0000","Thu, 23 May 2013 20:38:33 +0000",1120722,clean up code around compaction completion in HStore,0.4,0.4,neutral
hbase,8665,comment_10,"Will submit some simple patch tomorrow, we are hitting this a lot under high load. As soon as I can make the test slightly less ugly.",code_debt,low_quality_code,"Thu, 30 May 2013 23:21:38 +0000","Thu, 5 May 2022 02:11:33 +0000","Wed, 19 Jun 2013 00:42:56 +0000",1646478,"Will submit some simple patch tomorrow, we are hitting this a lot under high load. As soon as I can make the test slightly less ugly.",-0.5,-0.5,neutral
hbase,8665,comment_5,"W.r.t. property of the store: 1) Current priority is derived entirely from store state. In fact it only depends on number of files and blocking limit, so there's no such thing as important store other than store. 2) It seems to be the obvious cause of this particular issue, see above. When selection is added to queue, its priority is low. As store fills up it's ""real"" priority changes, but we have no way to influence it. Bumping its priority when we queue another one is just a hack around that imho; if it even helps. If blocked store comes into a queue full of non-blocked stores why should we bump them? And how much. 3) Additionally if we pre-select in advance we simply end up with inefficient compactions, for example in this case we could have compacted 6 small files in one go, but would have ended up compacting 3 and 3 if it spent less time in the queue.",code_debt,low_quality_code,"Thu, 30 May 2013 23:21:38 +0000","Thu, 5 May 2022 02:11:33 +0000","Wed, 19 Jun 2013 00:42:56 +0000",1646478,"W.r.t. property of the store: 1) Current priority is derived entirely from store state. In fact it only depends on number of files and blocking limit, so there's no such thing as important store other than blocked/about-to-block store. 2) It seems to be the obvious cause of this particular issue, see above. When selection is added to queue, its priority is low. As store fills up it's ""real"" priority changes, but we have no way to influence it. Bumping its priority when we queue another one is just a hack around that imho; if it even helps. If blocked store comes into a queue full of non-blocked stores why should we bump them? And how much. 3) Additionally if we pre-select in advance we simply end up with inefficient compactions, for example in this case we could have compacted 6 small files in one go, but would have ended up compacting 3 and 3 if it spent less time in the queue.",0.01944444444,0.002777777778,neutral
hbase,8904,description,* Clean up the Long overflow (oops) * Clean up the usage of tableName vs tableNameBytes * Add in more than just max time. * Add in tracing.,code_debt,low_quality_code,"Tue, 9 Jul 2013 17:44:59 +0000","Mon, 23 Sep 2013 19:22:31 +0000","Tue, 9 Jul 2013 23:46:09 +0000",21670,Clean up the Long overflow (oops) Clean up the usage of tableName vs tableNameBytes Add in more than just max time. Add in tracing.,0.06666666667,0.06666666667,neutral
hbase,9052,comment_1,nit: regionOffline should be renamed setRegionOffline or offlineRegion? I suppose you are following the precedent where this operation is done in a method named regionOffline (reads oddly). I see tightening up of allowed states but how are we prevening assign of MERGE and SPLIT?,code_debt,low_quality_code,"Fri, 26 Jul 2013 23:48:30 +0000","Mon, 23 Sep 2013 19:22:36 +0000","Tue, 30 Jul 2013 16:56:23 +0000",320873,nit: regionOffline should be renamed setRegionOffline or offlineRegion? I suppose you are following the precedent where this operation is done in a method named regionOffline (reads oddly). I see tightening up of allowed states but how are we prevening assign of MERGE and SPLIT?,0,0,neutral
hbase,9052,comment_6,Yeah. Its a bad name but should be consistent.,code_debt,low_quality_code,"Fri, 26 Jul 2013 23:48:30 +0000","Mon, 23 Sep 2013 19:22:36 +0000","Tue, 30 Jul 2013 16:56:23 +0000",320873,jxiang Yeah. Its a bad name but should be consistent.,-0.2,-0.2,negative
hbase,9158,comment_4,Cleaned up 0.94 patch.,code_debt,low_quality_code,"Thu, 8 Aug 2013 06:59:45 +0000","Wed, 21 Aug 2013 00:08:50 +0000","Thu, 8 Aug 2013 23:08:49 +0000",58144,Cleaned up 0.94 patch.,0,0,neutral
hbase,9303,comment_9,"ok, this explanation is really helpful, and I buy it. There are some subtle things going on here, so can we add comments in the code about why there are two mutate calls and why we must first delete and then rewrite (instead of reusing like we did before)?",code_debt,low_quality_code,"Thu, 22 Aug 2013 19:46:12 +0000","Tue, 24 Sep 2013 20:32:53 +0000","Fri, 23 Aug 2013 20:44:53 +0000",89921,"ok, this explanation is really helpful, and I buy it. There are some subtle things going on here, so can we add comments in the code about why there are two mutate calls and why we must first delete and then rewrite (instead of reusing like we did before)?",0.59375,0.59375,positive
hbase,9340,comment_1,nit: no need for that extra newline.,code_debt,low_quality_code,"Mon, 26 Aug 2013 19:04:22 +0000","Fri, 20 Nov 2015 11:53:42 +0000","Mon, 26 Aug 2013 20:18:20 +0000",4438,nit: no need for that extra newline.,0,0,negative
hbase,9371,summary,Eliminate log spam when tailing files,code_debt,low_quality_code,"Wed, 28 Aug 2013 21:51:02 +0000","Fri, 20 Nov 2015 11:53:15 +0000","Thu, 29 Aug 2013 00:38:30 +0000",10048,Eliminate log spam when tailing files,0.4,0.4,neutral
hbase,9433,description,OpenRegionHandler uses the timeout monitor checking period as the timeout. It should share the same default as in AssignmentManager.,code_debt,low_quality_code,"Wed, 4 Sep 2013 15:28:53 +0000","Fri, 20 Nov 2015 11:54:24 +0000","Wed, 4 Sep 2013 23:58:32 +0000",30579,OpenRegionHandler uses the timeout monitor checking period as the timeout. It should share the same default as in AssignmentManager.,-0.075,-0.075,neutral
hbase,9461,comment_0,"Here is a start. Adds start of a description of who is doing what to who. Removes thread local that made the RcpServer instance available -- not used (seemingly), ugly.",code_debt,low_quality_code,"Sat, 7 Sep 2013 17:30:25 +0000","Fri, 20 Nov 2015 11:55:25 +0000","Sun, 15 Sep 2013 02:24:06 +0000",636821,"Here is a start. Adds start of a description of who is doing what to who. Removes thread local that made the RcpServer instance available  not used (seemingly), ugly.",-0.3333333333,-0.3333333333,neutral
hbase,9461,comment_4,"More doc and untangling. More to do but this ok for now. RpcServer had internal CallRunner class. The scheduler was reaching over to use this inner class. Broke it out of RpcServer and made it Standalone. Made it take an RpcServerInterface instead of RpcServer. Undid CallRunner implementing Runnable (confusing -- how it is run is internal function of Scheduler implementation) Cleaned up RpcServerInterface explaining why a start and a startThreads and an openServer. Added a few methods to support CallRunner being outside of RpcServer. Added into CallRunner the managment of the call queue size. As it was, callQueueSize was incremented before we created a CallRunner but internal to CallRunner it was managing the decrement. Shutdown access on CallRunner constructor and CR#getStatus and other methods only for use in this package. Added javadoc all around to explain the cryptic. was like CallRunner, a class used by the Scheduler only it was an inner class of RpcServer. Moved it out. Removed unused thread local SERVER. Add simple test to demo being able to instantiate a CallRunner outside of RpcServer context.a",code_debt,low_quality_code,"Sat, 7 Sep 2013 17:30:25 +0000","Fri, 20 Nov 2015 11:55:25 +0000","Sun, 15 Sep 2013 02:24:06 +0000",636821,"More doc and untangling. More to do but this ok for now. RpcServer had internal CallRunner class. The scheduler was reaching over to use this inner class. Broke it out of RpcServer and made it Standalone. Made it take an RpcServerInterface instead of RpcServer. Undid CallRunner implementing Runnable (confusing  how it is run is internal function of Scheduler implementation) Cleaned up RpcServerInterface explaining why a start and a startThreads and an openServer. Added a few methods to support CallRunner being outside of RpcServer. Added into CallRunner the managment of the call queue size. As it was, callQueueSize was incremented before we created a CallRunner but internal to CallRunner it was managing the decrement. Shutdown access on CallRunner constructor and CR#getStatus and other methods only for use in this package. Added javadoc all around to explain the cryptic. RpcSchedulerContextImpl was like CallRunner, a class used by the Scheduler only it was an inner class of RpcServer. Moved it out. Removed unused thread local SERVER. Add simple test to demo being able to instantiate a CallRunner outside of RpcServer context.a",0.09417647059,0.09417647059,neutral
hbase,9461,comment_6,"Just asking, if you find the time. Some comments describing the intention (what this class is supposed to do) would be great. For example, the delayed calls, undelayed calls with delayed response are not trivial (are they used?). It's very difficult to understand what these classes are supposed to do and why. Or this low level, with a specific case with '==1' Basically, I wanted to try a Netty based implementation, but there are so many corner cases that even estimating the time needed for a hacked implementation is difficult....",code_debt,low_quality_code,"Sat, 7 Sep 2013 17:30:25 +0000","Fri, 20 Nov 2015 11:55:25 +0000","Sun, 15 Sep 2013 02:24:06 +0000",636821,"Just asking, if you find the time. Some comments describing the intention (what this class is supposed to do) would be great. For example, the delayed calls, undelayed calls with delayed response are not trivial (are they used?). It's very difficult to understand what these classes are supposed to do and why. Or this low level, with a specific case with '==1' Basically, I wanted to try a Netty based implementation, but there are so many corner cases that even estimating the time needed for a hacked implementation is difficult....",-1.11E-17,-1.11E-17,neutral
hbase,9461,comment_7,"Smile. I am in same boat as you. I want to untangle this rats nest so I can understand whats going on and try stuff (I want to try pool of bytebuffers -- direct bytebuffers even -- for incoming requests. Putting in netty too would be sweet). Unless you object, was going to commit this since it some progress. Was going to do more along this line soon but in other issues; I'm afraid the refactors will rot between getting time to work in here. The Delay stuff is unused I think. It was an experiment. Maybe I'll look at that next and purge it if I can.",code_debt,dead_code,"Sat, 7 Sep 2013 17:30:25 +0000","Fri, 20 Nov 2015 11:55:25 +0000","Sun, 15 Sep 2013 02:24:06 +0000",636821,"Smile. I am in same boat as you. I want to untangle this rats nest so I can understand whats going on and try stuff (I want to try pool of bytebuffers  direct bytebuffers even  for incoming requests. Putting in netty too would be sweet). Unless you object, was going to commit this since it some progress. Was going to do more along this line soon but in other issues; I'm afraid the refactors will rot between getting time to work in here. The Delay stuff is unused I think. It was an experiment. Maybe I'll look at that next and purge it if I can.",0.1312407407,0.1312407407,neutral
hbase,9461,summary,Some doc and cleanup in RPCServer,code_debt,low_quality_code,"Sat, 7 Sep 2013 17:30:25 +0000","Fri, 20 Nov 2015 11:55:25 +0000","Sun, 15 Sep 2013 02:24:06 +0000",636821,Some doc and cleanup in RPCServer,0,0,neutral
hbase,9493,comment_0,Simple braindead patch. New name makes it clear that allocation is going on (and also clearly points out that we are doing a lot of small array allocations in tests).,code_debt,low_quality_code,"Tue, 10 Sep 2013 20:44:08 +0000","Fri, 20 Nov 2015 11:53:02 +0000","Tue, 10 Sep 2013 23:10:15 +0000",8767,Simple braindead patch. New name makes it clear that allocation is going on (and also clearly points out that we are doing a lot of small array allocations in tests).,0,0,negative
hbase,9493,description,"To make reading code easier, we should rename the CellUtil#get*Array methods to to CellUtil#clone*. This eliminates the possibly confusion between the semantics of these methods (which allocate-copy) and that of Cell#get*Array (which essentially just returns a pointer).",code_debt,low_quality_code,"Tue, 10 Sep 2013 20:44:08 +0000","Fri, 20 Nov 2015 11:53:02 +0000","Tue, 10 Sep 2013 23:10:15 +0000",8767,"To make reading code easier, we should rename the CellUtil#get*Array methods (Value,Qualifier,Family,Row) to to CellUtil#clone*. This eliminates the possibly confusion between the semantics of these methods (which allocate-copy) and that of Cell#get*Array (which essentially just returns a pointer).",0,0,neutral
hbase,9545,comment_0,"Stack trace It looks like assumes its {{masterMonitor}} field is never null, but if there is no connection,that isn't true. The close() operation should be made a bit more robust, so as not to hide the underlying RPC failures I expect to see",code_debt,low_quality_code,"Mon, 16 Sep 2013 17:30:00 +0000","Thu, 16 Jun 2022 18:06:24 +0000","Tue, 17 Sep 2013 09:07:39 +0000",56259,"Stack trace It looks like MasterMonitorCallable.close() assumes its masterMonitor field is never null, but if there is no connection,that isn't true. The close() operation should be made a bit more robust, so as not to hide the underlying RPC failures I expect to see",-0.1198333333,-0.07988888889,negative
hbase,9689,comment_7,"Looks good Correct the comment pls. Seems copy paste :) set_get_attributes, set_put_attributes seems same code repeating. We can make this one def?",code_debt,duplicated_code,"Tue, 1 Oct 2013 09:52:39 +0000","Fri, 20 Nov 2015 11:53:36 +0000","Thu, 17 Oct 2013 17:36:36 +0000",1410237,"Looks good Correct the comment pls. Seems copy paste set_scan_attributes, set_get_attributes, set_put_attributes seems same code repeating. We can make this one def?",0.339,0.2056666667,positive
hbase,9806,comment_0,"Here's an initial version, based heavily on HFilerPerfEval code. Would be nice to reduce duplication between the two. Sample output, for reference.",code_debt,duplicated_code,"Fri, 18 Oct 2013 22:13:17 +0000","Thu, 16 Jun 2022 18:22:32 +0000","Thu, 16 Jun 2022 18:22:30 +0000",273269353,"Here's an initial version, based heavily on HFilerPerfEval code. Would be nice to reduce duplication between the two. Sample output, for reference.",0.275,0.275,neutral
hbase,9806,comment_3,"Updated patch after running with a couple different configurations. Hopefully I'll get to run on a larger memory machine tomorrow. I have a couple more things to expose as configuration and I'd like to get it to the point where it can automatically determine the amount of data to write based on different cache:data ratios. I also tried running multiple concurrent the cache consumers, but saw warnings about double-storing blocks. Will investigate that further before adding.  how much RAM do your test machines have?",code_debt,low_quality_code,"Fri, 18 Oct 2013 22:13:17 +0000","Thu, 16 Jun 2022 18:22:32 +0000","Thu, 16 Jun 2022 18:22:30 +0000",273269353,"Updated patch after running with a couple different configurations. Hopefully I'll get to run on a larger memory machine tomorrow. I have a couple more things to expose as configuration and I'd like to get it to the point where it can automatically determine the amount of data to write based on different cache:data ratios. I also tried running multiple concurrent the cache consumers, but saw warnings about double-storing blocks. Will investigate that further before adding. jmspaggi how much RAM do your test machines have?",0.1125,0.1125,positive
hbase,9901,comment_2,"The toString is ugly. Fix on commit: + return ""HTable{"" + ""connection="" + connection + "", tableName="" + tableName + '}'; Make it just: connection + "","" + tableName It will look like: Or Our logs are too profuse already -- they need paring. Let the above be the convention for tablename string. No need of having the '{' and the HTable preamble? If you do above, +1 on patch for branch and trunk",code_debt,low_quality_code,"Wed, 6 Nov 2013 08:50:52 +0000","Mon, 16 Dec 2013 18:46:55 +0000","Wed, 6 Nov 2013 18:32:18 +0000",34886,"The toString is ugly. Fix on commit: + return ""HTable {"" + ""connection="" + connection + "", tableName="" + tableName + '} '; Make it just: connection + "","" + tableName It will look like: hconnection-0x020234343,bigtable Or bigtable,hconnection-0x020234343 Our logs are too profuse already  they need paring. Let the above be the convention for tablename string. No need of having the '{' and the HTable preamble? If you do above, +1 on patch for branch and trunk",-0.13,-0.13,negative
hbase,1849,comment_5,"It's not perfect, but the client has come a long way. No action on this issue for a long time, resolving as Incomplete",defect_debt,uncorrected_known_defects,"Thu, 17 Sep 2009 05:44:41 +0000","Sat, 11 Jun 2022 21:03:00 +0000","Sat, 19 Jul 2014 00:46:24 +0000",152564503,"It's not perfect, but the client has come a long way. No action on this issue for a long time, resolving as Incomplete",-0.2,-0.2,negative
hbase,10083,description,"When RegionServer failed to load a bloom block from HDFS due to any timeout or other reasons, it threw out the exception and disable the entire bloom filter for this HFile. This behavior does not make too much sense, especially for the compound bloom filter. Instead of disabling the bloom filter for the entire file, it could just return a potentially false positive result (true) and keep the bloom filter available.",design_debt,non-optimal_design,"Thu, 5 Dec 2013 00:47:55 +0000","Fri, 17 Jun 2022 04:51:04 +0000","Mon, 25 Sep 2017 19:05:11 +0000",120161836,"When RegionServer failed to load a bloom block from HDFS due to any timeout or other reasons, it threw out the exception and disable the entire bloom filter for this HFile. This behavior does not make too much sense, especially for the compound bloom filter. Instead of disabling the bloom filter for the entire file, it could just return a potentially false positive result (true) and keep the bloom filter available.",0.1733333333,0.1733333333,negative
hbase,10115,comment_5,"Why not another coproc endpoint or improve the existing one? As long as it doesn't affect perf too much... we can already wrap (or replace) compaction scanner thru coproc. ScanQueryMatcher is what decides what KVs get skipped, so if a wrapping scanner could influence that (or even just be able to react to .match call), it should be almost sufficient. It has KV, it has the default verdict. The only problem is with seek to next something results, which can skip KVs. This will no longer be allowed if all deleted KVs are to be examined if I understand that code correctly, so coproc will have to also prevent matcher from doing that?",design_debt,non-optimal_design,"Mon, 9 Dec 2013 23:33:33 +0000","Fri, 17 Jun 2022 04:53:21 +0000","Tue, 29 Apr 2014 00:12:35 +0000",12098342,"Why not another coproc endpoint or improve the existing one? As long as it doesn't affect perf too much... we can already wrap (or replace) compaction scanner thru coproc. ScanQueryMatcher is what decides what KVs get skipped, so if a wrapping scanner could influence that (or even just be able to react to .match call), it should be almost sufficient. It has KV, it has the default verdict. The only problem is with seek to next something results, which can skip KVs. This will no longer be allowed if all deleted KVs are to be examined if I understand that code correctly, so coproc will have to also prevent matcher from doing that?",0.04471428571,0.04471428571,neutral
hbase,1012,description,Ning Li up on list has stated that getting blocks using hdfs though the block is local takes almost the same amount of time as accesing the block over the network. See if can do something smarter when the data is known to be local short-circuiting hdfs if we can in a subclass of DFSClient (George Porter suggestion).,design_debt,non-optimal_design,"Wed, 19 Nov 2008 20:59:26 +0000","Sat, 11 Jun 2022 20:28:55 +0000","Fri, 24 Feb 2012 19:12:41 +0000",102982395,Ning Li up on list has stated that getting blocks using hdfs though the block is local takes almost the same amount of time as accesing the block over the network. See if can do something smarter when the data is known to be local short-circuiting hdfs if we can in a subclass of DFSClient (George Porter suggestion).,0.2125,0.2125,neutral
hbase,10213,comment_17,"the intention of this jira is good:-), but by examining the patch: the metric above only reflects the log read/parse rate, not the desired replicating data to peer cluster rate, since the read/parsed log files may contain many kvs from column-families with replication scope=0 which will be filtered out and removed from the entries list before the real replicating to peer cluster occurs... why not use currentSize, the size of all entries which will be really replicated to the peer cluster?",design_debt,non-optimal_design,"Fri, 20 Dec 2013 09:34:06 +0000","Sat, 21 Feb 2015 23:29:00 +0000","Tue, 7 Jan 2014 22:15:26 +0000",1600880,"However, it is not clear enough to know how many bytes replicating to peer cluster from these metrics. In production environment, it may be important to know the size of replicating data per second the intention of this jira is good, but by examining the patch: the metric above only reflects the log read/parse rate, not the desired replicating data to peer cluster rate, since the read/parsed log files may contain many kvs from column-families with replication scope=0 which will be filtered out and removed from the entries list before the real replicating to peer cluster occurs... why not use currentSize, the size of all entries which will be really replicated to the peer cluster?",0.2586666667,-0.1206666667,neutral
hbase,1054,description,"I had some tables refuse to find their indexes even though they were defined and had been updated. Scanning .META. I see that some regions from the table don't have the indexes... Oddly, during the .META. scan, I would see that I had 3 entries per table. (I have very little data in each table, def not splitting yet) But when I visited the UI it showed just one region per table... This patch also addes some toStrings that helped in diagnostics, and a new null check that I found I needed in IndexedTableScanner",design_debt,non-optimal_design,"Wed, 10 Dec 2008 00:42:51 +0000","Sun, 13 Sep 2009 22:26:34 +0000","Wed, 10 Dec 2008 16:23:55 +0000",56464,"I had some tables refuse to find their indexes even though they were defined and had been updated. Scanning .META. I see that some regions from the table don't have the indexes... Oddly, during the .META. scan, I would see that I had 3 entries per table. (I have very little data in each table, def not splitting yet) But when I visited the UI it showed just one region per table... This patch also addes some toStrings that helped in diagnostics, and a new null check that I found I needed in IndexedTableScanner",0.007142857143,0.007142857143,neutral
hbase,10702,comment_3,"You sure you want to use deleteColumn (vs. deleteColumns)? Delete.deleteColumn will only target the current latest version of that column for deletion (as determined by the time the delete arrives at the server). Not only is this very expensive (a Get on the server just to find the last ts), it also only targets the very latest version. (I mentioned before that the Delete API is confusing. This reminds me, maybe there's time to fix it before 1.0.)",design_debt,non-optimal_design,"Fri, 7 Mar 2014 22:16:14 +0000","Fri, 17 Jun 2022 05:10:24 +0000","Sat, 8 Mar 2014 02:15:01 +0000",14327,"You sure you want to use deleteColumn (vs. deleteColumns)? Delete.deleteColumn will only target the current latest version of that column for deletion (as determined by the time the delete arrives at the server). Not only is this very expensive (a Get on the server just to find the last ts), it also only targets the very latest version. (I mentioned before that the Delete API is confusing. This reminds me, maybe there's time to fix it before 1.0.)",0.02716666667,0.02716666667,neutral
hbase,10892,comment_0,"looks ok to me, (I've also tried it a bit) but there are a couple of things I'm not sure about I think that the glob only for ""all tables"" may be confusing, because then you can't do ""tableName*"" you must use ""tableName.*"" but I'm ok having it as a special ""all tables"" case. is and was committed as Deprecated. I know that is used in other places, but we should decide if we want to use that for new code or not and later replace it where is used.",design_debt,non-optimal_design,"Tue, 1 Apr 2014 23:32:09 +0000","Sat, 21 Feb 2015 23:30:24 +0000","Fri, 25 Apr 2014 22:35:01 +0000",2070172,"looks ok to me, (I've also tried it a bit) but there are a couple of things I'm not sure about I think that the glob only for ""all tables"" may be confusing, because then you can't do ""tableName*"" you must use ""tableName.*"" but I'm ok having it as a special ""all tables"" case. admin.getTableNames() is and was committed as Deprecated. I know that is used in other places, but we should decide if we want to use that for new code or not and later replace it where is used.",0.41975,0.3358,neutral
hbase,1089,comment_5,Committed. Thanks for the nice patch Samuel. We might consider adding this to hbase 0.19.1. Could be used to indicate cruft in the hbase.rootdir; e.g. there is loads of cruft in pset hbase.rootdir since its been migrated across multiple versions.,design_debt,non-optimal_design,"Wed, 24 Dec 2008 18:03:58 +0000","Sun, 13 Sep 2009 22:24:17 +0000","Mon, 19 Jan 2009 21:45:12 +0000",2259674,Committed. Thanks for the nice patch Samuel. We might consider adding this to hbase 0.19.1. Could be used to indicate cruft in the hbase.rootdir; e.g. there is loads of cruft in pset hbase.rootdir since its been migrated across multiple versions.,0.1854166667,0.1854166667,positive
hbase,10925,comment_7,"Looks good. This should be a configuration: + long maxRowSize = 10 * 1024 * 1024; On the below: + totalReadSize += kv.getRowLength() + + + + + if (totalReadSize + throw new size of row is :"" + maxRowSize + + "", but the row is bigger than that.""); + } ... it is a bit of a pain having to do the above. I suppose we should add a getSize to CellUtils. Could do that in another patch. Was also thinking what if the Cell is offheap but then above is probably fine still because while it is offheap in the server, in the client it may not be and a 10G rowsize is likely larger than it can swallow w/o OOME.",design_debt,non-optimal_design,"Mon, 7 Apr 2014 21:48:10 +0000","Tue, 21 Nov 2017 22:55:14 +0000","Wed, 30 Apr 2014 00:08:59 +0000",1909249,"Looks good. This should be a configuration: + long maxRowSize = 10 * 1024 * 1024; On the below: + totalReadSize += kv.getRowLength() + kv.getFamilyLength() + + kv.getQualifierLength() + kv.getValueLength(); + if (totalReadSize > maxRowSize) { + throw new RowTooBigException(""Max size of row is :"" + maxRowSize + + "", but the row is bigger than that.""); + } ... it is a bit of a pain having to do the above. I suppose we should add a getSize to CellUtils. Could do that in another patch. Was also thinking what if the Cell is offheap but then above is probably fine still because while it is offheap in the server, in the client it may not be and a 10G rowsize is likely larger than it can swallow w/o OOME.",0.05995238095,0.04196666667,positive
hbase,10968,description,"Here is related code: context was dereferenced first, leaving the null check ineffective.",design_debt,non-optimal_design,"Sat, 12 Apr 2014 01:10:49 +0000","Sat, 21 Feb 2015 23:30:17 +0000","Sat, 12 Apr 2014 01:56:36 +0000",2747,"Here is related code: context was dereferenced first, leaving the null check ineffective.",-0.583,-0.583,neutral
hbase,10968,summary,Null check in is redundant,design_debt,non-optimal_design,"Sat, 12 Apr 2014 01:10:49 +0000","Sat, 21 Feb 2015 23:30:17 +0000","Sat, 12 Apr 2014 01:56:36 +0000",2747,Null check in TableSnapshotInputFormat#TableSnapshotRegionRecordReader#initialize() is redundant,0,0,negative
hbase,11011,description,"On load we already have a StoreFileInfo and we create it from the path, this will result in an extra fs.getFileStatus() call. In we do a fs.exists() and later a fs.getFileStatus() to create the StoreFileInfo, we can avoid the exists.",design_debt,non-optimal_design,"Thu, 17 Apr 2014 02:06:05 +0000","Sat, 21 Feb 2015 23:35:09 +0000","Fri, 18 Apr 2014 17:19:58 +0000",141233,"On load we already have a StoreFileInfo and we create it from the path, this will result in an extra fs.getFileStatus() call. In completeCompactionMarker() we do a fs.exists() and later a fs.getFileStatus() to create the StoreFileInfo, we can avoid the exists.",-0.04,-0.04,neutral
hbase,11011,summary,Avoid extra getFileStatus() calls on Region startup,design_debt,non-optimal_design,"Thu, 17 Apr 2014 02:06:05 +0000","Sat, 21 Feb 2015 23:35:09 +0000","Fri, 18 Apr 2014 17:19:58 +0000",141233,Avoid extra getFileStatus() calls on Region startup,-0.2,-0.2,neutral
hbase,11229,description,"See parent issue. Small changes in the hit percentage can have large implications, even when movement is inside a single percent: i.e. going from 99.11 to 99.87 percent. As is, percents are ints. Make them doubles.",design_debt,non-optimal_design,"Wed, 21 May 2014 22:05:09 +0000","Sat, 21 Feb 2015 23:32:31 +0000","Thu, 22 May 2014 18:44:45 +0000",74376,"See parent issue. Small changes in the hit percentage can have large implications, even when movement is inside a single percent: i.e. going from 99.11 to 99.87 percent. As is, percents are ints. Make them doubles.",0,0,neutral
hbase,11229,summary,Change block cache percentage metrics to be doubles rather than ints,design_debt,non-optimal_design,"Wed, 21 May 2014 22:05:09 +0000","Sat, 21 Feb 2015 23:32:31 +0000","Thu, 22 May 2014 18:44:45 +0000",74376,Change block cache percentage metrics to be doubles rather than ints,-0.2,-0.2,neutral
hbase,11511,comment_4,"Thanks Stack for taking a look. Good point. We writelock the updatesLock, then start the mvcc transaction, so it should be the case that sync() and sync(trx) should be the same thing I guess. But let me change it to be plain old sync() to be failure-proof. Indeed. The block above is a catch block for catching an ignoring any exception from writing the abort marker to WAL. The call to should still happen in the outer block, resulting in",design_debt,non-optimal_design,"Mon, 14 Jul 2014 18:14:43 +0000","Fri, 6 Apr 2018 17:51:19 +0000","Tue, 15 Jul 2014 21:52:15 +0000",99452,"Thanks Stack for taking a look. That is ok? What if an edit after the start flush edit was added? Good point. We writelock the updatesLock, then start the mvcc transaction, so it should be the case that sync() and sync(trx) should be the same thing I guess. But let me change it to be plain old sync() to be failure-proof. ... or are we still dealing with failure at this point? Indeed. The block above is a catch block for catching an ignoring any exception from writing the abort marker to WAL. The call to wal.abortCacheFlush() should still happen in the outer block, resulting in DroppedSnapshotException.",0.2618809524,0.2294242424,neutral
hbase,11511,description,"We used to write COMPLETE_FLUSH event to WAL until it got removed in 0.96 in issue HBASE-7329. For secondary region replicas, it is important to share the data files with the primary region. So we should reintroduce the flush wal markers so that the secondary region replicas can pick up the newly flushed files from the WAL and start serving data from those. A design doc which explains the context a bit better can be found in HBASE-11183.",design_debt,non-optimal_design,"Mon, 14 Jul 2014 18:14:43 +0000","Fri, 6 Apr 2018 17:51:19 +0000","Tue, 15 Jul 2014 21:52:15 +0000",99452,"We used to write COMPLETE_FLUSH event to WAL until it got removed in 0.96 in issue HBASE-7329. For secondary region replicas, it is important to share the data files with the primary region. So we should reintroduce the flush wal markers so that the secondary region replicas can pick up the newly flushed files from the WAL and start serving data from those. A design doc which explains the context a bit better can be found in HBASE-11183.",0.2,0.2,neutral
hbase,11612,description,Can be removed now from 0.98 onwards? The javadoc says it should be removed on a major release after 0.96. It does a full Meta scan which takes quite some time especially if there are around 1M regions.,design_debt,non-optimal_design,"Wed, 30 Jul 2014 00:16:23 +0000","Fri, 17 Jun 2022 05:52:59 +0000","Wed, 30 Jul 2014 00:23:17 +0000",414,Can MetaMigrationConvertingToPB be removed now from 0.98 onwards? The javadoc says it should be removed on a major release after 0.96. It does a full Meta scan which takes quite some time especially if there are around 1M regions.,0,0,neutral
hbase,11935,comment_13,This one has held up to initial testing. Will post a trunk patch soon and report back with more actual cluster testing tomorrow. The core of the change is that we do schedule every single queue on a thread (there might be 1000's or even 100's of 1000's). Instead we schedule a thread per failed RS and handle all that RSs queue inside the same thread (by calling ReplicationSource's run() inline). While we looked at the code we all felt it is time to rethink the architecture and rewrite the replication source side from scratch. It has reached the point where incremental changes are no longer appropriate... But that is for another jira.,design_debt,non-optimal_design,"Wed, 10 Sep 2014 18:47:37 +0000","Fri, 17 Jun 2022 17:13:44 +0000","Fri, 13 Feb 2015 01:59:31 +0000",13417914,This one has held up to initial testing. Will post a trunk patch soon and report back with more actual cluster testing tomorrow. The core of the change is that we do schedule every single queue on a thread (there might be 1000's or even 100's of 1000's). Instead we schedule a thread per failed RS and handle all that RSs queue inside the same thread (by calling ReplicationSource's run() inline). While we looked at the code we all felt it is time to rethink the architecture and rewrite the replication source side from scratch. It has reached the point where incremental changes are no longer appropriate... But that is for another jira.,-0.1125,-0.1125,neutral
hbase,12017,description,"Now that Connection and ConnectionFactory are in place, internal code should use them instead of HTable constructors as per HBASE-11825.",design_debt,non-optimal_design,"Thu, 18 Sep 2014 15:27:55 +0000","Wed, 4 Apr 2018 23:05:46 +0000","Thu, 27 Nov 2014 16:03:35 +0000",6050140,"Now that Connection and ConnectionFactory are in place, internal code should use them instead of HTable constructors as per HBASE-11825.",0,0,neutral
hbase,12059,description,Different versions of hadoop have different annotations. We can smooth this out by providing our own.,design_debt,non-optimal_design,"Tue, 23 Sep 2014 01:04:57 +0000","Fri, 6 Apr 2018 17:54:27 +0000","Wed, 24 Sep 2014 03:18:32 +0000",94415,Different versions of hadoop have different annotations. We can smooth this out by providing our own.,0.1875,0.1875,neutral
hbase,12211,description,"In the hbase-daemon.sh file line 199, it has the content: The variable loglog is defined as: For my understanding, this information should be printed to the ""logout"" variable; we should not mix this ""ulimit"" information with the actual log printed by hbase java program.",design_debt,non-optimal_design,"Thu, 9 Oct 2014 02:15:21 +0000","Fri, 17 Jun 2022 17:40:25 +0000","Thu, 6 Jul 2017 19:00:08 +0000",86546687,"In the hbase-daemon.sh file line 199, it has the content: echo ""`ulimit -a`"" >> $loglog 2>&1 The variable loglog is defined as: logout=$HBASE_LOG_DIR/$HBASE_LOG_PREFIX.out loggc=$HBASE_LOG_DIR/$HBASE_LOG_PREFIX.gc loglog=""${HBASE_LOG_DIR}/${HBASE_LOGFILE}"" For my understanding, this information should be printed to the ""logout"" variable; we should not mix this ""ulimit"" information with the actual log printed by hbase java program.",0.25,0.125,neutral
hbase,12238,comment_0,In standalone mode this is a little disorientating... it shows up frequently:,design_debt,non-optimal_design,"Mon, 13 Oct 2014 02:23:59 +0000","Fri, 6 Apr 2018 17:55:53 +0000","Thu, 30 Oct 2014 04:54:55 +0000",1477856,In standalone mode this is a little disorientating... it shows up frequently:,0,0,negative
hbase,12238,description,"Let me fix a few innocuous exceptions that show on startup (saw testing 0.99.1), even when regular -- will throw people off. Here is one: More to follow...",design_debt,non-optimal_design,"Mon, 13 Oct 2014 02:23:59 +0000","Fri, 6 Apr 2018 17:55:53 +0000","Thu, 30 Oct 2014 04:54:55 +0000",1477856,"Let me fix a few innocuous exceptions that show on startup (saw testing 0.99.1), even when regular  will throw people off. Here is one: More to follow...",-0.5,-0.5,negative
hbase,12238,summary,A few ugly exceptions on startup,design_debt,non-optimal_design,"Mon, 13 Oct 2014 02:23:59 +0000","Fri, 6 Apr 2018 17:55:53 +0000","Thu, 30 Oct 2014 04:54:55 +0000",1477856,A few ugly exceptions on startup,-1,-1,negative
hbase,12271,comment_6,"ExportSnapshot will skip files that are the same (either via checksum or via name+length comparison). We use this to cut down on transfer time. First we snapshot the table, then export that snapshot to the remote DFS. Next run, we instead start by copying the previously exported snapshot into the current snapshot's destination (all on the remote DFS). We then do the snapshot and export dance again, and it dutifully copies only the changed files. Our (fb) HDFS has hard links (last I heard upstream does not) so we can copy backups around pretty cheaply. In the future we may just throw everything into one directory and let the manifest be the decider of what files are involved, but in the mean time it's copy and differential.",design_debt,non-optimal_design,"Wed, 15 Oct 2014 16:59:00 +0000","Fri, 6 Apr 2018 17:54:40 +0000","Wed, 15 Oct 2014 17:46:45 +0000",2865,"ExportSnapshot will skip files that are the same (either via checksum or via name+length comparison). We use this to cut down on transfer time. First we snapshot the table, then export that snapshot to the remote DFS. Next run, we instead start by copying the previously exported snapshot into the current snapshot's destination (all on the remote DFS). We then do the snapshot and export dance again, and it dutifully copies only the changed files. Our (fb) HDFS has hard links (last I heard upstream does not) so we can copy backups around pretty cheaply. In the future we may just throw everything into one directory and let the manifest be the decider of what files are involved, but in the mean time it's copy and differential.",-0.03514285714,-0.03514285714,neutral
hbase,12293,comment_1,"tests should be at info level at the minimum, as in production: if not we will discover in test that we log too much (or worse triggers NPE or stuff like this). For the same reason, I prefer to use the debug level in tests, to be sure that I won't have surprises (NPE) if I try to use them. What I did in the past is reusing the info from the apache build (run time and logs), and looked at the both the log size and the log rate per test to prioritize the tests I was looking at. Then I was just improving the logs around these area.",design_debt,non-optimal_design,"Sun, 19 Oct 2014 19:53:26 +0000","Fri, 17 Jun 2022 17:48:21 +0000","Wed, 29 Oct 2014 18:05:26 +0000",857520,"tests should be at info level at the minimum, as in production: if not we will discover in production/integration test that we log too much (or worse triggers NPE or stuff like this). For the same reason, I prefer to use the debug level in tests, to be sure that I won't have surprises (NPE) if I try to use them. What I did in the past is reusing the info from the apache build (run time and logs), and looked at the both the log size and the log rate per test to prioritize the tests I was looking at. Then I was just improving the logs around these area.",0.2625,0.2625,neutral
hbase,12293,description,"In trying to solve HBASE-12285, it was pointed out that tests are writing too much to output again. At best, this is a sloppy practice and, at worst, it leaves us open to builds breaking when our test tools can't handle the flood. If  would be willing give me a little bit of mentoring on how he dealt with this problem a few years back, I'd be happy to add it to my plate.",design_debt,non-optimal_design,"Sun, 19 Oct 2014 19:53:26 +0000","Fri, 17 Jun 2022 17:48:21 +0000","Wed, 29 Oct 2014 18:05:26 +0000",857520,"In trying to solve HBASE-12285, it was pointed out that tests are writing too much to output again. At best, this is a sloppy practice and, at worst, it leaves us open to builds breaking when our test tools can't handle the flood. If nkeywal would be willing give me a little bit of mentoring on how he dealt with this problem a few years back, I'd be happy to add it to my plate.",-0.07222222222,-0.07222222222,negative
hbase,12293,summary,Tests are logging too much,design_debt,non-optimal_design,"Sun, 19 Oct 2014 19:53:26 +0000","Fri, 17 Jun 2022 17:48:21 +0000","Wed, 29 Oct 2014 18:05:26 +0000",857520,Tests are logging too much,-0.5,-0.5,negative
hbase,12428,comment_8,"the problem is that is doesn't handle the port correctly. If there isn't one specified, it will throw an exception when it tries to cleanup old files. However, we are also doing the ""... unless port"" check in two different places, which is not as clean as it could be. So this just sets up the port in a single place, if its not set via the command line.",design_debt,non-optimal_design,"Tue, 4 Nov 2014 20:56:08 +0000","Fri, 6 Apr 2018 17:54:55 +0000","Thu, 6 Nov 2014 21:04:28 +0000",173300,"lhofhansl the problem is that is doesn't handle the port correctly. If there isn't one specified, it will throw an exception when it tries to cleanup old files. However, we are also doing the ""... unless port"" check in two different places, which is not as clean as it could be. So this just sets up the port in a single place, if its not set via the command line.",-0.0125,-0.0125,negative
hbase,12464,comment_3,"It's not good for meta to stuck in FAILED_OPEN. Agree we should handle it differently. The patch looks good. Just couples things: 1. Can we add a log (info/debug level may be fine) when we reset the retry count to 0? 2. We also need to prevent meta region goes to FAILED_OPEN at method How about FAILED_CLOSE? It should be fine since the meta region is still available? Is this essentially the same as setting maximumAttempts to a huge number? In many cases, a region may not be able to heal automatically without a pill. Personally, I think a better monitoring system could be better in this case.",design_debt,non-optimal_design,"Wed, 12 Nov 2014 18:12:22 +0000","Fri, 6 Apr 2018 17:55:10 +0000","Thu, 20 Nov 2014 22:02:39 +0000",705017,"It's not good for meta to stuck in FAILED_OPEN. Agree we should handle it differently. The patch looks good. Just couples things: 1. Can we add a log (info/debug level may be fine) when we reset the retry count to 0? 2. We also need to prevent meta region goes to FAILED_OPEN at method AssignmentManger#onRegionFailedOpen. How about FAILED_CLOSE? It should be fine since the meta region is still available? Fix#2 (more involved): if a region is in FAILED_OPEN state, we should provide a way to automatically trigger AssignmentManager::assign() after a short period of time (leaving any region in FAILED_OPEN state or other states like 'FAILED_CLOSE' is undesirable, should have some way to retrying and auto-heal the region). Is this essentially the same as setting maximumAttempts to a huge number? In many cases, a region may not be able to heal automatically without a pill. Personally, I think a better monitoring system could be better in this case.",0.3284090909,0.2086538462,negative
hbase,12464,description,"meta table region assignment could reach to the 'FAILED_OPEN' state, which makes the region not available unless the target region server shutdown or manual resolution. This is undesirable state for meta tavle region. Here is the sequence how this could happen (the code is in Step 1: Master detects a region server (RS1) that hosts one meta table region is down, it changes the meta region state from 'online' to 'offline' Step 2: In a loop (with configuable maximumAttempts count, default is 10, and minimal is 1), AssignmentManager tries to find a RS to host the meta table region. If there is no RS available, it would loop forver by resetting the loop count (BUG#1 from this logic - a small bug) Step 3: Once a new RS is found (RS2), inside the same loop as Step 2, AssignmentManager tries to assign the meta region to RS2 (OFFLINE, RS1 = Based on the document ( ), this is by design - ""17. For regions in FAILED_OPEN or FAILED_CLOSE states , the master tries to close them again when they are reassigned by an operator via HBase Shell."". However, this is bad design, espcially for meta table region (it is arguable that the design is good for regular table - for this ticket, I am more focus on fixing the meta region availablity issue). I propose 2 possible fixes: Fix#1 (band-aid change): in Step 3, just like Step 2, if the region is a meta table region, reset the loop count so that it would not leave the loop with meta table region in FAILED_OPEN state. Fix#2 (more involved): if a region is in FAILED_OPEN state, we should provide a way to automatically trigger after a short period of time (leaving any region in FAILED_OPEN state or other states like 'FAILED_CLOSE' is undesirable, should have some way to retrying and auto-heal the region). I think at least for 1.0.0, Fix#1 is good enough. We can open a task-type of JIRA for Fix#2 in future release.",design_debt,non-optimal_design,"Wed, 12 Nov 2014 18:12:22 +0000","Fri, 6 Apr 2018 17:55:10 +0000","Thu, 20 Nov 2014 22:02:39 +0000",705017,"meta table region assignment could reach to the 'FAILED_OPEN' state, which makes the region not available unless the target region server shutdown or manual resolution. This is undesirable state for meta tavle region. Here is the sequence how this could happen (the code is in AssignmentManager#assign()): Step 1: Master detects a region server (RS1) that hosts one meta table region is down, it changes the meta region state from 'online' to 'offline' Step 2: In a loop (with configuable maximumAttempts count, default is 10, and minimal is 1), AssignmentManager tries to find a RS to host the meta table region. If there is no RS available, it would loop forver by resetting the loop count (BUG#1 from this logic - a small bug) Step 3: Once a new RS is found (RS2), inside the same loop as Step 2, AssignmentManager tries to assign the meta region to RS2 (OFFLINE, RS1 => PENDING_OPEN, RS2). If for some reason that opening the region in RS2 failed (eg. the target RS2 is not ready to serve - ServerNotRunningYetException), AssignmentManager would change the state from (PENDING_OPEN, RS2) to (FAILED_OPEN, RS2). then it would retry (and even change the RS server to go to). The retry is up to maximumAttempts. Once the maximumAttempts is reached, the meta region will be in the 'FAILED_OPEN' state, unless either (1). RS2 shutdown to trigger region assignment again or (2). it is reassigned by an operator via HBase Shell. Based on the document ( http://hbase.apache.org/book/regions.arch.html ), this is by design - ""17. For regions in FAILED_OPEN or FAILED_CLOSE states , the master tries to close them again when they are reassigned by an operator via HBase Shell."". However, this is bad design, espcially for meta table region (it is arguable that the design is good for regular table - for this ticket, I am more focus on fixing the meta region availablity issue). I propose 2 possible fixes: Fix#1 (band-aid change): in Step 3, just like Step 2, if the region is a meta table region, reset the loop count so that it would not leave the loop with meta table region in FAILED_OPEN state. Fix#2 (more involved): if a region is in FAILED_OPEN state, we should provide a way to automatically trigger AssignmentManager::assign() after a short period of time (leaving any region in FAILED_OPEN state or other states like 'FAILED_CLOSE' is undesirable, should have some way to retrying and auto-heal the region). I think at least for 1.0.0, Fix#1 is good enough. We can open a task-type of JIRA for Fix#2 in future release.",0.02106060606,0.007477777778,neutral
hbase,1263,description,"As some of us have been discussing, allowing the client to manually set the timestamp of a put breaks the general semantics of versioning and I'd like to see it removed as part of HBASE-880 (a more appropriate place to debate that). However, one trick being used when you don't want the overhead of versions on a frequently updated column (which are only cleared on compactions even if set to 1), was to use the same timestamp. Since that would create an identical key it would just overwrite the value not create a new version. It's a very common use-case, and this hack is being used as part of the committed increment ops from Rather than making a special optimization for counters, an optimization on single-version families that never stores more than one version of a column.",design_debt,non-optimal_design,"Sun, 15 Mar 2009 07:08:10 +0000","Sat, 11 Jun 2022 20:35:01 +0000","Wed, 16 Jul 2014 21:45:45 +0000",168446255,"As some of us have been discussing, allowing the client to manually set the timestamp of a put breaks the general semantics of versioning and I'd like to see it removed as part of HBASE-880 (a more appropriate place to debate that). However, one trick being used when you don't want the overhead of versions on a frequently updated column (which are only cleared on compactions even if set to 1), was to use the same timestamp. Since that would create an identical key it would just overwrite the value not create a new version. It's a very common use-case, and this hack is being used as part of the committed increment ops from HBASE-868/HBASE-1252. Rather than making a special optimization for counters, an optimization on single-version families that never stores more than one version of a column.",0.334375,0.2675,negative
hbase,12673,comment_6,Thanks for the pointer in HMobStore @ 312. The retry there should address the situation I was concerned about. Looking at that code again one other concern comes up -- do you know if line 319 in there will throw another exception if we got the FNFE on line 313? (we'd have an open file instance that got moved  not sure what would happen on close on line 319). Can we change it so that we capture the other exceptions that could be caught there [1] and then try the next location? HFileLink essentially makes this file redirection mechanism transparent and would potentially make the code easier to follow. I'd prefer it if we could use that code so if we find other cases we can just fix it in one centralized place. [1],design_debt,non-optimal_design,"Thu, 11 Dec 2014 03:41:47 +0000","Fri, 17 Jun 2022 05:56:42 +0000","Tue, 3 Mar 2015 02:37:17 +0000",7080930,Thanks for the pointer in HMobStore @ 312. The retry there should address the situation I was concerned about. Looking at that code again one other concern comes up  do you know if line 319 in there will throw another exception if we got the FNFE on line 313? (we'd have an open file instance that got moved  not sure what would happen on close on line 319). Can we change it so that we capture the other exceptions that could be caught there [1] and then try the next location? HFileLink essentially makes this file redirection mechanism transparent and would potentially make the code easier to follow. I'd prefer it if we could use that code so if we find other cases we can just fix it in one centralized place. [1] https://github.com/apache/hbase/blob/hbase-11339/hbase-server/src/main/java/org/apache/hadoop/hbase/io/FileLink.java#L124,0,0,neutral
hbase,12673,comment_9,"Ok, my main ask is to use the wrapping that the FileLink/HFileLink provides when a reader to the file is opened. You don't need to use the funny encoded name sine we aren't in a situation where we have a placeholder file present. We are in a place where we open a file and it could move to the archive. I spent a little bit of time in there and between the StoreFile, StoreFileInfo, Reference and HFileLink it is a bit messy. I'm planning to spend a little bit of time to refactor/clean things up in there so we have only have one mechanism to use. I think the short term solution is to add the other exception checks -- e.g. handle more than just the FNFE and catch these two other exception cases.",design_debt,non-optimal_design,"Thu, 11 Dec 2014 03:41:47 +0000","Fri, 17 Jun 2022 05:56:42 +0000","Tue, 3 Mar 2015 02:37:17 +0000",7080930,"Ok, my main ask is to use the wrapping that the FileLink/HFileLink provides when a reader to the file is opened. You don't need to use the funny encoded name sine we aren't in a situation where we have a placeholder file present. We are in a place where we open a file and it could move to the archive. I spent a little bit of time in there and between the StoreFile, StoreFileInfo, Reference and HFileLink it is a bit messy. I'm planning to spend a little bit of time to refactor/clean things up in there so we have only have one mechanism to use. I think the short term solution is to add the other exception checks  e.g. handle more than just the FNFE and catch these two other exception cases.",0.008333333333,0.008333333333,neutral
hbase,12729,comment_0,This is mildly complicated by the fact we can't change the Public annotated interface HConnection in 0.98. Hacking around with alternatives to find something reasonable.,design_debt,non-optimal_design,"Fri, 19 Dec 2014 19:49:36 +0000","Fri, 20 Nov 2015 11:54:27 +0000","Tue, 20 Jan 2015 01:07:09 +0000",2697453,This is mildly complicated by the fact we can't change the Public annotated interface HConnection in 0.98. Hacking around with alternatives to find something reasonable.,0.271,0.271,negative
hbase,12833,comment_11,"A couple of comments on this: 1) the interface changes and the managed/unmanaged connection issues are separate concerns. 2) On the mailing list you said: ""Yes, the concept of connection caching, aka, managed connections are going away."" If managed connections are going away, then let's make them go away across the board. If they're staying, then let's keep them and rework the interfaces to support managed connections as a first class citizen. I only see disadvantages for inconsistency in this aspect. That said, this can totally be something that I'm missing. Do you see any advantages to keeping a mix of managed and unmanaged connections around?",design_debt,non-optimal_design,"Fri, 9 Jan 2015 20:40:05 +0000","Fri, 6 Apr 2018 17:55:48 +0000","Fri, 16 Jan 2015 19:55:32 +0000",602127,"A couple of comments on this: 1) the interface changes and the managed/unmanaged connection issues are separate concerns. 2) On the mailing list you said: ""Yes, the concept of connection caching, aka, managed connections are going away."" If managed connections are going away, then let's make them go away across the board. If they're staying, then let's keep them and rework the interfaces to support managed connections as a first class citizen. I only see disadvantages for inconsistency in this aspect. That said, this can totally be something that I'm missing. Do you see any advantages to keeping a mix of managed and unmanaged connections around?",0.1821428571,0.1821428571,neutral
hbase,12846,summary,is unstable in 0.98,design_debt,non-optimal_design,"Tue, 13 Jan 2015 06:54:02 +0000","Fri, 17 Jun 2022 18:49:12 +0000","Mon, 26 Jan 2015 01:24:03 +0000",1103401,TestZKLessAMOnCluster is unstable in 0.98,-0.4,-0.4,neutral
hbase,1309,comment_2,"In my opinion HBase should not dictate to users what they can or cannot store into the system. I did not say anything about empty keys, which of course makes no sense. The particular circumstance in this place is a crawling application that stores content indexed by SHA-1 hash but also includes a family 'url:' which stores, for each unique object according to hash, the list of URLs where it was encountered. The URLs are stored as column qualifiers to produce a list of unique values with no need for scanning for duplicate elimination etc. There is no need to store data also because all the data is already in the key.",design_debt,non-optimal_design,"Sun, 5 Apr 2009 02:18:31 +0000","Sun, 13 Sep 2009 22:24:31 +0000","Tue, 7 Apr 2009 18:23:08 +0000",230677,"In my opinion HBase should not dictate to users what they can or cannot store into the system. I did not say anything about empty keys, which of course makes no sense. The particular circumstance in this place is a crawling application that stores content indexed by SHA-1 hash but also includes a family 'url:' which stores, for each unique object according to hash, the list of URLs where it was encountered. The URLs are stored as column qualifiers to produce a list of unique values with no need for scanning for duplicate elimination etc. There is no need to store data also because all the data is already in the key.",0.1692,0.1692,neutral
hbase,13710,description,HttpServer makes use of Hadoop's ReflectionUtil instead of our own. AFAICT it's using 1 extra method. Just copy that one over to our own ReflectionUtil.,design_debt,non-optimal_design,"Tue, 19 May 2015 05:58:23 +0000","Fri, 24 Jun 2022 18:50:45 +0000","Thu, 28 May 2015 05:37:17 +0000",776334,HttpServer makes use of Hadoop's ReflectionUtil instead of our own. AFAICT it's using 1 extra method. Just copy that one over to our own ReflectionUtil.,0,0,neutral
hbase,13905,description,The reruns failed because the test is not idempotent. Perhaps we should have the test startup clean up it's workspace before starting.,design_debt,non-optimal_design,"Mon, 15 Jun 2015 17:09:18 +0000","Mon, 31 Aug 2015 22:39:39 +0000","Tue, 16 Jun 2015 00:42:25 +0000",27187,The reruns failed because the test is not idempotent. Perhaps we should have the test startup clean up it's workspace before starting.,0,0,negative
hbase,13942,comment_0,It is becoming increasingly difficult with having 256 threads/cluster. Is it not possible to reduce this? We are afraid the same issue might prop up in case of drastically reducing the threads.,design_debt,non-optimal_design,"Mon, 22 Jun 2015 05:29:08 +0000","Fri, 24 Jun 2022 19:23:12 +0000","Mon, 13 Feb 2017 19:25:27 +0000",52062979,It is becoming increasingly difficult with having 256 threads/cluster. Is it not possible to reduce this? We are afraid the same issue might prop up in case of drastically reducing the threads.,-0.0435,-0.0435,negative
hbase,14517,description,"In production env, regionservers may be removed from the cluster for hardware problems and rejoined the cluster after the repair. There is a potential risk that the version of rejoined regionserver may diff from others because the cluster has been upgraded through many versions. To solve this, we can show the all regionservers' version in the server list of master's status page, and highlight the regionserver when its version is different from the master's version, similar to HDFS-3245 Suggestions are welcome~",design_debt,non-optimal_design,"Wed, 30 Sep 2015 05:30:06 +0000","Fri, 1 Jul 2022 21:38:21 +0000","Fri, 9 Oct 2015 22:15:05 +0000",837899,"In production env, regionservers may be removed from the cluster for hardware problems and rejoined the cluster after the repair. There is a potential risk that the version of rejoined regionserver may diff from others because the cluster has been upgraded through many versions. To solve this, we can show the all regionservers' version in the server list of master's status page, and highlight the regionserver when its version is different from the master's version, similar to HDFS-3245 Suggestions are welcome~",-0.11,-0.11,neutral
hbase,14604,description,"The code in MoveCoseFunction: It uses cluster.numRegions + META_MOVE_COST_MULT as the max value when scale moveCost to [0,1]. But this should use maxMoves as the max value when cluster have a lot of regions. Assume a cluster have 10000 regions, maxMoves is 2500, it only scale moveCost to [0, 0.25]. Improve moveCost by use maxMoves) as the max cost, so it can scale moveCost to [0,1].",design_debt,non-optimal_design,"Wed, 14 Oct 2015 09:54:05 +0000","Tue, 20 Oct 2015 15:20:36 +0000","Tue, 20 Oct 2015 09:40:16 +0000",517571,"The code in MoveCoseFunction: It uses cluster.numRegions + META_MOVE_COST_MULT as the max value when scale moveCost to [0,1]. But this should use maxMoves as the max value when cluster have a lot of regions. Assume a cluster have 10000 regions, maxMoves is 2500, it only scale moveCost to [0, 0.25]. Improve moveCost by use Math.min(cluster.numRegions, maxMoves) as the max cost, so it can scale moveCost to [0,1].",0.08,0.05714285714,neutral
hbase,1492,description,Make the region split size a table configuration parameter. It would help make a smaller but higher concurrent table split sooner and improve scalability on smaller data sets.,design_debt,non-optimal_design,"Sat, 6 Jun 2009 08:54:43 +0000","Sun, 13 Sep 2009 22:24:41 +0000","Sat, 6 Jun 2009 20:17:01 +0000",40938,Make the region split size a table configuration parameter. It would help make a smaller but higher concurrent table split sooner and improve scalability on smaller data sets.,0.2,0.2,neutral
hbase,14941,comment_5,Can we use begin ensure block to ensure we do not leak it ?,design_debt,non-optimal_design,"Mon, 7 Dec 2015 17:44:41 +0000","Fri, 11 Dec 2015 03:53:34 +0000","Thu, 10 Dec 2015 21:35:48 +0000",273067,Can we use begin ensure block to ensure we do not leak it ?,0.1,0.1,neutral
hbase,15617,comment_1,"The new depends on some internal details likely to change. We should treat ZK state as a black box and avoid relying on it. This is a layering violation, really. Why not use the supported Admin API's to get the live server list to enumerate? That unit test failure appears unrelated.",design_debt,non-optimal_design,"Fri, 8 Apr 2016 16:55:56 +0000","Thu, 19 May 2016 17:52:21 +0000","Thu, 19 May 2016 02:21:41 +0000",3489945,"The new getDrainingServers() depends on some internal details likely to change. We should treat ZK state as a black box and avoid relying on it. This is a layering violation, really. Why not use the supported Admin API's Admin#getClusterStatus to get the live server list to enumerate? That unit test failure appears unrelated.",-0.28,-0.28,negative
hbase,15617,description,"When running in regionserver mode the Canary is expected to probe for service health one time per regionserver during a probe interval. Each time the canary chore fires, we create a which uses (via to enumerate over all table descriptors, find the locations for each table, then assemble the list of regionservers to probe from this result. The list may not contain all live regionservers, if there is a regionserver up but for some reason not serving any regions. To ensure we have the complete list of live regionservers I think it would be better to use and enumerate the live server list returned in the result.",design_debt,non-optimal_design,"Fri, 8 Apr 2016 16:55:56 +0000","Thu, 19 May 2016 17:52:21 +0000","Thu, 19 May 2016 02:21:41 +0000",3489945,"When running in regionserver mode the Canary is expected to probe for service health one time per regionserver during a probe interval. Each time the canary chore fires, we create a RegionServerMonitor, which uses filterRegionServerByName (via getAllRegionServerByName) to enumerate over all table descriptors, find the locations for each table, then assemble the list of regionservers to probe from this result. The list may not contain all live regionservers, if there is a regionserver up but for some reason not serving any regions. To ensure we have the complete list of live regionservers I think it would be better to use Admin#getClusterStatus and enumerate the live server list returned in the result.",0.0875,0.0875,neutral
hbase,15640,comment_0,"Here is small patch which ups our limit to 1M blocks from 100k so less likely we'll hit limit and then if we do, puts in place a message in red with how to address the limit (I couldn't figure what the previous message was trying to say... didn't seem to make sense). This could be better.. but will do for now.",design_debt,non-optimal_design,"Tue, 12 Apr 2016 23:28:31 +0000","Thu, 9 Nov 2017 00:43:41 +0000","Wed, 20 Apr 2016 21:28:00 +0000",683969,"Here is small patch which ups our limit to 1M blocks from 100k so less likely we'll hit limit and then if we do, puts in place a message in red with how to address the limit (I couldn't figure what the previous message was trying to say... didn't seem to make sense). This could be better.. but will do for now.",0.325,0.325,neutral
hbase,15640,description,"I was baffled looking at L1 Cache because it was stuck at 100k blocks and only 'loading' 3G of data according to the UI. When I looked in log, all the numbers looked write in the Cache summary reported in log. Turns out our buckecache UI template will cut off calculating stats at a limit --which is probably what we want -- but the fact that it has done this should be more plain.",design_debt,non-optimal_design,"Tue, 12 Apr 2016 23:28:31 +0000","Thu, 9 Nov 2017 00:43:41 +0000","Wed, 20 Apr 2016 21:28:00 +0000",683969,"I was baffled looking at L1 Cache because it was stuck at 100k blocks and only 'loading' 3G of data according to the UI. When I looked in log, all the numbers looked write in the Cache summary reported in log. Turns out our buckecache UI template will cut off calculating stats at a limit --which is probably what we want  but the fact that it has done this should be more plain.",-0.09444444444,-0.09444444444,negative
hbase,15640,summary,L1 cache doesn't give fair warning that it is showing partial stats only when it hits limit,design_debt,non-optimal_design,"Tue, 12 Apr 2016 23:28:31 +0000","Thu, 9 Nov 2017 00:43:41 +0000","Wed, 20 Apr 2016 21:28:00 +0000",683969,L1 cache doesn't give fair warning that it is showing partial stats only when it hits limit,0.1,0.1,negative
hbase,15704,comment_2,"Was checking the code, but it is not clear whether these classes are actually getting used or not. It seems that you can actually configure in your configuration and in-theory use for archiving table's hfiles. However, I don't think this is getting used much if at all. If not, I would rather get rid of these, instead of putting it in the example package where they will just bit-rot. HBASE-5547 added these, so maybe  or  might give some insight.",design_debt,non-optimal_design,"Mon, 25 Apr 2016 19:09:20 +0000","Fri, 1 Jul 2022 20:14:53 +0000","Thu, 24 Aug 2017 20:32:02 +0000",41995362,"Was checking the code, but it is not clear whether these classes are actually getting used or not. It seems that you can actually configure LongTermArchivingHFileCleaner in your configuration and in-theory use ZKTableArchiveClient for archiving table's hfiles. However, I don't think this is getting used much if at all. If not, I would rather get rid of these, instead of putting it in the example package where they will just bit-rot. HBASE-5547 added these, so maybe jesse_yates or lhofhansl might give some insight.",0,0,neutral
hbase,15707,comment_7,"Yeah, the function has an anonymous class inline so making it exceeding the 150 line limit. I was considering to separate the anonymous class..., thanks .",design_debt,non-optimal_design,"Mon, 25 Apr 2016 23:26:14 +0000","Thu, 9 Nov 2017 00:48:29 +0000","Wed, 27 Apr 2016 20:51:13 +0000",163499,"Yeah, the function has an anonymous class inline so making it exceeding the 150 line limit. I was considering to separate the anonymous class..., thanks ted_yu.",0.2,0.2,neutral
hbase,15835,comment_5,"Curious: It appears that TestFSHLogProvider is (inadvertently, I imagine) instantiating an HBaseTestingUtility with a *null* Configuration (this within the context of kicking off a thread). This might be worthy of further investigation (since it's probably not intended to be this way), but for now I will simply precede my newly inserted logic with a check for a null Configuration, and thus avoid the that my original patch encountered.",design_debt,non-optimal_design,"Mon, 16 May 2016 08:54:16 +0000","Fri, 1 Jul 2022 20:27:59 +0000","Wed, 9 May 2018 14:58:37 +0000",62489061,"Curious: It appears that TestFSHLogProvider is (inadvertently, I imagine) instantiating an HBaseTestingUtility with a null Configuration (this within the context of kicking off a WALPerformanceEvaluation thread). This might be worthy of further investigation (since it's probably not intended to be this way), but for now I will simply precede my newly inserted logic with a check for a null Configuration, and thus avoid the null-pointer-exception that my original patch encountered.",0.096875,0.096875,neutral
hbase,15835,description,"When a MiniCluster is being started with the method (most typically in the context of JUnit testing), if a local HBase instance is already running (or for that matter, another thread with another MiniCluster is already running), the startup will fail with a RuntimeException saying ""HMasterAddress already in use"", referring explicitly to contention for the same default master info port (16010). This problem most recently came up in conjunction with HBASE-14876 and its sub-JIRAs (development of new HBase-oriented Maven archetypes), but this is apparently a known issue to veteran developers, who tend to set up the @BeforeClass sections of their test modules with code similar to the following: A comprehensive solution modeled on this should be put directly into HBaseTestUtility's main constructor, using one of the following options: OPTION 1 (always force random port assignment): OPTION 2 (always force random port assignment if user has not explicitly defined alternate port):",design_debt,non-optimal_design,"Mon, 16 May 2016 08:54:16 +0000","Fri, 1 Jul 2022 20:27:59 +0000","Wed, 9 May 2018 14:58:37 +0000",62489061,"When a MiniCluster is being started with the HBaseTestUtility#startMiniCluster method (most typically in the context of JUnit testing), if a local HBase instance is already running (or for that matter, another thread with another MiniCluster is already running), the startup will fail with a RuntimeException saying ""HMasterAddress already in use"", referring explicitly to contention for the same default master info port (16010). This problem most recently came up in conjunction with HBASE-14876 and its sub-JIRAs (development of new HBase-oriented Maven archetypes), but this is apparently a known issue to veteran developers, who tend to set up the @BeforeClass sections of their test modules with code similar to the following: A comprehensive solution modeled on this should be put directly into HBaseTestUtility's main constructor, using one of the following options: OPTION 1 (always force random port assignment): OPTION 2 (always force random port assignment if user has not explicitly defined alternate port):",0.03,0.03,neutral
hbase,16273,description,Currently store file size is reported in MB and index & bloom filter in kb. There are two problems here. 1) They are calculated from bytes by dividing by 1024 / 1024 and then converted to an int. So for small regions it always reports 0. 2) And this conversion is done per store so even if there are multiple stores and all of them combined would be larger than 1 MB it'd be reported as 0 I'd suggest to count them in bytes and then when displaying them on the RegionServer UI page decide on an appropriate unit to use.,design_debt,non-optimal_design,"Fri, 22 Jul 2016 13:46:06 +0000","Fri, 22 Jul 2016 21:13:33 +0000","Fri, 22 Jul 2016 21:13:33 +0000",26847,Currently store file size is reported in MB and index & bloom filter in kb. There are two problems here. 1) They are calculated from bytes by dividing by 1024 / 1024 and then converted to an int. So for small regions it always reports 0. 2) And this conversion is done per store so even if there are multiple stores and all of them combined would be larger than 1 MB it'd be reported as 0 I'd suggest to count them in bytes and then when displaying them on the RegionServer UI page decide on an appropriate unit to use.,0.07,0.07,negative
hbase,1655,comment_11,"I've submitted another patch, that reflects a different approach taken to improve HTablePool. In the new approach, there will be no static methods at all. Instead, the normal usage of the class will be to instantiate an HTablePool directly and use the getTable and putTable methods to get an HTable and return an HTable to the pool. The pool class will automatically instantiate a new HTable in the case that no HTable for the given table name is in the pool OR if the internal pool for that table name is full (has more elements than its configured max size). The default max size is Integer.MAX_VALUE. While we lose the convenience of calling static methods, we gain a design with less code that is easy to understand, easy to use in an IoC container, and easy to unit test. This patch also includes a small modification to HTable that allows an instantiation to HTable with a null HBaseConfiguration object. This makes it possible to unit test classes such as HTablePool that internally use an HTable without requiring an environment in which the HTable can get and use a connection. Finally, this patch includes modifications to the stargate package reflecting the new HTablePool API. Let me know if this is suitable for inclusion in the project. Thanks to Jon Gray and Stack for their help and input.",design_debt,non-optimal_design,"Tue, 14 Jul 2009 19:52:59 +0000","Wed, 12 Aug 2020 23:02:36 +0000","Fri, 17 Jul 2009 19:26:30 +0000",257611,"I've submitted another patch, HBASE-1655-v3.patch, that reflects a different approach taken to improve HTablePool. In the new approach, there will be no static methods at all. Instead, the normal usage of the class will be to instantiate an HTablePool directly and use the getTable and putTable methods to get an HTable and return an HTable to the pool. The pool class will automatically instantiate a new HTable in the case that no HTable for the given table name is in the pool OR if the internal pool for that table name is full (has more elements than its configured max size). The default max size is Integer.MAX_VALUE. While we lose the convenience of calling static methods, we gain a design with less code that is easy to understand, easy to use in an IoC container, and easy to unit test. This patch also includes a small modification to HTable that allows an instantiation to HTable with a null HBaseConfiguration object. This makes it possible to unit test classes such as HTablePool that internally use an HTable without requiring an environment in which the HTable can get and use a connection. Finally, this patch includes modifications to the stargate package reflecting the new HTablePool API. Let me know if this is suitable for inclusion in the project. Thanks to Jon Gray and Stack for their help and input.",0.2026,0.1870153846,neutral
hbase,1655,comment_6,"- Even though TreeMap uses the comparator rather than the equals method to compare keys, using a byte[] as the key seems to break the contract of a java.util.Map. The last paragraph of the [Map says that impls are free to avoid calling equals() but the containsKey(Object key) method should ""return true if and only if this map contains a mapping for a key k such that (key==null ? k==null : key.equals(k))"". As you mentioned, you'd need a byte[] wrapper in order to be compliant. So a byte[] key will work in a TreeMap (and not in a HashMap), but we'd break the Map contract. Seems better to just use String and HashMap which works well and satisfies the Map contract. - Allowing both the static methods and the public construction would bring me back to the complaint I had originally with this class. :) It becomes confusing for a user trying to quickly understand how he/she is supposed to interact with this class. I guess this can all be explained away with documentation, but, it doesn't feel ""good"". - Also, yes, there would be a problem with re-instantiating the static map multiple times, but this could be prevented by implementing a Singleton pattern so that the static access operates on an internal singleton instance with a member Map rather than a static Map. - I agree that there is no reason to construct HBC's for each invocation. An internal HBC instance can be constructed and reused. - Is there a checkstyle.xml file for HBase? That would make it easy to check my code for formatting problems before submitting patches.",design_debt,non-optimal_design,"Tue, 14 Jul 2009 19:52:59 +0000","Wed, 12 Aug 2020 23:02:36 +0000","Fri, 17 Jul 2009 19:26:30 +0000",257611,"Even though TreeMap uses the comparator rather than the equals method to compare keys, using a byte[] as the key seems to break the contract of a java.util.Map. The last paragraph of the Map API says that impls are free to avoid calling equals() but the containsKey(Object key) method should ""return true if and only if this map contains a mapping for a key k such that (key==null ? k==null : key.equals(k))"". As you mentioned, you'd need a byte[] wrapper in order to be compliant. So a byte[] key will work in a TreeMap (and not in a HashMap), but we'd break the Map contract. Seems better to just use String and HashMap which works well and satisfies the Map contract. Allowing both the static methods and the public construction would bring me back to the complaint I had originally with this class. It becomes confusing for a user trying to quickly understand how he/she is supposed to interact with this class. I guess this can all be explained away with documentation, but, it doesn't feel ""good"". Also, yes, there would be a problem with re-instantiating the static map multiple times, but this could be prevented by implementing a Singleton pattern so that the static access operates on an internal singleton instance with a member Map rather than a static Map. I agree that there is no reason to construct HBC's for each invocation. An internal HBC instance can be constructed and reused. Is there a checkstyle.xml file for HBase? That would make it easy to check my code for formatting problems before submitting patches.",0.1284951531,0.04042724868,neutral
hbase,1655,comment_7,".bq So a byte[] key will work in a TreeMap (and not in a HashMap), but we'd break the Map contract. Seems better to just use String and HashMap which works well and satisfies the Map contract. Looking at the TreeMap equals implementation, yes, it breaks the above contract for Map. We're impure (insert lots of sack cloth and ashes, self-flagellation, etc., here -- smile). We are zealous about using byte [] everywhere. As Jon allows above, client-side, we can relax some. IMO, we should not be allowing public construction and static methods. +1 on Singleton to prevent multiple instantiations. There is no checkstyle (maybe there is one up in hadoop). In general 80 characters per line and two spaces for tab. Anything else we'll take. Don't worry about it too much.",design_debt,non-optimal_design,"Tue, 14 Jul 2009 19:52:59 +0000","Wed, 12 Aug 2020 23:02:36 +0000","Fri, 17 Jul 2009 19:26:30 +0000",257611,".bq So a byte[] key will work in a TreeMap (and not in a HashMap), but we'd break the Map contract. Seems better to just use String and HashMap which works well and satisfies the Map contract. Looking at the TreeMap equals implementation, yes, it breaks the above contract for Map. We're impure (insert lots of sack cloth and ashes, self-flagellation, etc., here  smile). We are zealous about using byte [] everywhere. As Jon allows above, client-side, we can relax some. IMO, we should not be allowing public construction and static methods. +1 on Singleton to prevent multiple instantiations. There is no checkstyle (maybe there is one up in hadoop). In general 80 characters per line and two spaces for tab. Anything else we'll take. Don't worry about it too much.",0.2391153846,0.2391153846,neutral
hbase,1655,summary,Usability improvements to HTablePool,design_debt,non-optimal_design,"Tue, 14 Jul 2009 19:52:59 +0000","Wed, 12 Aug 2020 23:02:36 +0000","Fri, 17 Jul 2009 19:26:30 +0000",257611,Usability improvements to HTablePool,0.5,0.5,neutral
hbase,16789,comment_2,", here are a few points that are discussed: * This is an offline Compaction Tool (CT). Without MR option, CT will compact files for input table/ region/ column family on local node where CT is run. * Current CT, decides on node to run MR jobs based on location of first block of a first file in an input directory. * This can be improved to consider nodes based on last know region assignments with fallback on location of first block of first file in a table/ region/ column family. This will provide better locality. * Even with the improved logic, locality cannot be guaranteed. * So, whether to run with MR and MR job node selection can be determined by code outside of CT or a User. CT will be just responsible for compaction of files for input table/ region/ cf without deciding on MR or node selection for MR. * CT may query/ consider local regions and only compact files belonging to local regions. Workaround with -force option can be provided for the default behavior.",design_debt,non-optimal_design,"Fri, 7 Oct 2016 00:44:02 +0000","Thu, 23 Jun 2022 20:30:29 +0000","Thu, 20 Oct 2016 20:50:08 +0000",1195566,"busbey, here are a few points that are discussed: This is an offline Compaction Tool (CT). Without MR option, CT will compact files for input table/ region/ column family on local node where CT is run. Current CT, decides on node to run MR jobs based on location of first block of a first file in an input directory. This can be improved to consider nodes based on last know region assignments with fallback on location of first block of first file in a table/ region/ column family. This will provide better locality. Even with the improved logic, locality cannot be guaranteed. So, whether to run with MR and MR job node selection can be determined by code outside of CT or a User. CT will be just responsible for compaction of files for input table/ region/ cf without deciding on MR or node selection for MR. CT may query/ consider local regions and only compact files belonging to local regions. Workaround with -force option can be provided for the default behavior.",0.05,0.05,neutral
hbase,16817,comment_1,It might be odd to pass this boolean.. Even withTags we plan to remove. Intentionally it was removed that the oswrite within Cell impl write the length of the cell. Whether the length to be written or no is up to the caller. In case of HFIle write we dont need to write the length. In case of KVCodec we need it. Some other codec may come tomorrow which want to write the length not as an int but as a varint. So IMO it is better to leave it and not club the length write part within write method in Cell. Got ur point of one extra calc but it is ok.,design_debt,non-optimal_design,"Wed, 12 Oct 2016 14:38:08 +0000","Thu, 13 Oct 2016 12:26:13 +0000","Thu, 13 Oct 2016 12:26:13 +0000",78485,"int write(OutputStream out, boolean withTags, boolean withLengthHeader) It might be odd to pass this boolean.. Even withTags we plan to remove. Intentionally it was removed that the oswrite within Cell impl write the length of the cell. Whether the length to be written or no is up to the caller. In case of HFIle write we dont need to write the length. In case of KVCodec we need it. Some other codec may come tomorrow which want to write the length not as an int but as a varint. So IMO it is better to leave it and not club the length write part within write method in Cell. Got ur point of one extra calc but it is ok.",0.0875,0.0875,neutral
hbase,16817,description,"Current we write length header before it is more efficient to write length header inside it, so we only to calculate the length only once.",design_debt,non-optimal_design,"Wed, 12 Oct 2016 14:38:08 +0000","Thu, 13 Oct 2016 12:26:13 +0000","Thu, 13 Oct 2016 12:26:13 +0000",78485,"Current we write length header before KeyValueUtil#oswrite, it is more efficient to write length header inside it, so we only to calculate the length only once.",0,0,neutral
hbase,16856,summary,Exception message in SyncRunner.run() should print currentSequence,design_debt,non-optimal_design,"Mon, 17 Oct 2016 06:22:14 +0000","Tue, 18 Oct 2016 02:49:44 +0000","Mon, 17 Oct 2016 14:24:16 +0000",28922,Exception message in SyncRunner.run() should print currentSequence,0,0,neutral
hbase,17101,description,"As described in the doc (see HBASE-15531), we would like to start with user tables for favored nodes. This task ensures FN does not apply to system tables. System tables are in memory and won't benefit from favored nodes. Since we also maintain FN information for user regions in meta, it helps to keep implementation simpler by ignoring system tables for the first iterations.",design_debt,non-optimal_design,"Tue, 15 Nov 2016 18:35:53 +0000","Thu, 23 Jun 2022 19:12:57 +0000","Tue, 31 Jan 2017 18:59:41 +0000",6654228,"As described in the doc (see HBASE-15531), we would like to start with user tables for favored nodes. This task ensures FN does not apply to system tables. System tables are in memory and won't benefit from favored nodes. Since we also maintain FN information for user regions in meta, it helps to keep implementation simpler by ignoring system tables for the first iterations.",0.064,0.064,neutral
hbase,1723,comment_7,substitute setTimeRange with setTimeStamp in several methods for consistent behavior. Also adjust TestThriftServer.,design_debt,non-optimal_design,"Thu, 30 Jul 2009 08:30:33 +0000","Sat, 11 Jun 2022 21:00:56 +0000","Tue, 19 Jan 2010 18:12:34 +0000",14982121,substitute setTimeRange with setTimeStamp in several methods for consistent behavior. Also adjust TestThriftServer.,0,0,neutral
hbase,17338,comment_10,"Thanks for the reviews Stack & Ram.. I have to fix the failed test as those were having assert on heapOverhead size after cell addition to Memstore. Now we track heapSize not overhead. Will add more comments as Stack asked Regarding Ram's comment. I have different opinion. I dont think we need change this way. We track dataSize (irrespective of cell data in on heap or off heap area).. This dataSize been used at Segment level for in memory flush decision, Region level for on disk flushes and globally to force flush some regions. At the 1st 2 levels, it is not doubt that we have to track all the cell data size together. Now the point Ram says is when we have off heap configured and max off heap global size is say 12 GB, once the data size globally reaches this level, we will force flush some regions. So his point is for this tracking, we have to consider only off heap Cells and on heap Cell's data size should not get accounted in the data size but only in the heapSize. (At global level. But at region and segment level it has to get applied). 2 reasons why I am not in favor of this 1. This makes the impl so complex. We need to add isOffheap check down the layers. Also at 2 layers we have to consider these on heap cell data size and one level not. 2. When off heap is enabled, (We have the MSLAB pool off heap in place), we will end up in having on heap Cells when the pool is out of BBs. We will create on demand LABs on heap. If we dont consider those cell's data size at global level, we may reach forced flush level a bit late. That is the only gain. But here the on demand LAB creation is a sign that the write load is so high and delaying the flush will add more pressure to the MSLAB and more and more on demand BBs (2 MB sized) need to get created. One aim of the off heap work is to reduce the max heap space need for the servers. So lets consider the cell data size globally also (how we do now) and make global flushes. Now even if MSLAB is used, the append/increment use cases wont use MSLAB. The cells will be on heap then. But for such a use case, enabling MSLAB (and pool) is totally waste. That is a mis configuration. More and more on demand BB creation, when MSLAB pool is a bad sign. We have a WARN log for just one time as of now.. May be we should repeat this log at certain intervals. (Like for every 100th pool miss or so.) We should be able to turn MSLAB usage ON/OFF per table also. Now this is possible? Am not sure. These 2 things need to be checked and done in another jiras IMO.",design_debt,non-optimal_design,"Mon, 19 Dec 2016 13:18:49 +0000","Fri, 1 Jul 2022 21:33:56 +0000","Fri, 10 Mar 2017 05:43:36 +0000",6971087,"Thanks for the reviews Stack & Ram.. I have to fix the failed test as those were having assert on heapOverhead size after cell addition to Memstore. Now we track heapSize not overhead. Will add more comments as Stack asked Regarding Ram's comment. I have different opinion. I dont think we need change this way. We track dataSize (irrespective of cell data in on heap or off heap area).. This dataSize been used at Segment level for in memory flush decision, Region level for on disk flushes and globally to force flush some regions. At the 1st 2 levels, it is not doubt that we have to track all the cell data size together. Now the point Ram says is when we have off heap configured and max off heap global size is say 12 GB, once the data size globally reaches this level, we will force flush some regions. So his point is for this tracking, we have to consider only off heap Cells and on heap Cell's data size should not get accounted in the data size but only in the heapSize. (At global level. But at region and segment level it has to get applied). 2 reasons why I am not in favor of this 1. This makes the impl so complex. We need to add isOffheap check down the layers. Also at 2 layers we have to consider these on heap cell data size and one level not. 2. When off heap is enabled, (We have the MSLAB pool off heap in place), we will end up in having on heap Cells when the pool is out of BBs. We will create on demand LABs on heap. If we dont consider those cell's data size at global level, we may reach forced flush level a bit late. That is the only gain. But here the on demand LAB creation is a sign that the write load is so high and delaying the flush will add more pressure to the MSLAB and more and more on demand BBs (2 MB sized) need to get created. One aim of the off heap work is to reduce the max heap space need for the servers. So lets consider the cell data size globally also (how we do now) and make global flushes. Now even if MSLAB is used, the append/increment use cases wont use MSLAB. The cells will be on heap then. But for such a use case, enabling MSLAB (and pool) is totally waste. That is a mis configuration. More and more on demand BB creation, when MSLAB pool is a bad sign. We have a WARN log for just one time as of now.. May be we should repeat this log at certain intervals. (Like for every 100th pool miss or so.) We should be able to turn MSLAB usage ON/OFF per table also. Now this is possible? Am not sure. These 2 things need to be checked and done in another jiras IMO.",-0.0004852941176,-0.0004852941176,negative
hbase,17338,comment_3,"In case of on heap MSLAB, all the size calc doing global memstore data size + heap overhead based. That is why was not adding the cell data size in case of on heap. But I think I can do it bit diff way.. So instead of tracking the data size and heap overhead, we will track cell data size and heap size. Pls see it is not heap overhead alone.. This is the total heap space occupied by memstore(s). I can change the calc accordingly. For on heap MSLAB , this heap space count will be the same as what we have before off heap write path work (ie. It will include all cell data size and overhead part). The new accounting ie. cellDataSize will include only the cell data bytes size part. This will be a subset of the former one then. Will do all necessary changes.. This will be a bigger patch then as I will rename all the related area from heapOverhead to heapSize or so. I believe that way will look cleaner. Thoughts? Append/Increment is not adding cells into MSLAB area.. This is to avoid MSLAB wastage. Say same cell is getting incremented 1000 times and cell key+ value size is 100 bytes. If every increment (add to memstore) cell was added to MSLAB, we will overall take ~100KB MSLAB space whereas only 100 bytes is valid at any point.. All the former cells are getting deleted by the addition of a new cell.. The Cell POJO as such is removed from CSLM. But we can not free up that space in MSLAB.. MSLAB is not designed to do this way. That the chunk allocation and offset allocation within a chunk is serially incremented way happening. We can not mark some in btw space as free and reuse.. That will make things very complex for us.. So to avoid these, the simplest way was to NOT use MSLAB for upsert.",design_debt,non-optimal_design,"Mon, 19 Dec 2016 13:18:49 +0000","Fri, 1 Jul 2022 21:33:56 +0000","Fri, 10 Mar 2017 05:43:36 +0000",6971087,"In case of on heap MSLAB, all the size calc doing global memstore data size + heap overhead based. That is why was not adding the cell data size in case of on heap. But I think I can do it bit diff way.. So instead of tracking the data size and heap overhead, we will track cell data size and heap size. Pls see it is not heap overhead alone.. This is the total heap space occupied by memstore(s). I can change the calc accordingly. For on heap MSLAB , this heap space count will be the same as what we have before off heap write path work (ie. It will include all cell data size and overhead part). The new accounting ie. cellDataSize will include only the cell data bytes size part. This will be a subset of the former one then. Will do all necessary changes.. This will be a bigger patch then as I will rename all the related area from heapOverhead to heapSize or so. I believe that way will look cleaner. Thoughts? need to read on why Append/Increment can't be out in offheap. Append/Increment is not adding cells into MSLAB area.. This is to avoid MSLAB wastage. Say same cell is getting incremented 1000 times and cell key+ value size is 100 bytes. If every increment (add to memstore) cell was added to MSLAB, we will overall take ~100KB MSLAB space whereas only 100 bytes is valid at any point.. All the former cells are getting deleted by the addition of a new cell.. The Cell POJO as such is removed from CSLM. But we can not free up that space in MSLAB.. MSLAB is not designed to do this way. That the chunk allocation and offset allocation within a chunk is serially incremented way happening. We can not mark some in btw space as free and reuse.. That will make things very complex for us.. So to avoid these, the simplest way was to NOT use MSLAB for upsert.",0.1058823529,0.1,neutral
hbase,17338,comment_9,Rest is all good. But as discussed internally that isOffheap() addition to MSLAB is better so that we can really avoid adding the dataSize to the MemstoreSize when the memstore is offheap but still the entire Cell is onheap. Now currently we just account for the dataSize also. We can do in another JIRA. +1 otherwise.,design_debt,non-optimal_design,"Mon, 19 Dec 2016 13:18:49 +0000","Fri, 1 Jul 2022 21:33:56 +0000","Fri, 10 Mar 2017 05:43:36 +0000",6971087,Rest is all good. But as discussed internally that isOffheap() addition to MSLAB is better so that we can really avoid adding the dataSize to the MemstoreSize when the memstore is offheap but still the entire Cell is onheap. Now currently we just account for the dataSize also. We can do in another JIRA. +1 otherwise.,0.2939333333,0.2939333333,neutral
hbase,17338,description,"We have only data size and heap overhead being tracked globally. Off heap memstore works with off heap backed MSLAB pool. But a cell, when added to memstore, not always getting copied to MSLAB. Append/Increment ops doing an upsert, dont use MSLAB. Also based on the Cell size, we sometimes avoid MSLAB copy. But now we track these cell data size also under the global memstore data size which indicated off heap size in case of off heap memstore. For global checks for flushes (against lower/upper watermark levels), we check this size against max off heap memstore size. We do check heap overhead against global heap memstore size (Defaults to 40% of xmx) But for such cells the data size also should be accounted under the heap overhead.",design_debt,non-optimal_design,"Mon, 19 Dec 2016 13:18:49 +0000","Fri, 1 Jul 2022 21:33:56 +0000","Fri, 10 Mar 2017 05:43:36 +0000",6971087,"We have only data size and heap overhead being tracked globally. Off heap memstore works with off heap backed MSLAB pool. But a cell, when added to memstore, not always getting copied to MSLAB. Append/Increment ops doing an upsert, dont use MSLAB. Also based on the Cell size, we sometimes avoid MSLAB copy. But now we track these cell data size also under the global memstore data size which indicated off heap size in case of off heap memstore. For global checks for flushes (against lower/upper watermark levels), we check this size against max off heap memstore size. We do check heap overhead against global heap memstore size (Defaults to 40% of xmx) But for such cells the data size also should be accounted under the heap overhead.",-0.0375,-0.0375,neutral
hbase,17883,comment_12,"I resolved HBASE-19429 by disabling checkstyle reporting in the site build, so, a workaround, not a fix. There's something off about Maven's behavior when executing the site target in general, but we do get a good binary build as a result.",design_debt,non-optimal_design,"Wed, 5 Apr 2017 17:06:11 +0000","Thu, 25 Jan 2018 02:16:37 +0000","Thu, 25 Jan 2018 02:16:37 +0000",25434626,"I resolved HBASE-19429 by disabling checkstyle reporting in the site build, so, a workaround, not a fix. There's something off about Maven's behavior when executing the site target in general, but we do get a good binary build as a result.",0.119,0.119,neutral
hbase,1849,comment_0,"Great issue, stack. Also, we need to consider how to best add additional threading to HTable. Specifically, for which Erik and I are working on right now over in HBASE-1845. We are not at all taking advantage of running batch puts in parallel right now, and it's especially important for MultiGet which could drastically improve performance by distributing the calls in parallel. Additional discussion of the specifics should happen in the other issue, just wanted to link these up.",design_debt,non-optimal_design,"Thu, 17 Sep 2009 05:44:41 +0000","Sat, 11 Jun 2022 21:03:00 +0000","Sat, 19 Jul 2014 00:46:24 +0000",152564503,"Great issue, stack. Also, we need to consider how to best add additional threading to HTable. Specifically, for MultiGet/Put/Delete, which Erik and I are working on right now over in HBASE-1845. We are not at all taking advantage of running batch puts in parallel right now, and it's especially important for MultiGet which could drastically improve performance by distributing the calls in parallel. Additional discussion of the specifics should happen in the other issue, just wanted to link these up.",0.3485466667,0.3485466667,positive
hbase,1849,comment_2,"I've been working on this for the past 2 weeks, although I'm guessing that my solution won't be really satisfactory for this issue. I wrote another HBase client from scratch, and it's been written from the ground up to work well in a multi-threaded environment. I'll open-source it in a few days, stay tuned.",design_debt,non-optimal_design,"Thu, 17 Sep 2009 05:44:41 +0000","Sat, 11 Jun 2022 21:03:00 +0000","Sat, 19 Jul 2014 00:46:24 +0000",152564503,"I've been working on this for the past 2 weeks, although I'm guessing that my solution won't be really satisfactory for this issue. I wrote another HBase client from scratch, and it's been written from the ground up to work well in a multi-threaded environment. I'll open-source it in a few days, stay tuned.",0.2107222222,0.2107222222,negative
hbase,1849,comment_3,some of the original complaints have been fixed. HTablePool does some things. The advice has generally been dont share HTable between threads. The granularity of the locks in HCM were improved and while not all better there are substantial improvements since this issue was filed.,design_debt,non-optimal_design,"Thu, 17 Sep 2009 05:44:41 +0000","Sat, 11 Jun 2022 21:03:00 +0000","Sat, 19 Jul 2014 00:46:24 +0000",152564503,some of the original complaints have been fixed. HTablePool does some things. The advice has generally been dont share HTable between threads. The granularity of the locks in HCM were improved and while not all better there are substantial improvements since this issue was filed.,-0.175,-0.175,positive
hbase,1863,description,"does not support read/write of unknown Writable object (will throw in addition, writing a known Writable object, e.g., HColumnDescriptor, will write the code twice. furthermore, it may be useful to change addToMap from private to public. not causing any problem with hbase, but will be nice to have the above corrected, especially part of the code is already there.",design_debt,non-optimal_design,"Wed, 23 Sep 2009 18:33:22 +0000","Sat, 11 Jun 2022 21:03:14 +0000","Sat, 18 May 2013 20:28:48 +0000",115178126,"o.a.h.h.i.HbaseObjectWritable does not support read/write of unknown Writable object (will throw UnsupportedIoerationException); in addition, writing a known Writable object, e.g., HColumnDescriptor, will write the code twice. furthermore, it may be useful to change addToMap from private to public. not causing any problem with hbase, but will be nice to have the above corrected, especially part of the code is already there.",0.3263888889,0.3263888889,negative
hbase,1863,summary,"HbaseObjectWritable does not support unknown Writable, and writes code twice for known writables",design_debt,non-optimal_design,"Wed, 23 Sep 2009 18:33:22 +0000","Sat, 11 Jun 2022 21:03:14 +0000","Sat, 18 May 2013 20:28:48 +0000",115178126,"HbaseObjectWritable does not support unknown Writable, and writes code twice for known writables",-0.4,-0.4,negative
hbase,18646,description,The default procedure timeout of 60 sec and pool size (1) may be not optimal for large deployements,design_debt,non-optimal_design,"Mon, 21 Aug 2017 22:32:06 +0000","Fri, 1 Jul 2022 20:12:05 +0000","Fri, 1 Sep 2017 17:18:52 +0000",931606,The default procedure timeout of 60 sec and pool size (1) may be not optimal for large deployements,-0.75,-0.75,negative
hbase,1897,description,"Presumption is that WAL is a sequencefile. I just spent some time looking again at my old buddy SF and its kinda heavy-duty for our needs. Do we need the sync bytes it writes into the stream every time you call a sync? Maybe it'd help recovering logs of edits? We don't need the compression by record or block, metadata info, etc. Its also currently unsuited because its sync actually doesn't do a sync on the backing stream: We should move all of our HLog stuff into a wal package and rename the classes as WAL, WALEdit, etc. Splitting code and replay code should reference a WAL.Reader rather than a etc.",design_debt,non-optimal_design,"Fri, 9 Oct 2009 04:48:03 +0000","Sat, 11 Jun 2022 21:03:37 +0000","Fri, 9 Oct 2009 18:38:09 +0000",49806,"Presumption is that WAL is a sequencefile. I just spent some time looking again at my old buddy SF and its kinda heavy-duty for our needs. Do we need the sync bytes it writes into the stream every time you call a sync? Maybe it'd help recovering logs of edits? We don't need the compression by record or block, metadata info, etc. Its also currently unsuited because its sync actually doesn't do a sync on the backing stream: We should move all of our HLog stuff into a wal package and rename the classes as WAL, WALEdit, etc. Splitting code and replay code should reference a WAL.Reader rather than a SequenceFile.Reader, etc.",0.1380952381,0.1208333333,neutral
hbase,19073,description,"- Remove the configuration - Keep following interface since they nicely separate ZK based implementation: ProcedureMemberRpcs - Replace CSM (interface) + BCSM (unnecessary middle hierarchy) with single CSM interface. - Don't pass whole CSM object around (with server in it which gives acess to pretty much everything), only pass the relevant dependencies. Discussion thread on dev@ mailing list.",design_debt,non-optimal_design,"Mon, 23 Oct 2017 22:47:02 +0000","Wed, 1 Aug 2018 06:22:22 +0000","Wed, 25 Oct 2017 03:05:27 +0000",101905,"Remove the configuration hbase.coordinated.state.manager.class Keep following interface since they nicely separate ZK based implementation: SplitLogWorkerCoordination, SplitLogManagerCoordination, ProcedureCoordinatorRpcs, ProcedureMemberRpcs Replace CSM (interface) + BCSM (unnecessary middle hierarchy) with single CSM interface. Don't pass whole CSM object around (with server in it which gives acess to pretty much everything), only pass the relevant dependencies. Discussion thread on dev@ mailing list. http://mail-archives.apache.org/mod_mbox/hbase-dev/201710.mbox/%3CCAAjhxrqjOg90Fdi73kZZe_Gxtrqq8ff%2B%3DAj_epptO_XO812Abg%40mail.gmail.com%3E",0.1905,0.047625,neutral
hbase,19633,description,"In the previous implementation, we can not always cleanly remove all the replication queues when removing a peer since the removing work is done by RS and if an RS is crashed then some queues may left there forever. That's why we need to check if there are already some queues for a newly created peer since we may reuse the peer id and causes problem. With the new procedure based replication peer modification, I think we can do it cleanly. After the are done on all RSes, we can make sure that no RS will create queue for this peer again, then we can iterate over all the queues for all Rses and do another round of clean up.",design_debt,non-optimal_design,"Tue, 26 Dec 2017 13:32:49 +0000","Fri, 24 Jun 2022 19:14:30 +0000","Tue, 2 Jan 2018 02:01:39 +0000",563330,"In the previous implementation, we can not always cleanly remove all the replication queues when removing a peer since the removing work is done by RS and if an RS is crashed then some queues may left there forever. That's why we need to check if there are already some queues for a newly created peer since we may reuse the peer id and causes problem. With the new procedure based replication peer modification, I think we can do it cleanly. After the RefreshPeerProcedures are done on all RSes, we can make sure that no RS will create queue for this peer again, then we can iterate over all the queues for all Rses and do another round of clean up.",-0.025,-0.025,neutral
hbase,1964,description,"When a hadoop/hbase cluster is under heavy load it will inevitably reach a tipping point where data is lost or corrupted. A graceful method is needed to put the cluster into safe mode until more resources can be added or the load on the cluster has been reduced. St.Ack has suggested the following short-term task: ""Meantime, it should be possible to have a cron run a script that checks cluster resources from time-to-time -- e.g. how full hdfs is, how much each regionserver is carrying -- and when it determines the needle is in the red, flip the cluster to be read-only.""",design_debt,non-optimal_design,"Mon, 9 Nov 2009 19:16:05 +0000","Sat, 11 Jun 2022 23:10:39 +0000","Sat, 13 Apr 2013 01:10:01 +0000",108021236,"When a hadoop/hbase cluster is under heavy load it will inevitably reach a tipping point where data is lost or corrupted. A graceful method is needed to put the cluster into safe mode until more resources can be added or the load on the cluster has been reduced. St.Ack has suggested the following short-term task: ""Meantime, it should be possible to have a cron run a script that checks cluster resources from time-to-time  e.g. how full hdfs is, how much each regionserver is carrying  and when it determines the needle is in the red, flip the cluster to be read-only.""",0.01388888889,0.01388888889,neutral
hbase,1968,description,"From a Trend dev team: Their further analysis explains in detail the scenario, which I will summarize here: 1) An invalid put is added to the writeBuffer by put(Put put). It will trigger a once it goes to the region server. 2) At some point, the buffer is flushed. 3) When the invalid put is processed, an exception is thrown. The finally clause of flushCommits() removes all successful puts from the writebuffer list but the failed put remains at the top. This entry becomes an immovable blocker which prevents any subsequent entry from being processed. 4) Subsequent puts will add more entries to the write buffer until the buffer limit is reached, compounding the problem by allowing more edits to be queued which can never be processed. A workaround could be for the client to call getWriteBuffer() -- on trunk -- and remove the entry at the head of the list manually, but without the patch on this issue, the client cannot get access to the list on branch.",design_debt,non-optimal_design,"Tue, 10 Nov 2009 04:53:49 +0000","Fri, 12 Oct 2012 06:13:23 +0000","Tue, 10 Nov 2009 04:56:59 +0000",190,"From a Trend dev team: When insert rows into one table by calling the method public synchronized void put(final Put put), if the column family of one row does not exist, the insert operation will failed and throw NoSuchColumnFamilyException.. We observed that all the following insert operation will fails even though all of them have valid column family. That is one exception of insert operation can cause failure of all the following insert operation. Their further analysis explains in detail the scenario, which I will summarize here: 1) An invalid put is added to the writeBuffer by put(Put put). It will trigger a NoSuchColumnFamilyException once it goes to the region server. 2) At some point, the buffer is flushed. 3) When the invalid put is processed, an exception is thrown. The finally clause of flushCommits() removes all successful puts from the writebuffer list but the failed put remains at the top. This entry becomes an immovable blocker which prevents any subsequent entry from being processed. 4) Subsequent puts will add more entries to the write buffer until the buffer limit is reached, compounding the problem by allowing more edits to be queued which can never be processed. A workaround could be for the client to call getWriteBuffer()  on trunk  and remove the entry at the head of the list manually, but without the patch on this issue, the client cannot get access to the list on branch.",0.103125,0.0025,neutral
hbase,19862,description,"We have temporary (added in HBASE-19007) and concept of CoreCoprocessors which require that whichever they get, it should also implement This test builds mock RegionCpEnv for TokenProvider but it falls short of what's expected and results in following exceptions in test logs Patch adds the missing interface to the mock. Also, uses Mockito to mock the interfaces rather the crude way.",design_debt,non-optimal_design,"Thu, 25 Jan 2018 22:01:27 +0000","Wed, 21 Mar 2018 22:23:45 +0000","Fri, 26 Jan 2018 08:27:33 +0000",37566,"We have temporary HasRegionServerServices (added in HBASE-19007) and concept of CoreCoprocessors which require that whichever *CoprocessorEnvironment they get, it should also implement HasRegionServerServices. This test builds mock RegionCpEnv for TokenProvider (RegionCoprocessor), but it falls short of what's expected and results in following exceptions in test logs Patch adds the missing interface to the mock. Also, uses Mockito to mock the interfaces rather the crude way.",-0.35,-0.2333333333,neutral
hbase,1990,comment_10,"The concept of is good, but unfortunately it might not work even for simpler use cases. Let's discuss the following example: The above example forces you to pick a data type for values at the instantiation of the Put object. But in most cases (at least in our software) we have different data types in a row such as Long, String, Custom Object etc. Even a typical relational database table always have multiple data types in a row. If you exclude the value and keep the value as byte array, it should be sufficient for 80% of the use cases. (Even though we have many columns where the column name is not a string, they are a minority)",design_debt,non-optimal_design,"Wed, 18 Nov 2009 21:48:59 +0000","Sat, 11 Jun 2022 23:15:15 +0000","Mon, 12 May 2014 00:40:37 +0000",141274298,"The concept of TestHTableGenerics.java is good, but unfortunately it might not work even for simpler use cases. Let's discuss the following example: The above example forces you to pick a data type for values at the instantiation of the Put object. But in most cases (at least in our software) we have different data types in a row such as Long, String, Custom Object etc. Even a typical relational database table always have multiple data types in a row. If you exclude the value and keep the value as byte array, it should be sufficient for 80% of the use cases. (Even though we have many columns where the column name is not a string, they are a minority)",-0.1171388889,-0.1004047619,neutral
hbase,19969,comment_5,One more comment about the new test : You can store the return value from in a variable. The count would not change in between the log and the assertion. I looped the following tests locally which passed:,design_debt,non-optimal_design,"Fri, 9 Feb 2018 22:57:42 +0000","Wed, 28 Mar 2018 02:50:38 +0000","Sat, 10 Mar 2018 17:49:55 +0000",2487133,One more comment about the new test : You can store the return value from TEST_UTIL.countRows() in a variable. The count would not change in between the log and the assertion. I looped the following tests locally which passed:,0,0,neutral
hbase,19969,description,"Some file system operations are not fault tolerant during merge. We delete backup data in a backup file system, then copy new data over to backup destination. Deletes can be partial, copy can fail as well",design_debt,non-optimal_design,"Fri, 9 Feb 2018 22:57:42 +0000","Wed, 28 Mar 2018 02:50:38 +0000","Sat, 10 Mar 2018 17:49:55 +0000",2487133,"Some file system operations are not fault tolerant during merge. We delete backup data in a backup file system, then copy new data over to backup destination. Deletes can be partial, copy can fail as well",-0.09483333333,-0.09483333333,negative
hbase,19969,summary,Improve fault tolerance in backup merge operation,design_debt,non-optimal_design,"Fri, 9 Feb 2018 22:57:42 +0000","Wed, 28 Mar 2018 02:50:38 +0000","Sat, 10 Mar 2018 17:49:55 +0000",2487133,Improve fault tolerance in backup merge operation,0.4,0.4,neutral
hbase,20100,comment_4,Linked to a follow-on issue. The AssignProcedure is too coarsely grained (feedback from  that makes sense). HBASE-20103 is about breaking it up to do finer steps. Will do in follow-on.,design_debt,non-optimal_design,"Tue, 27 Feb 2018 19:59:19 +0000","Tue, 7 May 2019 16:08:43 +0000","Thu, 1 Mar 2018 16:55:27 +0000",161768,Linked to a follow-on issue. The AssignProcedure is too coarsely grained (feedback from Apache9 that makes sense). HBASE-20103 is about breaking it up to do finer steps. Will do in follow-on.,-0.40625,-0.40625,neutral
hbase,20108,comment_5,"looks like jline shows up in the modules now. the rest of the approach looks really good, though! I assume this works both in dev tree and out of a bin tarball?",design_debt,non-optimal_design,"Wed, 28 Feb 2018 22:01:52 +0000","Wed, 1 Aug 2018 06:23:17 +0000","Wed, 7 Mar 2018 21:01:46 +0000",601194,"looks like jline shows up in the shaded-client/mapreduce modules now. the rest of the approach looks really good, though! I assume this works both in dev tree and out of a bin tarball?",0.4586666667,0.4586666667,positive
hbase,20302,description,"when catalogJanitor is disabled it has various condition for which it can be true to disable CatalogJanitor and sends this message ""CatalogJanitor disabled! Not running scan."" Since this is an async thread, it is difficult to identify what is the exact reason for it disable. We could log all conditions alongwith disabled! Not running scan."") for better debugging.",design_debt,non-optimal_design,"Wed, 28 Mar 2018 11:54:56 +0000","Fri, 1 Feb 2019 20:05:01 +0000","Thu, 29 Mar 2018 17:45:03 +0000",107407,"when catalogJanitor is disabled it has various condition for which it can be true to disableCatalogJanitor and sends this message ""CatalogJanitor disabled! Not running scan."" Since this is an async thread, it is difficult to identify what is the exact reason for it disable. We could log all conditions alongwithLOG.warn(""CatalogJanitor disabled! Not running scan."") for better debugging.",0.1969166667,0.1116428571,negative
hbase,20595,comment_0,"The issue I was thinking of was HBASE-20500 which maintains one server in the 'default' group, but that is not the full scope of what we should have. We should guarantee the placement, specifically, of ""special tables"" into a rsgroup that must always have a nonzero number of servers, and not assume that will be the 'default' group. In fact I think we should have two default rsgroups, very similar to how we do namespacing: a ""default"" group into which goes all user level stuff not otherwise specified; and a system group into which goes system/special tables (in namespace terms, akin to the 'hbase' namespace). Special tables should not be allowed to move into rsgroups for user tables.",design_debt,non-optimal_design,"Wed, 16 May 2018 21:37:08 +0000","Mon, 22 Apr 2019 16:10:14 +0000","Wed, 23 May 2018 18:58:44 +0000",595296,"The issue I was thinking of was HBASE-20500 which maintains one server in the 'default' group, but that is not the full scope of what we should have. We should guarantee the placement, specifically, of ""special tables"" into a rsgroup that must always have a nonzero number of servers, and not assume that will be the 'default' group. In fact I think we should have two default rsgroups, very similar to how we do namespacing: a ""default"" group into which goes all user level stuff not otherwise specified; and a system group into which goes system/special tables (in namespace terms, akin to the 'hbase' namespace). Special tables should not be allowed to move into rsgroups for user tables.",-0.01875,-0.01875,neutral
hbase,20595,description,"Regionserver groups needs to specially handle what it calls ""special tables"", tables upon which core or other modular functionality depends. They need to be excluded from normal rsgroup processing during bootstrap to avoid circular dependencies or errors due to insufficiently initialized state. I think we also want to ensure that such tables are always given a rsgroup assignment with nonzero servers. (IIRC another issue already raises that point, we can link it later.) Special tables include: * The system tables in the 'hbase:' namespace * The ACL table if the AccessController coprocessor is installed * The Labels table if the coprocessor is installed * The Quotas table if the FS quotas feature is active Either we need a facility where ""special tables"" can be registered, which should be in core. Or, we institute a blanket rule that core and all extensions that need a ""special table"" must put them into the 'hbase:' namespace, so the test will return TRUE for all, and then rsgroups simply needs to test for that.",design_debt,non-optimal_design,"Wed, 16 May 2018 21:37:08 +0000","Mon, 22 Apr 2019 16:10:14 +0000","Wed, 23 May 2018 18:58:44 +0000",595296,"Regionserver groups needs to specially handle what it calls ""special tables"", tables upon which core or other modular functionality depends. They need to be excluded from normal rsgroup processing during bootstrap to avoid circular dependencies or errors due to insufficiently initialized state. I think we also want to ensure that such tables are always given a rsgroup assignment with nonzero servers. (IIRC another issue already raises that point, we can link it later.) Special tables include: The system tables in the 'hbase:' namespace The ACL table if the AccessController coprocessor is installed The Labels table if the VisibilityController coprocessor is installed The Quotas table if the FS quotas feature is active Either we need a facility where ""special tables"" can be registered, which should be in core. Or, we institute a blanket rule that core and all extensions that need a ""special table"" must put them into the 'hbase:' namespace, so the TableName#isSystemTable() test will return TRUE for all, and then rsgroups simply needs to test for that.",0.1026666667,0.1026666667,neutral
hbase,20693,comment_1,"This patch takes care of point 1. Point 2 is optional. I think we should do it as its always a good idea to break up jsp s into header/footers. Also as the rest/thrift UIs grow more complex, we will have to do it someday. Keeping it open for now, will do it if asked for.",design_debt,non-optimal_design,"Wed, 6 Jun 2018 20:20:51 +0000","Wed, 8 Jan 2025 12:30:17 +0000","Fri, 27 Sep 2024 12:55:43 +0000",199125292,"This patch takes care of point 1. Point 2 is optional. I think we should do it as its always a good idea to break up jsp s into header/footers. Also as the rest/thrift UIs grow more complex, we will have to do it someday. Keeping it open for now, will do it if asked for.",0.2352,0.2352,neutral
hbase,2085,description,Some references in toString() converted from StringBuffer to StringBuilder as concurrency is probably not needed in those contexts as the references do not get out of scope.,design_debt,non-optimal_design,"Thu, 31 Dec 2009 20:44:43 +0000","Fri, 20 Nov 2015 13:01:56 +0000","Thu, 31 Dec 2009 22:34:35 +0000",6592,Some references in toString() converted from StringBuffer to StringBuilder as concurrency is probably not needed in those contexts as the references do not get out of scope.,0,0,neutral
hbase,20975,comment_8,"The failed UTs are related, upload a patch to fix it. My fix reveals a bug which exists all the time... the acquire and release lock logic in procedure is a bit of mess here... For different procedures, we have holdLock() set to true of false, for procedures which holdLock() == true, we need to override hasLock() method to test whether we have the lock... when executing procedure, we acquire the lock and release it immediately. And when the procedure is success, we release the lock again in It works, since for procedure with holdLock = true, we call releaseLock() with force = false the first time, the lock won't release here, but in where we call releaseLock() with force = true, the lock will be released there. For procedure with holdLock = false, it also works since when the first time we call releaseLock() with force = false, the lock will release, and the second time we call relseaseLock in we only release it lock if procedure with holdLock=true. So releasing lock won't called twice But when rolling back, we acquire the lock but only release the lock for procedure with holdLock = true... *Rolling back for procedures with holdLock=false(e.g. most of the Table Procedures) will never release the lock...*",design_debt,non-optimal_design,"Mon, 30 Jul 2018 07:56:06 +0000","Tue, 14 Aug 2018 11:44:47 +0000","Mon, 13 Aug 2018 12:24:30 +0000",1225704,"The failed UTs are related, upload a patch to fix it. My fix reveals a bug which exists all the time... the acquire and release lock logic in procedure is a bit of mess here... For different procedures, we have holdLock() set to true of false, for procedures which holdLock() == true, we need to override hasLock() method to test whether we have the lock... when executing procedure, we acquire the lock and release it immediately. And when the procedure is success, we release the lock again in execCompletionCleanup()... It works, since for procedure with holdLock = true, we call releaseLock() with force = false the first time, the lock won't release here, but in execCompletionCleanup, where we call releaseLock() with force = true, the lock will be released there. For procedure with holdLock = false, it also works since when the first time we call releaseLock() with force = false, the lock will release, and the second time we call relseaseLock in execCompletionCleanup(), we only release it lock if procedure with holdLock=true. So releasing lock won't called twice But when rolling back, we acquire the lock but only release the lock for procedure with holdLock = true... Rolling back for procedures with holdLock=false(e.g. most of the Table Procedures) will never release the lock...",0.08738611111,0.1048633333,negative
hbase,20990,comment_1,"Then you need to record the exceptions in the memory and send them back to master when reporting. The sync RPC call become a async one, what if the RS restarts before sending this info. The procedure in master even don't know whether the open/close procedure is executing, whether a RPC retry is needed.",design_debt,non-optimal_design,"Wed, 1 Aug 2018 03:40:40 +0000","Thu, 21 Nov 2019 13:59:28 +0000","Thu, 21 Nov 2019 13:59:28 +0000",41249928,"I prefer not returning anything when calling executeProcedure, instead, using reportRegionTransition and reportProcedureResult to send back the response... Then you need to record the exceptions in the memory and send them back to master when reporting. The sync RPC call become a async one, what if the RS restarts before sending this info. The procedure in master even don't know whether the open/close procedure is executing, whether a RPC retry is needed.",0.5,0.5,neutral
hbase,20990,description,"In AMv2, we batch open/close region operations and call RS with executeProcedures API. But, in this API, if one of the region's operations throws an exception, all the operations in the batch will receive the same exception. Actually, some of the operations in the batch is executing normally in the RS. I think we should try catch exceptions respectively, and call remoteCallFailed or remoteCallCompleted in respectively. Otherwise, there will be some very strange behave. Such as this one: The AssignProcedure failed with a what??? It is very strange, actually, the AssignProcedure successes on the RS, another CloseRegion operation failed in the operation batch was causing the exception. To correct this, we need to modify the response of executeProcedures API, which is the proto, to return infos(status, exceptions) per operation. This issue alone won't cause much trouble, so not so hurry to change the behave here, but indeed we need to consider this one when we want do some reconstruct to AMv2.",design_debt,non-optimal_design,"Wed, 1 Aug 2018 03:40:40 +0000","Thu, 21 Nov 2019 13:59:28 +0000","Thu, 21 Nov 2019 13:59:28 +0000",41249928,"In AMv2, we batch open/close region operations and call RS with executeProcedures API. But, in this API, if one of the region's operations throws an exception, all the operations in the batch will receive the same exception. Actually, some of the operations in the batch is executing normally in the RS. I think we should try catch exceptions respectively, and call remoteCallFailed or remoteCallCompleted in RegionTransitionProcedure respectively. Otherwise, there will be some very strange behave. Such as this one: The AssignProcedure failed with a NotServingRegionException, what??? It is very strange, actually, the AssignProcedure successes on the RS, another CloseRegion operation failed in the operation batch was causing the exception. To correct this, we need to modify the response of executeProcedures API, which is the ExecuteProceduresResponse proto, to return infos(status, exceptions) per operation. This issue alone won't cause much trouble, so not so hurry to change the behave here, but indeed we need to consider this one when we want do some reconstruct to AMv2.",-0.04492592593,-0.04492592593,neutral
hbase,21160,comment_1,"Since the catch block currently re-throws IOException, that means the catch block is no longer needed. Please run with the change locally before attaching patch. Thanks",design_debt,non-optimal_design,"Thu, 6 Sep 2018 10:56:25 +0000","Tue, 18 Sep 2018 13:34:47 +0000","Mon, 17 Sep 2018 15:25:42 +0000",966557,"Since the catch block currently re-throws IOException, that means the catch block is no longer needed. Please run with the change locally before attaching patch. Thanks",0.1333333333,0.1333333333,neutral
hbase,21160,comment_2,Hi I found so many re-throws blocks in the file of . Should we resolve it all?,design_debt,non-optimal_design,"Thu, 6 Sep 2018 10:56:25 +0000","Tue, 18 Sep 2018 13:34:47 +0000","Mon, 17 Sep 2018 15:25:42 +0000",966557,Hi yuzhihong@gmail.com I found so many re-throws blocks in the file of TestVisibilityLabelsWithDeletes.java . Should we resolve it all?,0.1,0.06666666667,neutral
hbase,21173,comment_2,"Thanks for working on this JIRA, . In previous discussion, I thought calling on closed/null region is no harm, while deleting the duplicate close may make some tests unhappy. So we were not very strict to make region close only once. Good discussion to revisit this. - The last one in was added after HBASE-21138 and can be removed here. - The in was followed by the assertion to verify that the .regioninfo file is still there. I saw the close-and-assertion happens in the same test method multiple times so I was not sure we could remove the close here. - In {{testSequenceId}} and the pattern in this patch, i.e. ""{{region.close() && region = null}}"", is not correct. The reason is that, it makes the in {{teardown()}} a no-op, leaving WAL not closed. One fix is to not set the null value and leave the test as-is; a better one I think is as suggested, we can replace the {{region.close()}} with and set {{this.region}} null value. - Other places to set {{this.region}} null value after is good to explicitly make the in {{tearDown}} a no-op.",design_debt,non-optimal_design,"Sat, 8 Sep 2018 14:28:39 +0000","Fri, 1 Feb 2019 20:12:31 +0000","Tue, 11 Sep 2018 09:52:38 +0000",242639,"Thanks for working on this JIRA, andrewcheng. In previous discussion, I thought calling HTU.closeRegionAndWAL() on closed/null region is no harm, while deleting the duplicate close may make some tests unhappy. So we were not very strict to make region close only once. Good discussion to revisit this. The last one in testBulkLoadReplicationEnabled() was added after HBASE-21138 and can be removed here. The HTU.closeRegionAndWAL() in testRegionInfoFileCreation() was followed by the assertion to verify that the .regioninfo file is still there. I saw the close-and-assertion happens in the same test method multiple times so I was not sure we could remove the close here. In testSequenceId and testCloseCarryingSnapshot, the pattern in this patch, i.e. ""region.close() && region = null"", is not correct. The reason is that, it makes the HTU.closeRegionAndWAL() in teardown() a no-op, leaving WAL not closed. One fix is to not set the null value and leave the test as-is; a better one I think is as yuzhihong@gmail.com suggested, we can replace the region.close() with HTU.closeRegionAndWAL(), and set this.region null value. Other places to set this.region null value after HTU.closeRegionAndWAL() is good to explicitly make the HTU.closeRegionAndWAL() in tearDown a no-op.",0.1465,0.07990909091,neutral
hbase,21238,description,Correct way of handling error condition is through return value of run method.,design_debt,non-optimal_design,"Wed, 26 Sep 2018 15:17:05 +0000","Tue, 16 Oct 2018 13:29:39 +0000","Mon, 15 Oct 2018 20:17:39 +0000",1659634,Correct way of handling error condition is through return value of run method.,0.2375,0.2375,neutral
hbase,21816,description,"User may get confused, to understanding our HBase configurations which are loaded for replication. Sometimes, User may place source and destination cluster conf under ""/etc/hbase/conf"" directory. It will create uncertainty because our log points that all the configurations are co-located. Existing Logs, But it should be something like, This jira only to change the log-line, no issue with the functionality.",design_debt,non-optimal_design,"Fri, 1 Feb 2019 02:03:31 +0000","Sun, 12 May 2019 19:44:01 +0000","Fri, 8 Feb 2019 05:16:04 +0000",616353,"User may get confused, to understanding our HBase configurations which are loaded for replication. Sometimes,User mayplace source and destination clusterconf under ""/etc/hbase/conf"" directory. It willcreate uncertainty because our log pointsthat all the configurations are co-located.  Existing Logs, But it should be something like,  Thisjira only to change the log-line, no issuewith the functionality.",-0.1,-0.1,neutral
hbase,22174,summary,Remove error prone from our precommit javac check,design_debt,non-optimal_design,"Sat, 6 Apr 2019 01:33:08 +0000","Sat, 23 Nov 2019 01:48:24 +0000","Sat, 4 May 2019 12:58:42 +0000",2460334,Remove error prone from our precommit javac check,-0.4,-0.4,neutral
hbase,22193,comment_1,"Talked with  offline, the problem here is not the retry number, but the retry interval. When a region is failed open, we will try to reassign it ASAP, the intention here is to make the region online soon. But sometimes, the region can not online on any RS because of config error or some other problems, then it is not a good idea to retry immediately as it will lead to so many proc wals... So the first thing is to detect this problem and increase the retry interval... And for a long term solution, I think we need to find out a way to better deal with config error. For now, the will hang there forever and the only way is to use HBCK2 to bypass the procedure and fix the table state, which is a bit difficult. For hbase version before 2.0, I think there is a straight forward way to fix this is to disable the table, fix the schema, and enable it again...",design_debt,non-optimal_design,"Tue, 9 Apr 2019 03:21:48 +0000","Tue, 30 Apr 2019 13:15:19 +0000","Sat, 13 Apr 2019 03:17:46 +0000",345358,"Talked with zghaobac offline, the problem here is not the retry number, but the retry interval. When a region is failed open, we will try to reassign it ASAP, the intention here is to make the region online soon. But sometimes, the region can not online on any RS because of config error or some other problems, then it is not a good idea to retry immediately as it will lead to so many proc wals... So the first thing is to detect this problem and increase the retry interval... And for a long term solution, I think we need to find out a way to better deal with config error. For now, the ModifyTableProcedure will hang there forever and the only way is to use HBCK2 to bypass the procedure and fix the table state, which is a bit difficult. For hbase version before 2.0, I think there is a straight forward way to fix this is to disable the table, fix the schema, and enable it again...",-0.2019,-0.2019,neutral
hbase,22400,comment_1,Please ignore my RR. I was just fed up with the ugly adapter code and can't wait to remove them. :) Please continue.,design_debt,non-optimal_design,"Sun, 12 May 2019 07:16:29 +0000","Wed, 2 Oct 2019 17:12:32 +0000","Sat, 18 May 2019 06:14:38 +0000",514689,Please ignore my RR. I was just fed up with the ugly adapter code and can't wait to remove them. Please continue.,-0.1083333333,-0.1416666667,negative
hbase,22707,comment_0,"Interesting. We need something like this. I like that you hook it into assigns. I'm wary though of re-use of joinCluster. The messaging in logs will look strange. Will say stuff like 'Joining cluster...' and 'Waiting for RegionServers to join;....'. Then we re-add chores, do the unnecessary wait on RS. Should we just add a method that gets the new row in hbase:meta and then does what call does?",design_debt,non-optimal_design,"Wed, 17 Jul 2019 17:04:59 +0000","Tue, 7 Apr 2020 22:45:30 +0000","Fri, 2 Aug 2019 10:47:20 +0000",1359741,"Interesting. We need something like this. I like that you hook it into assigns. I'm wary though of re-use of joinCluster. The messaging in logs will look strange. Will say stuff like 'Joining cluster...' and 'Waiting for RegionServers to join;....'. Then we re-add chores, do the unnecessary wait on RS. Should we just add a method that gets the new row in hbase:meta and then does what loadMeta#visitRegionState call does?",0.1055555556,0.1055555556,neutral
hbase,22707,comment_1,"Yeah, using _joinCluster_ was a bit too much of an attempt of reusing existing code without changing it as much as possible, but I agree with all the side effects you had pointed out, . Had done some refactoring to re-use some of the adding a with the changes. Let me know what you think, if you feel this PR approach is fine, I will work on some UTs for it.",design_debt,non-optimal_design,"Wed, 17 Jul 2019 17:04:59 +0000","Tue, 7 Apr 2020 22:45:30 +0000","Fri, 2 Aug 2019 10:47:20 +0000",1359741,"Yeah, usingjoinClusterwas a bit too much of an attempt of reusing existing code without changing it as much as possible, but I agree with all the side effects you had pointed out, stack. Had done some refactoring to re-use some of the loadMeta#visitRegionState, adding aPR with the changes. Let me know what you think, if you feel this PR approach is fine, I will work on some UTs for it.",0.1166666667,0.1166666667,neutral
hbase,22933,description,"The old implementation is a bit strange, the isStuck method is like this: It can only return true when there are ongoing procedures. But if we have a procedure, then the procedure will try to reassign region. Scheduling a new procedure does not make sense here, at least for branch-2.2+. I suggest we just remove the related code, since the default retry number for assigning a region is Integer.MAX_VALUE. And even if user set this to small value and finally the region is left in FAILED_OPEN state without a procedure, HBCK2 is used to deal with this, it is not necessary to deal it automatically.",design_debt,non-optimal_design,"Tue, 27 Aug 2019 13:34:03 +0000","Mon, 2 Sep 2019 09:03:19 +0000","Fri, 30 Aug 2019 03:21:32 +0000",222449,"The old implementation is a bit strange, the isStuck method is like this: It can only return true when there are ongoing procedures. But if we have a procedure, then the procedure will try to reassign region. Scheduling a new procedure does not make sense here, at least for branch-2.2+. I suggest we just remove the related code, since the default retry number for assigning a region is Integer.MAX_VALUE. And even if user set this to small value and finally the region is left in FAILED_OPEN state without a procedure, HBCK2 is used to deal with this, it is not necessary to deal it automatically.",-0.03764285714,-0.03764285714,negative
hbase,2295,comment_6,"@Dhruba If you want to use a SortedSet and pass a comparator instead, so you can retain byte [] elements, there is a bytes comparator here: Otherwise, patch looks good. Regards TRUNK, its been mavenized so to build it, you need maven -- see -- and then to build and run tests do something like mvn install",design_debt,non-optimal_design,"Fri, 5 Mar 2010 23:31:34 +0000","Fri, 20 Nov 2015 12:43:37 +0000","Tue, 9 Mar 2010 17:11:23 +0000",322789,"@Dhruba If you want to use a SortedSet and pass a comparator instead, so you can retain byte [] elements, there is a bytes comparator here: http://hadoop.apache.org/hbase/docs/r0.20.3/api/org/apache/hadoop/hbase/util/Bytes.html#BYTES_COMPARATOR Otherwise, patch looks good. Regards TRUNK, its been mavenized so to build it, you need maven  see http://wiki.apache.org/hadoop/Hbase/MavenPrimer  and then to build and run tests do something like mvn install",0.0585,0.0585,positive
hbase,23061,description,We are using Jackson to emit JSON in at least one place in common and client. We don't need all of Jackson and all the associated trouble just to do that. Use a suitably licensed JSON library with no known vulnerability. This will avoid problems downstream because we are trying to avoid having them pull in a vulnerable Jackson via us so Jackson is a 'provided' scope transitive dependency of client and its in-project dependencies (like common). Here's where I am referring to:,design_debt,non-optimal_design,"Sat, 21 Sep 2019 00:53:27 +0000","Fri, 24 Jun 2022 17:55:18 +0000","Sat, 21 Sep 2019 03:28:56 +0000",9329,We are using Jackson to emit JSON in at least one place in common and client. We don't need all of Jackson and all the associated trouble just to do that. Use a suitably licensed JSON library with no known vulnerability. This will avoid problems downstream because we are trying to avoid having them pull in a vulnerable Jackson via us so Jackson is a 'provided' scope transitive dependency of client and its in-project dependencies (like common). Here's where I am referring to: org.apache.hadoop.hbase.util.JsonMapper.<clinit>(JsonMapper.java:37) at org.apache.hadoop.hbase.client.Operation.toJSON(Operation.java:70) at org.apache.hadoop.hbase.client.Operation.toString(Operation.java:96),0.1749,0.03363461538,neutral
hbase,23651,description,"HBASE-17178 Add region balance throttling, but it can not be disabled, sometimes we need no throttle and balance the cluster as fast as possible.",design_debt,non-optimal_design,"Mon, 6 Jan 2020 07:51:00 +0000","Fri, 10 Jan 2020 09:21:20 +0000","Wed, 8 Jan 2020 11:08:04 +0000",184624,"HBASE-17178 Add region balance throttling, but it can not be disabled, sometimes we need no throttle and balance the cluster as fast as possible.",0,0,neutral
hbase,2555,summary,Get rid of,design_debt,non-optimal_design,"Sun, 16 May 2010 21:36:30 +0000","Fri, 20 Nov 2015 12:42:41 +0000","Sun, 16 May 2010 22:47:17 +0000",4247,Get rid of HColumnDescriptor.MAPFILE_INDEX_INTERVAL?,0,0,negative
hbase,2925,comment_6,"@stack, thank you for picking up this issue. Feel free to reuse the code provided in in any way you want. (Let me know if this is not enough I could submit another file with Apache license added.) Back to the discussion, I agree that removing the {{hasCode()}} and {{equals()}} methods does resolve this issue. However, I wonder if it defeats the purpose of having a connection cache from the first place. You also alluded to the problem of the configuration settings changing after the connection is open, which, if we rely solely on Object equality, could lead to subtle error or unexpected behavior. Hence my suggestion to pin down the configuration once the TableServers instance is created. The disadvantage there is that small/unrelated changes in the configuration will invalidate the cache entry as well. If that is deemed a real problem given the common use cases, I was thinking maybe we define ""equality"" of the configuration for the purposes of caching as the equality of all properties that start with {{hbase.}} or {{zk.}} or any other prefix whose properties could be used as possible connection parameters. Not sure if the comparison becomes too slow in that case, but what is more important is whether correctness is compromised if we make the assumption that only properties starting with those prefixes affect the connection.",design_debt,non-optimal_design,"Tue, 17 Aug 2010 21:27:52 +0000","Fri, 20 Nov 2015 12:42:13 +0000","Mon, 6 Sep 2010 15:52:27 +0000",1707875,"@stack, thank you for picking up this issue. Feel free to reuse the code provided in SimpleHConnectionManagerLeakReplicator.java in any way you want. (Let me know if this is not enough I could submit another file with Apache license added.) Back to the discussion, I agree that removing the hasCode() and equals() methods does resolve this issue. However, I wonder if it defeats the purpose of having a connection cache from the first place. You also alluded to the problem of the configuration settings changing after the connection is open, which, if we rely solely on Object equality, could lead to subtle error or unexpected behavior. Hence my suggestion to pin down the configuration once the TableServers instance is created. The disadvantage there is that small/unrelated changes in the configuration will invalidate the cache entry as well. If that is deemed a real problem given the common use cases, I was thinking maybe we define ""equality"" of the configuration for the purposes of caching as the equality of all properties that start with hbase. or zk. or any other prefix whose properties could be used as possible connection parameters. Not sure if the comparison becomes too slow in that case, but what is more important is whether correctness is compromised if we make the assumption that only properties starting with those prefixes affect the connection.",0.09033333333,0.09876923077,neutral
hbase,2969,description,"Considering that the method _getTable(String)_ in is invoked by multiple threads, it may happen that while _'queue == null'_ is true, it is possible to have a queue mapped to that name into the tables map when queue)'_ is executed. However, I don't think it will cause any harm because the overwritten queue will eventually be garbage-collected. :-)",design_debt,non-optimal_design,"Wed, 8 Sep 2010 17:58:53 +0000","Fri, 20 Nov 2015 12:43:50 +0000","Wed, 8 Sep 2010 22:42:04 +0000",16991,"Considering that the method getTable(String) in org.apache.hadoop.hbase.client.HTablePool is invoked by multiple threads, it may happen that while 'queue == null' is true, it is possible to have a queue mapped to that name into the tables map when 'tables.put(tableName, queue)' is executed. However, I don't think it will cause any harm because the overwritten queue will eventually be garbage-collected.",0.2164444444,0.0685,neutral
hbase,3324,description,"The block cache has lots of configuration parameters but they aren't using Configuration like they should. It would also be nice to have a better way of doing hit ratios, like a rolling window.",design_debt,non-optimal_design,"Thu, 9 Dec 2010 08:44:19 +0000","Sun, 12 Jun 2022 00:16:59 +0000","Sun, 18 May 2014 04:08:54 +0000",108501875,"The block cache has lots of configuration parameters but they aren't using Configuration like they should. It would also be nice to have a better way of doing hit ratios, like a rolling window.",0.23125,0.23125,negative
hbase,3368,comment_3,"As we discussed, I think comment is a little misleading with ""just in case"". I agree that this should not be necessary but that's because the current design is suboptimal. Otherwise this seems fine. When were we doing the regionOffline previously?",design_debt,non-optimal_design,"Thu, 16 Dec 2010 06:37:55 +0000","Fri, 20 Nov 2015 12:41:50 +0000","Mon, 21 Mar 2011 21:04:17 +0000",8259982,"As we discussed, I think comment is a little misleading with ""just in case"". I agree that this should not be necessary but that's because the current design is suboptimal. Otherwise this seems fine. When were we doing the regionOffline previously?",0,0,negative
hbase,3510,comment_1,Same comment as last time - it's difficult to get to the RS name from here. So I just added the port - that makes it consistent with the other IPC threads.,design_debt,non-optimal_design,"Mon, 7 Feb 2011 20:44:11 +0000","Fri, 20 Nov 2015 12:43:36 +0000","Mon, 7 Feb 2011 22:02:23 +0000",4692,Same comment as last time - it's difficult to get to the RS name from here. So I just added the port - that makes it consistent with the other IPC threads.,-0.1,-0.1,neutral
hbase,3510,description,"The IPC readers come out of a thread pool but have no name, which is annoying.",design_debt,non-optimal_design,"Mon, 7 Feb 2011 20:44:11 +0000","Fri, 20 Nov 2015 12:43:36 +0000","Mon, 7 Feb 2011 22:02:23 +0000",4692,"The IPC readers come out of a thread pool but have no name, which is annoying.",-1,-1,negative
hbase,3528,description,"Our maven build exports a lot of deps, and they flow to clients who depend on us. for example we shouldnt export etc. Clients should be able to depend on any version of the above libraries or NOT depend on them.",design_debt,non-optimal_design,"Sat, 12 Feb 2011 06:44:16 +0000","Sun, 12 Jun 2022 17:29:05 +0000","Mon, 29 Dec 2014 19:06:16 +0000",122386920,"Our maven build exports a lot of deps, and they flow to clients who depend on us. for example we shouldnt export thrift,protobuf,jetty,jruby,tomcat,jersey,guava, etc. Clients should be able to depend on any version of the above libraries or NOT depend on them.",-0.344,0.344,neutral
hbase,3625,description,"Currently the surefire plugin configuration defines the following exclusion: AFAICT the '{{/$**}}' does not resolve to anything meaningful. Adding support to exclude one or more tests via Maven property, i.e.",design_debt,non-optimal_design,"Fri, 11 Mar 2011 07:22:17 +0000","Fri, 20 Nov 2015 12:41:35 +0000","Tue, 15 Mar 2011 01:11:43 +0000",323366,"Currently the surefire plugin configuration defines the following exclusion: AFAICT the '**/*$*' does not resolve to anything meaningful. Adding support to exclude one or more tests via Maven property, i.e. '-Dtest.exclude=<TESTCLASS>' would be useful.",-0.1541666667,-0.05277777778,negative
hbase,3673,description,"In the case of medium-to-large sized HTable pools, the amount of time the client spends blocking on the underlying map and queue data structures turns out to be quite significant. Using an efficient wait-free implementation of maps and queues might serve to reduce the contention on the pool. In particular, I was wondering if we should replace the synchronized map with a concurrent hash map, and linked list with a concurrent linked queue.",design_debt,non-optimal_design,"Fri, 18 Mar 2011 22:20:04 +0000","Fri, 20 Nov 2015 12:40:39 +0000","Thu, 24 Mar 2011 22:28:49 +0000",518925,"In the case of medium-to-large sized HTable pools, the amount of time the client spends blocking on the underlying map and queue data structures turns out to be quite significant. Using an efficient wait-free implementation of maps and queues might serve to reduce the contention on the pool. In particular, I was wondering if we should replace the synchronized map with a concurrent hash map, and linked list with a concurrent linked queue.",0.06666666667,0.06666666667,neutral
hbase,3807,description,Currently the metrics are a mix of MB and bytes. Its confusing.,design_debt,non-optimal_design,"Thu, 21 Apr 2011 06:28:51 +0000","Fri, 20 Nov 2015 12:43:21 +0000","Wed, 10 Aug 2011 19:43:28 +0000",9638077,Currently the metrics are a mix of MB and bytes. Its confusing.,-0.2185,-0.2185,negative
hbase,3840,description,A common user error (and even hbase dev error) is to pass a vanilla Hadoop Configuration into HBase methods that expect to see all of the relevant hbase defaults from hbase-default.xml. This often results in NPE or issues locating ZK. We should add a method like which ensures that the conf has incorporated hbase-default.xml. We can do this by checking for existence of,design_debt,non-optimal_design,"Sun, 1 May 2011 05:56:34 +0000","Sun, 12 Jun 2022 18:18:23 +0000","Wed, 5 Sep 2012 18:08:27 +0000",42639113,A common user error (and even hbase dev error) is to pass a vanilla Hadoop Configuration into HBase methods that expect to see all of the relevant hbase defaults from hbase-default.xml. This often results in NPE or issues locating ZK. We should add a method like HBaseConfiguration.verify(conf) which ensures that the conf has incorporated hbase-default.xml. We can do this by checking for existence of hbase.defaults.for.version.,-0.07222222222,-0.04333333333,negative
hbase,3840,summary,Add sanity checks on Configurations to make sure hbase confs have been loaded,design_debt,non-optimal_design,"Sun, 1 May 2011 05:56:34 +0000","Sun, 12 Jun 2022 18:18:23 +0000","Wed, 5 Sep 2012 18:08:27 +0000",42639113,Add sanity checks on Configurations to make sure hbase confs have been loaded,0.625,0.625,neutral
hbase,4016,comment_0,"Your code is storing strings in the cells, but expects a big-endian encoded long, not a string.",design_debt,non-optimal_design,"Tue, 21 Jun 2011 23:07:46 +0000","Fri, 20 Nov 2015 11:53:05 +0000","Tue, 28 Jun 2011 23:36:32 +0000",606526,"Your code is storing strings in the cells, but incrementColumnValue expects a big-endian encoded long, not a string.",0.2,0.2,neutral
hbase,4016,description,"We wanted to use an int (32-bit) atomic counter and we initialize it with a certain value when the row is created. Later, we increment the counter using This call results in one of two outcomes. 1. The call succeeds, but the column value now is a long (64-bit) and is corrupt (by additional data that was in the buffer read). 2. Throws offset (65547) + length (8) exceed the capacity of the array: 65551 Source) Based on our incorrect usage of counters (initializing it with a 32 bit value and later using it as a counter), I would expect that we fail consistently with mode 2 rather than silently corrupting data with mode 1. However, the exception is thrown only rarely and I am not sure what determines the case to be executed. I am wondering if this has something to do with flush. Here is a HRegion unit test that can reproduce this problem. We modified our code to initialize the counter with a 64 bit value. But, I was also wondering if something has to change in to handle inconsistent counter sizes gracefully without corrupting existing data. Please let me know if you need additional information.",design_debt,non-optimal_design,"Tue, 21 Jun 2011 23:07:46 +0000","Fri, 20 Nov 2015 11:53:05 +0000","Tue, 28 Jun 2011 23:36:32 +0000",606526,"We wanted to use an int (32-bit) atomic counter and we initialize it with a certain value when the row is created. Later, we increment the counter using HTable.incrementColumnValue(). This call results in one of two outcomes. 1. The call succeeds, but the column value now is a long (64-bit) and is corrupt (by additional data that was in the buffer read). 2. Throws IOException/IllegalArgumentException. Java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: offset (65547) + length (8) exceed the capacity of the array: 65551 at org.apache.hadoop.hbase.util.Bytes.explainWrongLengthOrOffset(Bytes.java:502) at org.apache.hadoop.hbase.util.Bytes.toLong(Bytes.java:480) at org.apache.hadoop.hbase.regionserver.HRegion.incrementColumnValue(HRegion.java:3139) at org.apache.hadoop.hbase.regionserver.HRegionServer.incrementColumnValue(HRegionServer.java:2468) at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570) at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1039) Based on our incorrect usage of counters (initializing it with a 32 bit value and later using it as a counter), I would expect that we fail consistently with mode 2 rather than silently corrupting data with mode 1. However, the exception is thrown only rarely and I am not sure what determines the case to be executed. I am wondering if this has something to do with flush. Here is a HRegion unit test that can reproduce this problem. http://paste.lisp.org/display/122822 We modified our code to initialize the counter with a 64 bit value. But, I was also wondering if something has to change in HRegion.incrementColumnValue() to handle inconsistent counter sizes gracefully without corrupting existing data. Please let me know if you need additional information.",0.09166666667,0.01486486486,neutral
hbase,4311,description,"When refactoring TestHBaseFsckRepair to add more hbck test cases, I noticed that uses an existing table with empty region, adds more regions, and then attempts to remove the region. The region remains in meta and is causes hbck to report at inconsistency. Ideally these test table generation utility functions should generate clean tables.",design_debt,non-optimal_design,"Wed, 31 Aug 2011 16:31:47 +0000","Sun, 12 Jun 2022 19:20:18 +0000","Thu, 12 Apr 2012 16:49:37 +0000",19441070,"When refactoring TestHBaseFsckRepair to add more hbck test cases, I noticed that HBaseTestingUtility.createMultiRegions uses an existing table with empty region, adds more regions, and then attempts to remove the region. The region remains in meta and is causes hbck to report at inconsistency. Ideally these test table generation utility functions should generate clean tables.",0.06666666667,0.05,negative
hbase,4341,description,"This's the reason of why did get failure . In this test, one case was timeout and cause the whole test process got killed. [logs] Here's the related logs(From [Analysis] One region was opened during the RS's stopping. This is method of HRS#onlineRegions is a ConcurrentHashMap. So walk down this map may not get all the data if some entries are been added during the traverse. Once one region was missed, it can't be closed anymore. And this regionserver will not be stopped normally. Then the following logs occurred:",design_debt,non-optimal_design,"Wed, 7 Sep 2011 02:42:48 +0000","Fri, 20 Nov 2015 11:54:46 +0000","Thu, 8 Sep 2011 15:39:20 +0000",132992,"This's the reason of why did ""https://builds.apache.org/job/hbase-0.90/282"" get failure . In this test, one case was timeout and cause the whole test process got killed. [logs] Here's the related logs(From org.apache.hadoop.hbase.mapreduce.TestTableMapReduce-output.txt): [Analysis] One region was opened during the RS's stopping. This is method of ""HRS#closeAllRegions"": HRS#onlineRegions is a ConcurrentHashMap. So walk down this map may not get all the data if some entries are been added during the traverse. Once one region was missed, it can't be closed anymore. And this regionserver will not be stopped normally. Then the following logs occurred:",-0.2115,-0.128,negative
hbase,4437,comment_3,Move to 0.20.205.0 hadoop. Also includes edits to our jsp and jamon templates commenting out DOCTYPE to get around bug where css is served as text/html. See tail of HBASE-2110 for discussion.,design_debt,non-optimal_design,"Mon, 19 Sep 2011 16:59:13 +0000","Fri, 20 Nov 2015 11:55:14 +0000","Mon, 24 Oct 2011 21:56:44 +0000",3041851,Move to 0.20.205.0 hadoop. Also includes edits to our jsp and jamon templates commenting out DOCTYPE to get around bug where css is served as text/html. See tail of HBASE-2110 for discussion.,0,0,neutral
hbase,4459,description,"There are about 90 classes/codes in HbaseObjectWritable currently and Byte.MAX_VALUE is 127. In addition, anyone wanting to add custom classes but not break compatibility might want to leave a gap before using codes and that's difficult in such limited space. Eventually we should get rid of this pattern that makes compatibility difficult (better client/server protocol handshake) but we should probably at least bump this to a short for 0.94.",design_debt,non-optimal_design,"Thu, 22 Sep 2011 18:46:58 +0000","Fri, 20 Nov 2015 11:55:50 +0000","Thu, 20 Oct 2011 18:23:23 +0000",2417785,"There are about 90 classes/codes in HbaseObjectWritable currently and Byte.MAX_VALUE is 127. In addition, anyone wanting to add custom classes but not break compatibility might want to leave a gap before using codes and that's difficult in such limited space. Eventually we should get rid of this pattern that makes compatibility difficult (better client/server protocol handshake) but we should probably at least bump this to a short for 0.94.",0.0625,0.0625,neutral
hbase,467,comment_1,"This is looking ugly. HRegion references a number of methods in HStore and HStoreFile. HStore references static methods from HRegion. HRegion and HStore are also tightly coupled with HLog. We can probably factor out the inner classes of Hregion, HStore and HStoreFile into but trying to tease these apart using the current plan will either be a) a ton of work or b) turn out to not be possible. Comments, please!",design_debt,non-optimal_design,"Sun, 24 Feb 2008 05:57:43 +0000","Fri, 22 Aug 2008 21:35:04 +0000","Sun, 24 Feb 2008 22:10:11 +0000",58348,"This is looking ugly. HRegion references a number of methods in HStore and HStoreFile. HStore references static methods from HRegion. HRegion and HStore are also tightly coupled with HLog. We can probably factor out the inner classes of Hregion, HStore and HStoreFile into o.a.h.h.regionserver. {region,store,storefile(??)} , but trying to tease these apart using the current plan will either be a) a ton of work or b) turn out to not be possible. Comments, please!",-0.2458333333,-0.184375,negative
hbase,467,comment_2,"Looking at the issue further, it appears that making subpackages for region, store, etc inner classes is not going to work well. Although many of the inner classes are static, in order for the parent class to access them, too much would have to be made public. Just factoring out inner classes (into and making them package scope would yield the highest containment.",design_debt,non-optimal_design,"Sun, 24 Feb 2008 05:57:43 +0000","Fri, 22 Aug 2008 21:35:04 +0000","Sun, 24 Feb 2008 22:10:11 +0000",58348,"Looking at the issue further, it appears that making subpackages for region, store, etc inner classes is not going to work well. Although many of the inner classes are static, in order for the parent class to access them, too much would have to be made public. Just factoring out inner classes (into o.a.h.h.regionserver) and making them package scope would yield the highest containment.",-0.1367777778,-0.1367777778,negative
hbase,467,description,"Per Jim's suggestions on HBase-419, let's move all the store-related files into a subpackage. It should either be o.a.h.h.store or If we push it down another level, I think that we should make sure we never use any of those files outside of",design_debt,non-optimal_design,"Sun, 24 Feb 2008 05:57:43 +0000","Fri, 22 Aug 2008 21:35:04 +0000","Sun, 24 Feb 2008 22:10:11 +0000",58348,"Per Jim's suggestions on HBase-419, let's move all the store-related files into a subpackage. It should either be o.a.h.h.store or o.a.h.h.regionserver.store. If we push it down another level, I think that we should make sure we never use any of those files outside of o.a.h.h.regionserver.",-0.146,-0.073,neutral
hbase,5052,comment_0,"We could work on better cleaning the region name, but I think it is far better to depend on something like the hashed name",design_debt,non-optimal_design,"Fri, 16 Dec 2011 08:44:39 +0000","Fri, 20 Nov 2015 11:55:28 +0000","Sat, 7 Jan 2012 22:22:08 +0000",1949849,"We could work on better cleaning the region name, but I think it is far better to depend on something like the hashed name (HRehionInfo.hashCode()).",0.5333333333,0.2666666667,neutral
hbase,506,description,"Every so often, we find ourselves trying to debug a problem that happens in HTable where we exhaust all our retries trying to contact the region server hosting the region we want to operate on. Oftentimes the last exception that comes out is something like which should just never be the case. As a way to improve our debugging capabilities, when we decide to throw an exception out of ServerCallable, let's show not just the last exception but all the exceptions that caused all the retries in the first place. This will help us understand the sequence of events that led to us running out of retries.",design_debt,non-optimal_design,"Wed, 12 Mar 2008 00:20:42 +0000","Fri, 22 Aug 2008 21:13:11 +0000","Sat, 15 Mar 2008 21:27:18 +0000",335196,"Every so often, we find ourselves trying to debug a problem that happens in HTable where we exhaust all our retries trying to contact the region server hosting the region we want to operate on. Oftentimes the last exception that comes out is something like WrongRegionException, which should just never be the case. As a way to improve our debugging capabilities, when we decide to throw an exception out of ServerCallable, let's show not just the last exception but all the exceptions that caused all the retries in the first place. This will help us understand the sequence of events that led to us running out of retries.",0.1,0.1,negative
hbase,5333,description,"Currently if the cannot keep up with the writeload, we block writers up to milliseconds (default is 90000). Would be nice if there was a concept of a soft ""backpressure"" that slows writing clients gracefully *before* we reach this condition. From the log: ""2012-02-04 00:00:06,963 WARN Region <table",design_debt,non-optimal_design,"Sat, 4 Feb 2012 01:04:30 +0000","Sun, 12 Jun 2022 20:17:27 +0000","Thu, 23 Aug 2012 04:31:10 +0000",17378800,"Currently if the memstore/flush/compaction cannot keep up with the writeload, we block writers up to hbase.hstore.blockingWaitTime milliseconds (default is 90000). Would be nice if there was a concept of a soft ""backpressure"" that slows writing clients gracefully before we reach this condition. From the log: ""2012-02-04 00:00:06,963 WARN org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Region <table>,,1328313512779.c2761757621ddf8fb78baf5288d71271. has too many store files; delaying flush up to 90000ms""",-0.07916666667,-0.04895833333,negative
hbase,5523,comment_3,"I remember the initial motivation for the +1 shift now. If somebody accidentally places a Delete at T it would not be possible to get at any Puts of T with normal Scan API. The +1 allow setting an interval that includes the Puts but not the Delete. I.e. setting the range to [0,T+1) would include the Puts and Deletes. (note that the lower bound inclusive and the upper bound is exclusive hence the [x,y) notation). With the +1 shift [0,T+1) would not contain the Delete, but [0,T+2) would. This is very confusing, since [0,T+1) doesn't actually mean that when it comes to deletes. To recover the above mentioned Puts one could use a raw scan, instead. I'm going to commit the attached patch; it makes these scenarios much clearer.",design_debt,non-optimal_design,"Mon, 5 Mar 2012 22:43:26 +0000","Tue, 26 Feb 2013 08:12:56 +0000","Tue, 6 Mar 2012 06:22:22 +0000",27536,"I remember the initial motivation for the +1 shift now. If somebody accidentally places a Delete at T it would not be possible to get at any Puts of T with normal Scan API. The +1 allow setting an interval that includes the Puts but not the Delete. I.e. setting the range to [0,T+1) would include the Puts and Deletes. (note that the lower bound inclusive and the upper bound is exclusive hence the [x,y) notation). With the +1 shift [0,T+1) would not contain the Delete, but [0,T+2) would. This is very confusing, since [0,T+1) doesn't actually mean that when it comes to deletes. To recover the above mentioned Puts one could use a raw scan, instead. I'm going to commit the attached patch; it makes these scenarios much clearer.",-0.03188888889,-0.03188888889,neutral
hbase,5523,description,"A Delete at time T marks a Put at time T as deleted. In parent I invented special logic that insert a virtual millisecond into the tr if the encountered KV is a delete marker. This was so that there is a way to specify a timerange that would allow to see the put but not the delete: Discussed this today with a coworker and he convinced me that this is very confusing and also not needed. When we have a Delete and Put at the same time T, there *is* not timerange that can include the Put but not the Delete. So I will change the code to this (and fix the tests): It's easier to understand, and does not lead to strange scenarios when the TS is used as a controlled counter. Needs to be done before 0.94 goes out.",design_debt,non-optimal_design,"Mon, 5 Mar 2012 22:43:26 +0000","Tue, 26 Feb 2013 08:12:56 +0000","Tue, 6 Mar 2012 06:22:22 +0000",27536,"A Delete at time T marks a Put at time T as deleted. In parent I invented special logic that insert a virtual millisecond into the tr if the encountered KV is a delete marker. This was so that there is a way to specify a timerange that would allow to see the put but not the delete: Discussed this today with a coworker and he convinced me that this is very confusing and also not needed. When we have a Delete and Put at the same time T, there is not timerange that can include the Put but not the Delete. So I will change the code to this (and fix the tests): It's easier to understand, and does not lead to strange scenarios when the TS is used as a controlled counter. Needs to be done before 0.94 goes out.",0.05483333333,0.05483333333,neutral
hbase,5635,comment_0,"Yes, I think, continuing without SplitLogWroker may not be a good behaviour. Because that particular regionServer may have more capacity to take up the new regions. With the current behaviour it may not compete for taking any new splilog work. I feel we can retry for some times and then we can shutdown regionServer? or other option is to retry forever on any ZK exception. And can exit only on interrupted exception. Also i am seeing this issue may be bit dangerous bacause, if ZK is not available for some time, all RegionServer may face this problem and no one will take up the splitlog work. will return null only if node does not exist. If it is not able to find any children then it will return empty list. So, will be always set. On Other keeperExceptions like ZK unavalability and all, we have to handle.",design_debt,non-optimal_design,"Mon, 26 Mar 2012 09:56:31 +0000","Tue, 26 Feb 2013 17:02:58 +0000","Mon, 23 Apr 2012 16:50:31 +0000",2444040,"Yes, I think, continuing without SplitLogWroker may not be a good behaviour. Because that particular regionServer may have more capacity to take up the new regions. With the current behaviour it may not compete for taking any new splilog work. I feel we can retry for some times and then we can shutdown regionServer? or other option is to retry forever on any ZK exception. And can exit only on interrupted exception. Also i am seeing this issue may be bit dangerous bacause, if ZK is not available for some time, all RegionServer may face this problem and no one will take up the splitlog work. listChildrenAndWatchForNewChildren will return null only if node does not exist. If it is not able to find any children then it will return empty list. So, zookeeper.znode.splitlog will be always set. On Other keeperExceptions like ZK unavalability and all, we have to handle.",-0.06503030303,-0.05502564103,negative
hbase,5635,comment_7,Then we should log a message saying what we wait for every X minutes so that user doesn't have to use jstack.,design_debt,non-optimal_design,"Mon, 26 Mar 2012 09:56:31 +0000","Tue, 26 Feb 2013 17:02:58 +0000","Mon, 23 Apr 2012 16:50:31 +0000",2444040,Then we should log a message saying what we wait for every X minutes so that user doesn't have to use jstack.,0,0,neutral
hbase,6009,comment_0,How do we define the markers ? Through a series of magic bytes ? Looks like total size field for ClusterStatus is better choice.,design_debt,non-optimal_design,"Wed, 16 May 2012 00:17:14 +0000","Mon, 13 Jun 2022 16:39:46 +0000","Mon, 25 Jun 2012 21:56:10 +0000",3533936,or we can have start and end markers How do we define the markers ? Through a series of magic bytes ? Looks like total size field for ClusterStatus is better choice.,0.1666666667,0.1666666667,neutral
hbase,6009,comment_1,"I looked at the total size field option for this, starting from the write case. To calculate total size written, you have to know how many bytes were written for each write() call on ClusterStatus, including any objects contained inside it. The DataOutput interface for Writables doesn't have a way to return how many bytes were written to the stream. This is not a problem for primitive types as we can figure that out trivially. Even for somewhat more complicated situations such as modified UTF-8s written with the writeUTF call, the number of written bytes for a String can at least be calculated based on the formula for modified UTF-8 conversion. However, for calls to Object's write functions (e.g. for HRegionLoad), this becomes somewhat more problematic as there is no obvious answer as to how many bytes were written. We could use reflection to grab the fields, but then there is no guarantee that all of the fields of the Object are actually written to the stream when write() is called. So you'd have to introduce some hardcoded way of knowing how much was written for each Object, which is Bad. I'm tempted to say that we shouldn't add any more fields to ClusterStatus or similar APIs until 0.96, when hopefully our wire compatibility efforts will kick in and we can do this in a compatible way without having to jump through hoops.",design_debt,non-optimal_design,"Wed, 16 May 2012 00:17:14 +0000","Mon, 13 Jun 2022 16:39:46 +0000","Mon, 25 Jun 2012 21:56:10 +0000",3533936,"I looked at the total size field option for this, starting from the write case. To calculate total size written, you have to know how many bytes were written for each write() call on ClusterStatus, including any objects contained inside it. The DataOutput interface for Writables doesn't have a way to return how many bytes were written to the stream. This is not a problem for primitive types as we can figure that out trivially. Even for somewhat more complicated situations such as modified UTF-8s written with the writeUTF call, the number of written bytes for a String can at least be calculated based on the formula for modified UTF-8 conversion. However, for calls to Object's write functions (e.g. for HRegionLoad), this becomes somewhat more problematic as there is no obvious answer as to how many bytes were written. We could use reflection to grab the fields, but then there is no guarantee that all of the fields of the Object are actually written to the stream when write() is called. So you'd have to introduce some hardcoded way of knowing how much was written for each Object, which is Bad. I'm tempted to say that we shouldn't add any more fields to ClusterStatus or similar APIs until 0.96, when hopefully our wire compatibility efforts will kick in and we can do this in a compatible way without having to jump through hoops.",0.03522222222,0.03522222222,neutral
hbase,6009,comment_4,"The immediate issue here is that HBASE-5209 was committed in 0.92.1, and that broke compatibility with 0.92.0. I suppose anyone who cares about 0.92 branch has moved to 0.92.1 so that there is no practical hit. You are right that adding a size would be another incompatible change, hence my later comment about ""let's just not make any more changes until 0.96"". :D Anyway in the absence of any changes, I can at least add a release note to 0.92.1 stating this incompatibility with 0.92.0. I'll use this JIRA to track that.",design_debt,non-optimal_design,"Wed, 16 May 2012 00:17:14 +0000","Mon, 13 Jun 2022 16:39:46 +0000","Mon, 25 Jun 2012 21:56:10 +0000",3533936,"The immediate issue here is that HBASE-5209 was committed in 0.92.1, and that broke compatibility with 0.92.0. I suppose anyone who cares about 0.92 branch has moved to 0.92.1 so that there is no practical hit. You are right that adding a size would be another incompatible change, hence my later comment about ""let's just not make any more changes until 0.96"". Anyway in the absence of any changes, I can at least add a release note to 0.92.1 stating this incompatibility with 0.92.0. I'll use this JIRA to track that.",0.2929,0.1729,neutral
hbase,6009,description,"The additions to add backup masters to ClusterStatus are technically incompatible between clients and servers. Older clients will basically not read the extra bits that the newer server pushes for the backup masters, thus screwing up the serialization for the next blob in the pipe. For the Writable, we can add a total size field for ClusterStatus at the beginning, or we can have start and end markers. I can make a patch for either approach; interested in whatever folks have to suggest. Would be good to get this in soon to limit the damage to 0.92.1 (don't know if we can get this in in time for 0.94.0). Either change will make us forward-compatible starting with when the change goes in, but will not fix the backwards incompatibility, which we will have to mark with a release note as there have already been releases with this change. Hopefully we can do this in a cleaner way when wire compat rolls around in 0.96.",design_debt,non-optimal_design,"Wed, 16 May 2012 00:17:14 +0000","Mon, 13 Jun 2022 16:39:46 +0000","Mon, 25 Jun 2012 21:56:10 +0000",3533936,"The additions to add backup masters to ClusterStatus are technically incompatible between clients and servers. Older clients will basically not read the extra bits that the newer server pushes for the backup masters, thus screwing up the serialization for the next blob in the pipe. For the Writable, we can add a total size field for ClusterStatus at the beginning, or we can have start and end markers. I can make a patch for either approach; interested in whatever folks have to suggest. Would be good to get this in soon to limit the damage to 0.92.1 (don't know if we can get this in in time for 0.94.0). Either change will make us forward-compatible starting with when the change goes in, but will not fix the backwards incompatibility, which we will have to mark with a release note as there have already been releases with this change. Hopefully we can do this in a cleaner way when wire compat rolls around in 0.96.",0.2191190476,0.2191190476,neutral
hbase,6050,description,The scenario is like this There if the regiondir doesnot exist we tend to create and then add the recovered.edits. Because of this HBCK thinks it to be an orphan region because we have the regiondir but with no regioninfo. Ideally cluster is fine but we it is misleading.,design_debt,non-optimal_design,"Fri, 18 May 2012 15:41:10 +0000","Tue, 26 Feb 2013 08:16:18 +0000","Sun, 27 May 2012 16:38:55 +0000",781065,The scenario is like this -> A region is getting splitted. -> The master is still not processed the split . -> Region server goes down. -> Split log manager starts splitting the logs and creates the recovered.edits in the splitlog path. -> CJ starts and deletes the entry from META and also just completes the deletion of the region dir. -> in hlogSplitter on final step we rename the recovered.edits to come under the regiondir. There if the regiondir doesnot exist we tend to create and then add the recovered.edits. Because of this HBCK thinks it to be an orphan region because we have the regiondir but with no regioninfo. Ideally cluster is fine but we it is misleading.,-0.04375,-0.001714285714,negative
hbase,6050,summary,"HLogSplitter renaming recovered.edits and CJ removing the parent directory race, making the HBCK think cluster is inconsistent.",design_debt,non-optimal_design,"Fri, 18 May 2012 15:41:10 +0000","Tue, 26 Feb 2013 08:16:18 +0000","Sun, 27 May 2012 16:38:55 +0000",781065,"HLogSplitter renaming recovered.edits and CJ removing the parent directory race, making the HBCK think cluster is inconsistent.",0,0,negative
hbase,6093,description,"Many applications run with maxVersions=1 and do not care about timestamps, or they will specify one timestamp per row as a normal KeyValue rather than per-cell. Then, DataBlockEncoders like those in HBASE-4218 and HBASE-4676 often encode timestamps as diffs from the previous or diffs from the minimum timestamp in the block. If all timestamps in a block are the same, they will all compress to basically <= 8 bytes total per block. This can be 10% to 25% space savings for some schemas, and that savings is realized both on disk and in block cache. We could add a ColumnFamily setting If true, then all timestamps are modified during a flush/compaction to the currentTimeMillis() at the start of the flush/compaction. If all timestamps are made identical in a file, then the encoder will be able to eliminate them. The simplest use case is probably that where all inserts are type=Put, there are no overwrites, and there are no deletes. As use cases get more complex, then so does the implementation. For example, what happens when there is a Put and a Delete of the same cell in the same memstore? Maybe for a flush at t=flushStartTime, the Put gets timestamp=t, and the Delete gets timestamp=t+1. Or maybe HBASE-4241 could take care of this problem.",design_debt,non-optimal_design,"Fri, 25 May 2012 06:15:44 +0000","Mon, 13 Jun 2022 16:34:39 +0000","Sat, 11 Apr 2015 01:33:27 +0000",90789463,"Many applications run with maxVersions=1 and do not care about timestamps, or they will specify one timestamp per row as a normal KeyValue rather than per-cell. Then, DataBlockEncoders like those in HBASE-4218 and HBASE-4676 often encode timestamps as diffs from the previous or diffs from the minimum timestamp in the block. If all timestamps in a block are the same, they will all compress to basically <= 8 bytes total per block. This can be 10% to 25% space savings for some schemas, and that savings is realized both on disk and in block cache. We could add a ColumnFamily setting flattenTimestamps=[true/false]. If true, then all timestamps are modified during a flush/compaction to the currentTimeMillis() at the start of the flush/compaction. If all timestamps are made identical in a file, then the encoder will be able to eliminate them. The simplest use case is probably that where all inserts are type=Put, there are no overwrites, and there are no deletes. As use cases get more complex, then so does the implementation. For example, what happens when there is a Put and a Delete of the same cell in the same memstore? Maybe for a flush at t=flushStartTime, the Put gets timestamp=t, and the Delete gets timestamp=t+1. Or maybe HBASE-4241 could take care of this problem.",0.04796969697,0.05951388889,neutral
hbase,6151,description,"See, for example: The HRegionServer calls HBaseServer: but the server can start accepting RPCs once the threads have been started, but if they do, they throw until openServer runs. We should probably 1) Catch the remote exception and retry on the master 2) Look into whether the start() behavior of HBaseServer makes any sense. Why would you start accepting RPCs only to throw back",design_debt,non-optimal_design,"Sat, 2 Jun 2012 00:51:46 +0000","Mon, 13 Jun 2022 16:37:09 +0000","Fri, 13 Jul 2012 00:48:01 +0000",3542175,"See, for example: The HRegionServer calls HBaseServer: but the server can start accepting RPCs once the threads have been started, but if they do, they throw ServerNotRunningException until openServer runs. We should probably 1) Catch the remote exception and retry on the master 2) Look into whether the start() behavior of HBaseServer makes any sense. Why would you start accepting RPCs only to throw back ServerNotRunningException?",0.3,0.3,neutral
hbase,6184,comment_0,"This change will affect the look up in the META table? When searchRow is created with passing newformat=true, it will add the encoded name also at the end[<tableName In your issue you are getting the result but in that result the HRegionInfo seems coming as null only? Do this above change really fix your issue? Do u facing some other issues?",design_debt,non-optimal_design,"Thu, 7 Jun 2012 05:21:15 +0000","Mon, 13 Jun 2022 16:41:37 +0000","Thu, 25 Oct 2018 22:39:02 +0000",201460667,"This change will affect the look up in the META table? When searchRow is created with passing newformat=true, it will add the encoded name also at the end[<tableName>,<row>,<regionid>.<encodedname>.]. But the searchRow is used to do metaTable.getRowOrBefore(). Any way after the row we add HConstants.NINES using which we need to get correct row from META table. I mean adding this encodedname might not be needed for this lookup In your issue you are getting the result but in that result the HRegionInfo seems coming as null only? Do this above change really fix your issue? Do u facing some other issues?",0.14325,0.1448,neutral
hbase,6201,comment_10,"Yes. At the current state, most of our unit tests, which are candidates to be upgraded to be system tests does start a mini-cluster of n-nodes, load some data, kill a few nodes, verify, etc. We are them to do the same things on the actual cluster. A particular test case, for example, starts 4 region servers, put some data, kills 1 RS, checks whether the regions are balanced, kills one more, checks agains, etc. Some basic functionality we can use from itest are: - Starting / stopping / sending a signal to daemons (start a region server on host1, kill master on host2, etc). For both HBase and Hadoop processes. - Basic cluster/node discovery (give me the nodes running hmaster) - Run this command on host3 (SSH)",design_debt,non-optimal_design,"Tue, 12 Jun 2012 02:33:19 +0000","Fri, 20 Nov 2015 11:53:53 +0000","Tue, 24 Sep 2013 21:21:05 +0000",40589266,"Are you saying that you would like the tests themeselves to get involved in the lifecycle of each service? Like bringing them up and down, etc? Yes. At the current state, most of our unit tests, which are candidates to be upgraded to be system tests does start a mini-cluster of n-nodes, load some data, kill a few nodes, verify, etc. We are converting/reimplementing them to do the same things on the actual cluster. A particular test case, for example, starts 4 region servers, put some data, kills 1 RS, checks whether the regions are balanced, kills one more, checks agains, etc. Some basic functionality we can use from itest are: Starting / stopping / sending a signal to daemons (start a region server on host1, kill master on host2, etc). For both HBase and Hadoop processes. Basic cluster/node discovery (give me the nodes running hmaster) Run this command on host3 (SSH)",-0.1138888889,-0.1083703704,neutral
hbase,6244,description,"Now, the RowResultGenerator and the will fit the column family if the request doesn't contain any column info. The will cost 10+ milliseconds in our hbase cluster each request. We can remove these code because the server will auto add the columns.",design_debt,non-optimal_design,"Wed, 20 Jun 2012 15:16:18 +0000","Tue, 26 Feb 2013 08:15:58 +0000","Wed, 20 Jun 2012 21:16:08 +0000",21590,"Now, the RowResultGenerator and the ScanerResultGenerator will fit the column family if the request doesn't contain any column info. The table.getTableDescriptor() will cost 10+ milliseconds in our hbase cluster each request. We can remove these code because the server will auto add the columns.",0.1943333333,0.14575,neutral
hbase,6969,comment_2,"As written, at end of tests, won't your passed in zkcluster be shutdown? That is probably not what you want? Should there be a to answer your added Is baseZKCluster necessary? Why not just an internal flag which has whether or not HBaseTestingUtility started the zk cluster? If we didn't start it, we shouldn't stop it on the way out? Otherwise, looks like useful functionality to add. Thanks Micah.",design_debt,non-optimal_design,"Wed, 10 Oct 2012 02:34:56 +0000","Tue, 14 Jun 2022 22:00:52 +0000","Sat, 11 Apr 2015 00:23:59 +0000",78875343,"As written, at end of tests, won't your passed in zkcluster be shutdown? That is probably not what you want? Should there be a getMiniZookeeperCluster to answer your added setMiniZookeeperCluster? Is baseZKCluster necessary? Why not just an internal flag which has whether or not HBaseTestingUtility started the zk cluster? If we didn't start it, we shouldn't stop it on the way out? Otherwise, looks like useful functionality to add. Thanks Micah.",0.05714285714,0.05,negative
hbase,6969,description,"While not an official part of the HBase API, it'd be nice if HBaseTestingUtility allowed for consumer to inject their own instance of the when executing tests. Currently there is no way to control how the ZK instance is started which we've had to hack around when there were ZK version conflicts (3.3 vs 3.4). Or if you want to control which specific port it starts on vs random. Allowing consumers to inject an instance (unstarted) gives them the freedom to implement their own without having to fork the entire HBaseTestingUtility class",design_debt,non-optimal_design,"Wed, 10 Oct 2012 02:34:56 +0000","Tue, 14 Jun 2022 22:00:52 +0000","Sat, 11 Apr 2015 00:23:59 +0000",78875343,"While not an official part of the HBase API, it'd be nice if HBaseTestingUtility allowed for consumer to inject their own instance of the MiniZookeeperCluster when executing tests. Currently there is no way to control how the ZK instance is started which we've had to hack around when there were ZK version conflicts (3.3 vs 3.4). Or if you want to control which specific port it starts on vs random. Allowing consumers to inject an instance (unstarted) gives them the freedom to implement their own without having to fork the entire HBaseTestingUtility class",0.090625,0.090625,neutral
hbase,7212,description,"This is a simplified version of what was proposed in HBASE-6573. Instead of claiming to be a 2pc or 3pc implementation (which implies logging at each actor, and recovery operations) this is just provides a best effort global barrier mechanism called a Procedure. Users need only to implement a methods to acquireBarrier, to act when insideBarrier, and to releaseBarrier that use the ExternalException cooperative error checking mechanism. Globally consistent snapshots require the ability to quiesce writes to a set of region servers before a the snapshot operation is executed. Also if any node fails, it needs to be able to notify them so that they abort. The first cut of other online snapshots don't need the fully barrier but may still use this for its error propagation mechanisms. This version removes the extra layer incurred in the previous implementation due to the use of generics, separates the coordinator and members, and reduces the amount of inheritance used in favor of composition.",design_debt,non-optimal_design,"Thu, 22 Nov 2012 19:08:26 +0000","Mon, 23 Sep 2013 18:30:35 +0000","Sat, 29 Dec 2012 06:18:52 +0000",3150626,"This is a simplified version of what was proposed in HBASE-6573. Instead of claiming to be a 2pc or 3pc implementation (which implies logging at each actor, and recovery operations) this is just provides a best effort global barrier mechanism called a Procedure. Users need only to implement a methods to acquireBarrier, to act when insideBarrier, and to releaseBarrier that use the ExternalException cooperative error checking mechanism. Globally consistent snapshots require the ability to quiesce writes to a set of region servers before a the snapshot operation is executed. Also if any node fails, it needs to be able to notify them so that they abort. The first cut of other online snapshots don't need the fully barrier but may still use this for its error propagation mechanisms. This version removes the extra layer incurred in the previous implementation due to the use of generics, separates the coordinator and members, and reduces the amount of inheritance used in favor of composition.",0.1198928571,0.1198928571,neutral
hbase,76,comment_1,"I think we should split the column into family and qualifier anyway, because it will benefit us when it comes time to switch mapfile implementation.",design_debt,non-optimal_design,"Mon, 7 Jan 2008 22:16:02 +0000","Fri, 22 Aug 2008 21:13:05 +0000","Thu, 15 May 2008 22:03:00 +0000",11144818,"I think we should split the column into family and qualifier anyway, because it will benefit us when it comes time to switch mapfile implementation.",0.688,0.688,neutral
hbase,76,comment_2,"TS made a difference profiling saving on Text creations but it can only be used in a few places; its dangerous using it everywhere (the underlying Text can move from under it). Profiling, an HSK that had family and qualifier members rather just than a column that we then have to split in many places would save us lots of object creations and CPU.",design_debt,non-optimal_design,"Mon, 7 Jan 2008 22:16:02 +0000","Fri, 22 Aug 2008 21:13:05 +0000","Thu, 15 May 2008 22:03:00 +0000",11144818,"TS made a difference profiling saving on Text creations but it can only be used in a few places; its dangerous using it everywhere (the underlying Text can move from under it). Profiling, an HSK that had family and qualifier members rather just than a column that we then have to split in many places would save us lots of object creations and CPU.",-0.103,-0.103,negative
hbase,76,comment_6,I think the important extraction from this issue is that we should split the column name into separate column family and cell qualifier text instances in HStoreKey. Then we'd never have to search the column name for the separator.,design_debt,non-optimal_design,"Mon, 7 Jan 2008 22:16:02 +0000","Fri, 22 Aug 2008 21:13:05 +0000","Thu, 15 May 2008 22:03:00 +0000",11144818,I think the important extraction from this issue is that we should split the column name into separate column family and cell qualifier text instances in HStoreKey. Then we'd never have to search the column name for the separator.,0.2,0.2,neutral
hbase,76,description,"Chatting with Jim while looking at profiler outputs, we should make an effort at purging the servers of the Text type so HRegionServer doesn't ever have to deal in Characters and the concomitant encode/decode to UTF-8. Toward this end, we'd make changes like moving HStoreKey to have four rather than 3 data members: column family, column family qualifier, row + timestamp done as a basic Writable -- -- and a long rather than a Text column, Text row and a timestamp long. This would save on our having to do the relatively expensive 'find' of the column family separator inside in extractFamily (",design_debt,non-optimal_design,"Mon, 7 Jan 2008 22:16:02 +0000","Fri, 22 Aug 2008 21:13:05 +0000","Thu, 15 May 2008 22:03:00 +0000",11144818,"Chatting with Jim while looking at profiler outputs, we should make an effort at purging the servers of the Text type so HRegionServer doesn't ever have to deal in Characters and the concomitant encode/decode to UTF-8. Toward this end, we'd make changes like moving HStoreKey to have four rather than 3 data members: column family, column family qualifier, row + timestamp done as a basic Writable  ImmutableBytesWritable?  and a long rather than a Text column, Text row and a timestamp long. This would save on our having to do the relatively expensive 'find' of the column family separator inside in extractFamily (>10% of CPU scanning). Chatting about it, we could effect the change without change in the public client API; clients could continue to take Text type for row and column and then client-side, the convertion to HStoreKey could be done before crossing the wire to the server.",0.1333333333,0.08,neutral
hbase,798,comment_1,"After discussing with Jim on IRC, there won't be very many additional functions exposed through HTable. It will actually just be one new version of each of: batchUpdate, deleteAll x 2, and deleteFamily. So it would be those 4, plus the lockRow/unlockRow. You think it would be okay to include just those in HTable w/o subclassing it?",design_debt,non-optimal_design,"Wed, 6 Aug 2008 04:29:41 +0000","Sun, 13 Sep 2009 22:33:29 +0000","Wed, 13 Aug 2008 02:35:03 +0000",597922,"After discussing with Jim on IRC, there won't be very many additional functions exposed through HTable. It will actually just be one new version of each of: batchUpdate, deleteAll x 2, and deleteFamily. So it would be those 4, plus the lockRow/unlockRow. You think it would be okay to include just those in HTable w/o subclassing it?",0.425,0.425,neutral
hbase,798,comment_2,"I""m fine w/ that. You might want to subclass anyways just because it groups this new functionality nicely.",design_debt,non-optimal_design,"Wed, 6 Aug 2008 04:29:41 +0000","Sun, 13 Sep 2009 22:33:29 +0000","Wed, 13 Aug 2008 02:35:03 +0000",597922,"I""m fine w/ that. You might want to subclass anyways just because it groups this new functionality nicely.",0.3,0.3,positive
hbase,8056,comment_4,Could make a new function to place the added code in StoreScanner and mark that it is used in stripe compactions. I think it would be clear for reader. The change is avaiable to me,design_debt,non-optimal_design,"Sat, 9 Mar 2013 00:40:12 +0000","Thu, 16 Jun 2022 05:49:14 +0000","Wed, 20 Mar 2013 01:13:44 +0000",952412,Could make a new function to place the added code in StoreScanner and mark that it is used in stripe compactions. I think it would be clear for reader. The change is avaiable to me,0,0,neutral
hbase,8056,comment_5,Can you store dropDeletesFromRow and dropDeletesToRow in ScanQueryMatcher and keep the logic local there?,design_debt,non-optimal_design,"Sat, 9 Mar 2013 00:40:12 +0000","Thu, 16 Jun 2022 05:49:14 +0000","Wed, 20 Mar 2013 01:13:44 +0000",952412,Can you store dropDeletesFromRow and dropDeletesToRow in ScanQueryMatcher and keep the logic local there?,0,0,neutral
hbase,8089,comment_0,"I'm beginning to think variable-length encoding for anything but char,byte arrays is an unnecessary micro-optimization. Instead of helping a user pack data via encoding, we should encourage the use of compression.",design_debt,non-optimal_design,"Wed, 13 Mar 2013 16:18:34 +0000","Thu, 16 Jun 2022 05:49:38 +0000","Sat, 11 Jun 2022 18:11:53 +0000",291779599,"I'm beginning to think variable-length encoding for anything but char,byte arrays is an unnecessary micro-optimization. Instead of helping a user pack data via encoding, we should encourage the use of compression.",0.2145,0.2145,negative
hbase,8089,comment_18,"Hey Nick, It might be worth updating this jira to reflect the latest state of the work. IIUC this work is about proving a client-side library that does the order-preserving serialization, that higher level projects (eg Phoenix & Kiji) can use for row keys and column qualifiers. Per the other jiras, cell serialization, defining types, and schema are out of scope. These are left to higher-level systems which may make different choices (eg in terms of how to create compound keys) and may have different type models, but at least will be able to share serialization. IMO it's worth considering creating a separate project for this as this is genuinely useful outside HBase (eg container formats) and would benefit from multiple language implementations (the serialization here is language agnostic right?) and so the HBase project may end up being a clunky place to maintain things. Thanks, Eli",design_debt,non-optimal_design,"Wed, 13 Mar 2013 16:18:34 +0000","Thu, 16 Jun 2022 05:49:38 +0000","Sat, 11 Jun 2022 18:11:53 +0000",291779599,"Hey Nick, It might be worth updating this jira to reflect the latest state of the work. IIUC this work is about proving a client-side library that does the order-preserving serialization, that higher level projects (eg Phoenix & Kiji) can use for row keys and column qualifiers. Per the other jiras, cell serialization, defining types, and schema are out of scope. These are left to higher-level systems which may make different choices (eg in terms of how to create compound keys) and may have different type models, but at least will be able to share serialization. IMO it's worth considering creating a separate project for this as this is genuinely useful outside HBase (eg container formats) and would benefit from multiple language implementations (the serialization here is language agnostic right?) and so the HBase project may end up being a clunky place to maintain things. Thanks, Eli",0.35325,0.35325,neutral
hbase,8089,description,"This proposal outlines an improvement to HBase that provides for a set of types, above and beyond the existing ""byte-bucket"" strategy. This is intended to reduce user-level duplication of effort, provide better support for 3rd-party integration, and provide an overall improved experience for developers using HBase.",design_debt,non-optimal_design,"Wed, 13 Mar 2013 16:18:34 +0000","Thu, 16 Jun 2022 05:49:38 +0000","Sat, 11 Jun 2022 18:11:53 +0000",291779599,"This proposal outlines an improvement to HBase that provides for a set of types, above and beyond the existing ""byte-bucket"" strategy. This is intended to reduce user-level duplication of effort, provide better support for 3rd-party integration, and provide an overall improved experience for developers using HBase.",0.496,0.496,positive
hbase,8324,comment_1,"Talked with , and we found this in the MRAppMaster's logs: This is related to MAPREDUCE-4880 which is in turn fixed by MAPREDUCE-4607 (a race in speculative task execution, fixed in 2.0.3-alpha). Compiling and running against hadoop-2.0.3-alpha fails out even earlier so instead of going that route, I'm going to try an alternate workaround -- disabling mapper and reducer speculative execution.",design_debt,non-optimal_design,"Thu, 11 Apr 2013 05:16:56 +0000","Mon, 23 Sep 2013 19:08:23 +0000","Fri, 12 Apr 2013 14:59:26 +0000",121350,"Talked with sandyr, and we found this in the MRAppMaster's logs: This is related to MAPREDUCE-4880 which is in turn fixed by MAPREDUCE-4607 (a race in speculative task execution, fixed in 2.0.3-alpha). Compiling and running against hadoop-2.0.3-alpha fails out even earlier so instead of going that route, I'm going to try an alternate workaround  disabling mapper and reducer speculative execution.",-0.1833333333,-0.1833333333,neutral
hbase,8324,comment_9,I don't think this is a one-off -- this would affect all of our MR tests that use more than one mapper/reducer. I don't think speculative execution should be on for our MR tests in general.,design_debt,non-optimal_design,"Thu, 11 Apr 2013 05:16:56 +0000","Mon, 23 Sep 2013 19:08:23 +0000","Fri, 12 Apr 2013 14:59:26 +0000",121350,I don't think this is a one-off  this would affect all of our MR tests that use more than one mapper/reducer. I don't think speculative execution should be on for our MR tests in general.,0.2,0.2,negative
hbase,8608,description,"Since we enabled blooms, there is a bunch of bloom spew.... We also do a bunch of logging around new file open....",design_debt,non-optimal_design,"Fri, 24 May 2013 00:09:52 +0000","Mon, 23 Sep 2013 19:08:34 +0000","Mon, 10 Jun 2013 22:00:16 +0000",1547424,"Since we enabled blooms, there is a bunch of bloom spew.... We also do a bunch of logging around new file open....",0,0,neutral
hbase,8608,summary,Do an edit of logs.. we log too much.,design_debt,non-optimal_design,"Fri, 24 May 2013 00:09:52 +0000","Mon, 23 Sep 2013 19:08:34 +0000","Mon, 10 Jun 2013 22:00:16 +0000",1547424,Do an edit of logs.. we log too much.,-0.5,-0.5,negative
hbase,8665,comment_11,"Here's the patch. It's rather simple, most complexity is in tests. It allows for both pre-selected compactions (from coprocessors and users) and non-pre-selected. When non-preselected compaction makes it to run() it is selected and executed. There are two special cases: 1) Store priority might have decreased since the compaction was queued; in that case it's re-queued with new priority to avoid inversion. 2) Without selecting, we don't know which pool to go to. We go to small pool by default, and if the compaction is large after selection, queue it to the large pool; it can bounce back if it becomes small while stuck in the large pool. Both of these cases can cause compaction to get continuously requeued, or bounced between pools, however it can only happen if store priority decreases (i.e. other compactions happen), or files are removed from store (same); or files are added in some very special pattern that causes policy to select continuously).",design_debt,non-optimal_design,"Thu, 30 May 2013 23:21:38 +0000","Thu, 5 May 2022 02:11:33 +0000","Wed, 19 Jun 2013 00:42:56 +0000",1646478,"Here's the patch. It's rather simple, most complexity is in tests. It allows for both pre-selected compactions (from coprocessors and users) and non-pre-selected. When non-preselected compaction makes it to run() it is selected and executed. There are two special cases: 1) Store priority might have decreased since the compaction was queued; in that case it's re-queued with new priority to avoid inversion. 2) Without selecting, we don't know which pool to go to. We go to small pool by default, and if the compaction is large after selection, queue it to the large pool; it can bounce back if it becomes small while stuck in the large pool. Both of these cases can cause compaction to get continuously requeued, or bounced between pools, however it can only happen if store priority decreases (i.e. other compactions happen), or files are removed from store (same); or files are added in some very special pattern that causes policy to select large-small-large-small-... continuously).",0.0406875,0.0406875,neutral
hbase,8665,comment_6,I was suggesting bumping the priority instead of queueing another. Bumping the compaction request to what would currently be computed seems reasonable. In this case pre-selecting doesn't seem to be a bad thing. Prioritizing a fast compaction over being a little more efficient seems like a trade off that most people would want. So since current implementation biases towards what I would expect users to want maybe we shouldn't change that until we can put those smarts into the compaction selection (plumbing reason the compaction was requested into selection).,design_debt,non-optimal_design,"Thu, 30 May 2013 23:21:38 +0000","Thu, 5 May 2022 02:11:33 +0000","Wed, 19 Jun 2013 00:42:56 +0000",1646478,"If blocked store comes into a queue full of non-blocked stores why should we bump them? And how much. I was suggesting bumping the priority instead of queueing another. Bumping the compaction request to what would currently be computed seems reasonable. 3) Additionally if we pre-select in advance we simply end up with inefficient compactions, for example in this case we could have compacted 6 small files in one go, but would have ended up compacting 3 and 3 if it spent less time in the queue. In this case pre-selecting doesn't seem to be a bad thing. Prioritizing a fast compaction over being a little more efficient seems like a trade off that most people would want. So since current implementation biases towards what I would expect users to want maybe we shouldn't change that until we can put those smarts into the compaction selection (plumbing reason the compaction was requested into selection).",0.3084,0.152125,neutral
hbase,8665,comment_7,"Well, the effect of getting a faster compaction in this case is a pure accident, if there was not a smaller one already queued, it would still compact 6 according to policy. Also, out of many possible faster compactions in this case, bad one (later files) is chosen, so it's not really what user would expect. Policy should make such decisions - if we prefer faster compactions for blocked store, we should have it in the policy, and so last-moment selection would still choose the best one. As for bumping the priority of current to what it would have been, it is actually equivalent to just sorting them by current store priority... I wonder if there's any fundamental reason to divorce selection from compaction? If we introduce compaction-based priority modifiers, not just store based, we could still apply them by doing selection in multiple stores and comparing priorities. Selecting is not that expensive, given how frequently we compact.",design_debt,non-optimal_design,"Thu, 30 May 2013 23:21:38 +0000","Thu, 5 May 2022 02:11:33 +0000","Wed, 19 Jun 2013 00:42:56 +0000",1646478,"Well, the effect of getting a faster compaction in this case is a pure accident, if there was not a smaller one already queued, it would still compact 6 according to policy. Also, out of many possible faster compactions in this case, bad one (later files) is chosen, so it's not really what user would expect. Policy should make such decisions - if we prefer faster compactions for blocked store, we should have it in the policy, and so last-moment selection would still choose the best one. As for bumping the priority of current to what it would have been, it is actually equivalent to just sorting them by current store priority... I wonder if there's any fundamental reason to divorce selection from compaction? If we introduce compaction-based priority modifiers, not just store based, we could still apply them by doing selection in multiple stores and comparing priorities. Selecting is not that expensive, given how frequently we compact.",0.1791777778,0.1791777778,negative
hbase,8665,description,"Note that this can be solved by bumping up the number of compaction threads but still it seems like this priority ""inversion"" should be dealt with. There's a store with 1 big file and 3 flushes (1 2 3 4) sitting around and minding its own business when it decides to compact. Compaction (2 3 4) is created and put in queue, it's low priority, so it doesn't get out of the queue for some time - other stores are compacting. Meanwhile more files are flushed and at (1 2 3 4 5 6 7) it decides to compact (5 6 7). This compaction now has higher priority than the first one. After that if the load is high it enters vicious cycle of compacting and compacting files as they arrive, with store being blocked on and off, with the (2 3 4) compaction staying in queue for up to ~20 minutes (that I've seen). I wonder why we do thing thing where we queue compaction and compact separately. Perhaps we should take snapshot of all store priorities, then do select in order and execute the first compaction we find. This will need starvation safeguard too but should probably be better. Btw, exploring compaction policy may be more prone to this, as it can select files from the middle, not just beginning, which, given the treatment of already selected files that was not changed from the old ratio-based one (all files with lower seqNums than the ones selected are also ineligible for further selection), will make more files ineligible (e.g. imagine with 10 blocking files, with 8 present (1-8), (6 7 8) being selected and getting stuck). Today I see the case that would also apply to old policy, but yesterday I saw file distribution something like this: 4,5g, 2,1g, 295,9m, 113,3m, 68,0m, 67,8m, 1,1g, 295,1m, 100,4m, unfortunately w/o enough logs to figure out how it resulted.",design_debt,non-optimal_design,"Thu, 30 May 2013 23:21:38 +0000","Thu, 5 May 2022 02:11:33 +0000","Wed, 19 Jun 2013 00:42:56 +0000",1646478,"Note that this can be solved by bumping up the number of compaction threads but still it seems like this priority ""inversion"" should be dealt with. There's a store with 1 big file and 3 flushes (1 2 3 4) sitting around and minding its own business when it decides to compact. Compaction (2 3 4) is created and put in queue, it's low priority, so it doesn't get out of the queue for some time - other stores are compacting. Meanwhile more files are flushed and at (1 2 3 4 5 6 7) it decides to compact (5 6 7). This compaction now has higher priority than the first one. After that if the load is high it enters vicious cycle of compacting and compacting files as they arrive, with store being blocked on and off, with the (2 3 4) compaction staying in queue for up to ~20 minutes (that I've seen). I wonder why we do thing thing where we queue compaction and compact separately. Perhaps we should take snapshot of all store priorities, then do select in order and execute the first compaction we find. This will need starvation safeguard too but should probably be better. Btw, exploring compaction policy may be more prone to this, as it can select files from the middle, not just beginning, which, given the treatment of already selected files that was not changed from the old ratio-based one (all files with lower seqNums than the ones selected are also ineligible for further selection), will make more files ineligible (e.g. imagine with 10 blocking files, with 8 present (1-8), (6 7 8) being selected and getting stuck). Today I see the case that would also apply to old policy, but yesterday I saw file distribution something like this: 4,5g, 2,1g, 295,9m, 113,3m, 68,0m, 67,8m, 1,1g, 295,1m, 100,4m, unfortunately w/o enough logs to figure out how it resulted.",0.1573151515,0.1573151515,neutral
hbase,8816,comment_1,"Should we change argument name concurrent_factor to num_tables? concurrent_factor is not that descriptive. You have changed the parsing of args from being in run(String[]) method to being in main. This breaks the usage model for the Tool interface. We should not do parsing in static main. Maybe you can use the instance which parsed the command line opts as the controller, and spawn all the other LTT instances from that, and join in the end. Can you please update the patch for trunk as well as 0.94. I think this is only for 0.94.",design_debt,non-optimal_design,"Fri, 28 Jun 2013 04:40:38 +0000","Wed, 21 Aug 2013 00:08:49 +0000","Sun, 4 Aug 2013 01:33:55 +0000",3185597,"Should we change argument name concurrent_factor to num_tables? concurrent_factor is not that descriptive. You have changed the parsing of args from being in run(String[]) method to being in main. This breaks the usage model for the Tool interface. We should not do parsing in static main. Maybe you can use the instance which parsed the command line opts as the controller, and spawn all the other LTT instances from that, and join in the end. Can you please update the patch for trunk as well as 0.94. I think this is only for 0.94.",0.1863125,0.1863125,negative
hbase,8816,comment_4,"Why 64 for upper limit on num_tables? We can do Short.MAX_VALUE. We need a trunk version of the patch as well. Lastly, instead of the command line parsing wizardry for passing a modified version of the args to child classes, maybe you can just construct LoadTestTool instances with the same args in WorkerThread (calling processOptions manually), and then call setNumTables(1), and + ""_"" + i).",design_debt,non-optimal_design,"Fri, 28 Jun 2013 04:40:38 +0000","Wed, 21 Aug 2013 00:08:49 +0000","Sun, 4 Aug 2013 01:33:55 +0000",3185597,"Why 64 for upper limit on num_tables? We can do Short.MAX_VALUE. We need a trunk version of the patch as well. Lastly, instead of the command line parsing wizardry for passing a modified version of the args to child classes, maybe you can just construct LoadTestTool instances with the same args in WorkerThread (calling processOptions manually), and then call setNumTables(1), and setTableName(parent.tableName + ""_"" + i).",0.2262,0.1885,neutral
hbase,8816,comment_8,How about passing a comma-separated list of table names to the -tn parameter? That way the one table is simply a special case.,design_debt,non-optimal_design,"Fri, 28 Jun 2013 04:40:38 +0000","Wed, 21 Aug 2013 00:08:49 +0000","Sun, 4 Aug 2013 01:33:55 +0000",3185597,How about passing a comma-separated list of table names to the -tn parameter? That way the one table is simply a special case.,-0.25,-0.25,neutral
hbase,8816,comment_9,"Sounds good, but if you want to run with 50 tables, 10 writer/readers each, then passing that parameter will become ugly, no?",design_debt,non-optimal_design,"Fri, 28 Jun 2013 04:40:38 +0000","Wed, 21 Aug 2013 00:08:49 +0000","Sun, 4 Aug 2013 01:33:55 +0000",3185597,"How about passing a comma-separated list of table names to the -tn parameter? Sounds good, but if you want to run with 50 tables, 10 writer/readers each, then passing that parameter will become ugly, no?",-0.008,-0.254,negative
hbase,8816,description,"Introducing an optional parameter 'num_tables' into LoadTestTool. When it's specified with positive integer n, LoadTestTool will load n tables parallely. -tn parameter value becomes table name prefix. Tables are created with name in format <tn The motivation is to add a handy way to load multiple tables concurrently. In addition, we could use this option to test resource leakage of long running clients.",design_debt,non-optimal_design,"Fri, 28 Jun 2013 04:40:38 +0000","Wed, 21 Aug 2013 00:08:49 +0000","Sun, 4 Aug 2013 01:33:55 +0000",3185597,"Introducing an optional parameter 'num_tables' into LoadTestTool. When it's specified with positive integer n, LoadTestTool will load n tables parallely. -tn parameter value becomes table name prefix. Tables are created with name in format <tn>_1...<tn>_n. A sample command line ""-tn test -num_tables 2"" will create & load tables:""test_1"" and ""test_2"" The motivation is to add a handy way to load multiple tables concurrently. In addition, we could use this option to test resource leakage of long running clients.",0.1955,0.1955,neutral
hbase,9052,description,"If a region is split/merged, before it's removed from meta, you can still assign it from the HBase shell. It's better to prevent this from happening.",design_debt,non-optimal_design,"Fri, 26 Jul 2013 23:48:30 +0000","Mon, 23 Sep 2013 19:22:36 +0000","Tue, 30 Jul 2013 16:56:23 +0000",320873,"If a region is split/merged, before it's removed from meta, you can still assign it from the HBase shell. It's better to prevent this from happening.",0.2155,0.2155,negative
hbase,9315,comment_1,This patch alters the test to wait for the cache to stabilize before asserting on the number of evictions run. This may be overkill.,design_debt,non-optimal_design,"Fri, 23 Aug 2013 00:17:13 +0000","Fri, 20 Nov 2015 11:52:24 +0000","Tue, 27 Aug 2013 16:36:36 +0000",404363,This patch alters the test to wait for the cache to stabilize before asserting on the number of evictions run. This may be overkill.,0,0,negative
hbase,9668,description,The Scan objects used to construct AggregateRequest's for aggregates supported by AggregationClient qualify as small scan because response from each region is small. We should utilize small scan for better performance.,design_debt,non-optimal_design,"Fri, 27 Sep 2013 01:53:26 +0000","Thu, 16 Jun 2022 18:12:40 +0000","Fri, 27 Sep 2013 02:55:22 +0000",3716,The Scan objects used to construct AggregateRequest's for aggregates supported by AggregationClient qualify as small scan because response from each region is small. We should utilize small scan for better performance.,0.45,0.45,neutral
hbase,9671,comment_3,I would expect it to throw an exception like it currently is. The point is that chaos monkey actions shouldn't be trying to avoid exceptions. It should be issuing commands at will and not trying to protect the cluster at all. Anything else can hide bugs.,design_debt,non-optimal_design,"Fri, 27 Sep 2013 14:01:01 +0000","Thu, 16 Jun 2022 18:12:48 +0000","Wed, 27 Nov 2013 23:00:31 +0000",5302770,Can you clarify what would be the expected behavior ? I would expect it to throw an exception like it currently is. The point is that chaos monkey actions shouldn't be trying to avoid exceptions. It should be issuing commands at will and not trying to protect the cluster at all. Anything else can hide bugs.,0.0125,0.01,neutral
hbase,9683,comment_9,"Hi , I agree that the 0.94 RPC is ugly in terms of expansion and it needs much more carefulness and efforts than the PB based one. I believe that the 0.94 series will remain in production for a while (at least for us and for several other users that I know of). Having this back ported to 0.94 will definitely benefit a lot of existing users.",design_debt,non-optimal_design,"Sun, 29 Sep 2013 11:20:55 +0000","Thu, 16 Jun 2022 18:11:25 +0000","Wed, 22 Apr 2015 00:41:58 +0000",49209663,"Hi eclark, I agree that the 0.94 RPC is ugly in terms of expansion and it needs much more carefulness and efforts than the PB based one. I believe that the 0.94 series will remain in production for a while (at least for us and for several other users that I know of). Having this back ported to 0.94 will definitely benefit a lot of existing users.",0.1848888889,0.1848888889,neutral
hbase,9689,comment_9,"There is no way to have a backwards compatible put command that does not take an attributes hash, just a timestamp? It's fine if timestamp has to be specified as part of the attribute hash if one is present.",design_debt,non-optimal_design,"Tue, 1 Oct 2013 09:52:39 +0000","Fri, 20 Nov 2015 11:53:36 +0000","Thu, 17 Oct 2013 17:36:36 +0000",1410237,"So this change is a new behaviour for specifying puts. There is no way to have a backwards compatible put command that does not take an attributes hash, just a timestamp? It's fine if timestamp has to be specified as part of the attribute hash if one is present.",-0.1125,-0.075,negative
hbase,9806,comment_12,"Sorry for the delays,  -- Strata and all that. I wanted to add a kind of self-tuning to the patch, give it the ability to write records until it fills up the cache and no more. No luck as of yet, which means determining how many rows to write requires a little guess-work and checking the logs. I've run the test for 3g (-r 1700000) and 20g (-r 12000000) heaps using the attached configs and 100 iterations. Attached also are the charts I generated from the logs. I tried 1000 iterations to see if anything happens over a longer interval but nothing exciting. I'm also updating my log-parsing script to overly the GC events. More to follow. On your fancy rig, can you run for maybe 8g, 16g, 20g, and 28g? You'll have to apply one of these conf patches and then adjust the heap size yourself. You'll also need to play with the -r param to work out how many rows you can fit into the cache w.o eviction (please let me know what you end up using!) -i 100 should be a good starting point, but if you have time I'd take logs from 500 or 1000. Thank you much!",design_debt,non-optimal_design,"Fri, 18 Oct 2013 22:13:17 +0000","Thu, 16 Jun 2022 18:22:32 +0000","Thu, 16 Jun 2022 18:22:30 +0000",273269353,"Sorry for the delays, jmspaggi  Strata and all that. I wanted to add a kind of self-tuning to the patch, give it the ability to write records until it fills up the cache and no more. No luck as of yet, which means determining how many rows to write requires a little guess-work and checking the logs. I've run the test for 3g (-r 1700000) and 20g (-r 12000000) heaps using the attached configs and 100 iterations. Attached also are the charts I generated from the logs. I tried 1000 iterations to see if anything happens over a longer interval but nothing exciting. I'm also updating my log-parsing script to overly the GC events. More to follow. On your fancy rig, can you run for maybe 8g, 16g, 20g, and 28g? You'll have to apply one of these conf patches and then adjust the heap size yourself. You'll also need to play with the -r param to work out how many rows you can fit into the cache w.o eviction (please let me know what you end up using!) -i 100 should be a good starting point, but if you have time I'd take logs from 500 or 1000. Thank you much!",0.1285961538,0.1285961538,neutral
hbase,9806,comment_7,"Any chance one of you has the configs laying around from the benchmarks run on the slabcache [announcement I'm attempting to reproduce and slabcache looks pretty unstable. I wonder if I'm exercising some edge-case based on my configuration or if there's some bit-rot happened. Specifically, I'm looking for values for HBASE_HEAPSIZE, and anything else that was using a non-default value. Thanks.",design_debt,non-optimal_design,"Fri, 18 Oct 2013 22:13:17 +0000","Thu, 16 Jun 2022 18:22:32 +0000","Thu, 16 Jun 2022 18:22:30 +0000",273269353,"tlipcon li jdcryans Any chance one of you has the configs laying around from the benchmarks run on the slabcache announcement post? I'm attempting to reproduce and slabcache looks pretty unstable. I wonder if I'm exercising some edge-case based on my configuration or if there's some bit-rot happened. Specifically, I'm looking for values for HBASE_HEAPSIZE, -XX:MaxDirectMemorySize, hfile.block.cache.size, hbase.regionserver.global.memstore.upperLimit, hbase.regionserver.global.memstore.lowerLimit, hbase.offheapcache.percentage, hbase.offheapcache.slab.proportions, and anything else that was using a non-default value. Thanks.",0.4499166667,0.08728571429,neutral
hbase,980,description,"Profiling, I learned that the HBASE-975 makes things worse rather than better. For every Reader opened -- one is opened per store file when we open a region as well as a Reader per file when we compact and then another Reader whenever a Scanner is opened -- the change adds about 4 seeks and at least in the case of compacting and scanning, to no benefit. Even where it is of benefit, when going against HalfMapFiles or when many Store files and we're testing to see if row is in file, it looks like the number of seeks saved are miniscule -- definetly not something that would show up in timings. This issue is about undoing the get of first and last key on open of a store file, the heart of HBASE-975 (975 included a bunch of cleanup refactoring. That'll stay). Profiling seeks, I did notice that we do an extra seek during a get, a reset that takes us to the start of the file. Then internally to getClosest, the core of our get, we're also doing a seek to closest index. Let me try undoing the extra seek and see if it breaks things.",design_debt,non-optimal_design,"Tue, 4 Nov 2008 06:46:48 +0000","Sun, 13 Sep 2009 22:26:31 +0000","Tue, 4 Nov 2008 08:29:14 +0000",6146,"Profiling, I learned that the HBASE-975 makes things worse rather than better. For every Reader opened  one is opened per store file when we open a region as well as a Reader per file when we compact and then another Reader whenever a Scanner is opened  the change adds about 4 seeks and at least in the case of compacting and scanning, to no benefit. Even where it is of benefit, when going against HalfMapFiles or when many Store files and we're testing to see if row is in file, it looks like the number of seeks saved are miniscule  definetly not something that would show up in timings. This issue is about undoing the get of first and last key on open of a store file, the heart of HBASE-975 (975 included a bunch of cleanup refactoring. That'll stay). Profiling seeks, I did notice that we do an extra seek during a get, a reset that takes us to the start of the file. Then internally to getClosest, the core of our get, we're also doing a seek to closest index. Let me try undoing the extra seek and see if it breaks things.",0.1453541667,0.1453541667,negative
hbase,9950,comment_2,"not sure yet. Since there is no notion of row level data in hbase storage, I would have to create some special KVs that are stored for the row, which sounds very hacky.  Replication scope is defined at the CF level, so I don't think Ill be able to use it. I do need to plug in custom replication policy though if this is not a core feature. There are no observers for replication, are there?",design_debt,non-optimal_design,"Mon, 11 Nov 2013 22:26:36 +0000","Thu, 16 Jun 2022 18:32:26 +0000","Tue, 14 May 2019 00:38:34 +0000",173585518,"stack not sure yet. Since there is no notion of row level data in hbase storage, I would have to create some special KVs that are stored for the row, which sounds very hacky. apurtell Replication scope is defined at the CF level, so I don't think Ill be able to use it. I do need to plug in custom replication policy though if this is not a core feature. There are no observers for replication, are there?",0.03193333333,0.03193333333,negative
hbase,9950,description,"We have a replication setup with the same table and column family being present in multiple data centers. Currently, all of them have exactly the same data, but each cluster doesn't need all the data. Rows need to be present in only x out of the total y clusters. This information varies at the row level and thus more granular replication cannot be achieved by setting up cluster level replication. Adding row level replication should solve this.",design_debt,non-optimal_design,"Mon, 11 Nov 2013 22:26:36 +0000","Thu, 16 Jun 2022 18:32:26 +0000","Tue, 14 May 2019 00:38:34 +0000",173585518,"We have a replication setup with the same table and column family being present in multiple data centers. Currently, all of them have exactly the same data, but each cluster doesn't need all the data. Rows need to be present in only x out of the total y clusters. This information varies at the row level and thus more granular replication cannot be achieved by setting up cluster level replication. Adding row level replication should solve this.",0.04,0.04,neutral
hbase,10074,comment_2,"I don't read docbook, so I cannot comment about the markup. However, the content is great! Here are some nits. Mind adding some JIRA references here? ""store file index to rise rising"" ? This sentence is confusing me. How about ""Hosting only 5 regions per RS will not be enough task splits for a mapreduce job, while 1000 regions will generate far too many map tasks."" In section {{<section +1",documentation_debt,low_quality_documentation,"Tue, 3 Dec 2013 22:47:13 +0000","Mon, 16 Dec 2013 18:46:36 +0000","Thu, 5 Dec 2013 23:22:28 +0000",174915,"I don't read docbook, so I cannot comment about the markup. However, the content is great! Here are some nits. <para>The master as is is allergic to tons of regions Mind adding some JIRA references here? tons of regions on a few RS can cause the store file index to rise raising heap usage and... ""store file index to rise rising"" ? This sentence is confusing me. Keeping 5 regions per RS would be too low for a job, whereas 1000 will generate too many maps. How about ""Hosting only 5 regions per RS will not be enough task splits for a mapreduce job, while 1000 regions will generate far too many map tasks."" In section <section xml:id=""ops.capacity.regions""><title>Determining region count and size</title> you suggest ""20-200 regions per RS"" but previously you said ""20-100"". +1",-0.023375,-0.01141666667,positive
hbase,10074,comment_4,+1 I see a few spelling issues and would maybe change some of the numbers and statements but can be done in another issue. This reorg is excellent. Thanks .,documentation_debt,low_quality_documentation,"Tue, 3 Dec 2013 22:47:13 +0000","Mon, 16 Dec 2013 18:46:36 +0000","Thu, 5 Dec 2013 23:22:28 +0000",174915,+1 I see a few spelling issues and would maybe change some of the numbers and statements but can be done in another issue. This reorg is excellent. Thanks sershe.,0.3833333333,0.3833333333,positive
hbase,10074,comment_6,"incorporated feedback, some spelling fixes and rephrases. I'd assume +1 stands, will commit in the afternoon",documentation_debt,low_quality_documentation,"Tue, 3 Dec 2013 22:47:13 +0000","Mon, 16 Dec 2013 18:46:36 +0000","Thu, 5 Dec 2013 23:22:28 +0000",174915,"incorporated feedback, some spelling fixes and rephrases. I'd assume +1 stands, will commit in the afternoon",0.1,0.1,neutral
hbase,10074,description,"Region count description is in config section; region size description is in architecture sections; both of these have a lot of good technical details, but imho we could do better in terms of admin-centric advice. Currently, there's a nearly-empty capacity section; I'd like to rewrite it to consolidate capacity sizing information, and some basic configuration pertaining to it.",documentation_debt,low_quality_documentation,"Tue, 3 Dec 2013 22:47:13 +0000","Mon, 16 Dec 2013 18:46:36 +0000","Thu, 5 Dec 2013 23:22:28 +0000",174915,"Region count description is in config section; region size description is in architecture sections; both of these have a lot of good technical details, but imho we could do better in terms of admin-centric advice. Currently, there's a nearly-empty capacity section; I'd like to rewrite it to consolidate capacity planning/sizing/region sizing information, and some basic configuration pertaining to it.",0.219,0.219,neutral
hbase,10074,summary,consolidate and improve capacity/sizing documentation,documentation_debt,low_quality_documentation,"Tue, 3 Dec 2013 22:47:13 +0000","Mon, 16 Dec 2013 18:46:36 +0000","Thu, 5 Dec 2013 23:22:28 +0000",174915,consolidate and improve capacity/sizing documentation,0.4,0.4,neutral
hbase,10081,description,"Discussed in HBASE-7091. It's not critical, but a little bit surprising, as the comments in bin/hbase doesn't say anything about this. If you create your own hbase-env then it's not an issue...",documentation_debt,outdated_documentation,"Wed, 4 Dec 2013 17:50:48 +0000","Fri, 17 Jun 2022 04:50:59 +0000","Wed, 22 May 2019 01:31:31 +0000",172309243,"Discussed in HBASE-7091. It's not critical, but a little bit surprising, as the comments in bin/hbase doesn't say anything about this. If you create your own hbase-env then it's not an issue...",0,0,neutral
hbase,10213,comment_10,Thanks for the comment  and I should pay more attention on spelling.,documentation_debt,low_quality_documentation,"Fri, 20 Dec 2013 09:34:06 +0000","Sat, 21 Feb 2015 23:29:00 +0000","Tue, 7 Jan 2014 22:15:26 +0000",1600880,Thanks for the comment apurtell and yuzhihong@gmail.com. I should pay more attention on spelling.,0.1,0.1,neutral
hbase,10213,comment_8,"Committed to 0.98. Thanks Ted. I fixed a spelling error on commit. Attached is an addendum for trunk to match what went into 0.98. I committed this trivial change to trunk using CTR as r1554361.  and : We can close this after a decision on 0.96 and 0.94. Almost feel bad pinging you, should be no big deal.",documentation_debt,low_quality_documentation,"Fri, 20 Dec 2013 09:34:06 +0000","Sat, 21 Feb 2015 23:29:00 +0000","Tue, 7 Jan 2014 22:15:26 +0000",1600880,"Committed to 0.98. Thanks Ted. I fixed a spelling error on commit. Attached is an addendum for trunk to match what went into 0.98. I committed this trivial change to trunk using CTR as r1554361. stack and lhofhansl: We can close this after a decision on 0.96 and 0.94. Almost feel bad pinging you, should be no big deal.",0.1285714286,0.1285714286,neutral
hbase,10213,comment_9,"Thanks for the catch, Andy. It would be not so good if a public method has spelling mistake.",documentation_debt,low_quality_documentation,"Fri, 20 Dec 2013 09:34:06 +0000","Sat, 21 Feb 2015 23:29:00 +0000","Tue, 7 Jan 2014 22:15:26 +0000",1600880,"Thanks for the catch, Andy. It would be not so good if a public method has spelling mistake.",0.294,0.294,negative
hbase,10334,description,The links to RS's seems to be broken in table.jsp after HBASE-9892.,documentation_debt,low_quality_documentation,"Tue, 14 Jan 2014 04:03:25 +0000","Sat, 21 Feb 2015 23:31:51 +0000","Tue, 14 Jan 2014 23:23:46 +0000",69621,The links to RS's seems to be broken in table.jsp after HBASE-9892.,-0.1,-0.1,negative
hbase,10334,summary,RegionServer links in table.jsp is broken,documentation_debt,low_quality_documentation,"Tue, 14 Jan 2014 04:03:25 +0000","Sat, 21 Feb 2015 23:31:51 +0000","Tue, 14 Jan 2014 23:23:46 +0000",69621,RegionServer links in table.jsp is broken,-0.1,-0.1,negative
hbase,10864,comment_5,"Oh dear, i realized what the problem was, I plopped the wrong patch file in. The other one fixes something like 30 spelling mistakes. It may still be too minor",documentation_debt,low_quality_documentation,"Fri, 28 Mar 2014 17:42:56 +0000","Sat, 21 Feb 2015 23:38:18 +0000","Sat, 29 Mar 2014 03:40:27 +0000",35851,"Oh dear, i realized what the problem was, I plopped the wrong patch file in. The other one fixes something like 30 spelling mistakes. It may still be too minor",-0.05288888889,-0.05288888889,negative
hbase,10864,description,We should really be more careful about spelling qualifier,documentation_debt,low_quality_documentation,"Fri, 28 Mar 2014 17:42:56 +0000","Sat, 21 Feb 2015 23:38:18 +0000","Sat, 29 Mar 2014 03:40:27 +0000",35851,We should really be more careful about spelling qualifier,0.4,0.4,negative
hbase,10864,summary,Spelling nit,documentation_debt,low_quality_documentation,"Fri, 28 Mar 2014 17:42:56 +0000","Sat, 21 Feb 2015 23:38:18 +0000","Sat, 29 Mar 2014 03:40:27 +0000",35851,Spelling nit,0,0,neutral
hbase,11053,comment_13,Javadoc fix. I missed this. Thanks Ted for the heads up.,documentation_debt,low_quality_documentation,"Wed, 23 Apr 2014 11:25:54 +0000","Sat, 21 Feb 2015 23:31:39 +0000","Sat, 26 Apr 2014 08:22:21 +0000",248187,Javadoc fix. I missed this. Thanks Ted for the heads up.,0,0,neutral
hbase,11421,description,"HTable#batch() APIs javadoc says * @param actions list of Get, Put, Delete, Increment, Append, RowMutations objects But we can not pass RowMutations in batch. Small patch to correct the doc.",documentation_debt,low_quality_documentation,"Fri, 27 Jun 2014 11:36:39 +0000","Sat, 21 Feb 2015 23:29:51 +0000","Fri, 27 Jun 2014 17:02:42 +0000",19563,"HTable#batch() APIs javadoc says @param actions list of Get, Put, Delete, Increment, Append, RowMutations objects But we can not pass RowMutations in batch. Small patch to correct the doc.",0.4375,0.4375,negative
hbase,11421,summary,HTableInterface javadoc correction,documentation_debt,low_quality_documentation,"Fri, 27 Jun 2014 11:36:39 +0000","Sat, 21 Feb 2015 23:29:51 +0000","Fri, 27 Jun 2014 17:02:42 +0000",19563,HTableInterface javadoc correction,0,0,neutral
hbase,11575,comment_1,"Looked into it and attached a patch that fixed some documentation mismatches, changed the pseudo distributed mode setting a little so that we just start one instance. Additional regionservers can be started with the local regionserver sh script.",documentation_debt,low_quality_documentation,"Wed, 23 Jul 2014 00:18:13 +0000","Fri, 6 Apr 2018 17:50:52 +0000","Thu, 24 Jul 2014 17:12:20 +0000",147247,"Looked into it and attached a patch that fixed some documentation mismatches, changed the pseudo distributed mode setting a little so that we just start one instance. Additional regionservers can be started with the local regionserver sh script.",0,0,neutral
hbase,11730,description,"New development goes against trunk and is backported as desired to existing release branches. From what I have seen on the jira, it looks like each branch's release manager makes the call on backporting a particular issue. We should document both this norm and who the relevant release manager is for each branch. In the current docs, I'd suggest adding the RM list to the ""Codelines"" section (18.11.1) and add a brief explanation of pinging the RM as a new section after ""submitting a patch again"" (18.12.6). Post HBASE-4593, the note about pinging a prior branch RM should just go as a bullet in the ""patch workflow.""",documentation_debt,low_quality_documentation,"Wed, 13 Aug 2014 15:12:15 +0000","Fri, 6 Apr 2018 17:55:46 +0000","Thu, 18 Sep 2014 02:52:28 +0000",3066013,"New development goes against trunk and is backported as desired to existing release branches. From what I have seen on the jira, it looks like each branch's release manager makes the call on backporting a particular issue. We should document both this norm and who the relevant release manager is for each branch. In the current docs, I'd suggest adding the RM list to the ""Codelines"" section (18.11.1) and add a brief explanation of pinging the RM as a new section after ""submitting a patch again"" (18.12.6). Post HBASE-4593, the note about pinging a prior branch RM should just go as a bullet in the ""patch workflow.""",0.18,0.18,neutral
hbase,12400,comment_1,"Thank you for taking this on . In this patch find See how it changes our example code to do the 'new' way. I was thinking of changing places where we have bits of code so there is a 1.0 version, the more prominent, and then the old way of doing it as it is now. I can do this one, np... might take you a good while more time than I since I've had my head in this a while now. Otherwise, ask more questions if not clear. When HBASE-12404 goes in, there will be more examples to pull from. Thanks.",documentation_debt,low_quality_documentation,"Fri, 31 Oct 2014 21:19:58 +0000","Fri, 6 Apr 2018 17:54:34 +0000","Wed, 26 Nov 2014 17:31:28 +0000",2232690,"Thank you for taking this on syuanjiang. In this patch https://issues.apache.org/jira/secure/attachment/12683500/12404v20.txt, find client/package-info.java See how it changes our example code to do the 'new' way. I was thinking of changing places where we have bits of code so there is a 1.0 version, the more prominent, and then the old way of doing it as it is now. I can do this one, np... might take you a good while more time than I since I've had my head in this a while now. Otherwise, ask more questions if not clear. When HBASE-12404 goes in, there will be more examples to pull from. Thanks.",0.3125714286,0.2735,positive
hbase,12400,description,"The refguide has bits of code in it. The code does 'new HTable' to get a table instance. Rather, it should be promoting the new style where we get a Connection and then do a getTable on it. Ditto for references to 'new HBaseAdmin'. See ConnectionFactory for new style. See also package-info.java in Client for updated example. Misty, if you are game for this one, I can help w/ how it should look.",documentation_debt,outdated_documentation,"Fri, 31 Oct 2014 21:19:58 +0000","Fri, 6 Apr 2018 17:54:34 +0000","Wed, 26 Nov 2014 17:31:28 +0000",2232690,"The refguide has bits of code in it. The code does 'new HTable' to get a table instance. Rather, it should be promoting the new style where we get a Connection and then do a getTable on it. Ditto for references to 'new HBaseAdmin'. See ConnectionFactory for new style. See also package-info.java in Client for updated example. Misty, if you are game for this one, I can help w/ how it should look.",0.09375,0.09375,neutral
hbase,12428,comment_11,"I believe you guys. :) Still weird, we're pulling this out of any method into the global context so it's executed as soon as the script is loaded by the interpreter, and set as a default everywhere? At least warrants a comment, I think.",documentation_debt,outdated_documentation,"Tue, 4 Nov 2014 20:56:08 +0000","Fri, 6 Apr 2018 17:54:55 +0000","Thu, 6 Nov 2014 21:04:28 +0000",173300,"I believe you guys. Still weird, we're pulling this out of any method into the global context so it's executed as soon as the script is loaded by the interpreter, and set as a default everywhere? At least warrants a comment, I think.",-0.05626666667,-0.1036666667,neutral
hbase,1271,comment_2,"Evgeny, - I like your approach with the ++port, but we need to note that you are changing it for the general use case, not just the tests. Any use of RegionServer or LocalHBaseCluster will now have the ++port logic in it. This means that a user who sets his port in a config to 5000 may spin up a server on port 5002 instead of producing an error. It's not clear to me that that is a good change necessarily? - LOG.info(""Faild binding Master to ... should fix spelling of ""Failed"" Looks great on the ZooKeeper side of things and otherwise. Thanks for taking this on :).",documentation_debt,low_quality_documentation,"Fri, 20 Mar 2009 05:00:09 +0000","Sun, 13 Sep 2009 22:24:29 +0000","Wed, 29 Apr 2009 23:15:43 +0000",3521734,"Evgeny, I like your approach with the ++port, but we need to note that you are changing it for the general use case, not just the tests. Any use of RegionServer or LocalHBaseCluster will now have the ++port logic in it. This means that a user who sets his port in a config to 5000 may spin up a server on port 5002 instead of producing an error. It's not clear to me that that is a good change necessarily? LOG.info(""Faild binding Master to ... should fix spelling of ""Failed"" Looks great on the ZooKeeper side of things and otherwise. Thanks for taking this on .",0.00819047619,0.02247619048,neutral
hbase,1271,comment_8,* moved port++ from LocalHBaseCluster to MiniHBaseCluster level * fixed spelling :) * Still auto port binding for Info Server. I think it is really useful.,documentation_debt,low_quality_documentation,"Fri, 20 Mar 2009 05:00:09 +0000","Sun, 13 Sep 2009 22:24:29 +0000","Wed, 29 Apr 2009 23:15:43 +0000",3521734,moved port++ from LocalHBaseCluster to MiniHBaseCluster level fixed spelling Still auto port binding for Info Server. I think it is really useful.,0.42025,0.3905,positive
hbase,12729,comment_9,"Did a 10 minute looksee, lgtm. Spelling Nit: Is the synchronized needed here: Could get the ServerStatitics, if null create one and then use putIfAbsent. In the race case we'd create the object in vain, but we'd save the synchronized. Is this even on the hot path?",documentation_debt,low_quality_documentation,"Fri, 19 Dec 2014 19:49:36 +0000","Fri, 20 Nov 2015 11:54:27 +0000","Tue, 20 Jan 2015 01:07:09 +0000",2697453,"Did a 10 minute looksee, lgtm. Spelling Nit: Is the synchronized needed here: Could get the ServerStatitics, if null create one and then use putIfAbsent. In the race case we'd create the object in vain, but we'd save the synchronized. Is this even on the hot path?",-0.02025,-0.02025,neutral
hbase,13184,comment_1,"HBASE-10513 added user documentation for Phase 1, but the book still does not contain changes reflecting Phase 2. I have written up something already, but did not get around to get them into asciidoc yet. I'll create a subtask in HBASE-10070 for that. Do you have any suggestions for how to organize it better ?",documentation_debt,outdated_documentation,"Tue, 10 Mar 2015 01:42:28 +0000","Thu, 23 Jun 2022 20:31:23 +0000","Wed, 29 Apr 2015 14:34:05 +0000",4366297,"I think the whole region replica area could use a rewrite and reorganization. HBASE-10513 added user documentation for Phase 1, but the book still does not contain changes reflecting Phase 2. I have written up something already, but did not get around to get them into asciidoc yet. I'll create a subtask in HBASE-10070 for that. Do you have any suggestions for how to organize it better ?",0.19525,0.1562,negative
hbase,13395,comment_19,please add a release note that lets downstream folks know what has happened and what they should do to account for it. This will be particularly important for folks coming from 0.98.,documentation_debt,outdated_documentation,"Fri, 3 Apr 2015 09:37:09 +0000","Thu, 16 Jun 2022 18:18:32 +0000","Sat, 25 Mar 2017 16:09:04 +0000",62404315,please add a release note that lets downstream folks know what has happened and what they should do to account for it. This will be particularly important for folks coming from 0.98.,0.3,0.3,neutral
hbase,13582,comment_0,I think I remember seeing some references to the old package (org.htrace) instead of the one for the ASF project (org.apache.htrace) in the book which could be updated as well.,documentation_debt,outdated_documentation,"Tue, 28 Apr 2015 15:39:49 +0000","Fri, 24 Jun 2022 18:23:50 +0000","Tue, 19 May 2015 23:06:06 +0000",1841177,I think I remember seeing some references to the old package (org.htrace) instead of the one for the ASF project (org.apache.htrace) in the book which could be updated as well.,0.15775,0.15775,neutral
hbase,13582,comment_1,This patch fixes all the issues I could find with the documentation. It also includes a few Asciidoc things and typos etc. so I changed the JIRA title slightly,documentation_debt,low_quality_documentation,"Tue, 28 Apr 2015 15:39:49 +0000","Fri, 24 Jun 2022 18:23:50 +0000","Tue, 19 May 2015 23:06:06 +0000",1841177,This patch fixes all the issues I could find with the documentation. It also includes a few Asciidoc things and typos etc. so I changed the JIRA title slightly,0,0,neutral
hbase,13582,description,the ref guide currently points to HTrace at its old location. update it to point at the ASF project. Should also verify that the usage example is still correct.,documentation_debt,outdated_documentation,"Tue, 28 Apr 2015 15:39:49 +0000","Fri, 24 Jun 2022 18:23:50 +0000","Tue, 19 May 2015 23:06:06 +0000",1841177,the ref guide currently points to HTrace at its old location. update it to point at the ASF project. Should also verify that the usage example is still correct.,0.1926666667,0.1926666667,neutral
hbase,13629,description,Parent introduced a typo. should be,documentation_debt,low_quality_documentation,"Wed, 6 May 2015 05:19:16 +0000","Fri, 24 Jun 2022 18:45:02 +0000","Wed, 6 May 2015 05:33:09 +0000",833,Parent introduced a typo. should be,0,0,negative
hbase,13799,comment_0,Fixup of javadoc on Scan class.,documentation_debt,low_quality_documentation,"Thu, 28 May 2015 23:17:02 +0000","Mon, 31 Aug 2015 22:39:33 +0000","Fri, 29 May 2015 18:54:02 +0000",70620,Fixup of javadoc on Scan class.,0,0,neutral
hbase,13871,comment_1,"LGTM. nit, createFirstOnRow can have a small doc.",documentation_debt,outdated_documentation,"Tue, 9 Jun 2015 06:25:08 +0000","Fri, 17 Jun 2022 05:25:11 +0000","Wed, 10 Jun 2015 13:12:06 +0000",110818,"LGTM. nit, createFirstOnRow can have a small doc.",0,0,neutral
hbase,13924,comment_0,Should we fix the doc to remove coprocessors? Or should we fix the code to use this config value to load jars for coprocessors?,documentation_debt,low_quality_documentation,"Wed, 17 Jun 2015 12:58:31 +0000","Fri, 24 Jun 2022 19:24:29 +0000","Mon, 10 Aug 2015 04:08:38 +0000",4633807,Should we fix the doc to remove coprocessors? Or should we fix the code to use this config value to load jars for coprocessors?,0,0,neutral
hbase,13924,comment_3,HBASE-13867 also targets to improve coprocessor documentation.,documentation_debt,low_quality_documentation,"Wed, 17 Jun 2015 12:58:31 +0000","Fri, 24 Jun 2022 19:24:29 +0000","Mon, 10 Aug 2015 04:08:38 +0000",4633807,HBASE-13867 also targets to improve coprocessor documentation.,0.4,0.4,neutral
hbase,13924,description,"The description in the following is wrong: The is *not* used for coprocessors, but only for filters, comparators, and exceptions. Fix.",documentation_debt,low_quality_documentation,"Wed, 17 Jun 2015 12:58:31 +0000","Fri, 24 Jun 2022 19:24:29 +0000","Mon, 10 Aug 2015 04:08:38 +0000",4633807,"The description in the following is wrong: The DynamicClassLoader is not used for coprocessors, but only for filters, comparators, and exceptions. Fix.",-0.125,-0.125,negative
hbase,13924,summary,Description for is wrong,documentation_debt,low_quality_documentation,"Wed, 17 Jun 2015 12:58:31 +0000","Fri, 24 Jun 2022 19:24:29 +0000","Mon, 10 Aug 2015 04:08:38 +0000",4633807,Description for hbase.dynamic.jars.dir is wrong,-0.25,-0.0625,negative
hbase,13973,comment_2,Some nits: 1. You should talk about the fact the storefile refresher is not needed when Async WAL replication is ON. Basically throw some color on this to make it clear as to when to use what. 2. There is a copy-paste issue in the section on configuration - the description of talks about meta replication which it shouldn't..,documentation_debt,low_quality_documentation,"Thu, 25 Jun 2015 22:54:28 +0000","Fri, 17 Jun 2022 04:50:43 +0000","Fri, 26 Jun 2015 22:31:54 +0000",85046,Some nits: 1. You should talk about the fact the storefile refresher is not needed when Async WAL replication is ON. Basically throw some color on this to make it clear as to when to use what. 2. There is a copy-paste issue in the section on configuration - the description of hbase.regionserver.storefile.refresh.period talks about meta replication which it shouldn't..,0.1,0.05555555556,neutral
hbase,14984,comment_1,+1. I can't figure out what the javadoc warning is supposed to be based on looking at the precommit artifacts and the patch.,documentation_debt,low_quality_documentation,"Tue, 15 Dec 2015 21:04:15 +0000","Thu, 17 Dec 2015 23:59:36 +0000","Wed, 16 Dec 2015 18:59:38 +0000",78923,+1. I can't figure out what the javadoc warning is supposed to be based on looking at the precommit artifacts and the patch.,0.3,0.3,negative
hbase,15835,comment_9,"Submitting a revised patch which includes all of the following... Subtask 1: Remove instances of setting the ports to -1 in existing tests. The following modules were modified to remove their (now apparently extraneous) setting of master-info-port and region-server-port: Subtask 2: Add some class-level javadoc. The following was added to the HBaseTestingUtility class-level javadoc comment: Subtask 3: Add a debug-level logging message for when port values are overridden to ""-1"". The following code now appears at the end of the main constructor for Note the ""debug"" logging that has been added: Subtask 4: Add new method to for testing port overrides. The following new method assures that port override is taking place when it should, and is NOT taking place when it should NOT:",documentation_debt,low_quality_documentation,"Mon, 16 May 2016 08:54:16 +0000","Fri, 1 Jul 2022 20:27:59 +0000","Wed, 9 May 2018 14:58:37 +0000",62489061,"Submitting a revised patch which includes all of the following... Subtask 1: Remove instances of setting the ports to -1 in existing tests. The following modules were modified to remove their (now apparently extraneous) setting of master-info-port and region-server-port: Subtask 2: Add some class-level javadoc. The following was added to the HBaseTestingUtility class-level javadoc comment: Subtask 3: Add a debug-level logging message for when port values are overridden to ""-1"". The following code now appears at the end of the main constructor for HBaseTestingUtility. Note the ""debug"" logging that has been added: Subtask 4: Add new method to TestHBaseTestingUtility for testing port overrides. The following new method assures that port override is taking place when it should, and is NOT taking place when it should NOT:",0.1,0.08333333333,neutral
hbase,15892,comment_6,Pushed to master branch. Lets try it.  helped me w/ test. I needed to do this: Run this first : pip install gitpython && pip install rbtools Excellent work  Bit of doc please in refguide when you get a chance so we can easily point folks this route.,documentation_debt,outdated_documentation,"Thu, 26 May 2016 12:17:12 +0000","Tue, 31 May 2016 02:27:03 +0000","Thu, 26 May 2016 16:20:02 +0000",14570,Pushed to master branch. Lets try it. appy helped me w/ test. I needed to do this: Run this first : pip install gitpython && pip install rbtools Excellent work appy Bit of doc please in refguide when you get a chance so we can easily point folks this route.,0.246875,0.246875,positive
hbase,1655,comment_12,"I'm +1 on the changes described. I'm not a big fan of putting a big comment in the HTable constructor javadoc about being able to pass it a null. IMO Something like that belongs in a // comment in the code, not exposed in the javadoc of the most client-facing class we have. It's for developers, not users. Users who pass a null HBC will soon learn that this will not work, why do they care about unit testing?",documentation_debt,low_quality_documentation,"Tue, 14 Jul 2009 19:52:59 +0000","Wed, 12 Aug 2020 23:02:36 +0000","Fri, 17 Jul 2009 19:26:30 +0000",257611,"I'm +1 on the changes described. I'm not a big fan of putting a big comment in the HTable constructor javadoc about being able to pass it a null. IMO Something like that belongs in a // comment in the code, not exposed in the javadoc of the most client-facing class we have. It's for developers, not users. Users who pass a null HBC will soon learn that this will not work, why do they care about unit testing?",-0.0644,-0.0644,negative
hbase,16856,description,"A very small bug, a typo in exception message: It should print currentSequence and syncFutureSequence, but print two syncFutureSequence",documentation_debt,low_quality_documentation,"Mon, 17 Oct 2016 06:22:14 +0000","Tue, 18 Oct 2016 02:49:44 +0000","Mon, 17 Oct 2016 14:24:16 +0000",28922,"A very small bug, a typo in exception message: It should print currentSequence and syncFutureSequence, but print two syncFutureSequence",0,0,negative
hbase,16872,comment_0,"Ah there is a typo in the comment. ""We need to the MultiRequest"" = Will fix on commit.",documentation_debt,low_quality_documentation,"Tue, 18 Oct 2016 14:48:06 +0000","Thu, 20 Oct 2016 09:12:21 +0000","Thu, 20 Oct 2016 01:41:18 +0000",125592,"Ah there is a typo in the comment. ""We need to the MultiRequest"" => ""We need the MultiRequest"" Will fix on commit.",0.1,0.1,negative
hbase,16872,comment_2,Please fix below javadoc warnings as well as the comment mentioned above: Other parts lgtm. +1,documentation_debt,low_quality_documentation,"Tue, 18 Oct 2016 14:48:06 +0000","Thu, 20 Oct 2016 09:12:21 +0000","Thu, 20 Oct 2016 01:41:18 +0000",125592,Please fix below javadoc warnings as well as the comment mentioned above: Other parts lgtm. +1,0.0385,0.0385,neutral
hbase,16872,comment_3,Fix javadoc issues.,documentation_debt,low_quality_documentation,"Tue, 18 Oct 2016 14:48:06 +0000","Thu, 20 Oct 2016 09:12:21 +0000","Thu, 20 Oct 2016 01:41:18 +0000",125592,Fix javadoc issues.,0,0,negative
hbase,17338,comment_18,Yes Stack. I have done edit to the doc so as to change the solution detailing part as per the discussion in this jira. Thanks for the remind. Are you ok with this patch as such. I may have to rebase it. Let me check. Tks,documentation_debt,low_quality_documentation,"Mon, 19 Dec 2016 13:18:49 +0000","Fri, 1 Jul 2022 21:33:56 +0000","Fri, 10 Mar 2017 05:43:36 +0000",6971087,Yes Stack. I have done edit to the doc so as to change the solution detailing part as per the discussion in this jira. Thanks for the remind. Are you ok with this patch as such. I may have to rebase it. Let me check. Tks,0.2214285714,0.2214285714,neutral
hbase,17394,comment_1,"Update, the current logic is correct, for TimeRangeTracker, it is [minimumTimestamp, maximumTimestamp], when it converts to TimeRange, the TimeRange is also [minStamp, maxStamp]. I am going to update comments in the code.",documentation_debt,outdated_documentation,"Fri, 30 Dec 2016 01:04:58 +0000","Mon, 9 Jan 2017 19:51:04 +0000","Mon, 9 Jan 2017 19:51:03 +0000",931565,"Update, the current logic is correct, for TimeRangeTracker, it is [minimumTimestamp, maximumTimestamp], when it converts to TimeRange, the TimeRange is also [minStamp, maxStamp]. I am going to update comments in the code.",0.4375,0.4375,neutral
hbase,17500,comment_2,Fix the javadoc warning.,documentation_debt,low_quality_documentation,"Fri, 20 Jan 2017 07:35:51 +0000","Wed, 25 Jan 2017 08:12:45 +0000","Wed, 25 Jan 2017 03:35:08 +0000",417557,Fix the javadoc warning.,-0.6,-0.6,negative
hbase,1770,comment_1,Patch looks good. Javadoc is a little bit odd because it seems like it's asking the client to call flushCommits rather than it happening automatically. Can fix on commit. Should we put this into 0.20 branch and trunk?,documentation_debt,low_quality_documentation,"Mon, 17 Aug 2009 22:32:50 +0000","Sun, 13 Sep 2009 22:24:54 +0000","Mon, 17 Aug 2009 23:44:31 +0000",4301,Patch looks good. Javadoc is a little bit odd because it seems like it's asking the client to call flushCommits rather than it happening automatically. Can fix on commit. Should we put this into 0.20 branch and trunk?,0.144,0.144,neutral
hbase,17918,description,"It looks like HBASE-9465 addresses one of the major flaws in our existing replication (namely that order of delivery is not assured). All I see in the reference guide is a note on Instead we should cover this in the replication section, especially given that we call out the order of delivery limitation.",documentation_debt,low_quality_documentation,"Fri, 14 Apr 2017 16:43:32 +0000","Thu, 16 Jun 2022 18:03:11 +0000","Tue, 10 Apr 2018 01:45:51 +0000",31136539,"It looks like HBASE-9465 addresses one of the major flaws in our existing replication (namely that order of delivery is not assured). All I see in the reference guide is a note on hbase.serial.replication.waitingMs. Instead we should cover this in the replication section, especially given that we call out the order of delivery limitation.",-0.447,-0.149,negative
hbase,18549,comment_3,"Yes i believe that as well. The patch mostly lgtm. The word region is misspelled in some exception text. That exception could provide more information for debugging. You have: Add the region name, the znode path, and the stacktrace to the log line.",documentation_debt,low_quality_documentation,"Wed, 9 Aug 2017 20:58:19 +0000","Fri, 1 Feb 2019 19:55:54 +0000","Tue, 2 Oct 2018 01:51:52 +0000",36132813,"my takeaway from this JIRA's description is to providing metrics regarding failed to recover replication queue. Yes i believe that as well. The patch mostly lgtm. The word region is misspelled in some exception text. That exception could provide more information for debugging. You have: Add the region name, the znode path, and the stacktrace to the log line.",0.0831,0.002583333333,neutral
hbase,18549,comment_7,+1 modulo minor spelling nit: What do you think about a patch for branch-1 too?,documentation_debt,low_quality_documentation,"Wed, 9 Aug 2017 20:58:19 +0000","Fri, 1 Feb 2019 19:55:54 +0000","Tue, 2 Oct 2018 01:51:52 +0000",36132813,+1 modulo minor spelling nit: What do you think about a patch for branch-1 too? xucang,0,0,neutral
hbase,18549,comment_8,added branch-1 patch. added master branch patch to address typo issue Andrew mentioned.,documentation_debt,low_quality_documentation,"Wed, 9 Aug 2017 20:58:19 +0000","Fri, 1 Feb 2019 19:55:54 +0000","Tue, 2 Oct 2018 01:51:52 +0000",36132813,added branch-1 patch. added master branch patch to address typo issue Andrew mentioned.,0.25,0.25,neutral
hbase,18909,comment_0,"The API you are suggesting to use is marked deprecated below in the patch, So we should suggest to use in the first place also. also There is reference to this method in java doc of other public APIs to use this one, I suggest to remove their also and point it to actual API we would like user to use. General practice what I follow is when ever I mark a API as deprecated I replace all of its reference from the code with the one we are suggesting to use in the java doc. If you also want to do this please also take care of the references in ruby scripts...",documentation_debt,outdated_documentation,"Sat, 30 Sep 2017 03:40:48 +0000","Wed, 21 Mar 2018 22:21:06 +0000","Sat, 7 Oct 2017 13:30:27 +0000",640179,"The API you are suggesting to use is marked deprecated below in the patch, So we should suggest to use listTableDescriptors(Pattern) in the first place also. also There is reference to this method in java doc of other public APIs to use this one, I suggest to remove their also and point it to actual API we would like user to use. General practice what I follow is when ever I mark a API as deprecated I replace all of its reference from the code with the one we are suggesting to use in the java doc. If you also want to do this please also take care of the references in ruby scripts...",0.06666666667,0.06666666667,neutral
hbase,18909,comment_11,Attach a 005 patch to fix the whitespace and javadoc warnings.,documentation_debt,low_quality_documentation,"Sat, 30 Sep 2017 03:40:48 +0000","Wed, 21 Mar 2018 22:21:06 +0000","Sat, 7 Oct 2017 13:30:27 +0000",640179,Attach a 005 patch to fix the whitespace and javadoc warnings.,-0.6,-0.6,neutral
hbase,19031,comment_1,s change on HBASE-19043 helps with {{HTableWrapper}}. In {{RemoteHTable}} the {{public Boolean[] exists(List<Get> gets)}} method is marked as deprecated but it does not have any documentation when it will be removed. Is it possible to remove that method before alpha4?,documentation_debt,outdated_documentation,"Tue, 17 Oct 2017 20:28:32 +0000","Wed, 21 Mar 2018 22:20:10 +0000","Sat, 28 Oct 2017 23:24:46 +0000",960974,stack's change on HBASE-19043 helps with HTableWrapper. In RemoteHTable the public Boolean[] exists(List<Get> gets) method is marked as deprecated but it does not have any documentation when it will be removed. Is it possible to remove that method before alpha4?,0.1333333333,0.1333333333,neutral
hbase,19241,comment_0,Add more javadoc for AsyncAdmin. Also cleanup the warnings of RawAsyncHBaseAdmin.,documentation_debt,low_quality_documentation,"Sat, 11 Nov 2017 12:43:01 +0000","Wed, 21 Mar 2018 22:21:08 +0000","Mon, 13 Nov 2017 09:02:52 +0000",159591,Add more javadoc for AsyncAdmin. Also cleanup the warnings of RawAsyncHBaseAdmin.,-0.3,-0.3,neutral
hbase,19241,comment_3,+1 Fix javadoc complaint on commit.,documentation_debt,low_quality_documentation,"Sat, 11 Nov 2017 12:43:01 +0000","Wed, 21 Mar 2018 22:21:08 +0000","Mon, 13 Nov 2017 09:02:52 +0000",159591,+1 Fix javadoc complaint on commit.,-0.1,-0.1,neutral
hbase,19478,comment_1,"It seems 'Future' is not needed in above sentence. For WALFiles, 'are' should be used - w.r.t. meaning of values in its returned Map, since you have this in the caller: please modify the javadoc to match the actual meaning.",documentation_debt,low_quality_documentation,"Sun, 10 Dec 2017 17:10:15 +0000","Mon, 1 Jan 2018 17:45:29 +0000","Mon, 1 Jan 2018 14:55:59 +0000",1892744,"It seems 'Future' is not needed in above sentence. For WALFiles, 'are' should be used - areWALFilesDeletable(). w.r.t. meaning of values in its returned Map, since you have this in the caller: please modify the javadoc to match the actual meaning.",0.2416666667,0.1611111111,neutral
hbase,19570,comment_6,Looks great  Fix this misspelling on commit. +1 PROJET_PERSONALITY,documentation_debt,low_quality_documentation,"Wed, 20 Dec 2017 21:04:59 +0000","Fri, 1 Feb 2019 20:20:24 +0000","Fri, 22 Dec 2017 01:47:48 +0000",103369,Looks great appy Fix this misspelling on commit. +1 PROJET_PERSONALITY,0.05,0.05,positive
hbase,19570,comment_7,Fixed spelling. Thanks for the review stack. Pushed all the way back till branch-1.1.,documentation_debt,low_quality_documentation,"Wed, 20 Dec 2017 21:04:59 +0000","Fri, 1 Feb 2019 20:20:24 +0000","Fri, 22 Dec 2017 01:47:48 +0000",103369,Fixed spelling. Thanks for the review stack. Pushed all the way back till branch-1.1.,0.1,0.1,positive
hbase,19969,comment_3,"I looked at the calls to where return value is wrapped by Path. It would be cleaner if returns Path. Can you add javadoc to the above method ? It is easier to understand what the two Paths are for. The method throws IOException. If newPath doesn't exist, throwing IOException is better. Unused variable.",documentation_debt,low_quality_documentation,"Fri, 9 Feb 2018 22:57:42 +0000","Wed, 28 Mar 2018 02:50:38 +0000","Sat, 10 Mar 2018 17:49:55 +0000",2487133,"I looked at the calls to HBackupFileSystem.getBackupTmpDirForBackupId where return value is wrapped by Path. It would be cleaner if HBackupFileSystem.getBackupTmpDirForBackupId() returns Path. Can you add javadoc to the above method ? It is easier to understand what the two Paths are for. The method throws IOException. If newPath doesn't exist, throwing IOException is better. Unused variable.",0.1285714286,0.1,neutral
hbase,19969,comment_4,Would be nice to see some javadoc on the methods added to Feels like this should be its own utility method. I think Ted covered all of the other stuff I noticed.,documentation_debt,low_quality_documentation,"Fri, 9 Feb 2018 22:57:42 +0000","Wed, 28 Mar 2018 02:50:38 +0000","Sat, 10 Mar 2018 17:49:55 +0000",2487133,Would be nice to see some javadoc on the methods added to MapReduceBackupMergeJob Feels like this should be its own utility method. I think Ted covered all of the other stuff I noticed.,0.4125,0.4125,positive
hbase,20595,comment_1,"that makes sense. is the ""system group"" treated like a normal rsgroup in terms of exclusivity? If so, how do we handle it and the user group needing to be distinct? if we just delegate it to manual operator config that means we can't have the rsgroup on by default ever right? we'll probably need to document ""can't turn on rsgroup feature"" as a limitation of single-node deployments.",documentation_debt,outdated_documentation,"Wed, 16 May 2018 21:37:08 +0000","Mon, 22 Apr 2019 16:10:14 +0000","Wed, 23 May 2018 18:58:44 +0000",595296,"that makes sense. is the ""system group"" treated like a normal rsgroup in terms of exclusivity? If so, how do we handle it and the user group needing to be distinct? if we just delegate it to manual operator config that means we can't have the rsgroup on by default ever right? we'll probably need to document ""can't turn on rsgroup feature"" as a limitation of single-node deployments.",-0.0427,-0.0427,neutral
hbase,20693,comment_4,"Attached which refactors {{rest.jsp}} and {{thrift.jsp}} and extracts header and footer out of them. Also, fixed following typo in thrift.jsp. Please review. Ping , , .",documentation_debt,low_quality_documentation,"Wed, 6 Jun 2018 20:20:51 +0000","Wed, 8 Jan 2025 12:30:17 +0000","Fri, 27 Sep 2024 12:55:43 +0000",199125292,"AttachedHBASE-20693.master.001.patchwhich refactors rest.jsp and thrift.jsp and extracts header and footer out of them. Also, fixed following typo in thrift.jsp. Please review. Ping tianjingyun, zghaobac, stack.",0.02857142857,0.07,neutral
hbase,20823,summary,Wrong param name in javadoc for,documentation_debt,low_quality_documentation,"Fri, 29 Jun 2018 16:19:40 +0000","Fri, 20 Jul 2018 13:58:36 +0000","Thu, 19 Jul 2018 10:33:06 +0000",1707206,Wrong param name in javadoc for HRegionServer#buildRegionSpaceUseReportRequest,-0.25,-0.25,negative
hbase,22206,description,"The dist.apache.org server is only intended for use by developers in staging releases. It must not be used on public download pages. Please use www.apache.org/dist (for KEYS, hashes and sigs) and the mirror system instead. The current download page has lots of references to dist.a.o; please replace thes.",documentation_debt,low_quality_documentation,"Wed, 10 Apr 2019 21:43:14 +0000","Mon, 22 Apr 2019 22:37:35 +0000","Mon, 22 Apr 2019 14:12:44 +0000",1009770,"The dist.apache.org server is only intended for use by developers in staging releases. It must not be used on public download pages. Please use www.apache.org/dist (for KEYS, hashes and sigs) and the mirror system instead. The current download page has lots of references to dist.a.o; please replace thes.",0.1555555556,0.1555555556,neutral
hbase,22225,comment_1,"After checking our there're some prerequisites to use this profiler servlet. However, not all users would check the book before clicking the button, so maybe a more comprehensive error message plus a link to the refguide is a better idea? Anyway I don't think this should be marked as a blocker any more.",documentation_debt,low_quality_documentation,"Fri, 12 Apr 2019 13:05:16 +0000","Mon, 7 Oct 2019 18:39:35 +0000","Mon, 29 Apr 2019 20:42:55 +0000",1496259,"After checking our refguide, there're some prerequisites to use this profiler servlet. However, not all users would check the book before clicking the button, so maybe a more comprehensive error message plus a link to the refguide is a better idea? Anyway I don't think this should be marked as a blocker any more.",0.1073333333,0.1073333333,negative
hbase,22378,comment_14,This commit landed on all branches but Fix versions were set to None. I checked the branches and release dates and set it accordingly. The release notes for the affected releases are most probably incorrect.,documentation_debt,outdated_documentation,"Wed, 8 May 2019 00:33:51 +0000","Thu, 23 Jan 2020 14:10:35 +0000","Mon, 13 May 2019 19:47:02 +0000",501191,This commit landed on all branches but Fix versions were set to None. I checked the branches and release dates and set it accordingly. The release notes for the affected releases are most probably incorrect.,0,0,negative
hbase,23200,summary,incorrect description in,documentation_debt,low_quality_documentation,"Tue, 22 Oct 2019 06:37:56 +0000","Tue, 11 Aug 2020 18:12:19 +0000","Tue, 29 Oct 2019 03:55:00 +0000",595024,incorrect description in SortedCompactionPolicy.getNextMajorCompactTime,0,0,negative
hbase,23651,comment_3,Please update the Release Note. Thanks.,documentation_debt,outdated_documentation,"Mon, 6 Jan 2020 07:51:00 +0000","Fri, 10 Jan 2020 09:21:20 +0000","Wed, 8 Jan 2020 11:08:04 +0000",184624,binlijinPlease update the Release Note. Thanks.,0.3,0.3,neutral
hbase,2621,description,"There's a bad link in the HFile javadoc; should point to HBASE-61, but instead point to HBASE-3315. Also cleaned up the reference to TFile.",documentation_debt,low_quality_documentation,"Fri, 28 May 2010 09:38:24 +0000","Fri, 20 Nov 2015 12:40:31 +0000","Fri, 28 May 2010 18:23:37 +0000",31513,"There's a bad link in the HFile javadoc; should point to HBASE-61, but instead point to HBASE-3315. Also cleaned up the reference to TFile.",-0.3,-0.3,negative
hbase,2621,summary,Fix broken link in HFile Javadoc,documentation_debt,low_quality_documentation,"Fri, 28 May 2010 09:38:24 +0000","Fri, 20 Nov 2015 12:40:31 +0000","Fri, 28 May 2010 18:23:37 +0000",31513,Fix broken link in HFile Javadoc,-0.2,-0.2,neutral
hbase,274,comment_4,v3 fixes javadoc warning.,documentation_debt,low_quality_documentation,"Sat, 5 Jan 2008 01:51:53 +0000","Thu, 2 May 2013 02:29:12 +0000","Tue, 15 Jan 2008 10:16:24 +0000",894271,v3 fixes javadoc warning.,-0.6,-0.6,neutral
hbase,2925,comment_7,"@Robert ...I wonder if it defeats the purpose of having a connection cache from the first place. As I see it, the main benefit to the cache is saving on region lookups, setup of zk connection, and master proxy setup. Having the likes of the following config cached is secondary: e.g. In fact, I'm now thinking that if a user changes any of the above in a Configuration that is being used as a key in that its ok if we don't notice the change as long as we doc the fact that the Configuration is read on instantiation of the HTable and then no more. I don't tthink this should 'surprise' the user too much and they can just go create new HTable with the new Configuration if they really want their new config. to take hold (Making things work this way is similar to what you suggest only you would copy the Configuration before adding it as a key). As to your last suggestion, I think we should move away from trying to equate Configurations at all; there be daemons that way. Let me know what you think. If you are agreeable, I'll work up the patch some more mostly adding doc clarifying what we've agreed here.",documentation_debt,outdated_documentation,"Tue, 17 Aug 2010 21:27:52 +0000","Fri, 20 Nov 2015 12:42:13 +0000","Mon, 6 Sep 2010 15:52:27 +0000",1707875,"@Robert ...I wonder if it defeats the purpose of having a connection cache from the first place. As I see it, the main benefit to the cache is saving on region lookups, setup of zk connection, and master proxy setup. Having the likes of the following config cached is secondary: e.g. In fact, I'm now thinking that if a user changes any of the above in a Configuration that is being used as a key in HCM#HBASE_INSTANCES, that its ok if we don't notice the change as long as we doc the fact that the Configuration is read on instantiation of the HTable and then no more. I don't tthink this should 'surprise' the user too much and they can just go create new HTable with the new Configuration if they really want their new config. to take hold (Making things work this way is similar to what you suggest only you would copy the Configuration before adding it as a key). As to your last suggestion, I think we should move away from trying to equate Configurations at all; there be daemons that way. Let me know what you think. If you are agreeable, I'll work up the patch some more mostly adding doc clarifying what we've agreed here.",0.3023958333,0.3023958333,neutral
hbase,2925,comment_9,"This version of the patch just adds javadoc to HTable explaining the advantage of shared Configuration -- the sharing of zookeeper connection, cache of region locations, etc. If you have a minute, give it a gander Robert and if its good w/ you, I'll go ahead and commit.",documentation_debt,outdated_documentation,"Tue, 17 Aug 2010 21:27:52 +0000","Fri, 20 Nov 2015 12:42:13 +0000","Mon, 6 Sep 2010 15:52:27 +0000",1707875,"This version of the patch just adds javadoc to HTable explaining the advantage of shared Configuration  the sharing of zookeeper connection, cache of region locations, etc. If you have a minute, give it a gander Robert and if its good w/ you, I'll go ahead and commit.",0.4575,0.4575,positive
hbase,3181,comment_0,"Working on a document that goes over all this stuff, but I'd like stack to give a go at my current patch. There's a few fairly big fixes, not just to timeouts but also to server shutdown handling, and I'd like to see if it fixes the issues he's been seeing. Putting patch up on RB.",documentation_debt,low_quality_documentation,"Sun, 31 Oct 2010 19:28:35 +0000","Fri, 20 Nov 2015 12:42:59 +0000","Tue, 2 Nov 2010 02:05:45 +0000",110230,"Working on a document that goes over all this stuff, but I'd like stack to give a go at my current patch. There's a few fairly big fixes, not just to timeouts but also to server shutdown handling, and I'd like to see if it fixes the issues he's been seeing. Putting patch up on RB.",0.4166666667,0.4166666667,positive
hbase,3696,comment_6,"Resolving as implemented by HBASE-11218 (which should go in soon) at least for standalone. Data loss on fs has been doc'd also. Hopefully HBASE-11218 will do pseudo distributed mode too. If not, doc will need tweaking. Can do in new issue.",documentation_debt,outdated_documentation,"Thu, 24 Mar 2011 01:10:36 +0000","Sun, 12 Jun 2022 00:55:50 +0000","Wed, 28 May 2014 04:08:13 +0000",100321057,"Resolving as implemented by HBASE-11218 (which should go in soon) at least for standalone. Data loss on fs has been doc'd also. Hopefully HBASE-11218 will do pseudo distributed mode too. If not, doc will need tweaking. Can do in new issue.",0.15,0.15,neutral
hbase,396,description,"Currently javadoc for hbase contrib does not show up anywhere at Below is some discussion from hadoop-dev list on how to do the hbase contrib javadoc build. From: Doug Cutting Re: javadoc for hbase on apache.org I'd vote for including it as we have other contrib documentation, as a separate section in the main javadoc tree. Doug Michael Stack wrote: > St.Ack",documentation_debt,low_quality_documentation,"Mon, 21 May 2007 18:42:25 +0000","Mon, 4 Feb 2008 18:42:03 +0000","Mon, 21 May 2007 23:05:49 +0000",15804,"Currently javadoc for hbase contrib does not show up anywhere at lucene.apache.org/hadoop. Below is some discussion from hadoop-dev list on how to do the hbase contrib javadoc build. From: Doug Cutting <cutting@apache.org> Subject: Re: javadoc for hbase on apache.org I'd vote for including it as we have other contrib documentation, as a separate section in the main javadoc tree. Doug Michael Stack wrote: > Any chance of having the hbase javadoc show somewhere up on > lucene.apache.org/hadoop? > > It looks like other contribs  streaming and datajoin  have their > javadoc produced as part of the general hadoop javadoc target up in the > root build.xml. I could submit a patch like the below that adds hbase > but perhaps folks have other ideas such as a 'javadoc-contrib' target in > the root build.xml that calls down into subtargets under src/contrib? > > Thanks, > St.Ack",-0.05,0.02557142857,neutral
hbase,4311,comment_0,The description is slightly wrong. The entry is removed from meta but it may be assigned and its hdfs hfile stuff is still present which causes hbck to complain.,documentation_debt,low_quality_documentation,"Wed, 31 Aug 2011 16:31:47 +0000","Sun, 12 Jun 2022 19:20:18 +0000","Thu, 12 Apr 2012 16:49:37 +0000",19441070,The description is slightly wrong. The entry is removed from meta but it may be assigned and its hdfs hfile stuff is still present which causes hbck to complain.,-0.15475,-0.15475,negative
hbase,4804,comment_2,Haha.. I have a spelling problem and a tendency to omit words which may be incurable. :),documentation_debt,low_quality_documentation,"Wed, 16 Nov 2011 23:18:06 +0000","Fri, 20 Nov 2015 11:52:12 +0000","Wed, 16 Nov 2011 23:24:47 +0000",401,Haha.. I have a spelling problem and a tendency to omit words which may be incurable.,0.25,0.1,negative
hbase,4804,description,I was going through the 0.92 CHANGES and found are a few entries in CHANGES.txt where jira numbers don't match up descriptions.,documentation_debt,low_quality_documentation,"Wed, 16 Nov 2011 23:18:06 +0000","Fri, 20 Nov 2015 11:52:12 +0000","Wed, 16 Nov 2011 23:24:47 +0000",401,I was going through the 0.92 CHANGES and found are a few entries in CHANGES.txt where jira numbers don't match up descriptions.,0,0,negative
hbase,4804,summary,Minor Dyslexia in CHANGES.txt,documentation_debt,low_quality_documentation,"Wed, 16 Nov 2011 23:18:06 +0000","Fri, 20 Nov 2015 11:52:12 +0000","Wed, 16 Nov 2011 23:24:47 +0000",401,Minor Dyslexia in CHANGES.txt,0,0,neutral
hbase,5217,comment_14,Fixed spacing,documentation_debt,low_quality_documentation,"Tue, 17 Jan 2012 19:17:36 +0000","Sun, 12 Jun 2022 20:10:28 +0000","Wed, 22 Apr 2015 00:33:57 +0000",102834981,Fixed spacing,0,0,neutral
hbase,5635,comment_5,Typo. First line should read 'it gets the' Second line should read 'worker thread exited'. Please reduce the above interval. Do we need a timeout for the newly added loop ?,documentation_debt,low_quality_documentation,"Mon, 26 Mar 2012 09:56:31 +0000","Tue, 26 Feb 2013 17:02:58 +0000","Mon, 23 Apr 2012 16:50:31 +0000",2444040,Typo. First line should read 'it gets the' Second line should read 'worker thread exited'. Please reduce the above interval. Do we need a timeout for the newly added loop ?,0.05,0.05,neutral
hbase,5635,comment_8,Updated the patch with log message for retry and corrected the typo.,documentation_debt,low_quality_documentation,"Mon, 26 Mar 2012 09:56:31 +0000","Tue, 26 Feb 2013 17:02:58 +0000","Mon, 23 Apr 2012 16:50:31 +0000",2444040,Updated the patch with log message for retry and corrected the typo.,0,0,neutral
hbase,5636,comment_3,"I filed the new issue of the bug. By the way, the TestCase name is typo, isn't it? I will rename it to",documentation_debt,low_quality_documentation,"Mon, 26 Mar 2012 16:33:41 +0000","Tue, 26 Feb 2013 08:12:42 +0000","Mon, 2 Apr 2012 13:54:46 +0000",595265,"I filed the new issue of the bug. By the way, the TestCase name 'TestMulitthreadedTableMapper' is typo, isn't it? I will rename it to 'TestMultithreadedTableMapper'.",0,0,neutral
hbase,6264,comment_0,Patch to correct documentation typos.,documentation_debt,low_quality_documentation,"Sat, 23 Jun 2012 20:01:33 +0000","Mon, 23 Sep 2013 18:31:39 +0000","Thu, 30 Aug 2012 23:10:43 +0000",5886550,Patch to correct documentation typos.,0.875,0.875,neutral
hbase,6264,description,"In section 6.9: In section 9.2: There is ""are are"" twice. I'm not 100% sure what's the best way to propose a fix, so I have included the patch below. If that's fine, I will probably propose some other corrections. JM Index:  (rvision 1352979) +++ (copie de travail) @@ -828,7 +828,7 @@ Secondary Indexes and Alternate Query Paths </title <para- A common example on the dist-list is where a row-key is of the format ""user-timestamp"" but there are are reporting requirements on activity across users for certain + A common example on the dist-list is where a row-key is of the format ""user-timestamp"" but there are reporting requirements on activity across users for certain time ranges. Thus, selecting by user is easy because it is in the lead position of the key, but time is not. </para <para@@ -1324,7 +1324,7 @@ <section <title- <para+ <para of the HBase shell's <code </para <section",documentation_debt,low_quality_documentation,"Sat, 23 Jun 2012 20:01:33 +0000","Mon, 23 Sep 2013 18:31:39 +0000","Thu, 30 Aug 2012 23:10:43 +0000",5886550,"In section 6.9: http://hbase.apache.org/book/secondary.indexes.html In section 9.2: http://hbase.apache.org/book/arch.catalog.html There is ""are are"" twice. I'm not 100% sure what's the best way to propose a fix, so I have included the patch below. If that's fine, I will probably propose some other corrections. JM Index: branches/0.94/src/docbkx/book.xml ===================================================================  branches/0.94/src/docbkx/book.xml (rvision 1352979) +++ branches/0.94/src/docbkx/book.xml (copie de travail) @@ -828,7 +828,7 @@ Secondary Indexes and Alternate Query Paths </title> <para>This section could also be titled ""what if my table rowkey looks like <emphasis>this</emphasis> but I also want to query my table like <emphasis>that</emphasis>."" A common example on the dist-list is where a row-key is of the format ""user-timestamp"" but there are are reporting requirements on activity across users for certain + A common example on the dist-list is where a row-key is of the format ""user-timestamp"" but there are reporting requirements on activity across users for certain time ranges. Thus, selecting by user is easy because it is in the lead position of the key, but time is not. </para> <para>There is no single answer on the best way to handle this because it depends on... @@ -1324,7 +1324,7 @@ <section xml:id=""arch.catalog""> <title>Catalog Tables</title> <para>The catalog tables ROOT and .META. exist as HBase tables. They are are filtered out + <para>The catalog tables ROOT and .META. exist as HBase tables. They are filtered out of the HBase shell's <code>list</code> command, but they are in fact tables just like any other. </para> <section xml:id=""arch.catalog.root"">",0.03683333333,-0.0202,neutral
hbase,6264,summary,Typos in the book documentation,documentation_debt,low_quality_documentation,"Sat, 23 Jun 2012 20:01:33 +0000","Mon, 23 Sep 2013 18:31:39 +0000","Thu, 30 Aug 2012 23:10:43 +0000",5886550,Typos in the book documentation,0,0,negative
hbase,6835,comment_1,"Thanks for pointing to that, linking as related. HBASE-4198 seems like a good idea; for now, let's make the comment accurate.",documentation_debt,low_quality_documentation,"Wed, 19 Sep 2012 01:58:13 +0000","Mon, 23 Sep 2013 18:30:08 +0000","Wed, 19 Sep 2012 20:34:16 +0000",66963,"Thanks for pointing to that, linking as related. HBASE-4198 seems like a good idea; for now, let's make the comment accurate.",0.5815,0.5815,positive
hbase,7212,comment_4,"Doc looks great. Do you have to write it yourself? Anything already available that you could use? Say we used the zk curator client, it has a few barriers implemented already: If we were using curator say, could you use these receipes as building blocks so you didn't have to write this yourself? (This feature has to be backportable to 0.94?) Reading the diagram, I""m not sure what receivedreached is. Or sendReached. sendReached is the coordinator saying all participants responded/are participating? On your barrier you say ""...but does not support ACID semantics"" and thats ok because the 'transactions' we'll be running over this mechanism do not require it? Because they can proceed and complete in any order and result will come off the same? You say ""....Does not recover on failures"" ... because the operation just FAILs. Right? Only one of these precedures can be ongoingn at any one time? Is that right? How do I read these set of slides? There is a 'Barrier Procedure Coordination' and then there is 'Procedure Coordination'? So, the PC makes use of a BPC? BPC is the skeleton you hang PC on? Why you say this 'If we arent doing proper 2PC do we need all this infrastructure?'? Are you making a case for our not needing 2PC given what is being implemented? Coordinator can be any client? Does not have to be master? What is Does this barrier acquistion have any relation to zk barrier receipe? What is 'class' in the zk node hierarchy? Class of procedure? Procedure looks good to me.",documentation_debt,low_quality_documentation,"Thu, 22 Nov 2012 19:08:26 +0000","Mon, 23 Sep 2013 18:30:35 +0000","Sat, 29 Dec 2012 06:18:52 +0000",3150626,"Doc looks great. Do you have to write it yourself? Anything already available that you could use? Say we used the zk curator client, it has a few barriers implemented already: https://github.com/Netflix/curator/wiki/Recipes If we were using curator say, could you use these receipes as building blocks so you didn't have to write this yourself? (This feature has to be backportable to 0.94?) Reading the diagram, I""m not sure what receivedreached is. Or sendReached. sendReached is the coordinator saying all participants responded/are participating? On your barrier you say ""...but does not support ACID semantics"" and thats ok because the 'transactions' we'll be running over this mechanism do not require it? Because they can proceed and complete in any order and result will come off the same? You say ""....Does not recover on failures"" ... because the operation just FAILs. Right? Only one of these precedures can be ongoingn at any one time? Is that right? How do I read these set of slides? There is a 'Barrier Procedure Coordination' and then there is 'Procedure Coordination'? So, the PC makes use of a BPC? BPC is the skeleton you hang PC on? Why you say this 'If we arent doing proper 2PC do we need all this infrastructure?'? Are you making a case for our not needing 2PC given what is being implemented? Coordinator can be any client? Does not have to be master? What is ProcedureCoordinateComms? Does this barrier acquistion have any relation to zk barrier receipe? http://zookeeper.apache.org/doc/trunk/recipes.html#sc_recipes_eventHandles What is 'class' in the zk node hierarchy? Class of procedure? Procedure looks good to me.",0.03497530864,-0.004202380952,positive
hbase,7212,comment_5,"The online-snapshots is a 'class' (e.g. all online snapshots) while a procedure name is an actual name for a particular snapshotting request (snapshot121201, snapshot121202 etc). Off the top of my head I can't think of any other HBase processes that are ok with the procedure mechanism's semantics (other operations like enabling, disabling, schema change, splitting, merging probably want 2pc and its recovery requirements). I think this extra znode dir could probably get removed.",documentation_debt,low_quality_documentation,"Thu, 22 Nov 2012 19:08:26 +0000","Mon, 23 Sep 2013 18:30:35 +0000","Sat, 29 Dec 2012 06:18:52 +0000",3150626,"Thanks for taking a look. I'll get another rev out with cleaned up documentation in a day or two. Answers below. Do you have to write it yourself? Anything already available that you could use? Say we used the zk curator client, it has a few barriers implemented already: https://github.com/Netflix/curator/wiki/Recipes If we were using curator say, could you use these receipes as building blocks so you didn't have to write this yourself? (This feature has to be backportable to 0.94?) This is a simplified version of Jesse's patch. I just gave curator a quick it is similar to the double barrier (https://github.com/Netflix/curator/wiki/Double-barrier). If it is implemented as the recipe you pointed out, I think we'd still need to add in the ability for cancellation/abort to come from any of the members. Reading the diagram, I""m not sure what receivedreached is. Or sendReached. sendReached is the coordinator saying all participants responded/are participating? Yes  reached is sent when the coordinator figures out that it has ""reached"" the global barrier point because all members have taken their part of the global barrier. Basically, zk is being used for its async notifications and as the RPC mechanism. Arrows into the ZK column are calls writing to ZK, arrows out of ZK are callbacks being called at the target. So the red coordinator writes to zk via sendStart, zk node creation triggers a startNewOpearion callback on the the blue member1, and similarly on the the green member2. These names are short hand for the names in the review was posted  now sendStart -> sendBarrierStart, sendReached -> sendBarrierReached, startNewOperation -> Subprocedure's consturctor + acquireBarrier, receiveReached -> receiveReachedGlobalBarrier On your barrier you say ""...but does not support ACID semantics"" and thats ok because the 'transactions' we'll be running over this mechanism do not require it? Because they can proceed and complete in any order and result will come off the same? Previously, this code was called TwoPhaseCommit (2pc). While it had two phases, the code did not implement true two phase commit. The purpose of this explicit comparison is to make clear 2pc's purpose (distributed ACID guarantees), to point out that we don't have 2pc here, to point out that we don't need 2pc here, and to point out that we just need a global barrier. The online snapshot coordination does not need all of what 2pc provides. The first cut will have ""only on a sunny day"" semantics  e.g. it will only succeed if everything succeeds and if anything fails along the way whole attempt will be aborted. This is ok because the durable work that snapshots does goes into tmp dir (/hbase/.snapshots/.tmp/xxx) that is ""commited"" at the end atomically via HDFS dir rename, and that durable intermediate operation (e.g. new files from forcing a hlog roll or hlog flush) don't need to be undone to remain correct. You say ""....Does not recover on failures"" ... because the operation just FAILs. Right? Yup. Only one of these precedures can be ongoingn at any one time? Is that right? True for this first cut implementation, but not a fundamental limitation. This actually gets enforced at the snapshot manager level which may be visible in HBASE-7208 and definitely in HBASE-6866 when that gets posted. I believe as implemented if we picked a different class we could have multiple different kinds of procedure concurrently running on a different znode dir hierarchy. How do I read these set of slides? There is a 'Barrier Procedure Coordination' and then there is 'Procedure Coordination'? So, the PC makes use of a BPC? BPC is the skeleton you hang PC on? All those are synonymous  I've bee using procedure as a shorthand. The code implements one framework for a globally barriered procedure, and I've just tried to call it 'procedure' and 'subprocedure' everywhere (though from review I missed spots where it was called task, operation, or commit). This 'procedure' takes care of the global barrier coordination and cross process error propagation. Why you say this 'If we arent doing proper 2PC do we need all this infrastructure?'? Are you making a case for our not needing 2PC given what is being implemented? I could probably remove that line  I'm now convinced why we need what this code does. The main questions I had when I was initially understanding the previous implementation was ""Is this 2pc?"" and ""Do we need 2pc?"". The answers are: what we have implemented here has two phases but is not true two-phase commit. 2pc, as defined in the literature (http://www.cs.berkeley.edu/~brewer/cs262/Aries.pdf), requires that once the coordinator says something is committed, any failures at a member or coordinator must be recover by failing forward and completing it. The key point here is that while we will need a global barrier for one of the snapshot flavors (global), it don't need full 2PC because 1) the we don't need to undo work (like a log roll or flush) if some sub part of the first phase (our acquire/2pc's prepare) fails, and because 2) we don't need to recover failing forward if anything fails in the second phase (our release/2pc's commit). In the latter case we just fail and delete .snapshot/.tmp reminants in the fs, and carry on with extra flushed/rolled hlogs. Coordinator can be any client? Does not have to be master? It could be anywhere, but currently for snapshots the coordinator lives on the master. What is ProcedureCoordinateComms? This is actually a layer that separate the zk code (the rpc communications or comms code) from specific execution (snapshotting specific code). I could probably remove it, but the abstraction allows for testing the core pieces without zk. Does this barrier acquistion have any relation to zk barrier receipe? http://zookeeper.apache.org/doc/trunk/recipes.html#sc_recipes_eventHandles Yes. It is very similar to the double barrier. The main thing different here is this code allows for any member or coordinator to abort/cancel the whole shebang while the recipe doesn't seem to. From the recipe it seems that we could be a little bit more clever about how we use our znodes. (we might have one extra set). What is 'class' in the zk node hierarchy? Class of procedure? The online-snapshots is a 'class' (e.g. all online snapshots) while a procedure name is an actual name for a particular snapshotting request (snapshot121201, snapshot121202 etc). Off the top of my head I can't think of any other HBase processes that are ok with the procedure mechanism's semantics (other operations like enabling, disabling, schema change, splitting, merging probably want 2pc and its recovery requirements). I think this extra znode dir could probably get removed.",-0.065,0.008770502646,neutral
hbase,7212,comment_7,"On curator double-barrier, it would seem there is no 'abort' as you say. They do have timeouts on barrier enter and leave. Would that be enough See Rather than 'abort', you could just timeout? That might be simpler still? i.e. your ""Need to be able to force failure after a specified timeout elapses"" double-barrier does not seem to be enough though. There needs to be a means of telling cluster members to go for a particular snapshot barrier. To this end, I suppose all members need to be watching a snapshot dir and when a new snapshot appears, all try to 'enter' its barrier? Is it true that you do not want members to start 'snapshotting' until ALL participants have 'entered' the barrier? Does it matter if they start doing their work soon as they 'enter' the barrier (using curator/zk receipe terms). Reading on, it seems like its fine if members just go about their merry way....working on their part of snapshot. If not all members complete, the coordinator will clean up the incomplete. What do you think of the terms in the zk receipe: i.e. rather than 'reach' a barrier, 'enter' it? Some of the answers you give above should go into doc of this feature. They are quality. I buy your argument for going w/ the more basic barrier rather than 2pc function for snapshots (Yeah, 2pc would be useful for other distributed ops like table enable/disable w/ us 'failing forward' an interrupted table enable or disable) On 'Comms', it was just unclear to me what it was. Makes sense now.",documentation_debt,outdated_documentation,"Thu, 22 Nov 2012 19:08:26 +0000","Mon, 23 Sep 2013 18:30:35 +0000","Sat, 29 Dec 2012 06:18:52 +0000",3150626,"On curator double-barrier, it would seem there is no 'abort' as you say. They do have timeouts on barrier enter and leave. Would that be enough See http://www.jarvana.com/jarvana/view/com/netflix/curator/curator-recipes/0.6.4/curator-recipes-0.6.4-javadoc.jar!/com/netflix/curator/framework/recipes/barriers/DistributedDoubleBarrier.html#leave() Rather than 'abort', you could just timeout? That might be simpler still? i.e. your ""Need to be able to force failure after a specified timeout elapses"" double-barrier does not seem to be enough though. There needs to be a means of telling cluster members to go for a particular snapshot barrier. To this end, I suppose all members need to be watching a snapshot dir and when a new snapshot appears, all try to 'enter' its barrier? Yes  reached is sent when the coordinator figures out that it has ""reached"" the global barrier point because all members have taken their part of the global barrier. Is it true that you do not want members to start 'snapshotting' until ALL participants have 'entered' the barrier? Does it matter if they start doing their work soon as they 'enter' the barrier (using curator/zk receipe terms). Reading on, it seems like its fine if members just go about their merry way....working on their part of snapshot. If not all members complete, the coordinator will clean up the incomplete. What do you think of the terms in the zk receipe: i.e. rather than 'reach' a barrier, 'enter' it? Some of the answers you give above should go into doc of this feature. They are quality. I buy your argument for going w/ the more basic barrier rather than 2pc function for snapshots (Yeah, 2pc would be useful for other distributed ops like table enable/disable w/ us 'failing forward' an interrupted table enable or disable) On 'Comms', it was just unclear to me what it was. Makes sense now.",0.1078921569,0.09967592593,neutral
hbase,7212,comment_9,"I need to take a look at the source implementation of the curator double barrier and examples of its use to do a better job of comparing. Based on the api and the zk recipes, I'm going to make some assumptions here. As another analogy, it seems that our procedure mechanism is similar to a monitor (synchronized in java) that guarantees enter/acquire and leave/release of the barrier parts, while the curator one is lower level and leaves it to the implementer to enforce that invariant. So in this patch, the time-based abort trigger and a potential user-induced cancellation uses the same mechanism to notify all members (and the coordinator) that the procedure has aborted. I'm speculating but with think one assumption with this mechanism has vs the double barrier's is that we assume that the actions on the members may be slow (one implementation waits for a memstore flush per region) and may need to be interrupted before completion. The curator double barrier api doesn't have such a mechanism and we may have to wait for all operations to complete before we can abort them. I believe that would be the case if we used curator. I don't think we can't use it -- and the factoring out of the *Comms/*Rpcs would potentially allow us to move that in a future rev. At the end of the day, the full barrier is only required for the snapshot that completely blocks all writes to get a truly consistent snapshot. The weaker snapshots (either the timestamp based or log roll based) won't give those guarantees and doesn't actually need the full barrier. For the first cut however, I'm probably going to use it since it handles the error propagation and cross process cancellation. I'm fine with it -- I'll change the terms acquire - I'll do another rev of the docs to make it consistent with the changes being made.",documentation_debt,outdated_documentation,"Thu, 22 Nov 2012 19:08:26 +0000","Mon, 23 Sep 2013 18:30:35 +0000","Sat, 29 Dec 2012 06:18:52 +0000",3150626,"I need to take a look at the source implementation of the curator double barrier and examples of its use to do a better job of comparing. Based on the api and the zk recipes, I'm going to make some assumptions here. As another analogy, it seems that our procedure mechanism is similar to a monitor (synchronized in java) that guarantees enter/acquire and leave/release of the barrier parts, while the curator one is lower level and leaves it to the implementer to enforce that invariant. Rather than 'abort', you could just timeout? That might be simpler still? i.e. your ""Need to be able to force failure after a specified timeout elapses"" So in this patch, the time-based abort trigger and a potential user-induced cancellation uses the same mechanism to notify all members (and the coordinator) that the procedure has aborted. I'm speculating but with think one assumption with this mechanism has vs the double barrier's is that we assume that the actions on the members may be slow (one implementation waits for a memstore flush per region) and may need to be interrupted before completion. The curator double barrier api doesn't have such a mechanism and we may have to wait for all operations to complete before we can abort them. double-barrier does not seem to be enough though. There needs to be a means of telling cluster members to go for a particular snapshot barrier. To this end, I suppose all members need to be watching a snapshot dir and when a new snapshot appears, all try to 'enter' its barrier? I believe that would be the case if we used curator. I don't think we can't use it  and the factoring out of the *Comms/*Rpcs would potentially allow us to move that in a future rev. s it true that you do not want members to start 'snapshotting' until ALL participants have 'entered' the barrier? Does it matter if they start doing their work soon as they 'enter' the barrier (using curator/zk receipe terms). Reading on, it seems like its fine if members just go about their merry way....working on their part of snapshot. If not all members complete, the coordinator will clean up the incomplete. At the end of the day, the full barrier is only required for the snapshot that completely blocks all writes to get a truly consistent snapshot. The weaker snapshots (either the timestamp based or log roll based) won't give those guarantees and doesn't actually need the full barrier. For the first cut however, I'm probably going to use it since it handles the error propagation and cross process cancellation. What do you think of the terms in the zk receipe: i.e. rather than 'reach' a barrier, 'enter' it? I'm fine with it  I'll change the terms acquire -> enter, reached -> leave in the next rev I post. (in the v3 version I still need to clean up the nomenclature in the tests). I'll do another rev of the docs to make it consistent with the changes being made.",-0.1097222222,-0.01255333333,neutral
hbase,8026,summary,HBase Shell docs for scan command does not reference VERSIONS,documentation_debt,low_quality_documentation,"Thu, 7 Mar 2013 17:36:28 +0000","Fri, 6 Apr 2018 17:55:55 +0000","Fri, 16 Jan 2015 17:47:09 +0000",58752641,HBase Shell docs for scan command does not reference VERSIONS,0,0,neutral
hbase,8056,comment_1,"The comment in SQM should say ""set to false"", not ""set to true"", I will fix that.",documentation_debt,low_quality_documentation,"Sat, 9 Mar 2013 00:40:12 +0000","Thu, 16 Jun 2022 05:49:14 +0000","Wed, 20 Mar 2013 01:13:44 +0000",952412,"The comment in SQM should say ""set to false"", not ""set to true"", I will fix that.",-0.3865,-0.3865,negative
hbase,8256,comment_9,"I've documented the settings in the reference guide. Unfortunately, the surefire settings changed in 2.14, and it's not perfectly documented (see HBASE-4955 for how to use them), hence the risk of doing something that will have to be redone very soon...",documentation_debt,outdated_documentation,"Wed, 3 Apr 2013 18:07:36 +0000","Thu, 16 Jun 2022 05:57:12 +0000","Mon, 27 Jan 2014 18:03:21 +0000",25833345,"I've documented the settings in the reference guide. Unfortunately, the surefire settings changed in 2.14, and it's not perfectly documented (see HBASE-4955 for how to use them), hence the risk of doing something that will have to be redone very soon...",-0.275,-0.275,negative
hbase,8324,comment_5,v2 improves comments on patch.,documentation_debt,low_quality_documentation,"Thu, 11 Apr 2013 05:16:56 +0000","Mon, 23 Sep 2013 19:08:23 +0000","Fri, 12 Apr 2013 14:59:26 +0000",121350,v2 improves comments on patch.,0.4,0.4,neutral
hbase,847,comment_1,"Patch for the issue. OK, this is my first big(-ish) patch, so I am sure I am missing something :) Anyway, updates hbase as Jim Kellerman suggested. RowResult#getRow-s don't have any documentation yet. I will update them with a later patch. I also want to update scanners so that you can ask for multiple versions from them too (not done yet). (Also includes patch from HBASE-892.)",documentation_debt,outdated_documentation,"Wed, 27 Aug 2008 20:36:12 +0000","Sun, 13 Sep 2009 22:26:26 +0000","Wed, 3 Dec 2008 01:06:24 +0000",8397012,"Patch for the issue. OK, this is my first big(-ish) patch, so I am sure I am missing something Anyway, updates hbase as Jim Kellerman suggested. RowResult#getRow-s don't have any documentation yet. I will update them with a later patch. I also want to update scanners so that you can ask for multiple versions from them too (not done yet). (Also includes patch from HBASE-892.)",0.07291666667,0.06388888889,neutral
hbase,8665,comment_12,Mind adding javadoc for the selectNow parameter ? Add debug log for the above case ? Please add license header for Can be private ?,documentation_debt,outdated_documentation,"Thu, 30 May 2013 23:21:38 +0000","Thu, 5 May 2022 02:11:33 +0000","Wed, 19 Jun 2013 00:42:56 +0000",1646478,Mind adding javadoc for the selectNow parameter ? Add debug log for the above case ? Please add license header for StatefulStoreMockMaker.java Can BlockingStoreMockMaker be private ?,0.06666666667,0.05,neutral
hbase,8816,comment_12,The patch looks good. There is a typo in Can be addressed at commit. Jeff please commit the trunk first though.,documentation_debt,low_quality_documentation,"Fri, 28 Jun 2013 04:40:38 +0000","Wed, 21 Aug 2013 00:08:49 +0000","Sun, 4 Aug 2013 01:33:55 +0000",3185597,The patch looks good. There is a typo in tablaNameValueIndex. Can be addressed at commit. Jeff please commit the trunk first though.,0.442,0.3315,positive
hbase,9131,comment_1,"Thanks. I think we are somewhere between too little detail and too much detail. First, can we add the config variables to hbase-default.xml (with full descriptions and with units). Now to the meat: The patch doesn't tell the admin why or when they'd want to consider using this. The link/pdf requires having to search for the bucket cache sections in the 2nd page and then goes on into too much design detail for an average admin. (It also lacks the config variables / instructions). My suggestion: Take let's take the high-level parts from section 3 of the pdf, polish it and add it to the official docs. Here's a stab at the sections that I think would be good for the ref guide with the prose improved a little bit: Let me know what you think, and feel free to update/correct the draft.",documentation_debt,low_quality_documentation,"Mon, 5 Aug 2013 23:28:26 +0000","Thu, 16 Jun 2022 17:50:38 +0000","Wed, 18 Jun 2014 03:37:06 +0000",27317320,"zjushch Thanks. I think we are somewhere between too little detail and too much detail. First, can we add the config variables to hbase-default.xml (with full descriptions and with units). Now to the meat: The patch doesn't tell the admin why or when they'd want to consider using this. The link/pdf requires having to search for the bucket cache sections in the 2nd page and then goes on into too much design detail for an average admin. (It also lacks the config variables / instructions). My suggestion: Take let's take the high-level parts from section 3 of the pdf, polish it and add it to the official docs. Here's a stab at the sections that I think would be good for the ref guide with the prose improved a little bit: Design and Motivation The Bucket Cache is an alternate block cache implementation that is designed to take advantage of large amounts of memory or low-latency storage. (something about how big would be useful). It is implemented as an off-the-jvm-heap and which has the secondary benefit of reducing JVM heap fragmentation that eventually causes stop-the-world JVM garbage collection operations. If one were to rely upon the standard JVM memory allocation and GC policies with large heaps (>16GB RAM) one would periodically incur instability in hbase due to long stop-the-world GC pauses (10's of secs to minutes) that can be misinterpreted as region server failures. The storage of cached blocks is is not constrained to in RAM-only use; one could cache blocks in memory and also use a high speed disk, such as SSD's, Fusion-IO devices, or ram-disks as massive secondary cache. (probably need something about the persistence properties not being required, but having the masssive capacity as a huge benefit. Internally, the bucket cache divided storage into many buckets, each of which contains blocks of a particular range of sizes. (this is a little fuzzy, needs some clarification). Insertions and evictions of blocks backed by physical storage just overwrites blocks on the device or reads data from the storage device. Managing these larger blocks prevents external fragmentation that causes GC pauses at the cost of some minor wasted space (internal fragmentation). Configuration and Usage To configure the bucket cache... (something along the line of what the current patch has).... Let me know what you think, and feel free to update/correct the draft.",-0.0292,0.01437719298,neutral
hbase,9131,description,HBASE-7404 added the bucket cache but its configuration settings are currently undocumented. Without documentation developers would be the only ones aware of the feature. Specifically documentation about slide 23 from would be great to add!,documentation_debt,outdated_documentation,"Mon, 5 Aug 2013 23:28:26 +0000","Thu, 16 Jun 2022 17:50:38 +0000","Wed, 18 Jun 2014 03:37:06 +0000",27317320,HBASE-7404 added the bucket cache but its configuration settings are currently undocumented. Without documentation developers would be the only ones aware of the feature. Specifically documentation about slide 23 from http://www.slideshare.net/cloudera/operations-session-4 would be great to add!,-0.004333333333,-0.004333333333,neutral
hbase,9742,comment_1,"Some of the changes in HBASE-5732 aren't documented. In the configuration section, I read through the code to see what the changes are now.",documentation_debt,low_quality_documentation,"Thu, 10 Oct 2013 17:47:49 +0000","Fri, 20 Nov 2015 11:53:29 +0000","Fri, 11 Oct 2013 04:15:29 +0000",37660,"Some of the changes in HBASE-5732 aren't documented. In the configuration section, I read through the code to see what the changes are now.",0,0,neutral
hbase,9742,description,"The [security in the HBase book only talks about using Kerberos. There is a simple user access mode too that is not documented. This should be documented for development systems and HBase installs where security is not an issue, but they want to prevent user mistakes. I've added a section to the security chapter that talks about simple user access. The new section makes it very clear it is not secure.",documentation_debt,low_quality_documentation,"Thu, 10 Oct 2013 17:47:49 +0000","Fri, 20 Nov 2015 11:53:29 +0000","Fri, 11 Oct 2013 04:15:29 +0000",37660,"The security section in the HBase book only talks about using Kerberos. There is a simple user access mode too that is not documented. This should be documented for development systems and HBase installs where security is not an issue, but they want to prevent user mistakes. I've added a section to the security chapter that talks about simple user access. The new section makes it very clear it is not secure.",-0.05333333333,-0.05333333333,negative
hbase,9901,comment_1,"There is no javadoc in the patch, and the findbugs in hbase-client or hbase-servers seems unrelated...",documentation_debt,low_quality_documentation,"Wed, 6 Nov 2013 08:50:52 +0000","Mon, 16 Dec 2013 18:46:55 +0000","Wed, 6 Nov 2013 18:32:18 +0000",34886,"There is no javadoc in the patch, and the findbugs in hbase-client or hbase-servers seems unrelated...",0,0,negative
hbase,991,comment_0,"Cleanup of the mapreduce examples. Started new Will point folks at examples in here since its hard keeping up examples that have been modified so they'll sit in javadoc. Also changed HbaseMapWritable so it can take byte [] for values, not just Writable. Makes sense passing byte [] rather than make a new, temporary to go from map to reduce.",documentation_debt,low_quality_documentation,"Tue, 11 Nov 2008 06:24:39 +0000","Sun, 13 Sep 2009 22:26:32 +0000","Mon, 17 Nov 2008 02:28:58 +0000",504259,"Cleanup of the mapreduce examples. Started new src/examples/mapred. Will point folks at examples in here since its hard keeping up examples that have been modified so they'll sit in javadoc. Also changed HbaseMapWritable so it can take byte [] for values, not just Writable. Makes sense passing byte [] rather than make a new, temporary ImmutableBytesWritable, to go from map to reduce.",0.075,0.06,neutral
hbase,991,description,"The examples in package doc. are old making mention of the long deprecated Text, etc. Update them.",documentation_debt,outdated_documentation,"Tue, 11 Nov 2008 06:24:39 +0000","Sun, 13 Sep 2009 22:26:32 +0000","Mon, 17 Nov 2008 02:28:58 +0000",504259,"The examples in package doc. are old making mention of the long deprecated Text, etc. Update them.",0,0,negative
hbase,17259,description,"I'm noticing that while I have create and update APIs for quotas, I missed the remove functionality. Need to add public API for that and some tests.",requirement_debt,requirement_partially_implemented,"Mon, 5 Dec 2016 18:01:55 +0000","Fri, 6 Apr 2018 04:00:24 +0000","Mon, 13 Feb 2017 17:30:07 +0000",6046092,"I'm noticing that while I have create and update APIs for quotas, I missed the remove functionality. Need to add public API for that and some tests.",-0.2,-0.2,negative
hbase,17259,summary,Missing functionality to remove space quota,requirement_debt,requirement_partially_implemented,"Mon, 5 Dec 2016 18:01:55 +0000","Fri, 6 Apr 2018 04:00:24 +0000","Mon, 13 Feb 2017 17:30:07 +0000",6046092,Missing functionality to remove space quota,-0.4,-0.4,negative
hbase,17808,description,"FastPath for the FIFO rpcscheduler was introduced in HBASE-16023. But it is not implemented for RW queues. In this issue, I use in RW queues. So anyone who want to isolate their read/write requests can also benefit from the fastpath. I haven't test the performance yet. But since I haven't change any of the core implemention of it should have the same performance in HBASE-16023.",requirement_debt,requirement_partially_implemented,"Mon, 20 Mar 2017 09:13:13 +0000","Wed, 15 Dec 2021 02:48:27 +0000","Wed, 15 Dec 2021 02:48:27 +0000",149535314,"FastPath for the FIFO rpcscheduler was introduced in HBASE-16023. But it is not implemented for RW queues. In this issue, I use FastPathBalancedQueueRpcExecutor in RW queues. So anyone who want to isolate their read/write requests can also benefit from the fastpath. I haven't test the performance yet. But since I haven't change any of the core implemention of FastPathBalancedQueueRpcExecutor, it should have the same performance in HBASE-16023.",0.074,0.074,neutral
hbase,19031,comment_3,Fixed a leftover TODO in version 2.,requirement_debt,requirement_partially_implemented,"Tue, 17 Oct 2017 20:28:32 +0000","Wed, 21 Mar 2018 22:20:10 +0000","Sat, 28 Oct 2017 23:24:46 +0000",960974,Fixed a leftover TODO in version 2.,0,0,neutral
hbase,2925,comment_4,"Here's a start. Its not done yet but shows direction: i.e. removing hashcode and equals from HBC. TODO, is test that prove that HBASE-1251 is still fixed (I think thing to do here is just read configs down in TableServers out of the conf each time rather than once up front so if number of retries is changed mid-use, subsequent invocations will pick up new config. -- let me see).",requirement_debt,requirement_partially_implemented,"Tue, 17 Aug 2010 21:27:52 +0000","Fri, 20 Nov 2015 12:42:13 +0000","Mon, 6 Sep 2010 15:52:27 +0000",1707875,"Here's a start. Its not done yet but shows direction: i.e. removing hashcode and equals from HBC. TODO, is test that prove that HBASE-1251 is still fixed (I think thing to do here is just read configs down in TableServers out of the conf each time rather than once up front so if number of retries is changed mid-use, subsequent invocations will pick up new config.  let me see).",-0.001375,-0.001833333333,neutral
hbase,3696,description,"LocalFileSystem in Hadoop doesn't currently implement sync(), so when we're running in that case, we don't have any durability. This isn't a huge deal since it isn't a realistic deployment scenario, but it's probably worth documenting. It caused some confusion for a user when a table disappeared after killing a standalone instance that was hosting its data in the local FS.",requirement_debt,requirement_partially_implemented,"Thu, 24 Mar 2011 01:10:36 +0000","Sun, 12 Jun 2022 00:55:50 +0000","Wed, 28 May 2014 04:08:13 +0000",100321057,"LocalFileSystem in Hadoop doesn't currently implement sync(), so when we're running in that case, we don't have any durability. This isn't a huge deal since it isn't a realistic deployment scenario, but it's probably worth documenting. It caused some confusion for a user when a table disappeared after killing a standalone instance that was hosting its data in the local FS.",-0.1125,-0.1125,negative
hbase,7000,comment_1,"IMHO, the maybe changed in future, so the check statement should be always be there for safety",requirement_debt,non-functional_requirements_not_fully_satisfied,"Wed, 17 Oct 2012 08:36:32 +0000","Mon, 23 Sep 2013 18:31:23 +0000","Thu, 18 Oct 2012 17:19:08 +0000",117756,"IMHO, the HConstants.MAXIMUM_VALUE_LENGTH maybe changed in future, so the check statement should be always be there for safety",0.2,0.20625,neutral
hbase,8324,comment_8,Please TODO in this comment or file a new ticket to remove this configuration tweak after the issue is resolved upstream. I don't want one-offs like this to become lost and forgotten.,requirement_debt,requirement_partially_implemented,"Thu, 11 Apr 2013 05:16:56 +0000","Mon, 23 Sep 2013 19:08:23 +0000","Fri, 12 Apr 2013 14:59:26 +0000",121350,Please TODO in this comment or file a new ticket to remove this configuration tweak after the issue is resolved upstream. I don't want one-offs like this to become lost and forgotten.,0.25,0.25,negative
hbase,10008,description,"is flakey on Jenkins, this test has failed due to timeout a couple times for me on precommit. Here's the stacktrace.",test_debt,flaky_test,"Wed, 20 Nov 2013 02:44:57 +0000","Fri, 20 Nov 2015 11:53:20 +0000","Thu, 21 Nov 2013 17:28:03 +0000",139386,"TestNamespaceCommands is flakey on Jenkins, this test has failed due to timeout a couple times for me on precommit. Here's the stacktrace.",-0.2,-0.2,negative
hbase,10008,summary,is flakey on jenkins,test_debt,flaky_test,"Wed, 20 Nov 2013 02:44:57 +0000","Fri, 20 Nov 2015 11:53:20 +0000","Thu, 21 Nov 2013 17:28:03 +0000",139386,TestNamespaceCommands is flakey on jenkins,0,0,neutral
hbase,10702,comment_8,"Now, why is it working in 0.96.1.1 with deleteColumn? I will try to write a testcase for that to validate that it's behaving as expected... Will close this one. Thanks!",test_debt,low_coverage,"Fri, 7 Mar 2014 22:16:14 +0000","Fri, 17 Jun 2022 05:10:24 +0000","Sat, 8 Mar 2014 02:15:01 +0000",14327,"Now, why is it working in 0.96.1.1 with deleteColumn? I will try to write a testcase for that to validate that it's behaving as expected... Will close this one. Thanks!",0.4,0.4,neutral
hbase,11129,comment_3,"Passing the scanner on the jobconf is a private API, now that the serialization details are private methods. This implementation detail should be isolated within a single job -- either it picks up the 0.X.Y hbase-server jar or it has 0.X.Z version, there's no mixing. We'd need to test it out, but I think making this change could be acceptable for a patch release. Looking at either is respected, or params are used. What I propose does away with the former. This way, these configs become part of the public API.",test_debt,lack_of_tests,"Wed, 7 May 2014 17:05:01 +0000","Fri, 17 Jun 2022 05:44:12 +0000","Mon, 23 Oct 2017 03:17:38 +0000",109246357,"Passing the scanner on the jobconf is a private API, now that the serialization details are private methods. This implementation detail should be isolated within a single job  either it picks up the 0.X.Y hbase-server jar or it has 0.X.Z version, there's no mixing. We'd need to test it out, but I think making this change could be acceptable for a patch release. Looking at TableInputFormat#setConf, either ""hbase.mapreduce.scan"" is respected, or ""hbase.mapreduce.scan.*"" params are used. What I propose does away with the former. This way, these configs become part of the public API.",0.166375,0.1023846154,neutral
hbase,11293,comment_10,"+1, would be good to have a test also",test_debt,lack_of_tests,"Tue, 3 Jun 2014 18:12:23 +0000","Fri, 17 Jun 2022 05:50:20 +0000","Sun, 12 Jun 2022 19:08:10 +0000",253241747,"+1, would be good to have a test also",0.776,0.776,positive
hbase,11693,comment_2,"Right, I did only test this in threaded mode. I made several passes over the changes and believe all MR details are accounted for. I don't have access to a cluster but can test on a single node YARN setup. Will do that and report back.",test_debt,low_coverage,"Wed, 6 Aug 2014 22:37:16 +0000","Tue, 9 Sep 2014 04:19:26 +0000","Fri, 8 Aug 2014 00:52:35 +0000",94519,"Right, I did only test this in threaded mode. I made several passes over the changes and believe all MR details are accounted for. I don't have access to a cluster but can test on a single node YARN setup. Will do that and report back.",0.13175,0.13175,neutral
hbase,12030,comment_3,"looks ok to me, it will be nice having a test to cover the path when zk throws an exception.. maybe the easiest way there is mocking zk?",test_debt,low_coverage,"Fri, 19 Sep 2014 18:22:46 +0000","Fri, 17 Jun 2022 17:17:14 +0000","Thu, 25 Sep 2014 04:19:34 +0000",467808,"looks ok to me, it will be nice having a test to cover the path when zk throws an exception.. maybe the easiest way there is mocking zk?",0.3916666667,0.3916666667,positive
hbase,12673,comment_4,"Hi , I don't think this unit test covers the case I'm concerned about. This unit test covers a fairly coarse grained happening -- a read is happening and inbetween read operations a table delete happens. What I'm concerned about is finer grained -- Let's say I'm reading a from a snapshot mob file which is pointing to the mob in the original dir. While this happens (while still in the middle of the read operation) a table deletion on the original table happens which move the original mob file to the archive. The read operation may fail (can't find more data from the file). With the HFilelink, we'd intercept that exception and then point the read to the moved location if the file ends up in the correct place. With the current code, I believe we get an IO exception and fail to return the proper data, or return an error. I'm in the process of crafting a rig that will constantly exercise these concurrently and hopefully will be able to produce a stack trace when this fails in a day or two.",test_debt,low_coverage,"Thu, 11 Dec 2014 03:41:47 +0000","Fri, 17 Jun 2022 05:56:42 +0000","Tue, 3 Mar 2015 02:37:17 +0000",7080930,"Hi jiajia, I don't think this unit test covers the case I'm concerned about. This unit test covers a fairly coarse grained happening  a read is happening and inbetween read operations a table delete happens. What I'm concerned about is finer grained  Let's say I'm reading a from a snapshot mob file which is pointing to the mob in the original dir. While this happens (while still in the middle of the read operation) a table deletion on the original table happens which move the original mob file to the archive. The read operation may fail (can't find more data from the file). With the HFilelink, we'd intercept that exception and then point the read to the moved location if the file ends up in the correct place. With the current code, I believe we get an IO exception and fail to return the proper data, or return an error. I'm in the process of crafting a rig that will constantly exercise these concurrently and hopefully will be able to produce a stack trace when this fails in a day or two.",0.05429166667,0.05429166667,negative
hbase,12833,comment_16,"I guess it could be, but won't connections still be leaked when these classes are used? All this shows is our test usage of this part of the client is not as through as others. Whatever you say .",test_debt,low_coverage,"Fri, 9 Jan 2015 20:40:05 +0000","Fri, 6 Apr 2018 17:55:48 +0000","Fri, 16 Jan 2015 19:55:32 +0000",602127,"On closer inspection, it looks like SecurityAdmin and VisibilityLabelsAdmin need updated with this new style as well. That is a different jira right? I guess it could be, but won't connections still be leaked when these classes are used? All this shows is our test usage of this part of the client is not as through as others. Whatever you say enis.",0.0135,0.2397,negative
hbase,13629,comment_3,"Currently in the description of this issue the before and after examples are the same text. :-) More typo-ing, whee. On the parent I remarked we should figure out how to test the bin scripts. Or convert them all to Java utilities and test those. Let's not make that a requirement to get this fix in though.",test_debt,low_coverage,"Wed, 6 May 2015 05:19:16 +0000","Fri, 24 Jun 2022 18:45:02 +0000","Wed, 6 May 2015 05:33:09 +0000",833,"Currently in the description of this issue the before and after examples are the same text. More typo-ing, whee. On the parent I remarked we should figure out how to test the bin scripts. Or convert them all to Java utilities and test those. Let's not make that a requirement to get this fix in though.",0.08,0,neutral
hbase,13776,comment_8,lgtm : Is it possible to add a unit test so that there is no regression in the future ?,test_debt,lack_of_tests,"Tue, 26 May 2015 04:05:42 +0000","Fri, 18 Dec 2015 05:41:33 +0000","Fri, 29 May 2015 18:38:15 +0000",311553,lgtm byh0831: Is it possible to add a unit test so that there is no regression in the future ?,0,0,neutral
hbase,13776,comment_9,"Can we add the wrong values of min and max versions configured, in the message? Yes pls add a test to cover this scenario. Thanks.",test_debt,lack_of_tests,"Tue, 26 May 2015 04:05:42 +0000","Fri, 18 Dec 2015 05:41:33 +0000","Fri, 29 May 2015 18:38:15 +0000",311553,"Can we add the wrong values of min and max versions configured, in the message? Yes pls add a test to cover this scenario. Thanks.",0.1166666667,0.1166666667,neutral
hbase,13905,comment_9,"Test is still in the flakey category, occasionally timing out with this stack",test_debt,flaky_test,"Mon, 15 Jun 2015 17:09:18 +0000","Mon, 31 Aug 2015 22:39:39 +0000","Tue, 16 Jun 2015 00:42:25 +0000",27187,"Test is still in the flakey category, occasionally timing out with this stack",0.281,0.281,negative
hbase,14161,comment_4,"Ok. The tests in hbase-spark are run as regular unit tests now after HBASE-17574. We may need to write new integration tests for the spark module. But until that is available, there is real IT yet.",test_debt,lack_of_tests,"Tue, 28 Jul 2015 17:23:42 +0000","Sat, 28 Sep 2024 02:54:23 +0000","Mon, 17 Jul 2017 15:28:34 +0000",62201092,"Ok. The tests in hbase-spark are run as regular unit tests now after HBASE-17574. We may need to write new integration tests for the spark module. But until that is available, there is real IT yet.",0.2875,0.2875,neutral
hbase,14604,comment_1,Any chance of a unit test ?,test_debt,lack_of_tests,"Wed, 14 Oct 2015 09:54:05 +0000","Tue, 20 Oct 2015 15:20:36 +0000","Tue, 20 Oct 2015 09:40:16 +0000",517571,Any chance of a unit test ?,0.4,0.4,neutral
hbase,14622,comment_13,"zkless is the future.. It will be default in 2.0, or at least a version of it -- one w/ state preserved in procedure store rather than up in meta. zkless in 1.0 is not well tested and I thought no one was using it (Only case I know of someone using it, is on a branch made of 0.98) I removed the tests here because it was an 'unsupported' feature test running on top of read replicas, a feature that is not on by default throwing an NPE. Rather than try and figure what was up in the test, and not expecting that someone would be jumping at the opportunity to fix issues in here, I just removed these zkless tests from branch-1 so they can't fail again. I can put them back if you'd like , just say.",test_debt,lack_of_tests,"Thu, 15 Oct 2015 19:56:14 +0000","Sat, 2 Nov 2019 23:55:29 +0000","Thu, 15 Oct 2015 20:04:00 +0000",466,"zkless is the future.. It will be default in 2.0, or at least a version of it  one w/ state preserved in procedure store rather than up in meta. zkless in 1.0 is not well tested and I thought no one was using it (Only case I know of someone using it, is on a branch made of 0.98) I removed the tests here because it was an 'unsupported' feature test running on top of read replicas, a feature that is not on by default throwing an NPE. Rather than try and figure what was up in the test, and not expecting that someone would be jumping at the opportunity to fix issues in here, I just removed these zkless tests from branch-1 so they can't fail again. I can put them back if you'd like eclark, just say.",-0.1597,-0.1597,neutral
hbase,14753,comment_1,"Ok, sorry for the spam then. I was trying to write a shell test, and was confused. Do we need to keep this open for tracking re-enabling or HBASE-14678 covers that?",test_debt,lack_of_tests,"Tue, 3 Nov 2015 22:53:34 +0000","Thu, 9 Nov 2017 19:11:41 +0000","Wed, 17 Aug 2016 00:03:31 +0000",24800997,"Ok, sorry for the spam then. I was trying to write a shell test, and was confused. Do we need to keep this open for tracking re-enabling or HBASE-14678 covers that?",-0.1666666667,-0.1666666667,neutral
hbase,14753,comment_2,I was thinking HBASE-14678 would do. I'd file issues for stuff I don't want to bring back in because flakey still... tests on shell are pretty important.,test_debt,flaky_test,"Tue, 3 Nov 2015 22:53:34 +0000","Thu, 9 Nov 2017 19:11:41 +0000","Wed, 17 Aug 2016 00:03:31 +0000",24800997,I was thinking HBASE-14678 would do. I'd file issues for stuff I don't want to bring back in because flakey still... tests on shell are pretty important.,0.335125,0.335125,neutral
hbase,15192,comment_6,Looks like there are other flaky sub-tests.,test_debt,flaky_test,"Sat, 30 Jan 2016 00:05:31 +0000","Wed, 24 Feb 2016 06:39:27 +0000","Mon, 8 Feb 2016 23:34:56 +0000",862165,Looks like there are other flaky sub-tests.,0,0,negative
hbase,15192,comment_9,"If there is no objection, I plan to resolve this JIRA. There is still flaky subtest, e.g. : which we can track in separate issue(s)",test_debt,flaky_test,"Sat, 30 Jan 2016 00:05:31 +0000","Wed, 24 Feb 2016 06:39:27 +0000","Mon, 8 Feb 2016 23:34:56 +0000",862165,"If there is no objection, I plan to resolve this JIRA. There is still flaky subtest, e.g. : https://builds.apache.org/job/HBase-TRUNK_matrix/lastCompletedBuild/jdk=latest1.8,label=yahoo-not-h2/testReport/org.apache.hadoop.hbase.regionserver/TestRegionMergeTransactionOnCluster/testMergeWithReplicas/ which we can track in separate issue(s)",0.3405,0.3405,neutral
hbase,15192,description,"fails intermittently due to failed assertion on cleaned merge region count: Before calling the test does: newcount1 is not cleared at the beginning of the loop. This means that if the check for newcount1 <= 1 doesn't pass the first iteration, it wouldn't pass in subsequent iterations. After timeout is exhausted, is called. However, there is a chance that has been called by the Chore already (during the wait period), leaving the cleaned count 0 and failing the test.",test_debt,flaky_test,"Sat, 30 Jan 2016 00:05:31 +0000","Wed, 24 Feb 2016 06:39:27 +0000","Mon, 8 Feb 2016 23:34:56 +0000",862165,"TestRegionMergeTransactionOnCluster#testCleanMergeReference fails intermittently due to failed assertion on cleaned merge region count: Before calling CatalogJanitor#scan(), the test does: newcount1 is not cleared at the beginning of the loop. This means that if the check for newcount1 <= 1 doesn't pass the first iteration, it wouldn't pass in subsequent iterations. After timeout is exhausted, admin.runCatalogScan() is called. However, there is a chance that CatalogJanitor#scan() has been called by the Chore already (during the wait period), leaving the cleaned count 0 and failing the test.",-0.2,-0.16,negative
hbase,15192,summary,is flaky,test_debt,flaky_test,"Sat, 30 Jan 2016 00:05:31 +0000","Wed, 24 Feb 2016 06:39:27 +0000","Mon, 8 Feb 2016 23:34:56 +0000",862165,TestRegionMergeTransactionOnCluster#testCleanMergeReference is flaky,0,0,negative
hbase,15287,comment_6,Can you modify the following tests to cover your fix ?,test_debt,lack_of_tests,"Wed, 17 Feb 2016 22:01:45 +0000","Thu, 9 Nov 2017 00:42:02 +0000","Sun, 17 Apr 2016 16:30:17 +0000",5164112,Can you modify the following tests to cover your fix ? hbase-server/src/test//java/org/apache/hadoop/hbase/mapreduce/TestImportExport.java hbase-server/src/test//java/org/apache/hadoop/hbase/mapreduce/TestCopyTable.java hbase-server/src/test//java/org/apache/hadoop/hbase/mapreduce/TestCellCounter.java,0,0,neutral
hbase,1558,comment_1,"here's a prototype fix, but we need tests.",test_debt,lack_of_tests,"Sun, 21 Jun 2009 03:42:47 +0000","Sun, 13 Sep 2009 22:24:45 +0000","Mon, 22 Jun 2009 18:10:11 +0000",138444,"here's a prototype fix, but we need tests.",0,0,neutral
hbase,15835,comment_11,"Okay, the above-listed QA-failure prompted me to read in more detail those emails regarding the problem of ""flaky|flakey"" tests. I see that is indeed on the list, and also that testing of HBASE-15876 ran into exactly the same (apparently ""flaky|flakey"") failure. In this case, an awkward aspect is that the changes I made could conceivably have caused this kind of a failure!! So, just for good measure, I did a fresh clone/install of the Master branch and ran the same test: yep, it failed in exactly the same way.",test_debt,flaky_test,"Mon, 16 May 2016 08:54:16 +0000","Fri, 1 Jul 2022 20:27:59 +0000","Wed, 9 May 2018 14:58:37 +0000",62489061,"Okay, the above-listed QA-failure prompted me to read in more detail those emails regarding the problem of ""flaky|flakey"" tests. I see that TestRegionServerMetrics is indeed on the list, and also that testing of HBASE-15876 ran into exactly the same (apparently ""flaky|flakey"") failure. In this case, an awkward aspect is that the changes I made could conceivably have caused this kind of a failure!! So, just for good measure, I did a fresh clone/install of the Master branch and ran the same test: yep, it failed in exactly the same way.",-0.151625,-0.151625,negative
hbase,16157,comment_6,lgtm. Is the test stable enough? Looks like it might end up being a flaky test.,test_debt,flaky_test,"Thu, 30 Jun 2016 18:21:41 +0000","Wed, 6 Jul 2016 02:00:24 +0000","Tue, 5 Jul 2016 20:57:21 +0000",441340,lgtm. Is the test stable enough? Looks like it might end up being a flaky test.,0.15,0.15,negative
hbase,17025,comment_7,Is it possible to add some .rb test ? Thanks,test_debt,lack_of_tests,"Fri, 4 Nov 2016 18:42:27 +0000","Fri, 6 Apr 2018 04:00:24 +0000","Fri, 17 Feb 2017 16:51:07 +0000",9065320,Is it possible to add some .rb test ? Thanks,0.1333333333,0.1333333333,neutral
hbase,17883,comment_6,Now stuck on HBASE-19239. Many of the problems are in unit tests so need fixing in order for tests to be correct and actually cover functionality as intended.,test_debt,low_coverage,"Wed, 5 Apr 2017 17:06:11 +0000","Thu, 25 Jan 2018 02:16:37 +0000","Thu, 25 Jan 2018 02:16:37 +0000",25434626,Now stuck on HBASE-19239. Many of the problems are in unit tests so need fixing in order for tests to be correct and actually cover functionality as intended.,-0.015625,-0.015625,negative
hbase,18085,comment_6,Any chance for a micro benchmark tests Yu? Appreciate ur effort to consider all possible options :-),test_debt,lack_of_tests,"Fri, 19 May 2017 19:04:08 +0000","Wed, 1 Aug 2018 06:24:22 +0000","Wed, 24 May 2017 07:57:01 +0000",391973,Any chance for a micro benchmark tests Yu? Appreciate ur effort to consider all possible options,0.4,0.4,positive
hbase,18180,comment_8,You can attach branch-1 patch alone where TestLockProcedure is not flaky.,test_debt,flaky_test,"Wed, 7 Jun 2017 02:54:21 +0000","Wed, 1 Aug 2018 06:21:19 +0000","Mon, 19 Jun 2017 13:40:24 +0000",1075563,You can attach branch-1 patch alone where TestLockProcedure is not flaky.,-0.4,-0.4,neutral
hbase,19073,comment_2,"So the only test passing in QA is But it's weird, this is second time am seeing it failing in a precommit, and the last patch was completely unrelated. It's not even in the flaky list. Let me dig.",test_debt,flaky_test,"Mon, 23 Oct 2017 22:47:02 +0000","Wed, 1 Aug 2018 06:22:22 +0000","Wed, 25 Oct 2017 03:05:27 +0000",101905,"So the only test passing in QA is TestDistributedLogSplitting. But it's weird, this is second time am seeing it failing in a precommit, and the last patch was completely unrelated. It's not even in the flaky list. Let me dig.",-0.1333333333,-0.1,negative
hbase,19384,comment_1,"any chance you could share the specifics on how the coprocessors are set up in which you see this? Or, if you really want to go the extra mile, distill it down to a generic test case? Assuming there is a bug anyways, having a clear example of what is going wrong would be good both for identifying the problem as well as preventing a regression later :)",test_debt,lack_of_tests,"Thu, 30 Nov 2017 04:43:15 +0000","Wed, 1 Aug 2018 06:20:54 +0000","Tue, 5 Dec 2017 16:08:25 +0000",473110,"rajeshbabu any chance you could share the specifics on how the coprocessors are set up in which you see this? Or, if you really want to go the extra mile, distill it down to a generic test case? Assuming there is a bug anyways, having a clear example of what is going wrong would be good both for identifying the problem as well as preventing a regression later",0.1432857143,0.1207222222,neutral
hbase,19384,comment_3,Sure will write test case for this.  Correct. Yes it's because of removal of complete so we are not able to skip running subsequent coprocessors which do not have any implementation for preAppend or preIncrement hooks.,test_debt,lack_of_tests,"Thu, 30 Nov 2017 04:43:15 +0000","Wed, 1 Aug 2018 06:20:54 +0000","Tue, 5 Dec 2017 16:08:25 +0000",473110,"elserj Rajeshbabu Chintaguntla any chance you could share the specifics on how the coprocessors are set up in which you see this? Or, if you really want to go the extra mile, distill it down to a generic test case? Sure will write test case for this. stack So you have multiple coprocessors stacked on a region. One intercepts preAppend (or preIncrement) to return its own result instead. Are you saying that this result is overwritten by the null that subsequent coprocessors return? Correct. Is the problem our removal of 'complete'? i.e. HBASE-19123 Purge 'complete' support from Coprocesor Observers ? Thanks. Yes it's because of removal of complete so we are not able to skip running subsequent coprocessors which do not have any implementation for preAppend or preIncrement hooks.",0.2103333333,0.1333636364,neutral
hbase,19775,comment_4,"yep, @josh is right about the oneliner style - it's much more ruby-esque. {{cause = cause.getCause if cause.is_a? can we add a test in",test_debt,lack_of_tests,"Thu, 11 Jan 2018 21:27:45 +0000","Wed, 1 Aug 2018 06:23:45 +0000","Fri, 12 Jan 2018 18:02:33 +0000",74088,"yep, @josh is right about the oneliner style - it's much more ruby-esque. cause = cause.getCause if cause.is_a? java.io.UncheckedIOException can we add a test in hbase-shell/src/test/ruby/hbase/table_test.rb?",0.0727,0.0454375,positive
hbase,19815,summary,Flakey,test_debt,flaky_test,"Wed, 17 Jan 2018 18:53:33 +0000","Tue, 7 May 2019 16:08:47 +0000","Thu, 18 Jan 2018 19:33:43 +0000",88810,Flakey TestAssignmentManager.testAssignWithRandExec,0,0,neutral
hbase,19939,comment_1,"The flakey-finder fingered the above commit as breaking the split test. Here is when the split test went bad... Unstable Build #1372 (Feb 5, 2018 3:53:40 PM) add description Build Artifacts Changes HBASE-19703 Functionality added as part of HBASE-12583 is not working (detail) Started by timer This run spent: 3 min 39 sec waiting in the queue; 27 min building on an executor; 31 min total from scheduled to completion. Revision: Test Result (6 failures / +3) See how the commit is ""HBASE-19703 Functionality added as part of HBASE-12583 is not working (detail)""",test_debt,flaky_test,"Mon, 5 Feb 2018 20:07:16 +0000","Wed, 1 Aug 2018 06:22:34 +0000","Tue, 6 Feb 2018 04:53:36 +0000",31580,"The flakey-finder fingered the above commit as breaking the split test. Here is when the split test went bad... Unstable Build #1372 (Feb 5, 2018 3:53:40 PM) add description Build Artifacts Changes HBASE-19703 Functionality added as part of HBASE-12583 is not working (detail) Started by timer This run spent: 3 min 39 sec waiting in the queue; 27 min building on an executor; 31 min total from scheduled to completion. Revision: f0a5f12d97784f609ccd15e1228d424bcab59c41 refs/remotes/origin/branch-2 Test Result (6 failures / +3) org.apache.hadoop.hbase.client.TestAvoidCellReferencesIntoShippedBlocks.org.apache.hadoop.hbase.client.TestAvoidCellReferencesIntoShippedBlocks org.apache.hadoop.hbase.client.TestAvoidCellReferencesIntoShippedBlocks.org.apache.hadoop.hbase.client.TestAvoidCellReferencesIntoShippedBlocks org.apache.hadoop.hbase.client.TestMetaWithReplicas.org.apache.hadoop.hbase.client.TestMetaWithReplicas org.apache.hadoop.hbase.client.TestMetaWithReplicas.org.apache.hadoop.hbase.client.TestMetaWithReplicas org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure.testSplitWithoutPONR org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure.testRecoveryAndDoubleExecution See how the commit is ""HBASE-19703 Functionality added as part of HBASE-12583 is not working (detail)""",-0.1138888889,0.005327868852,negative
hbase,19939,comment_3,"is already in flaky list so the QA didn't run it for HBASE-19703.- -Ok, It is rather than I misunderstood the test name due to the topic...Let me correct the test name for the topic. The correct test name is rather than",test_debt,flaky_test,"Mon, 5 Feb 2018 20:07:16 +0000","Wed, 1 Aug 2018 06:22:34 +0000","Tue, 6 Feb 2018 04:53:36 +0000",31580,"TestSplitTableRegionProcedure is already in flaky list so the QA didn'trun it for HBASE-19703. Ok,It isTestSplitTableRegion rather thanTestSplitTableRegionProcedure I misunderstood the test name due to the topic...Let me correct the test name for the topic. The correct test name isTestSplitTableRegionProcedure rather thanTestSplitTableRegion.",0.6416666667,0.48125,negative
hbase,19939,comment_4,The latest QA in HBASE-19703 is shown below. The is already in flaky.  Do you intent to fix the test totally? I'm +1 to your patch even if the is still flaky.,test_debt,flaky_test,"Mon, 5 Feb 2018 20:07:16 +0000","Wed, 1 Aug 2018 06:22:34 +0000","Tue, 6 Feb 2018 04:53:36 +0000",31580,The flakey-finder fingered the above commit as breaking the split test. The latest QA in HBASE-19703 is shown below. TheTestSplitTableRegionProcedure is already in flaky. uagashe Do you intent to fix the test totally? I'm +1to your patch even if theTestSplitTableRegionProcedure is still flaky.,0.2265,0.2212,negative
hbase,19939,comment_5,Thanks for correcting the name ! Let me fix the commit description as well and resubmit the patch. After this fix if the test is still flaky then I will take another look at it.,test_debt,flaky_test,"Mon, 5 Feb 2018 20:07:16 +0000","Wed, 1 Aug 2018 06:22:34 +0000","Tue, 6 Feb 2018 04:53:36 +0000",31580,Thanks for correcting the name chia7712! Let me fix the commit description as well and resubmit the patch. After this fix if the test is still flaky then I will take another look at it.,0.3655,0.3655,neutral
hbase,19969,comment_20,", looks like you might have some flaky tests on Hadoop3. Would be good to take a quick look to rule out test issues (the cnxn refused sounds like it might be just be the node itself). Logs are at",test_debt,flaky_test,"Fri, 9 Feb 2018 22:57:42 +0000","Wed, 28 Mar 2018 02:50:38 +0000","Sat, 10 Mar 2018 17:49:55 +0000",2487133,"vrodionov, looks like you might have some flaky tests on Hadoop3. Would be good to take a quick look to rule out test issues (the cnxn refused sounds like it might be just be the node itself). Logs are at https://builds.apache.org/job/HBase%20Nightly/job/master/259/artifact/output-jdk8-hadoop3/",0.196,0.196,negative
hbase,19977,comment_1,Yes . Using AtomicInteger will solve the problem. Will test this in a cluster.,test_debt,lack_of_tests,"Mon, 12 Feb 2018 09:08:13 +0000","Wed, 1 Aug 2018 06:21:18 +0000","Tue, 13 Feb 2018 10:22:15 +0000",90842,Yes Apache9. Using AtomicInteger will solve the problem. Will test this in a cluster.,0.03333333333,0.03333333333,positive
hbase,19998,comment_4,Could we add some commit message to remind readers that the commit is for debug? It can help readers to realize that the flaky hasn't been fixed.,test_debt,flaky_test,"Wed, 14 Feb 2018 06:29:48 +0000","Tue, 7 May 2019 16:08:44 +0000","Fri, 16 Feb 2018 21:00:02 +0000",225014,Could we add some commit message to remind readers that the commit is for debug? It can help readers to realize that the flaky hasn't been fixed.,0.3,0.3,neutral
hbase,19998,comment_7,"The test failed in flakies a few times last night, lets see how it does.",test_debt,flaky_test,"Wed, 14 Feb 2018 06:29:48 +0000","Tue, 7 May 2019 16:08:44 +0000","Fri, 16 Feb 2018 21:00:02 +0000",225014,"The test failed in flakies a few times last night, https://builds.apache.org/job/HBASE-Flaky-Tests-branch2.0/2048/artifact/hbase-server/target/surefire-reports/org.apache.hadoop.hbase.security.visibility.TestVisibilityLabelsWithCustomVisLabService-output.txt, lets see how it does.",-0.4,-0.4,neutral
hbase,19998,summary,Flakey,test_debt,flaky_test,"Wed, 14 Feb 2018 06:29:48 +0000","Tue, 7 May 2019 16:08:44 +0000","Fri, 16 Feb 2018 21:00:02 +0000",225014,Flakey TestVisibilityLabelsWithDefaultVisLabelService,0,0,neutral
hbase,20072,comment_1,We don't have a test suite for the book other than building it and we don't have an automated check for building the book. :/,test_debt,lack_of_tests,"Sat, 24 Feb 2018 19:09:14 +0000","Fri, 6 Apr 2018 04:58:06 +0000","Tue, 6 Mar 2018 19:59:06 +0000",866992,We don't have a test suite for the book other than building it and we don't have an automated check for building the book. :/,-0.2,-0.2,negative
hbase,20072,comment_2,filed HBASE-20077 for the lack of a test for building the book. that jira also has a command you can run locally if you want to quickly check the result of this change.,test_debt,lack_of_tests,"Sat, 24 Feb 2018 19:09:14 +0000","Fri, 6 Apr 2018 04:58:06 +0000","Tue, 6 Mar 2018 19:59:06 +0000",866992,filed HBASE-20077 for the lack of a test for building the book. that jira also has a command you can run locally if you want to quickly check the result of this change.,-0.1,-0.1,neutral
hbase,20100,description,"Failed in the nightly in interesting way. A subprocedure of enable table is assigning regions. The test is doing a kill of the procedure store to ensure we can handle all state transtions even in face of failure. In this case, the kill of PE framework came in the finish of the assign procedure AFTER we'd updated hbase:meta and the internal AM state moving Region from OPENING to OPEN. Rerunning this step on restoration of the AMv2 WAL Store results in....",test_debt,low_coverage,"Tue, 27 Feb 2018 19:59:19 +0000","Tue, 7 May 2019 16:08:43 +0000","Thu, 1 Mar 2018 16:55:27 +0000",161768,"Failed in the nightly in interesting way. https://builds.apache.org/job/HBase%20Nightly/job/branch-2/398/ A subprocedure of enable table is assigning regions. The test is doing a kill of the procedure store to ensure we can handle all state transtions even in face of failure. In this case, the kill of PE framework came in the finish of the assign procedure AFTER we'd updated hbase:meta and the internal AM state moving Region from OPENING to OPEN. Rerunning this step on restoration of the AMv2 WAL Store results in....",-0.07333333333,-0.07333333333,neutral
hbase,20100,summary,flakey,test_debt,flaky_test,"Tue, 27 Feb 2018 19:59:19 +0000","Tue, 7 May 2019 16:08:43 +0000","Thu, 1 Mar 2018 16:55:27 +0000",161768,TestEnableTableProcedure flakey,0,0,neutral
hbase,20975,comment_4,Please include a small change in hbase-server module so we can run more tests?,test_debt,low_coverage,"Mon, 30 Jul 2018 07:56:06 +0000","Tue, 14 Aug 2018 11:44:47 +0000","Mon, 13 Aug 2018 12:24:30 +0000",1225704,Please include a small change in hbase-server module so we can run more tests?,0.2,0.2,neutral
hbase,21589,comment_2,"It is very strange, it never failed in my environment. , can you upload an output or something, I can't find the failing test in jenkins or Flaky test board.",test_debt,flaky_test,"Wed, 12 Dec 2018 19:12:09 +0000","Mon, 4 Mar 2019 08:58:21 +0000","Mon, 17 Dec 2018 17:32:49 +0000",426040,"It is very strange, it never failed in my environment. stack, can you upload an output or something, I can't find the failing test in jenkins PreCommit-HBASE-Build or Flaky test board.",0.2,0.25,negative
hbase,22034,comment_6,Unit test failure is not related as far as I can tell. Does not reproduce locally. It looks like a flake in the precommit env. I can fix the one remaining checkstyle nit upon commit. (Indentation at,test_debt,flaky_test,"Mon, 11 Mar 2019 19:15:33 +0000","Fri, 6 Sep 2019 00:32:11 +0000","Fri, 22 Mar 2019 00:28:40 +0000",882787,Unit test failure is not related as far as I can tell. Does not reproduce locally. It looks like a flake in the precommit env. I can fix the one remaining checkstyle nit upon commit. (Indentation at TestKeyValue.java:573),-0.04,-0.03333333333,negative
hbase,22424,comment_1,"nice findings. Just curious, have you ran the flaky tests multiple times (such as 10) and all are passing?",test_debt,flaky_test,"Wed, 15 May 2019 08:47:37 +0000","Fri, 17 May 2019 19:59:26 +0000","Fri, 17 May 2019 01:18:34 +0000",145857,"nice findings. Just curious, have you ran the flaky tests multiple times (such as 10) and all are passing?",0.5125,0.5125,positive
hbase,22424,description,"When running rsgroup test class folder to run all the UTs together, and will flaky. Because TestRSGroupsAdmin1, TestRSGroupsAdmin2 and TestRSGroupsBalance are all extends TestRSGroupsBase, which has a static variable INIT, controlling the initialize of 'master 'group and the number of rs in 'default' rsgroup. Output errors of is shown in HBASE-22420, and will encounter NPE in because `master` group has not been added.",test_debt,flaky_test,"Wed, 15 May 2019 08:47:37 +0000","Fri, 17 May 2019 19:59:26 +0000","Fri, 17 May 2019 01:18:34 +0000",145857,"When running rsgroup test class folder to run all the UTs together, TestRSGroupsAdmin2.testMoveServersAndTables and TestRSGroupsBalance.testGroupBalance will flaky. Because TestRSGroupsAdmin1, TestRSGroupsAdmin2 and TestRSGroupsBalance are all extends TestRSGroupsBase, which has a static variable INIT, controlling the initialize of 'master 'group and the number of rs in 'default' rsgroup. Output errors of TestRSGroupsBalance.testGroupBalance is shown in HBASE-22420, and TestRSGroupsAdmin2.testMoveServersAndTables will encounter NPE in ```rsGroupAdmin.getRSGroupInfo(""master"").containsServer(server.getAddress())``` because `master` group has not been added.",-0.03958333333,0.043125,neutral
hbase,22424,summary,Interactions in RSGroup test classes will cause and flaky,test_debt,flaky_test,"Wed, 15 May 2019 08:47:37 +0000","Fri, 17 May 2019 19:59:26 +0000","Fri, 17 May 2019 01:18:34 +0000",145857,Interactions in RSGroup test classes will cause TestRSGroupsAdmin2.testMoveServersAndTables and TestRSGroupsBalance.testGroupBalance flaky,0,0,negative
hbase,2341,comment_8,Cosmin suggests that we instrument the code with a coverage tool while running a long-running cluster test. We can then identify edge cases / dark corners better (and write tests to exercise them),test_debt,lack_of_tests,"Wed, 17 Mar 2010 23:25:26 +0000","Sat, 11 Jun 2022 23:22:01 +0000","Wed, 16 Jul 2014 20:45:12 +0000",136675186,Cosmin suggests that we instrument the code with a coverage tool while running a long-running cluster test. We can then identify edge cases / dark corners better (and write tests to exercise them),0.03125,0.03125,neutral
hbase,23752,comment_2,"Mind taking a look at this? This is the last (hopefully) patch that fixes all the test failures from the full nightly run. I verified that the two failed tests are flakes, ran them locally.",test_debt,flaky_test,"Tue, 28 Jan 2020 20:59:07 +0000","Wed, 8 Apr 2020 22:08:15 +0000","Mon, 3 Feb 2020 18:18:29 +0000",508762,"stack/andrew.purtell@gmail.com Mind taking a look at this? This is the last (hopefully) patch that fixes all the test failures from the full nightly run. I verified that the two failed tests are flakes, ran them locally.",0.35,0.35,neutral
hbase,23752,summary,Fix a couple more test failures from nightly run,test_debt,low_coverage,"Tue, 28 Jan 2020 20:59:07 +0000","Wed, 8 Apr 2020 22:08:15 +0000","Mon, 3 Feb 2020 18:18:29 +0000",508762,Fix a couple more test failures from nightly run,-0.4,-0.4,negative
hbase,23789,description,"We can't find the balancer rules we just read in the test in high load conditions Test then goes on to fail with: Instead, have tests write rules to local test dir.",test_debt,low_coverage,"Tue, 4 Feb 2020 05:49:24 +0000","Tue, 7 Apr 2020 21:39:02 +0000","Wed, 5 Feb 2020 00:42:09 +0000",67965,"We can't find the balancer rules we just read in the HeterogeneousRegionCountCostFunction test in high load conditions Test then goes on to fail with: Instead, have tests write rules to local test dir.",0.4,0.4,negative
hbase,23789,summary,[Flakey Tests] ERROR [Time-limited test] cannot read rules file located at ' ',test_debt,flaky_test,"Tue, 4 Feb 2020 05:49:24 +0000","Tue, 7 Apr 2020 21:39:02 +0000","Wed, 5 Feb 2020 00:42:09 +0000",67965,[Flakey Tests] ERROR [Time-limited test] balancer.HeterogeneousRegionCountCostFunction(199): cannot read rules file located at ' /tmp/hbase-balancer.rules ',-0.3,-0.2333333333,negative
hbase,23792,comment_0,"I can't repro this locally or find a source for the filesystem implementation getting flipped to a distributed fs. However, the only place in Hadoop code where I see this ""Wrong FS"" message thrown as an is in Looking closer at the xml report, I see that the test failed once with the above. Surefire tried to re-run it, but it failed the rerun with which implies to me that when surefire reruns a test method, it does not run the BeforeClass business. I also notice that the test method runs the same code twice, but both times it's using I think one of the invocations is supposed to be calling So. # Survive flakey rerunning by converting the static {{BeforeClass}} stuff into instance-level {{Before}}. # Break the test method into two, one for running over each of the snapshot manifest versions.",test_debt,flaky_test,"Tue, 4 Feb 2020 16:51:38 +0000","Fri, 31 Dec 2021 14:06:28 +0000","Fri, 31 Dec 2021 14:06:07 +0000",60124469,"I can't repro this locally or find a source for the filesystem implementation getting flipped to a distributed fs. However, the only place in Hadoop code where I see this ""Wrong FS"" message thrown as an IllegalArgumentException is in FileSystem#checkPath. Looking closer at the xml report, I see that the test failed once with the above. Surefire tried to re-run it, but it failed the rerun with which implies to me that when surefire reruns a test method, it does not run the BeforeClass business. I also notice that the test method runs the same code twice, but both times it's using createSnapshotV2... I think one of the invocations is supposed to be calling createSnapshotV1. So. Survive flakey rerunning by converting the static BeforeClass stuff into instance-level Before. Break the test method into two, one for running over each of the snapshot manifest versions.",-0.2666666667,-0.2138888889,negative
hbase,23792,summary,[Flakey Test],test_debt,flaky_test,"Tue, 4 Feb 2020 16:51:38 +0000","Fri, 31 Dec 2021 14:06:28 +0000","Fri, 31 Dec 2021 14:06:07 +0000",60124469,[Flakey Test] TestExportSnapshotNoCluster.testSnapshotWithRefsExportFileSystemState,0,0,neutral
hbase,23825,comment_5,Ah... We considered BoundedByteString case there but did not do a check whether that is subclass of LiteralByteString BTW why no 1.5.1 fix version when it says affected at 1.5.0?,test_debt,low_coverage,"Mon, 10 Feb 2020 22:14:22 +0000","Wed, 12 Feb 2020 23:53:24 +0000","Wed, 12 Feb 2020 01:18:51 +0000",97469,Ah... We considered BoundedByteString case there but did not do a check whether that is subclass of LiteralByteString BTW why no 1.5.1 fix version when it says affected at 1.5.0?,0.35,0.35,negative
hbase,23863,summary,[Flakey Test] improvements,test_debt,flaky_test,"Mon, 17 Feb 2020 21:12:37 +0000","Tue, 7 Apr 2020 21:37:34 +0000","Fri, 21 Feb 2020 18:29:11 +0000",335794,[Flakey Test] TestReplicationEndpointWithMultipleWAL#testInterClusterReplication improvements,0,0,neutral
hbase,23867,description,Test has this on it: @Test // Test is flakey. TODO: Fix! Let me cut it down. It runs for a while. It fails nearly every run since HBASE-23865 Up flakey history from 5 to 10 which upped the flakey test so it ran with more fork count.,test_debt,flaky_test,"Wed, 19 Feb 2020 06:38:28 +0000","Tue, 7 Apr 2020 20:38:07 +0000","Wed, 19 Feb 2020 06:42:59 +0000",271,Test has this on it: @Test // Test is flakey. TODO: Fix! Let me cut it down. It runs for a while. It fails nearly every run since HBASE-23865 Up flakey history from 5 to 10 which upped the flakey test so it ran with more fork count.,-0.1292,-0.1292,negative
hbase,23867,summary,[Flakey Test],test_debt,flaky_test,"Wed, 19 Feb 2020 06:38:28 +0000","Tue, 7 Apr 2020 20:38:07 +0000","Wed, 19 Feb 2020 06:42:59 +0000",271,[Flakey Test] TestStochasticLoadBalancerRegionReplicaSameHosts#testRegionReplicationOnMidClusterSameHosts,0,0,neutral
hbase,3057,summary,Race condition when closing regions that causes flakiness in TestRestartCluster,test_debt,flaky_test,"Thu, 30 Sep 2010 18:44:50 +0000","Fri, 20 Nov 2015 12:43:07 +0000","Thu, 30 Sep 2010 18:54:09 +0000",559,Race condition when closing regions that causes flakiness in TestRestartCluster,0,0,neutral
hbase,4459,comment_8,What is this about? Is it necessary to this patch? And this: Looks good Ram. Any chance of a test to prove it works the way it used to?,test_debt,lack_of_tests,"Thu, 22 Sep 2011 18:46:58 +0000","Fri, 20 Nov 2015 11:55:50 +0000","Thu, 20 Oct 2011 18:23:23 +0000",2417785,What is this about? Is it necessary to this patch? And this: Looks good Ram. Any chance of a test to prove it works the way it used to?,0.194,0.194,positive
hbase,4740,description,"Running more frequently seems to show that it has become flaky. It is difficult to tell if this is because of a unrecoverable failure or a recoverable failure. To make this visiable from test and for users, we need to make a change to bulkload call's interface on HRegionServer. The change should make successful rpcs return true, recoverable failures return false, and unrecoverable failure throw an IOException. This is an RPC change, so it would be really good to get this api right before the final 0.92 goes out.",test_debt,flaky_test,"Thu, 3 Nov 2011 17:58:14 +0000","Fri, 20 Nov 2015 11:53:11 +0000","Tue, 8 Nov 2011 14:40:38 +0000",420144,"Running TestHFileOutputFormat more frequently seems to show that it has become flaky. It is difficult to tell if this is because of a unrecoverable failure or a recoverable failure. To make this visiable from test and for users, we need to make a change to bulkload call's interface on HRegionServer. The change should make successful rpcs return true, recoverable failures return false, and unrecoverable failure throw an IOException. This is an RPC change, so it would be really good to get this api right before the final 0.92 goes out.",0.07381428571,0.07381428571,neutral
hbase,5217,description,"At some point we disabled tests for the thrift server. In addition, it looks like the getRegionInfo no longer functions. I'd like to reenable the tests and add one for getRegionInfo. I had to write this to test my changes in HBASE-2600 anyway. I figured I would break it out. We shouldn't commit it until we have fixed getting the regioninfo from the thriftserver.",test_debt,low_coverage,"Tue, 17 Jan 2012 19:17:36 +0000","Sun, 12 Jun 2022 20:10:28 +0000","Wed, 22 Apr 2015 00:33:57 +0000",102834981,"At some point we disabled tests for the thrift server. In addition, it looks like the getRegionInfo no longer functions. I'd like to reenable the tests and add one for getRegionInfo. I had to write this to test my changes in HBASE-2600 anyway. I figured I would break it out. We shouldn't commit it until we have fixed getting the regioninfo from the thriftserver.",-0.03333333333,-0.03333333333,negative
hbase,6806,comment_5,"python, c++ and java: worksforme I had no chance to test php, perl and ruby, they might even have syntactic errors in it...",test_debt,lack_of_tests,"Tue, 18 Sep 2012 13:55:58 +0000","Mon, 23 Sep 2013 18:30:49 +0000","Fri, 21 Sep 2012 04:08:14 +0000",223936,"python, c++ and java: worksforme I had no chance to test php, perl and ruby, they might even have syntactic errors in it...",-0.4,-0.4,negative
hbase,7000,comment_7,The two failed tests are flaky. Integrated to trunk. Thanks for the patch Liang.,test_debt,flaky_test,"Wed, 17 Oct 2012 08:36:32 +0000","Mon, 23 Sep 2013 18:31:23 +0000","Thu, 18 Oct 2012 17:19:08 +0000",117756,The two failed tests are flaky. Integrated to trunk. Thanks for the patch Liang.,0,0,negative
hbase,7172,comment_4,"Ops, I've found some more flaky tests: I think we can just increase the timeouts a la HBASE-7165. I'll do a v2 patch.",test_debt,flaky_test,"Fri, 16 Nov 2012 00:38:37 +0000","Tue, 26 Feb 2013 08:23:00 +0000","Wed, 28 Nov 2012 22:41:14 +0000",1116157,"Ops, I've found some more flaky tests: I think we can just increase the timeouts a la HBASE-7165. I'll do a v2 patch.",0.1,0.1,negative
hbase,7172,comment_6,"Attaching v2 patches, which hopefully fixes the remaining flaky tests.",test_debt,flaky_test,"Fri, 16 Nov 2012 00:38:37 +0000","Tue, 26 Feb 2013 08:23:00 +0000","Wed, 28 Nov 2012 22:41:14 +0000",1116157,"Attaching v2 patches, which hopefully fixes the remaining flaky tests.",0.4,0.4,positive
hbase,7172,description,"fails when run individually (run just that test case from eclipse). I've also noticed that it is flaky on windows. The reason is a rare race condition, which somehow does not happen that much when the whole class is run. The sequence of events is smt like this: - we create 1 log file to split - we call in its own thread. - is waiting in since there are no splitlogworkers, it keep waiting. - we delete the task znode from zk - SplitLogManager receives the zk callback from which will call setDone() and mark the task as success. - However, meanwhile the loops sees that remainingInZK == 0, and calls return concurrently to the above. - on return from fails because the znode delete callback has not completed yet. This race only happens when the last task is deleted from zk, and normally only the SplitLogManager deletes the task znodes after processing it, so I don't think this is a production issue.",test_debt,flaky_test,"Fri, 16 Nov 2012 00:38:37 +0000","Tue, 26 Feb 2013 08:23:00 +0000","Wed, 28 Nov 2012 22:41:14 +0000",1116157,"TestSplitLogManager.testVanishingTaskZNode fails when run individually (run just that test case from eclipse). I've also noticed that it is flaky on windows. The reason is a rare race condition, which somehow does not happen that much when the whole class is run. The sequence of events is smt like this: we create 1 log file to split we call splitLogDistributed() in its own thread. splitLogDistributed() is waiting in waitForSplittingCompletion() since there are no splitlogworkers, it keep waiting. we delete the task znode from zk SplitLogManager receives the zk callback from GetDataAsyncCallback, which will call setDone() and mark the task as success. However, meanwhile the waitForSplittingCompletion() loops sees that remainingInZK == 0, and calls return concurrently to the above. on return from waitForSplittingCompletion(), splitLogDistributed() fails because the znode delete callback has not completed yet. This race only happens when the last task is deleted from zk, and normally only the SplitLogManager deletes the task znodes after processing it, so I don't think this is a production issue.",-0.03,0.01,negative
hbase,7172,summary,fails when run individually and is flaky,test_debt,flaky_test,"Fri, 16 Nov 2012 00:38:37 +0000","Tue, 26 Feb 2013 08:23:00 +0000","Wed, 28 Nov 2012 22:41:14 +0000",1116157,TestSplitLogManager.testVanishingTaskZNode() fails when run individually and is flaky,-0.4,-0.2,negative
hbase,7933,comment_10,"I've run the tests with this once again. The ACL tests seem to be flaky. Master does not wait for _acl_ table to be created before accepting other create table statements. Will open another issue. I want to get this resolved sooner rather than later, since it affects the tests and pre-commit tests.",test_debt,flaky_test,"Mon, 25 Feb 2013 21:51:35 +0000","Mon, 23 Sep 2013 18:31:40 +0000","Wed, 27 Feb 2013 00:40:54 +0000",96559,"I've run the tests with this once again. The ACL tests seem to be flaky. Master does not wait for acl table to be created before accepting other create table statements. Will open another issue. I want to get this resolved sooner rather than later, since it affects the tests and pre-commit tests.",0.09,0.09,negative
hbase,7933,comment_6,TestConstraint is a medium test. Meaning lock manager tests were not run.,test_debt,low_coverage,"Mon, 25 Feb 2013 21:51:35 +0000","Mon, 23 Sep 2013 18:31:40 +0000","Wed, 27 Feb 2013 00:40:54 +0000",96559,TestConstraint is a medium test. Meaning lock manager tests were not run.,0.3125,0.3125,neutral
hbase,8026,comment_3,Could you expand the help to include mention of how one uses RAW/VERSIONS? Also please add a test in should be straight forward.,test_debt,lack_of_tests,"Thu, 7 Mar 2013 17:36:28 +0000","Fri, 6 Apr 2018 17:55:55 +0000","Fri, 16 Jan 2015 17:47:09 +0000",58752641,Could you expand the help to include mention of how one uses RAW/VERSIONS? Also please add a test in hbase-shell/src/test/ruby/shell; should be straight forward.,0.25,0.25,neutral
hbase,8026,comment_4,"1) Usage : Its already present and can be seen once we do help scan on shell. Excerpt that include info on versions and raw : "" Also for experts, there is an advanced option -- RAW -- which instructs the scanner to return all cells (including delete markers and uncollected deleted cells). This option cannot be combined with requesting specific COLUMNS. Disabled by default. Example: hbase"" 2) Test - This jira is not about functionality of RAW and VERSIONS command. Its just the mention of these commands missing in the help. If you meant writing a new unit test for versions and raw command, then I will go through the shell tests to see if there is any corresponding test there and if not present, I can raise a new Jira and can work on it.",test_debt,lack_of_tests,"Thu, 7 Mar 2013 17:36:28 +0000","Fri, 6 Apr 2018 17:55:55 +0000","Fri, 16 Jan 2015 17:47:09 +0000",58752641,"1) Usage : Its already present and can be seen once we do help scan on shell. Excerpt that include info on versions and raw : "" Also for experts, there is an advanced option  RAW  which instructs the scanner to return all cells (including delete markers and uncollected deleted cells). This option cannot be combined with requesting specific COLUMNS. Disabled by default. Example: hbase> scan 't1', {RAW => true, VERSIONS => 10} "" 2) Test - This jira is not about functionality of RAW and VERSIONS command. Its just the mention of these commands missing in the help. If you meant writing a new unit test for versions and raw command, then I will go through the shell tests to see if there is any corresponding test there and if not present, I can raise a new Jira and can work on it.",0.09285714286,0.1747142857,neutral
hbase,8056,comment_0,A fairly simple patch with lots of comments. I am trying to see if there's a viable way to test the code in this area.,test_debt,low_coverage,"Sat, 9 Mar 2013 00:40:12 +0000","Thu, 16 Jun 2022 05:49:14 +0000","Wed, 20 Mar 2013 01:13:44 +0000",952412,A fairly simple patch with lots of comments. I am trying to see if there's a viable way to test the code in this area.,0.01575,0.01575,neutral
hbase,8256,comment_10,I think this is ok because we went the flakey test category to be an empty set ideally. So lets get the builds green by segregating them and then fix them before we have to worry about new versions of surefire. Easy. :-),test_debt,flaky_test,"Wed, 3 Apr 2013 18:07:36 +0000","Thu, 16 Jun 2022 05:57:12 +0000","Mon, 27 Jan 2014 18:03:21 +0000",25833345,I think this is ok because we went the flakey test category to be an empty set ideally. So lets get the builds green by segregating them and then fix them before we have to worry about new versions of surefire. Easy.,0.1635,0.08466666667,positive
hbase,8256,comment_15,"I would prefer to keep a real 'small tests' category. Sharing the JVM for small tests saves us from a lot of fork and makes tests faster. As we're supposed to have more and more tests, I would love to have more and more *small* tests. The setting to use with newer surefire version is 'reuseFork'. last time I checked, test methods and categories were buggy, but may be it has been fixed. Would it work to: - use an exclusion group fo 'Flaky' Category - add a profile to run only the 'Flaky' You said previously that you need as well to convert the old test cases extending TestCase. May be this could be done in a separate JIRA?",test_debt,flaky_test,"Wed, 3 Apr 2013 18:07:36 +0000","Thu, 16 Jun 2022 05:57:12 +0000","Mon, 27 Jan 2014 18:03:21 +0000",25833345,"I would prefer to keep a real 'small tests' category. Sharing the JVM for small tests saves us from a lot of fork and makes tests faster. As we're supposed to have more and more tests, I would love to have more and more small tests. The setting to use with newer surefire version is 'reuseFork'. last time I checked, test methods and categories were buggy, but may be it has been fixed. Would it work to: use an exclusion group fo 'Flaky' Category add a profile to run only the 'Flaky' You said previously that you need as well to convert the old test cases extending TestCase. May be this could be done in a separate JIRA?",0.101952381,0.101952381,neutral
hbase,8256,comment_16,", Yes, I added a profile to run only flaky tests. I used categories to exclude flaky category. It works actually so far. As to converting old test cases extending TestCase, we can do it in a separate Jira. As Andy said, we can have this patch in, then you fix those surefire setting issue separately. It is also fine with me that you take this over and fix as you like before let the patch in. Yes, the patch is not as small as I expected. Sub-modules can inherit from parent pom. As to the pom related changes, I can defer to  for further fixes/improvements.",test_debt,flaky_test,"Wed, 3 Apr 2013 18:07:36 +0000","Thu, 16 Jun 2022 05:57:12 +0000","Mon, 27 Jan 2014 18:03:21 +0000",25833345,"nkeywal, Yes, I added a profile to run only flaky tests. I used categories to exclude flaky category. It works actually so far. As to converting old test cases extending TestCase, we can do it in a separate Jira. As Andy said, we can have this patch in, then you fix those surefire setting issue separately. It is also fine with me that you take this over and fix as you like before let the patch in. saint.ack@gmail.com, Yes, the patch is not as small as I expected. Sub-modules can inherit from parent pom. As to the pom related changes, I can defer to nkeywal for further fixes/improvements.",0.2263888889,0.2213541667,neutral
hbase,8256,comment_1,"I tried to use excludedGroups to exclude the Flaky methods. However, it excludes the whole test class, not just those flaky methods. Per we need to use test suite?",test_debt,flaky_test,"Wed, 3 Apr 2013 18:07:36 +0000","Thu, 16 Jun 2022 05:57:12 +0000","Mon, 27 Jan 2014 18:03:21 +0000",25833345,"I tried to use excludedGroups to exclude the Flaky methods. However, it excludes the whole test class, not just those flaky methods. Per https://github.com/junit-team/junit/wiki/Categories, we need to use test suite?",-0.06666666667,-0.06666666667,negative
hbase,8256,comment_21,"Posted a new patch which is not as radical. The issue with this patch is that if any method is marked as flaky, the whole test class is ignored in runAllTests, although runFlakyTests works fine. This could be related to SUREFIRE-862. Does our surefire have this fix?",test_debt,flaky_test,"Wed, 3 Apr 2013 18:07:36 +0000","Thu, 16 Jun 2022 05:57:12 +0000","Mon, 27 Jan 2014 18:03:21 +0000",25833345,"Posted a new patch which is not as radical. The issue with this patch is that if any method is marked as flaky, the whole test class is ignored in runAllTests, although runFlakyTests works fine. This could be related to SUREFIRE-862. Does our surefire have this fix?",0.05,0.05,negative
hbase,8256,description,"To make the Jenkin build more useful, it is good to keep it blue/green. We can mark those flaky tests flaky, and don't run them by default. However, people can still run them. We can also set up a Jekin build just for those flaky tests.",test_debt,flaky_test,"Wed, 3 Apr 2013 18:07:36 +0000","Thu, 16 Jun 2022 05:57:12 +0000","Mon, 27 Jan 2014 18:03:21 +0000",25833345,"To make the Jenkin build more useful, it is good to keep it blue/green. We can mark those flaky tests flaky, and don't run them by default. However, people can still run them. We can also set up a Jekin build just for those flaky tests.",0.35475,0.35475,neutral
hbase,8256,summary,Add category Flaky for tests which are flaky,test_debt,flaky_test,"Wed, 3 Apr 2013 18:07:36 +0000","Thu, 16 Jun 2022 05:57:12 +0000","Mon, 27 Jan 2014 18:03:21 +0000",25833345,Add category Flaky for tests which are flaky,0,0,negative
hbase,8324,comment_10,"Fair enough. However, that means we are leaving HBase's interaction with this feature untested. Does that mean we will advise users against using it with HBase? If we're not going to test it, that means we're not taking responsibility for it; thus I conclude that we must ""officially drop support"" for the feature, however that's done.",test_debt,lack_of_tests,"Thu, 11 Apr 2013 05:16:56 +0000","Mon, 23 Sep 2013 19:08:23 +0000","Fri, 12 Apr 2013 14:59:26 +0000",121350,"Fair enough. However, that means we are leaving HBase's interaction with this feature untested. Does that mean we will advise users against using it with HBase? If we're not going to test it, that means we're not taking responsibility for it; thus I conclude that we must ""officially drop support"" for the feature, however that's done.",0.0875,0.0875,negative
hbase,847,comment_2,"Patch does not apply. Patches must be in svn diff format to be accepted. Please add a test case to demonstrate that getting multiple versions works (should also include multiple versions with timestamp specified) Please do not include a patch for HBASE-52 and HBASE-33 in this patch. Even though they are similar, changes to scanners are more difficult. We try to limit the scope of a single patch in general. Insure that the sub issues of this Jira, HBASE-857, HBASE-31 and HBASE-44 are addressed. Thanks.",test_debt,lack_of_tests,"Wed, 27 Aug 2008 20:36:12 +0000","Sun, 13 Sep 2009 22:26:26 +0000","Wed, 3 Dec 2008 01:06:24 +0000",8397012,"Patch does not apply. Patches must be in svn diff format to be accepted. Please add a test case to demonstrate that getting multiple versions works (should also include multiple versions with timestamp specified) Please do not include a patch for HBASE-52 and HBASE-33 in this patch. Even though they are similar, changes to scanners are more difficult. We try to limit the scope of a single patch in general. Insure that the sub issues of this Jira, HBASE-857, HBASE-31 and HBASE-44 are addressed. Thanks.",0.1976190476,0.1976190476,neutral
hbase,9303,comment_6,"Ok, so we have a split parent with the SPLIT marker in the hri in meta. Is this the hri saved off as part of the snapshot manifest, or is it only in the meta hri? Is the problem only that the meta entry has SPLIT as an attribute? Could we have a unit test where we force meta to have the split marker and the restore?",test_debt,lack_of_tests,"Thu, 22 Aug 2013 19:46:12 +0000","Tue, 24 Sep 2013 20:32:53 +0000","Fri, 23 Aug 2013 20:44:53 +0000",89921,"Ok, so we have a split parent with the SPLIT marker in the hri in meta. Is this the hri saved off as part of the snapshot manifest, or is it only in the meta hri? Is the problem only that the meta entry has SPLIT as an attribute? Could we have a unit test where we force meta to have the split marker and the restore?",0.2375,0.2375,neutral
hbase,9689,comment_6,"+1 patch looks ok, would be good to have coverage in TestShell for this",test_debt,low_coverage,"Tue, 1 Oct 2013 09:52:39 +0000","Fri, 20 Nov 2015 11:53:36 +0000","Thu, 17 Oct 2013 17:36:36 +0000",1410237,"+1 patch looks ok, would be good to have coverage in TestShell for this",0.763,0.763,positive
hbase,9893,comment_5,"Excellent catch. Attached is a patch that also includes updated test cases. Please let me know if you have other value permutations you'd like to see tested. It'd be nice to have a more thorough test suite around this code, a la the suite Orderly has. From the commit message",test_debt,lack_of_tests,"Tue, 5 Nov 2013 08:28:54 +0000","Mon, 16 Dec 2013 18:46:46 +0000","Wed, 20 Nov 2013 02:55:44 +0000",1276010,"Excellent catch. Attached is a patch that also includes updated test cases. Please let me know if you have other value permutations you'd like to see tested. It'd be nice to have a more thorough test suite around this code, a la the suite Orderly has. From the commit message Correct an invalid assumption in remaining assertion code around OrderedBytes#decodeVarBlob. When an encoded value contains a 1-bit in its LSB position and the length of the encoded byte array is divisible by 7, the value remaining in variable t will be 0x80, resulting in the failed assertion coming out of the decoding loop. This patch preserves the assertion for the general case by resetting 't' at the conclusion of the 7-byte cycle.",0.4051,0.2804285714,positive
impala,1120,description,"We should update our code that fetches and updates column stats to use the new bulk APIs introduced in Hive .13. Instead of fetching a single column at a time, we can now fetch stats for a list of column names.",architecture_debt,using_obsolete_technology,"Tue, 29 Jul 2014 00:38:53 +0000","Tue, 13 Jan 2015 18:17:33 +0000","Tue, 13 Jan 2015 18:17:33 +0000",14578720,"We should update our code that fetches and updates column stats to use the new bulk APIs introduced in Hive .13. Instead of fetching a single column at a time, we can now fetch stats for a list of column names.",0.08333333333,0.08333333333,neutral
impala,1577,comment_1,Thanks for the feedback. I just looked at the code a little and didn't see any easy improvements. The main perf problem is most likely due to the function not being codegen'd. There's a note in the code about some LLVM limitations. Our LLVM version is about 2 years old so an upgrade might help. An upgrade is on our todo list but I couldn't say when that would be done. Also the function heavily relies on boost so that could be an issue.,architecture_debt,using_obsolete_technology,"Fri, 5 Dec 2014 01:16:08 +0000","Mon, 9 Jul 2018 16:45:30 +0000","Mon, 9 Jul 2018 16:45:30 +0000",113412562,Thanks for the feedback. I just looked at the code a little and didn't see any easy improvements. The main perf problem is most likely due to the function not being codegen'd. There's a note in the code about some LLVM limitations. Our LLVM version is about 2 years old so an upgrade might help. An upgrade is on our todo list but I couldn't say when that would be done. Also the function heavily relies on boost so that could be an issue.,0.1279285714,0.1279285714,negative
impala,4210,summary,Impala uses an older version of httpcore which breaks KMS integration,architecture_debt,using_obsolete_technology,"Wed, 28 Sep 2016 15:22:01 +0000","Wed, 28 Sep 2016 19:34:30 +0000","Wed, 28 Sep 2016 15:22:13 +0000",12,Impala uses an older version of httpcore which breaks KMS integration,0,0,negative
impala,4652,description,Kudu's utility library depends on We need to add the most recent version to the toolchain.,architecture_debt,using_obsolete_technology,"Mon, 12 Dec 2016 18:43:32 +0000","Thu, 12 Jan 2017 23:51:13 +0000","Thu, 12 Jan 2017 23:51:13 +0000",2696861,Kudu's utility library depends on crcutil. We need to add the most recent version to the toolchain.,0,0,neutral
impala,6080,description,One of the parts of this patch was cleanup of how descriptor tables were managed. As described in the commit message: That cleanup should be independent of the larger patch so we could get it in separately.,architecture_debt,violation_of_modularity,"Wed, 18 Oct 2017 23:53:05 +0000","Thu, 16 Nov 2017 22:12:16 +0000","Thu, 16 Nov 2017 22:12:16 +0000",2499551,One of the parts of this patch https://gerrit.cloudera.org/#/c/7065/ was cleanup of how descriptor tables were managed. As described in the commit message: That cleanup should be independent of the larger patch so we could get it in separately.,0.1,0.1,neutral
impala,7400,summary,"""SQL Statements to Remove or Adapt"" is out of date",architecture_debt,using_obsolete_technology,"Mon, 6 Aug 2018 16:34:24 +0000","Wed, 8 Aug 2018 21:07:52 +0000","Wed, 8 Aug 2018 20:43:20 +0000",187736,"""SQL Statements to Remove or Adapt"" is out of date",0,0,negative
impala,7869,summary,Split up for readability and compile time,architecture_debt,violation_of_modularity,"Mon, 19 Nov 2018 16:57:08 +0000","Tue, 27 Nov 2018 02:01:56 +0000","Tue, 27 Nov 2018 02:01:56 +0000",637488,Split up parquet-column-readers.cc for readability and compile time,0,0,neutral
impala,3252,description,There seems to be a configuration file checked in in git that changes every time we do a build. The file is This file should be removed from git.,build_debt,build_others,"Mon, 28 Mar 2016 17:55:33 +0000","Mon, 11 Jun 2018 20:57:57 +0000","Mon, 11 Jun 2018 20:42:56 +0000",69562043,There seems to be a configuration file checked in in git that changes every time we do a build. The file is thirdparty/hadoop-2.6.0-cdh5.8.0-SNAPSHOT/share/hadoop/kms/tomcat/conf/server.xml. This file should be removed from git.,0,0.025,negative
impala,3338,description,"The Impala-lzo build depends on the Impala build scripts and headers, but we don't need to do an entire Impala build to produce the Impala-lzo .so. Factor out the build logic from buildall.sh so it can be built independently if needed.",build_debt,build_others,"Wed, 13 Apr 2016 00:50:25 +0000","Tue, 7 Jun 2016 16:19:39 +0000","Tue, 7 Jun 2016 16:19:39 +0000",4807754,"The Impala-lzo build depends on the Impala build scripts and headers, but we don't need to do an entire Impala build to produce the Impala-lzo .so. Factor out the build logic from buildall.sh so it can be built independently if needed.",0.125,0.125,neutral
impala,4267,comment_2,", I think some scripts are in the ASF repo. If they are not, some should be - it will ease new contributor onboarding to have well-maintained Docker images available.",build_debt,build_others,"Mon, 10 Oct 2016 18:16:29 +0000","Thu, 20 Oct 2016 18:14:30 +0000","Thu, 20 Oct 2016 18:14:30 +0000",863881,"henryr, I think some scripts are in the ASF repo. If they are not, some should be - it will ease new contributor onboarding to have well-maintained Docker images available.",0.2406666667,0.2406666667,neutral
impala,8862,description,These get bundled into the containers but are not be invoked at runtime. We should be able to avoid including them as dependencies entirely.,build_debt,over-declared_dependencies,"Wed, 14 Aug 2019 18:39:35 +0000","Fri, 16 Aug 2019 17:47:59 +0000","Fri, 16 Aug 2019 17:47:59 +0000",169704,These get bundled into the containers but are not be invoked at runtime. We should be able to avoid including them as dependencies entirely.,0.122,0.122,neutral
impala,1013,description,This function is very simple and goes through the HS2 interface to retrieve a few functions. I believe this used to be instantaneously and now takes hundreds of seconds making this likely unusable.,code_debt,slow_algorithm,"Wed, 21 May 2014 21:54:09 +0000","Wed, 11 Jun 2014 19:22:37 +0000","Wed, 11 Jun 2014 19:22:37 +0000",1805308,This function is very simple and goes through the HS2 interface to retrieve a few functions. I believe this used to be instantaneously and now takes hundreds of seconds making this likely unusable.,-0.2,-0.2,negative
impala,1013,summary,is unreasonably slow,code_debt,slow_algorithm,"Wed, 21 May 2014 21:54:09 +0000","Wed, 11 Jun 2014 19:22:37 +0000","Wed, 11 Jun 2014 19:22:37 +0000",1805308,FrontTest.TestGetFunctions is unreasonably slow,0,0,negative
impala,1082,comment_2,We are running CDH 4.4 and would need to upgrade to a later build. This appears to be occurring on only 1 or 2 of our nodes on the cluster and thus I believe is data centric. Are they any workarounds or ways to identify/cleanup the data?,code_debt,low_quality_code,"Fri, 11 Jul 2014 15:14:05 +0000","Mon, 14 Jul 2014 17:57:10 +0000","Fri, 11 Jul 2014 15:26:15 +0000",730,We are running CDH 4.4 and would need to upgrade to a later build. This appears to be occurring on only 1 or 2 of our nodes on the cluster and thus I believe is data centric. Are they any workarounds or ways to identify/cleanup the data?,0.175,0.175,neutral
impala,1414,description,"The following query on a relatively small input takes ~70s with codegen and ~35 minutes without codegen, which seems unreasonably long. We should make sure we're not doing anything crazy in the non-codegen eval path. Executing with codegen: The query profile when running without codegen is attached. I also used perf (linux profiling tool) to collect a profile (sampled stacks) and printed the aggregated callstacks to the attached file The summary without codegen. Most of the time (28mins) is spent in the hash joins:",code_debt,slow_algorithm,"Thu, 23 Oct 2014 18:22:13 +0000","Thu, 29 Sep 2016 20:47:33 +0000","Thu, 29 Sep 2016 20:36:09 +0000",61092836,"The following query on a relatively small input takes ~70s with codegen and ~35 minutes without codegen, which seems unreasonably long. We should make sure we're not doing anything crazy in the non-codegen eval path. Executing with codegen: The query profile when running without codegen is attached. I also used perf (linux profiling tool) to collect a profile (sampled stacks) and printed the aggregated callstacks to the attached file perf-histograms.out. The summary without codegen. Most of the time (28mins) is spent in the hash joins:",0.025,0.01785714286,neutral
impala,1414,summary,Slow query without codegen,code_debt,slow_algorithm,"Thu, 23 Oct 2014 18:22:13 +0000","Thu, 29 Sep 2016 20:47:33 +0000","Thu, 29 Sep 2016 20:36:09 +0000",61092836,Slow query without codegen,0,0,negative
impala,1466,description,"The catalog cache is cold. Loading the table metadata when I've a single request is fast (within 2sec). However, when I've 100+ concurrent queries all requesting the same table metadata, the loading time extremely long (in 10s of seconds+). This shouldn't be the case. The first request should load the metadata and all subsequent request (even though they're concurrent) should be a no-op.",code_debt,slow_algorithm,"Tue, 11 Nov 2014 19:24:05 +0000","Thu, 26 Jan 2017 01:03:23 +0000","Thu, 26 Jan 2017 01:03:23 +0000",69658758,"The catalog cache is cold. Loading the table metadata when I've a single request is fast (within 2sec). However, when I've 100+ concurrent queries all requesting the same table metadata, the loading time extremely long (in 10s of seconds+). This shouldn't be the case. The first request should load the metadata and all subsequent request (even though they're concurrent) should be a no-op.",0,0,negative
impala,1466,summary,Excessively long table metadata loading time when processing concurrent request of the same table,code_debt,slow_algorithm,"Tue, 11 Nov 2014 19:24:05 +0000","Thu, 26 Jan 2017 01:03:23 +0000","Thu, 26 Jan 2017 01:03:23 +0000",69658758,Excessively long table metadata loading time when processing concurrent request of the same table,0,0,negative
impala,1552,description,"When running out of memory in PAGG the error message does not make it clear why we ran out of memory. In the example below we see that the limit is 100MB and the consumption is only 8MB, but we ran out of memory. In this particular case, the reason is that PAGG tries to allocate the initial min mem for each partition and fails (each partition needs ~8MB and we have 32 of them). It would be good to improve the error message with some more useful info, e.g. how much more memory was needed. Backend 0:Memory Limit Exceeded Limit: Limit=100.00 MB Consumption=8.03 MB Fragment Consumption=8.00 KB EXCHANGE_NODE (id=5): Consumption=0 DataStreamRecvr: Consumption=0 Block Manager: Consumption=0 Fragment Consumption=4.01 MB SORT_NODE (id=2): Consumption=0 AGGREGATION_NODE (id=4): Consumption=4.00 MB EXCHANGE_NODE (id=3): Consumption=0 DataStreamRecvr: Consumption=0 DataStreamSender: Consumption=4.00 KB Fragment Consumption=4.01 MB AGGREGATION_NODE (id=1): Consumption=4.00 MB HDFS_SCAN_NODE (id=0): Consumption=0 DataStreamSender: Consumption=4.00 KB",code_debt,low_quality_code,"Wed, 26 Nov 2014 02:12:58 +0000","Wed, 4 Jan 2017 23:58:13 +0000","Tue, 8 Sep 2015 16:37:21 +0000",24762263,"When running out of memory in PAGG the error message does not make it clear why we ran out of memory. In the example below we see that the limit is 100MB and the consumption is only 8MB, but we ran out of memory. In this particular case, the reason is that PAGG tries to allocate the initial min mem for each partition and fails (each partition needs ~8MB and we have 32 of them). It would be good to improve the error message with some more useful info, e.g. how much more memory was needed. Backend 0:Memory Limit Exceeded Query(3b428a0f1e22f611:722a79f624fcb0b5) Limit: Limit=100.00 MB Consumption=8.03 MB Fragment 3b428a0f1e22f611:722a79f624fcb0b6: Consumption=8.00 KB EXCHANGE_NODE (id=5): Consumption=0 DataStreamRecvr: Consumption=0 Block Manager: Consumption=0 Fragment 3b428a0f1e22f611:722a79f624fcb0b7: Consumption=4.01 MB SORT_NODE (id=2): Consumption=0 AGGREGATION_NODE (id=4): Consumption=4.00 MB EXCHANGE_NODE (id=3): Consumption=0 DataStreamRecvr: Consumption=0 DataStreamSender: Consumption=4.00 KB Fragment 3b428a0f1e22f611:722a79f624fcb0b8: Consumption=4.01 MB AGGREGATION_NODE (id=1): Consumption=4.00 MB HDFS_SCAN_NODE (id=0): Consumption=0 DataStreamSender: Consumption=4.00 KB",-0.1704857143,-0.1704857143,negative
impala,1552,summary,Improve info shown in the OOM message,code_debt,low_quality_code,"Wed, 26 Nov 2014 02:12:58 +0000","Wed, 4 Jan 2017 23:58:13 +0000","Tue, 8 Sep 2015 16:37:21 +0000",24762263,Improve info shown in the OOM message,0.4,0.4,positive
impala,1577,comment_0,"I've just signed on to the Impala issue tracker and was about to report the exact same problem as described above. We had to add timezone conversions to our queries lately and the query performance decreased massively when we started using the from_utc_timestamp function. Like you said it's not only slow, but the performance is inconsistent, it seems to get worse the more rows are processed. Unfortunately (at least for us ;-)) this issue doesn't seem to get much attention. It would be great if you could let me know whether this will be addressed sometime soon or if you have any other news on this issue. Thanks in advance! P.S. We're using Impala 2.2 with CDH5.4.2.",code_debt,slow_algorithm,"Fri, 5 Dec 2014 01:16:08 +0000","Mon, 9 Jul 2018 16:45:30 +0000","Mon, 9 Jul 2018 16:45:30 +0000",113412562,"I've just signed on to the Impala issue tracker and was about to report the exact same problem as described above. We had to add timezone conversions to our queries lately and the query performance decreased massively when we started using the from_utc_timestamp function. Like you said it's not only slow, but the performance is inconsistent, it seems to get worse the more rows are processed. Unfortunately (at least for us ) this issue doesn't seem to get much attention. It would be great if you could let me know whether this will be addressed sometime soon or if you have any other news on this issue. Thanks in advance! P.S. We're using Impala 2.2 with CDH5.4.2.",-0.02185,-0.02185,negative
impala,1577,comment_2,"IMPALA-3307 replaced the timezone implementation, which became generally faster and should be more consistent. The old implementation looked up timezone aliases much slower than canonical timezone names, while there shouldn't be any difference in the new one (many aliases were removed, while some were added to a map for fast lookups).",code_debt,slow_algorithm,"Fri, 5 Dec 2014 01:16:08 +0000","Mon, 9 Jul 2018 16:45:30 +0000","Mon, 9 Jul 2018 16:45:30 +0000",113412562,"IMPALA-3307 replaced the timezone implementation, which became generally faster and should be more consistent. The old implementation looked up timezone aliases much slower than canonical timezone names, while there shouldn't be any difference in the new one (many aliases were removed, while some were added to a map for fast lookups).",-0.25,-0.25,neutral
impala,1577,description,"The performance of from_utc_timestamp, besides being extremely poor compared to that of other builtin functions such as from_unixtime, depends greatly on the input timezone. Given that evaluating this function can dominate the total query time, it would be good to at least make its performance more consistent, and even better to improve its performance overall.",code_debt,slow_algorithm,"Fri, 5 Dec 2014 01:16:08 +0000","Mon, 9 Jul 2018 16:45:30 +0000","Mon, 9 Jul 2018 16:45:30 +0000",113412562,"The performance of from_utc_timestamp, besides being extremely poor compared to that of other builtin functions such as from_unixtime, depends greatly on the input timezone. Given that evaluating this function can dominate the total query time, it would be good to at least make its performance more consistent, and even better to improve its performance overall.",0.07933333333,0.07933333333,negative
impala,1577,summary,Improve from_utc_timestamp perf which is unpredictable and slow,code_debt,slow_algorithm,"Fri, 5 Dec 2014 01:16:08 +0000","Mon, 9 Jul 2018 16:45:30 +0000","Mon, 9 Jul 2018 16:45:30 +0000",113412562,Improve from_utc_timestamp perf which is unpredictable and slow,0.4,0.4,negative
impala,1584,comment_0,"You don't have to do it this time, but next time (and in general) I think we should add the source code snippet of the failed DCHECK along with the relevant backtrace to bugs. That will make it easier to both review the fix (since it'll be easier to understand what went wrong), and also easier to identify duplicates (for all of dev, cce, and support).",code_debt,low_quality_code,"Mon, 8 Dec 2014 20:45:26 +0000","Sat, 21 Mar 2015 22:51:10 +0000","Wed, 24 Dec 2014 23:59:12 +0000",1394026,"You don't have to do it this time, but next time (and in general) I think we should add the source code snippet of the failed DCHECK along with the relevant backtrace to bugs. That will make it easier to both review the fix (since it'll be easier to understand what went wrong), and also easier to identify duplicates (for all of dev, cce, and support).",-0.1625,-0.1625,neutral
impala,1596,description,Some the builtin decimal functions assume they can always use the val16 union field of a DecimalType. This isn't actually a valid assumption though. We should only initialize and use the smaller fields (val4/val8) for performance if possible.,code_debt,low_quality_code,"Wed, 10 Dec 2014 21:10:54 +0000","Tue, 5 May 2015 22:31:37 +0000","Tue, 5 May 2015 22:31:37 +0000",12619243,Some the builtin decimal functions assume they can always use the val16 union field of a DecimalType. This isn't actually a valid assumption though. We should only initialize and use the smaller fields (val4/val8) for performance if possible.,0,0,negative
impala,1618,comment_0,"Probably the best fix here would be to add additional buffering in PlanRootSink so that the hand-off between the fetching thread and the producing thread was not synchronous. We would need to think carefully about the design - e.g. how much buffering, how should the buffered rows be stored, etc.",code_debt,multi-thread_correctness,"Wed, 17 Dec 2014 21:44:47 +0000","Thu, 14 May 2020 17:44:56 +0000","Thu, 29 Aug 2019 20:19:54 +0000",148257307,"Probably the best fix here would be to add additional buffering in PlanRootSink so that the hand-off between the fetching thread and the producing thread was not synchronous. We would need to think carefully about the design - e.g. how much buffering, how should the buffered rows be stored, etc.",0.66875,0.66875,neutral
impala,1907,comment_0,"The receiver case just needs a simple fix -- cancel the receiver before closing it. The sender case is caused by accessing invalid memory. The consumption value is a counter from the runtime profile. The profile has already been destroyed by the time ~MemTracker is called. Since nothing else in ~MemTracker accesses the runtime profile, for 2.2 the DCHECK should just be removed. Here is the log with additional info added",code_debt,low_quality_code,"Sun, 22 Mar 2015 17:29:08 +0000","Sun, 20 Dec 2015 00:05:28 +0000","Wed, 19 Aug 2015 00:53:28 +0000",12900260,"The receiver case just needs a simple fix  cancel the receiver before closing it. The sender case is caused by accessing invalid memory. The consumption value is a counter from the runtime profile. The profile has already been destroyed by the time ~MemTracker is called. Since nothing else in ~MemTracker accesses the runtime profile, for 2.2 the DCHECK should just be removed. Here is the log with additional info added",-0.1333333333,-0.1333333333,negative
impala,1929,description,"While running the mem leak test with 4 concurrent clients using the release bits an impalad crashed at with the following stack trace. This can happen in case of RIGHT_OUTER, RIGHT_ANTI and FULL_OUTER joins when the hash table of the partition is NULL. The code has a dcheck that the hash_tbl is not null, which we didn't hit because we were using the release bits.",code_debt,low_quality_code,"Tue, 31 Mar 2015 23:44:24 +0000","Sun, 20 Dec 2015 00:05:28 +0000","Sun, 19 Jul 2015 02:18:49 +0000",9426865,"While running the mem leak test with 4 concurrent clients using the release bits an impalad crashed at PHJ::NextSpilledProbeRowBatch() with the following stack trace. This can happen in case of RIGHT_OUTER, RIGHT_ANTI and FULL_OUTER joins when the hash table of the partition is NULL. The code has a dcheck that the hash_tbl is not null, which we didn't hit because we were using the release bits.",0.02822222222,0.02822222222,neutral
impala,1963,description,"We have several data sources that return dates in ISO-8601 format. Following IMPALA-648, we are closer to having these convertable to timestamp without effort, but time zones are still not supported: Neither format with a time zone specifier will parse even though they are valid based on my understanding of ISO-8601 and For now we have worked around this by converting with substring to cut off the time zone identifier, but I'm not sure which time zone the dates will be interpreted in. If I cut off the Z, will my dates be parsed in server local time? Ideally, we should be able to cast dates with time zone specifiers to timestamps with cast easily.",code_debt,low_quality_code,"Wed, 22 Apr 2015 15:03:34 +0000","Fri, 5 Jun 2015 04:47:07 +0000","Fri, 5 Jun 2015 04:47:07 +0000",3764613,"We have several data sources that return dates in ISO-8601 format. Following IMPALA-648, we are closer to having these convertable to timestamp without effort, but time zones are still not supported: Neither format with a time zone specifier will parse even though they are valid based on my understanding of ISO-8601 and http://en.wikipedia.org/wiki/ISO_8601 For now we have worked around this by converting with substring to cut off the time zone identifier, but I'm not sure which time zone the dates will be interpreted in. If I cut off the Z, will my dates be parsed in server local time? Ideally, we should be able to cast dates with time zone specifiers to timestamps with cast easily.",0.09541666667,0.09541666667,neutral
impala,1984,description,"CDH 5.4 Impala 2.2.0 create TABLE EmptySrc ( F1 INT, F2 STRING ) ; create TABLE EmptyTgt ( F1 INT, F2 STRING ) ; insert OVERWRITE EmptyTgt SELECT * FROM EmptySrc ; Query: insert OVERWRITE EmptyTgt SELECT * FROM EmptySrc WARNINGS: Could not list directory: This warning is unnecessary when there are empty tables involved and should be removed.",code_debt,low_quality_code,"Thu, 7 May 2015 20:44:13 +0000","Fri, 15 May 2015 23:21:35 +0000","Fri, 15 May 2015 23:21:35 +0000",700642,"CDH 5.4 Impala 2.2.0 create TABLE EmptySrc ( F1 INT, F2 STRING ) ; create TABLE EmptyTgt ( F1 INT, F2 STRING ) ; insert OVERWRITE EmptyTgt SELECT * FROM EmptySrc ; [cdh54-2.ent.xxx.com:21000] > insert OVERWRITE EmptyTgt SELECT * FROM EmptySrc ; Query: insert OVERWRITE EmptyTgt SELECT * FROM EmptySrc WARNINGS: Could not list directory: hdfs://cdh54-1.ent.xxx.com:8020/user/hive/warehouse/emptytgt This warning is unnecessary when there are empty tables involved and should be removed.",0.3152,0.2508,neutral
impala,2099,comment_0,"You may not want to spend too much time debugging this at the moment as this class basically needs a full rewrite anyway. Also, maybe a dup of IMPALA-2072? I don't know what the stack trace in that case looked like though.",code_debt,low_quality_code,"Wed, 24 Jun 2015 23:07:23 +0000","Sun, 20 Dec 2015 00:05:31 +0000","Thu, 27 Aug 2015 00:24:46 +0000",5447843,"You may not want to spend too much time debugging this at the moment as this class basically needs a full rewrite anyway. Also, maybe a dup of IMPALA-2072? I don't know what the stack trace in that case looked like though.",-0.05,-0.05,negative
impala,2244,description,"This function now loops through each scan range and when there are many, is noticeably slow. For example, an EXPLAIN on a query against 1M blocks and 1000K partitions went from around 450 ms to 700 ms with this change.",code_debt,slow_algorithm,"Mon, 24 Aug 2015 23:08:00 +0000","Thu, 17 Sep 2015 06:17:46 +0000","Thu, 17 Sep 2015 06:17:46 +0000",2012986,"This function now loops through each scan range and when there are many, is noticeably slow. For example, an EXPLAIN on a query against 1M blocks and 1000K partitions went from around 450 ms to 700 ms with this change.",-0.1,-0.1,negative
impala,2244,summary,HdfsScanNode.java computeNumNodes() can be slow,code_debt,slow_algorithm,"Mon, 24 Aug 2015 23:08:00 +0000","Thu, 17 Sep 2015 06:17:46 +0000","Thu, 17 Sep 2015 06:17:46 +0000",2012986,HdfsScanNode.java computeNumNodes() can be slow,0,0,negative
impala,231,comment_0,"We have to mark local var using ""DeleteLocalRef"" when the local var is done. This will help reduce mem pressure.",code_debt,low_quality_code,"Thu, 11 Apr 2013 20:43:39 +0000","Wed, 17 Apr 2013 20:28:22 +0000","Wed, 17 Apr 2013 20:28:12 +0000",517473,"We have to mark local var using ""DeleteLocalRef"" when the local var is done. This will help reduce mem pressure.",0.05,0.05,neutral
impala,231,description,"Impala's HBase scan is very slow. For scanning 40000 rows from an colocated HBase table, it took ~5.7sec. The query is: select count(*) from (select * from hbasetbl limit 40000); Majority of the time is spent inside I've done some experiment to figure out where we spent the time by adding a few more timers. Here's the runtime profile: HBASE_SCAN_NODE (id=0):(5s714ms 99.98%) - BytesRead: 4.26 MB - 483.594us - 5s710ms - MemoryUsed: 0.00 - MyOwnTimer1: 1s387ms <-- We should trim this time. - MyOwnTimer2: 2s798ms <-- - MyOwnTimer3: 688.179ms - MyOwnTimer4: 0ns - MyOwnTimer5: 0ns - NumDisksAccessed: 0 - 5.14 MB/sec - RowsReturned: 40.00K (40000) - RowsReturnedRate: 7.00 K/sec - ScanRangesComplete: 0 - 0 - 0ns - 0ns - 0ns - 0ns - 0 - 827.604ms <-- we spent only 800ms on fetching from HBase - 775.05 KB/sec I've attached the code with more timers in the attached file When I increase the limit to 80000, the perf is much worse. Here's the profile: HBASE_SCAN_NODE (id=0):(23s027ms 99.99%) - BytesRead: 8.51 MB - 249.40us - 23s018ms - MemoryUsed: 0.00 - MyOwnTimer1: 5s680ms - MyOwnTimer2: 11s401ms - MyOwnTimer3: 2s829ms - MyOwnTimer4: 0ns - MyOwnTimer5: 0ns - NumDisksAccessed: 0 - 2.76 MB/sec - RowsReturned: 80.00K (80000) - RowsReturnedRate: 3.47 K/sec - ScanRangesComplete: 0 - 0 - 0ns - 0ns - 0ns - 0ns - 0 - 3s085ms - 376.52 KB/sec I didn't see any disk activity. So, I dont' think HBase is going to disk. Also, the read time is only 3sec. The big chuck of time is spent within the block of MyOwnTimer1 and MyOwnTimer2. We spent 5x more time there even though we only scan 2x more rows.",code_debt,slow_algorithm,"Thu, 11 Apr 2013 20:43:39 +0000","Wed, 17 Apr 2013 20:28:22 +0000","Wed, 17 Apr 2013 20:28:12 +0000",517473,"Impala's HBase scan is very slow. For scanning 40000 rows from an colocated HBase table, it took ~5.7sec. The query is: select count from (select * from hbasetbl limit 40000); Majority of the time is spent inside HBaseTableScanner::Next. I've done some experiment to figure out where we spent the time by adding a few more timers. Here's the runtime profile: HBASE_SCAN_NODE (id=0):(5s714ms 99.98%) BytesRead: 4.26 MB HBaseTableScanner.ScanSetup: 483.594us HBaseTableScanner::ResultScanner_next: 5s710ms MemoryUsed: 0.00 MyOwnTimer1: 1s387ms <-- We should trim this time. MyOwnTimer2: 2s798ms <-- MyOwnTimer3: 688.179ms MyOwnTimer4: 0ns MyOwnTimer5: 0ns NumDisksAccessed: 0 PerReadThreadRawHdfsThroughput: 5.14 MB/sec RowsReturned: 40.00K (40000) RowsReturnedRate: 7.00 K/sec ScanRangesComplete: 0 ScannerThreadsInvoluntaryContextSwitches: 0 ScannerThreadsTotalWallClockTime: 0ns MaterializeTupleTime: 0ns ScannerThreadsSysTime: 0ns ScannerThreadsUserTime: 0ns ScannerThreadsVoluntaryContextSwitches: 0 TotalRawHdfsReadTime: 827.604ms <-- we spent only 800ms on fetching from HBase TotalReadThroughput: 775.05 KB/sec I've attached the code with more timers in the attached file HBaseTAbleScanner.Next When I increase the limit to 80000, the perf is much worse. Here's the profile: HBASE_SCAN_NODE (id=0):(23s027ms 99.99%) BytesRead: 8.51 MB HBaseTableScanner.ScanSetup: 249.40us HBaseTableScanner::ResultScanner_next: 23s018ms MemoryUsed: 0.00 MyOwnTimer1: 5s680ms MyOwnTimer2: 11s401ms MyOwnTimer3: 2s829ms MyOwnTimer4: 0ns MyOwnTimer5: 0ns NumDisksAccessed: 0 PerReadThreadRawHdfsThroughput: 2.76 MB/sec RowsReturned: 80.00K (80000) RowsReturnedRate: 3.47 K/sec ScanRangesComplete: 0 ScannerThreadsInvoluntaryContextSwitches: 0 ScannerThreadsTotalWallClockTime: 0ns MaterializeTupleTime: 0ns ScannerThreadsSysTime: 0ns ScannerThreadsUserTime: 0ns ScannerThreadsVoluntaryContextSwitches: 0 TotalRawHdfsReadTime: 3s085ms TotalReadThroughput: 376.52 KB/sec I didn't see any disk activity. So, I dont' think HBase is going to disk. Also, the read time is only 3sec. The big chuck of time is spent within the block of MyOwnTimer1 and MyOwnTimer2. We spent 5x more time there even though we only scan 2x more rows.",0.03755555556,0.04914285714,neutral
impala,231,summary,Impala HBase scan is very slow,code_debt,slow_algorithm,"Thu, 11 Apr 2013 20:43:39 +0000","Wed, 17 Apr 2013 20:28:22 +0000","Wed, 17 Apr 2013 20:28:12 +0000",517473,Impala HBase scan is very slow,0,0,negative
impala,2341,description,"Ideally, we should make this query work. Alternatively, the error message should be improved. Stack:",code_debt,low_quality_code,"Mon, 14 Sep 2015 22:36:03 +0000","Wed, 16 Sep 2015 04:52:08 +0000","Wed, 16 Sep 2015 04:52:08 +0000",108965,"Ideally, we should make this query work. Alternatively, the error message should be improved. Stack:",0.2,0.2,neutral
impala,2355,description,"I am running a select query that is fetching only 3 rows but taking 15 mins to provide the resultset. I have gone through the Profile and observed that majority of the time is spend on unregister query or Client fetch wait time. I just want to deep dive to find the exact issue. Can someone please help me in the same. Query Info Duration: 15m, 2s Rows Produced: 3 Bytes Streamed: 182.8 MiB Client Fetch Wait Time: 15.0m Client Fetch Wait Time Percentage: 100 Query Timeline Start execution: 81.43us (81.43us) Planning finished: 27ms (27ms) Ready to start remote fragments: 29ms (2ms) Remote fragments started: 724ms (694ms) Rows available: 1.16s (440ms) First row fetched: 1.76s (594ms) Unregister query: 15.0m (15.0m)",code_debt,slow_algorithm,"Thu, 17 Sep 2015 08:03:36 +0000","Tue, 22 Sep 2015 04:39:24 +0000","Tue, 22 Sep 2015 04:39:24 +0000",419748,"I am running a select query that is fetching only 3 rows but taking 15 mins to provide the resultset. I have gone through the Profile and observed that majority of the time is spend on unregister query or Client fetch wait time. I just want to deep dive to find the exact issue. Can someone please help me in the same. Query Info Duration: 15m, 2s Rows Produced: 3 Bytes Streamed: 182.8 MiB Client Fetch Wait Time: 15.0m Client Fetch Wait Time Percentage: 100 Query Timeline Start execution: 81.43us (81.43us) Planning finished: 27ms (27ms) Ready to start remote fragments: 29ms (2ms) Remote fragments started: 724ms (694ms) Rows available: 1.16s (440ms) First row fetched: 1.76s (594ms) Unregister query: 15.0m (15.0m)",0.4205,0.4205,negative
impala,2457,description,"According to the math behind the PERCENT_RANK() value, a row group containing a single row yields a division by zero error and therefore Impala returns NaN in this case. In this example, there is only 1 'Reptile' row and that line in the result set has a NaN: Most RDBMS examples on the web use data with multiple rows per group, so they don't illustrate what's supposed to happen in this case. But I suspect the right return value in this case might be zero. Cf. ""The first row in any set has a PERCENT_RANK of 0."" That statement suggests the first row (and any tied rows I presume) could be special-cased and no calculation performed. Cf. The analytic example doesn't show all of the relevant output, but there are two DEPARTMENT_IDs in the output that have only one relevant row, and in both those cases the PERCENT_RANK result is shown as 0. I didn't find any PERCENT_RANK() in MySQL to try. I don't have a PostgreSQL instance handy.",code_debt,low_quality_code,"Thu, 1 Oct 2015 00:21:34 +0000","Wed, 7 Oct 2015 00:07:38 +0000","Wed, 7 Oct 2015 00:07:38 +0000",517564,"According to the math behind the PERCENT_RANK() value, a row group containing a single row yields a division by zero error and therefore Impala returns NaN in this case. In this example, there is only 1 'Reptile' row and that line in the result set has a NaN: Most RDBMS examples on the web use data with multiple rows per group, so they don't illustrate what's supposed to happen in this case. But I suspect the right return value in this case might be zero. Cf. http://docs.aws.amazon.com/redshift/latest/dg/r_WF_PERCENT_RANK.html ""The first row in any set has a PERCENT_RANK of 0."" That statement suggests the first row (and any tied rows I presume) could be special-cased and no calculation performed. Cf. http://docs.oracle.com/cd/B19306_01/server.102/b14200/functions109.htm The analytic example doesn't show all of the relevant output, but there are two DEPARTMENT_IDs in the output that have only one relevant row, and in both those cases the PERCENT_RANK result is shown as 0. I didn't find any PERCENT_RANK() in MySQL to try. I don't have a PostgreSQL instance handy.",-0.07198333333,-0.07198333333,neutral
impala,2642,description,"I just noticed this while reading the statestore code: {{OfferUpdate()}} takes if the update queue is full, but in one place that lock has already been taken by the caller: It's not as scary as it sounds, because if the update queue has > 10000 entries there's something wrong anyway, but we should fix this.",code_debt,multi-thread_correctness,"Fri, 6 Nov 2015 00:31:26 +0000","Tue, 6 Mar 2018 18:34:34 +0000","Tue, 6 Mar 2018 18:34:34 +0000",73591388,"I just noticed this while reading the statestore code: OfferUpdate() takes subscribers_lock_ if the update queue is full, but in one place that lock has already been taken by the caller: It's not as scary as it sounds, because if the update queue has > 10000 entries there's something wrong anyway, but we should fix this.",-0.4375,-0.4375,negative
impala,2707,description,"For each input row the aggregation node uses HashTable::Find() followed by HashTable::Insert() if the grouping key isn't already present in the table. Both of these methods probe the hash table to find the same bucket. If we added a FindOrInsert() method to the hash table that returned a modifiable iterator pointing to the bucket, we could save a significant number of hash table probes. There is already a TODO in the code for this, so I'm creating a JIRA to track the issue. This could speed up aggregations with large output size significantly, e.g. TPC-H query 13 (see IMPALA-2470).",code_debt,slow_algorithm,"Tue, 24 Nov 2015 18:00:29 +0000","Wed, 9 Dec 2015 00:19:33 +0000","Mon, 7 Dec 2015 19:34:23 +0000",1128834,"For each input row the aggregation node uses HashTable::Find() followed by HashTable::Insert() if the grouping key isn't already present in the table. Both of these methods probe the hash table to find the same bucket. If we added a FindOrInsert() method to the hash table that returned a modifiable iterator pointing to the bucket, we could save a significant number of hash table probes. There is already a TODO in the partitioned-aggregation-node-ir.cc code for this, so I'm creating a JIRA to track the issue. This could speed up aggregations with large output size significantly, e.g. TPC-H query 13 (see IMPALA-2470).",0.16,0.1333333333,neutral
impala,2962,summary,redactor: Rule assignment operator infinite loop,code_debt,low_quality_code,"Mon, 8 Feb 2016 16:47:53 +0000","Tue, 9 Feb 2016 08:01:39 +0000","Tue, 9 Feb 2016 08:01:39 +0000",54826,redactor: Rule assignment operator infinite loop,0,0,neutral
impala,3103,comment_1,The following fix improves serialisation efficiency by 20x (!),code_debt,slow_algorithm,"Tue, 1 Mar 2016 00:12:14 +0000","Tue, 1 Mar 2016 15:35:11 +0000","Tue, 1 Mar 2016 15:35:11 +0000",55377,The following fix improves serialisation efficiency by 20x https://github.com/cloudera/Impala/commit/055bd7e4088de8286c3abd35a88ec70b15be4be9,0.4,0.4,positive
impala,3103,description,"{{TBloomFilters}} have a 'directory' structure that is a list of individual buckets (buckets are about 64k wide). The total size of the directory can be 1MB or even much more. That leads to a lot of buckets, and very inefficient deserialisation as each bucket has to be allocated on the heap. Instead, the {{TBloomFilter}} representation should use one contiguous string (like the real {{BloomFilter}} does, so that it can be allocated with a single operation (and deserialized with a single copy).",code_debt,slow_algorithm,"Tue, 1 Mar 2016 00:12:14 +0000","Tue, 1 Mar 2016 15:35:11 +0000","Tue, 1 Mar 2016 15:35:11 +0000",55377,"TBloomFilters have a 'directory' structure that is a list of individual buckets (buckets are about 64k wide). The total size of the directory can be 1MB or even much more. That leads to a lot of buckets, and very inefficient deserialisation as each bucket has to be allocated on the heap. Instead, the TBloomFilter representation should use one contiguous string (like the real BloomFilter does, so that it can be allocated with a single operation (and deserialized with a single copy).",-0.1875,-0.1875,neutral
impala,3103,summary,Improve efficiency of BloomFilter Thrift serialisation,code_debt,slow_algorithm,"Tue, 1 Mar 2016 00:12:14 +0000","Tue, 1 Mar 2016 15:35:11 +0000","Tue, 1 Mar 2016 15:35:11 +0000",55377,Improve efficiency of BloomFilter Thrift serialisation,0.4,0.4,neutral
impala,3252,comment_1,", thanks a lot for nailing this; it has been a nuisance for ages.",code_debt,low_quality_code,"Mon, 28 Mar 2016 17:55:33 +0000","Mon, 11 Jun 2018 20:57:57 +0000","Mon, 11 Jun 2018 20:42:56 +0000",69562043,"tarmstrong, thanks a lot for nailing this; it has been a nuisance for ages.",0.4,0.4,negative
impala,326,description,"The validation for supported encodings for parquet is too strict. We also need to allow RLE and BIT_PACKED in the enums check. Also, the enums should print the strings in the error message, not the numerical values.",code_debt,low_quality_code,"Mon, 29 Apr 2013 16:55:44 +0000","Sun, 20 Dec 2015 00:05:02 +0000","Mon, 6 May 2013 03:44:37 +0000",557333,"The validation for supported encodings for parquet is too strict. We also need to allow RLE and BIT_PACKED in the enums check. Also, the enums should print the strings in the error message, not the numerical values.",0.06666666667,0.06666666667,negative
impala,3276,description,PrepareForRead() allows callers to use it in two different modes. In one of the modes it does not report pin failures. The code was written assuming that it will have reservations. This can result it hitting a DCHECK or going down some strange code paths:,code_debt,low_quality_code,"Wed, 30 Mar 2016 19:27:38 +0000","Tue, 5 Jul 2016 18:10:14 +0000","Tue, 12 Apr 2016 02:27:12 +0000",1061974,PrepareForRead() allows callers to use it in two different modes. In one of the modes it does not report pin failures. The code was written assuming that it will have reservations. This can result it hitting a DCHECK or going down some strange code paths:,-0.2115,-0.2115,neutral
impala,330,description,"Web page requests hold for the duration of the callbacks that render the page. This isn't terrible for short-lived requests, but anything that takes a while to render will block other requests. I tried the obvious thing, and Mongoose started behaving very weirdly (rendering the wrong page, rejecting a lot of concurrent requests).",code_debt,multi-thread_correctness,"Tue, 30 Apr 2013 21:38:18 +0000","Thu, 27 Feb 2014 22:36:34 +0000","Thu, 27 Feb 2014 22:36:34 +0000",26182696,"Web page requests hold path_handlers_lock_ for the duration of the callbacks that render the page. This isn't terrible for short-lived requests, but anything that takes a while to render will block other requests. I tried the obvious thing, and Mongoose started behaving very weirdly (rendering the wrong page, rejecting a lot of concurrent requests).",-0.008333333333,-0.008333333333,negative
impala,3344,description,"The sorter code does not have its internal invariants well-documented and can be tricky to understand when working on it. E.g. what are the valid states for merged versus initial runs, how are end-of-block cases handled. This issue is to clean up the sorter code while documenting important invariants as preparatory work for porting the sorter to the new buffer pool.",code_debt,low_quality_code,"Wed, 13 Apr 2016 17:43:23 +0000","Thu, 8 Sep 2016 16:28:02 +0000","Fri, 3 Jun 2016 04:02:31 +0000",4357148,"The sorter code does not have its internal invariants well-documented and can be tricky to understand when working on it. E.g. what are the valid states for merged versus initial runs, how are end-of-block cases handled. This issue is to clean up the sorter code while documenting important invariants as preparatory work for porting the sorter to the new buffer pool.",-0.03677777778,-0.03677777778,neutral
impala,3344,summary,Simplify and document invariants in Sorter,code_debt,low_quality_code,"Wed, 13 Apr 2016 17:43:23 +0000","Thu, 8 Sep 2016 16:28:02 +0000","Fri, 3 Jun 2016 04:02:31 +0000",4357148,Simplify and document invariants in Sorter,0,0,neutral
impala,3742,description,"Inserts into Kudu tables should be partitioned (i.e. rows hashed using the same hash partitioning as the Kudu table) and, at the table sink, sorted on the primary key. This would significantly improve performance. This will require a local sort (IMPALA-2521), and support from Kudu to provide the partitioning.",code_debt,slow_algorithm,"Tue, 14 Jun 2016 20:31:16 +0000","Fri, 1 Sep 2017 21:45:00 +0000","Thu, 4 May 2017 16:00:08 +0000",27977332,"Inserts into Kudu tables should be partitioned (i.e. rows hashed using the same hash partitioning as the Kudu table) and, at the table sink, sorted on the primary key. This would significantly improve performance. This will require a local sort (IMPALA-2521), and support from Kudu to provide the partitioning.",0.4333333333,0.4333333333,neutral
impala,3780,description,"I observed after adding logging that the scanner was issuing many small 1024-byte scan ranges: This appears to be based on the constant in the text scanner, which is used to try and find the end of a field that extends into the next HDFS block. We could avoid this in a couple of ways: * Increase the constant. It's unclear why it's so low: I think the cost of reading additional data is probably negligible, at least up to 10's or 100's of KB or so. * Ramp up the read size, e.g. recursive doubling up to 8MB.",code_debt,slow_algorithm,"Thu, 23 Jun 2016 17:29:42 +0000","Mon, 18 Jul 2016 15:56:38 +0000","Mon, 18 Jul 2016 15:56:38 +0000",2154416,"I observed after adding logging that the scanner was issuing many small 1024-byte scan ranges: This appears to be based on the ""NEXT_BLOCK_READ_SIZE"" constant in the text scanner, which is used to try and find the end of a field that extends into the next HDFS block. We could avoid this in a couple of ways: Increase the constant. It's unclear why it's so low: I think the cost of reading additional data is probably negligible, at least up to 10's or 100's of KB or so. Ramp up the read size, e.g. recursive doubling up to 8MB.",-0.05,-0.06666666667,neutral
impala,3780,summary,Uncompressed text scanner is slow when reading strings that significantly exceed the HDFS block size,code_debt,slow_algorithm,"Thu, 23 Jun 2016 17:29:42 +0000","Mon, 18 Jul 2016 15:56:38 +0000","Mon, 18 Jul 2016 15:56:38 +0000",2154416,Uncompressed text scanner is slow when reading strings that significantly exceed the HDFS block size,-0.2,-0.2,negative
impala,4027,comment_2,"Despite this is pretty rare, I mean that variable ""probe_expr_ctxs_"" is added twice while variable ""filter_expr_ctxs_"" is not called by function AddExprCtxsToFree",code_debt,low_quality_code,"Fri, 26 Aug 2016 02:54:27 +0000","Tue, 30 Aug 2016 16:18:12 +0000","Tue, 30 Aug 2016 16:18:12 +0000",393825,"Despite this is pretty rare, I mean that variable ""probe_expr_ctxs_"" is added twice while variable ""filter_expr_ctxs_"" is not called by function AddExprCtxsToFree",-0.275,-0.275,neutral
impala,4027,summary,Memory leak with ExprCtxs not free,code_debt,low_quality_code,"Fri, 26 Aug 2016 02:54:27 +0000","Tue, 30 Aug 2016 16:18:12 +0000","Tue, 30 Aug 2016 16:18:12 +0000",393825,Memory leak with ExprCtxs not free,-0.2,-0.2,negative
impala,4231,summary,Performance regression in TPC-H Q2 due to delay in filter arrival,code_debt,slow_algorithm,"Fri, 30 Sep 2016 18:19:07 +0000","Mon, 17 Oct 2016 00:30:56 +0000","Mon, 17 Oct 2016 00:30:56 +0000",1404709,Performance regression in TPC-H Q2 due to delay in filter arrival,-0.2,-0.2,negative
impala,4291,comment_0,"IMPALA-4291: Reduce LLVM module's preparation time Previously, when creating a LlvmCodeGen object, we run an O(mn) algorithm to map the IRFunction::Type to the actual LLVM::Function object in the module. m is the size of IRFunction::Type enum and n is the total number of functions in the module. This is a waste of time if we only use few functions from the module. This change reduces the preparation time of a simple query from 23ms to 10ms. select count(*) from where l_orderkey > 20;",code_debt,slow_algorithm,"Thu, 13 Oct 2016 18:37:43 +0000","Thu, 30 Aug 2018 18:49:49 +0000","Thu, 20 Oct 2016 05:58:34 +0000",559251,"https://github.com/apache/incubator-impala/commit/47b8aa3a9e7682ebb182696901916900d3323039 IMPALA-4291: Reduce LLVM module's preparation time Previously, when creating a LlvmCodeGen object, we run an O(mn) algorithm to map the IRFunction::Type to the actual LLVM::Function object in the module. m is the size of IRFunction::Type enum and n is the total number of functions in the module. This is a waste of time if we only use few functions from the module. This change reduces the preparation time of a simple query from 23ms to 10ms. select count from tpch100_parquet.lineitem where l_orderkey > 20;",-0.2185,-0.1748,neutral
impala,4291,description,"For simple queries with one or two functions to codegen, we spent about 23ms or more out of 40ms of codegen time in preparation. We can save some time by not eagerly populating the map of to {{llvm::Function*}} map. This should help with the fixes for IMPALA-3638 when we create the LLVM module unconditionally.",code_debt,slow_algorithm,"Thu, 13 Oct 2016 18:37:43 +0000","Thu, 30 Aug 2018 18:49:49 +0000","Thu, 20 Oct 2016 05:58:34 +0000",559251,"For simple queries with one or two functions to codegen, we spent about 23ms or more out of 40ms of codegen time in preparation. We can save some time by not eagerly populating the map of IRFunction::Type to llvm::Function* map. This should help with the fixes for IMPALA-3638 when we create the LLVM module unconditionally.",0.2666666667,0.2666666667,neutral
impala,4328,description,"The config variable has hostnames that are private to Cloudera. Can you make those environment variables or configured at the command line or something, rather than hardcoding ""Cloudera"" into the script?",code_debt,low_quality_code,"Thu, 20 Oct 2016 18:18:38 +0000","Thu, 27 Jul 2017 02:55:35 +0000","Thu, 27 Jul 2017 02:55:35 +0000",24136617,"The config variable has hostnames that are private to Cloudera. Can you make those environment variables or configured at the command line or something, rather than hardcoding ""Cloudera"" into the script?",0,0,neutral
impala,4548,description,"As shown in IMPALA-4532, the build async thread can still be running after the query has completed. This may lead use-after-free issue in IMPALA-4532. A more appropriate design is for to wait for the completion of the build thread.",code_debt,multi-thread_correctness,"Mon, 28 Nov 2016 22:32:33 +0000","Fri, 21 Apr 2017 21:24:13 +0000","Fri, 21 Apr 2017 21:20:43 +0000",12437290,"As shown in IMPALA-4532, the build async thread can still be running after the query has completed. This may lead use-after-free issue in IMPALA-4532. A more appropriate design is for BlockingJoinNode::Close() to wait for the completion of the build thread.",0.4103333333,0.4103333333,neutral
impala,4617,description,"Currently Expr.isConstant() and Expr::IsConstant() in the frontend and backend have duplicate logic to do exactly the same analysis. They need to be kept exactly in sync to avoid problems. We should plumb through the value of isConstant() from the frontend to avoid this duplication. We need to be a little careful about how we do this in the frontend: Alex Behn mentioned that storing state in Exprs was risky, and also that naively calling isConstant() for each expr node was problematic since it could result in traversing the Expr tree many times.",code_debt,duplicated_code,"Wed, 7 Dec 2016 21:56:41 +0000","Tue, 31 Jan 2017 18:09:21 +0000","Tue, 31 Jan 2017 18:09:21 +0000",4738360,"Currently Expr.isConstant() and Expr::IsConstant() in the frontend and backend have duplicate logic to do exactly the same analysis. They need to be kept exactly in sync to avoid problems. We should plumb through the value of isConstant() from the frontend to avoid this duplication. We need to be a little careful about how we do this in the frontend: Alex Behn mentioned that storing state in Exprs was risky, and also that naively calling isConstant() for each expr node was problematic since it could result in traversing the Expr tree many times.",-0.05666666667,-0.05666666667,neutral
impala,4617,summary,Remove duplication of isConstant() and IsConstant() in frontend and backend,code_debt,duplicated_code,"Wed, 7 Dec 2016 21:56:41 +0000","Tue, 31 Jan 2017 18:09:21 +0000","Tue, 31 Jan 2017 18:09:21 +0000",4738360,Remove duplication of isConstant() and IsConstant() in frontend and backend,0,0,neutral
impala,4713,description,"I'm trying to debug failure queries where an is thrown wrapping a from an ""invalidate metadata <table The error message shows up in impala-shell but it doesn't appear that the Java backtrace is logged anywhere in the logs. This makes it impossible to determine where the NPE has come from.",code_debt,low_quality_code,"Sun, 25 Dec 2016 00:40:46 +0000","Fri, 2 Nov 2018 23:56:49 +0000","Fri, 2 Nov 2018 23:56:49 +0000",58576563,"I'm trying to debug failure queries where an IllegalStateException is thrown wrapping a NullPointerException from an ""invalidate metadata <table>"" query. The error message shows up in impala-shell but it doesn't appear that the Java backtrace is logged anywhere in the logs. This makes it impossible to determine where the NPE has come from.",-0.2166666667,-0.2833333333,negative
impala,4996,description,"Since we will multi-thread query execution at the fragment level, we should rework KuduScanNode to only use a single thread (the one that's executing the fragment).",code_debt,multi-thread_correctness,"Sat, 25 Feb 2017 16:11:23 +0000","Tue, 21 Mar 2017 20:00:28 +0000","Tue, 21 Mar 2017 20:00:28 +0000",2087345,"Since we will multi-thread query execution at the fragment level, we should rework KuduScanNode to only use a single thread (the one that's executing the fragment).",-0.5,-0.5,neutral
impala,5042,description,"Loading metadata for partitions with custom paths is 4x slower compared to partitions without custom paths, the slow down is due to an N2 lookups to check if a partition already exists. The List should ideally be replaced with a Set. From From Java mission control",code_debt,slow_algorithm,"Wed, 8 Mar 2017 08:21:24 +0000","Wed, 22 Mar 2017 18:33:30 +0000","Fri, 17 Mar 2017 17:27:01 +0000",810337,"Loading metadata for partitions with custom paths is 4x slower compared to partitions without custom paths, the slow down is due to an N2 lookups to check if a partition already exists. The List should ideally be replaced with a Set. From https://github.com/apache/incubator-impala/blob/master/fe/src/main/java/org/apache/impala/catalog/HdfsTable.java From Java mission control",-0.09733333333,-0.09733333333,negative
impala,5042,summary,"Loading metadata for partitioned tables is slow due to usage of an ArrayList, potential 4x speedup",code_debt,slow_algorithm,"Wed, 8 Mar 2017 08:21:24 +0000","Wed, 22 Mar 2017 18:33:30 +0000","Fri, 17 Mar 2017 17:27:01 +0000",810337,"Loading metadata for partitioned tables is slow due to usage of an ArrayList, potential 4x speedup",0,0,negative
impala,5070,description,Issues like IMPALA-92 with multiple attachments with the same name did not have all of their attachments translate properly.,code_debt,low_quality_code,"Mon, 13 Mar 2017 22:10:27 +0000","Thu, 16 Mar 2017 04:09:38 +0000","Thu, 16 Mar 2017 04:09:19 +0000",194332,Issues like IMPALA-92 with multiple attachments with the same name did not have all of their attachments translate properly.,0,0,negative
impala,5070,summary,Some attachments were not translated properly,code_debt,low_quality_code,"Mon, 13 Mar 2017 22:10:27 +0000","Thu, 16 Mar 2017 04:09:38 +0000","Thu, 16 Mar 2017 04:09:19 +0000",194332,Some attachments were not translated properly,0,0,negative
impala,5130,description,"can run concurrently with There is no synchronisation between the two. I saw a crash with this stack on a development branch, which I believe is caused by this: on this line: This method is not currently used in query execution, but we need to fix this before switching on the buffer pool for query execution.",code_debt,multi-thread_correctness,"Tue, 28 Mar 2017 18:59:35 +0000","Thu, 30 Mar 2017 00:21:05 +0000","Thu, 30 Mar 2017 00:21:05 +0000",105690,"MemTracker::EnableReservationReporting() can run concurrently with MemTracker::LogUsage(). There is no synchronisation between the two. I saw a crash with this stack on a development branch, which I believe is caused by this: on this line: This method is not currently used in query execution, but we need to fix this before switching on the buffer pool for query execution.",-0.2,-0.1333333333,negative
impala,5273,description,"Replacing StringCompare (which uses SSE4.2 instructions) with a call to glibc's memcmp results in a memcmp on my machine mainly uses sse4.1's ptest, after detecting at run-time that I have sse4.1 instructions available. The StringCompare benchmark is 5 years old and likely out-of-date by now. To replicate:",code_debt,slow_algorithm,"Tue, 2 May 2017 21:46:55 +0000","Tue, 9 May 2017 16:39:42 +0000","Tue, 9 May 2017 16:39:42 +0000",586367,"Replacing StringCompare (which uses SSE4.2 instructions) with a call to glibc's dynamically-dispatched memcmp results in a >5x improvement for large strings. memcmp on my machine mainly uses sse4.1's ptest, after detecting at run-time that I have sse4.1 instructions available. The StringCompare benchmark is 5 years old and likely out-of-date by now. To replicate:",0,0.07742857143,neutral
impala,5273,summary,StringCompare is very slow,code_debt,slow_algorithm,"Tue, 2 May 2017 21:46:55 +0000","Tue, 9 May 2017 16:39:42 +0000","Tue, 9 May 2017 16:39:42 +0000",586367,StringCompare is very slow,0,0,negative
impala,530,description,"I was experimenting with queries on year / month / day fields stored as strings in a TEXT data file. In addition to the need to CAST them to INT to get correct sorting during ORDER BY, I noticed that the sorting of the CAST-to-INT values was 2x faster than the original string values (this is with about 500M rows): select distinct year, month, day from raw_data_ymd order by year, month, day limit 7500; ... Returned 5124 row(s) in 102.45s select distinct year, month, day from raw_data_ymd order by cast (year as int), cast (month as int), cast (day as int) limit 7500; ... Returned 5124 row(s) in 41.87s But all the strings are very short, 1-2 bytes for the day and month fields, and 4 bytes for the year field. I would expect these could be sorted in about the same time as the corresponding integers, especially since the integer sorting in the above query has the overhead of 3 CASTs per row. I wonder if there is some latent UTF-8 processing slowing down the string sorting even though all the values are ASCII, or if the unknown-in-advance length of the strings makes the string sort harder to optimize. Could there be some special casing when the string data is known to be ASCII (i.e. like currently), and Impala could infer a binary collation? E.g. the strings could be compared 2, 4, or 8 bytes at a time as if they were integers. I first thought of this as an optimization for short string columns where the data is of consistent length, but if the strings are null-terminated and held in a buffer with a size rounded to an even number, I think the technique would apply for strings of different or unknown-in-advance lengths.",code_debt,slow_algorithm,"Tue, 13 Aug 2013 18:11:25 +0000","Fri, 5 Dec 2014 07:28:44 +0000","Fri, 5 Dec 2014 07:28:44 +0000",41347039,"I was experimenting with queries on year / month / day fields stored as strings in a TEXT data file. In addition to the need to CAST them to INT to get correct sorting during ORDER BY, I noticed that the sorting of the CAST-to-INT values was 2x faster than the original string values (this is with about 500M rows): select distinct year, month, day from raw_data_ymd order by year, month, day limit 7500; ... Returned 5124 row(s) in 102.45s select distinct year, month, day from raw_data_ymd order by cast (year as int), cast (month as int), cast (day as int) limit 7500; ... Returned 5124 row(s) in 41.87s But all the strings are very short, 1-2 bytes for the day and month fields, and 4 bytes for the year field. I would expect these could be sorted in about the same time as the corresponding integers, especially since the integer sorting in the above query has the overhead of 3 CASTs per row. I wonder if there is some latent UTF-8 processing slowing down the string sorting even though all the values are ASCII, or if the unknown-in-advance length of the strings makes the string sort harder to optimize. Could there be some special casing when the string data is known to be ASCII (i.e. like currently), and Impala could infer a binary collation? E.g. the strings could be compared 2, 4, or 8 bytes at a time as if they were integers. I first thought of this as an optimization for short string columns where the data is of consistent length, but if the strings are null-terminated and held in a buffer with a size rounded to an even number, I think the technique would apply for strings of different or unknown-in-advance lengths.",0.1458333333,0.1458333333,neutral
impala,530,summary,Could ASCII string sorting be faster?,code_debt,slow_algorithm,"Tue, 13 Aug 2013 18:11:25 +0000","Fri, 5 Dec 2014 07:28:44 +0000","Fri, 5 Dec 2014 07:28:44 +0000",41347039,Could ASCII string sorting be faster?,0,0,neutral
impala,5481,description,"One of the {{RowBatch}} c'tors copies the row descriptor into the row batch. This leads to a lot of allocation churn since {{RowDescriptor}} contains some vector members, and since the descriptor is usually the same the copies are unnecessary. Instead, we should consider allocating the {{RowDescriptor}} once from an object pool, and sharing it amongst all row batches that need that descriptor. In some tests, {{RowDescriptor()}} shows up as 20% of the tcmalloc allocation time.",code_debt,slow_algorithm,"Sat, 10 Jun 2017 00:55:46 +0000","Wed, 21 Jun 2017 03:02:29 +0000","Wed, 21 Jun 2017 03:02:28 +0000",958002,"One of the RowBatch c'tors copies the row descriptor into the row batch. This leads to a lot of allocation churn since RowDescriptor contains some vector members, and since the descriptor is usually the same the copies are unnecessary. Instead, we should consider allocating the RowDescriptor once from an object pool, and sharing it amongst all row batches that need that descriptor. In some tests, RowDescriptor() shows up as 20% of the tcmalloc allocation time.",-0.1405,-0.1405,neutral
impala,5600,summary,Small cleanups left over from IMPALA-5344,code_debt,low_quality_code,"Wed, 28 Jun 2017 22:40:19 +0000","Thu, 29 Jun 2017 04:28:45 +0000","Thu, 29 Jun 2017 04:27:30 +0000",20831,Small cleanups left over from IMPALA-5344,0,0,neutral
impala,5618,description,The extra time appears to be in AddRowCustom() via I think allocating the boost::function object is causing the slowdown.,code_debt,slow_algorithm,"Thu, 6 Jul 2017 00:51:03 +0000","Fri, 7 Jul 2017 18:02:02 +0000","Fri, 7 Jul 2017 18:02:02 +0000",148259,The extra time appears to be in AddRowCustom() via PartitionedAggregationNode::ConstructIntermediateTuple(). I think allocating the boost::function object is causing the slowdown.,-0.181,-0.0905,neutral
impala,5849,description,"IMPALA-5800, IMPALA-5775 and IMPALA-5743 added TLS configuration to Impala and Squeasel. Since Impala is often built against different versions of OpenSSL (with different TLS capabilities), we used compile-time definitions to avoid using symbols from OpenSSL 1.0.1 that weren't available. This works great if we can ensure that the machine on which Impala is built is the same environment as the one on which it executes, but we have discovered that the installed version of OpenSSL can vary between minor releases of Linux distributions. It appears possible to write the support for TLS1.1+ in terms of symbols that are available in OpenSSL 1.0.0 only. The only downside is that Impala can't then tell whether or not the runtime supports TLS 1.2, and so the error messages won't be quite as clear. However, the benefit of a single binary and Thrift toolchain dependency for all supported versions of OpenSSL is well worth it.",code_debt,low_quality_code,"Fri, 25 Aug 2017 18:54:25 +0000","Tue, 5 Sep 2017 16:49:06 +0000","Tue, 5 Sep 2017 16:49:06 +0000",942881,"IMPALA-5800, IMPALA-5775 and IMPALA-5743 added TLS configuration to Impala and Squeasel. Since Impala is often built against different versions of OpenSSL (with different TLS capabilities), we used compile-time definitions to avoid using symbols from OpenSSL 1.0.1 that weren't available. This works great if we can ensure that the machine on which Impala is built is the same environment as the one on which it executes, but we have discovered that the installed version of OpenSSL can vary between minor releases of Linux distributions. It appears possible to write the support for TLS1.1+ in terms of symbols that are available in OpenSSL 1.0.0 only. The only downside is that Impala can't then tell whether or not the runtime supports TLS 1.2, and so the error messages won't be quite as clear. However, the benefit of a single binary and Thrift toolchain dependency for all supported versions of OpenSSL is well worth it.",0.1673452381,0.1673452381,neutral
impala,596,comment_0,I actually do not think the leak is related to the HDFS close error now that I look at the log timestamps.,code_debt,low_quality_code,"Thu, 19 Sep 2013 22:07:11 +0000","Thu, 12 Feb 2015 09:21:39 +0000","Thu, 19 Sep 2013 22:23:33 +0000",982,I actually do not think the leak is related to the HDFS close error now that I look at the log timestamps.,0.3,0.3,negative
impala,6026,comment_2,"Thanks, Juan. Do you have the steps to repro this (for an older release)? I recently came across a setup that could repro this on 2.9.0 and we narrowed down the problem to be with the files in the partition mapping to the table root directory. For example '/tmp/foo' is the table root directory and one of the partition directories mapped to '/tmp/foo' (by mistake). We have some weird logic with adding a ""default partition"" with an immutable partition-key list and I'm guessing that the above state has caused a mixup and we tried appending to the Immutable list causing this behavior. In the above setup, we fixed the partition structure to unblock the table operations, fwiw. Interestingly I'm not able to reproduce it locally either. I'll keep this open for a while incase someone else runs into this and we have a more reliable repro.",code_debt,complex_code,"Sat, 7 Oct 2017 00:18:31 +0000","Fri, 19 Oct 2018 17:21:18 +0000","Fri, 19 Oct 2018 17:21:18 +0000",32634167,"Thanks, Juan. Do you have the steps to repro this (for an older release)? I recently came across a setup that could repro this on 2.9.0 and we narrowed down the problem to be with the files in the partition mapping to the table root directory. For example '/tmp/foo' is the table root directory and one of the partition directories mapped to '/tmp/foo' (by mistake). We have some weird logic with adding a ""default partition"" with an immutable partition-key list and I'm guessing that the above state has caused a mixup and we tried appending to the Immutable list causing this behavior. In the above setup, we fixed the partition structure to unblock the table operations, fwiw. Interestingly I'm not able to reproduce it locally either. I'll keep this open for a while incase someone else runs into this and we have a more reliable repro.",0.01683333333,0.01683333333,neutral
impala,6048,comment_3,"So, as explained, this could be triggered by heavy spilling under high concurrency so some nodes are slow to consume row batches, leading to long RPC wait time. This seems to be very similar to what's being tracked in IMPALA-6294.",code_debt,slow_algorithm,"Thu, 12 Oct 2017 23:54:47 +0000","Tue, 14 May 2019 01:53:18 +0000","Tue, 14 May 2019 01:53:18 +0000",49946311,"So, as explained, this could be triggered by heavy spilling under high concurrency so some nodes are slow to consume row batches, leading to long RPC wait time. This seems to be very similar to what's being tracked in IMPALA-6294.",0.2,0.2,neutral
impala,6048,description,"When running 32 concurrent queries from TPCDS a couple of instances from TPC-DS Q78 9 hours to finish and it appeared to be hung. On an idle cluster the query finished in under 5 minutes, profiles attached. When the query ran for long fragments reported +16 hours of network send/receive time The logs show there is a lot of messages like the one below, there are incidents for this log message where a node waited too long from an RPC from itself",code_debt,slow_algorithm,"Thu, 12 Oct 2017 23:54:47 +0000","Tue, 14 May 2019 01:53:18 +0000","Tue, 14 May 2019 01:53:18 +0000",49946311,"When running 32 concurrent queries from TPCDS a couple of instances from TPC-DS Q78 9 hours to finish and it appeared to be hung. On an idle cluster the query finished in under 5 minutes, profiles attached. When the query ran for long fragments reported +16 hours of network send/receive time The logs show there is a lot of messages like the one below, there are incidents for this log message where a node waited too long from an RPC from itself",0,0,neutral
impala,6048,summary,Queries make very slow progress and report WaitForRPC() stuck for too long,code_debt,slow_algorithm,"Thu, 12 Oct 2017 23:54:47 +0000","Tue, 14 May 2019 01:53:18 +0000","Tue, 14 May 2019 01:53:18 +0000",49946311,Queries make very slow progress and report  WaitForRPC() stuck for too long,0,0,negative
impala,6077,description,"As a follow-on to IMPALA-6076, we should remove support for the encoding in a compat-breaking release so that we no longer have to maintain this code.",code_debt,dead_code,"Wed, 18 Oct 2017 20:36:54 +0000","Fri, 23 Mar 2018 21:54:15 +0000","Mon, 12 Feb 2018 22:15:10 +0000",10114696,"As a follow-on to IMPALA-6076, we should remove support for the encoding in a compat-breaking release so that we no longer have to maintain this code.",0.4,0.4,neutral
impala,6080,comment_1,"I don't think anyone has picked that up due to a lack of time - I agree it would be good to get in but I think parts of that patch require deeper understanding than this subset, which is fairly mechanical cleanup. Separating this out would shrink that patch and I think make it easier to focus on the meat of it.",code_debt,low_quality_code,"Wed, 18 Oct 2017 23:53:05 +0000","Thu, 16 Nov 2017 22:12:16 +0000","Thu, 16 Nov 2017 22:12:16 +0000",2499551,"I don't think anyone has picked that up due to a lack of time - I agree it would be good to get in but I think parts of that patch require deeper understanding than this subset, which is fairly mechanical cleanup. Separating this out would shrink that patch and I think make it easier to focus on the meat of it.",-0.096,-0.096,neutral
impala,6080,summary,Clean up descriptor table handling in coordinator,code_debt,low_quality_code,"Wed, 18 Oct 2017 23:53:05 +0000","Thu, 16 Nov 2017 22:12:16 +0000","Thu, 16 Nov 2017 22:12:16 +0000",2499551,Clean up descriptor table handling in coordinator,0.4,0.4,neutral
impala,6131,description,Currently we (ab-)use to track the last update time of statistics. Instead we should introduce a separate counter to track the last update. With that we should also remove all occurrences of from and fall back to Hive's default behavior.,code_debt,low_quality_code,"Mon, 30 Oct 2017 23:49:52 +0000","Thu, 2 May 2019 21:48:05 +0000","Mon, 11 Jun 2018 15:23:35 +0000",19323223,Currently we (ab-)use transient_lastDdlTime to track the last update time of statistics. Instead we should introduce a separate counter to track the last update. With that we should also remove all occurrences of catalog_.updateLastDdlTime() from CatalogOpExecutor and fall back to Hive's default behavior.,-0.1666666667,-0.125,neutral
impala,6132,comment_0,"I think the use in is incorrect: the destructor for the {{unique_ptr}} calls {{delete}} on the raw pointer underneath when its scope ends at the end of the {{while}} loop. Meanwhile, that pointer leaked to {{std::string * path}}.",code_debt,low_quality_code,"Tue, 31 Oct 2017 05:44:54 +0000","Tue, 31 Oct 2017 21:18:44 +0000","Tue, 31 Oct 2017 21:18:44 +0000",56030,"I think the use in GetExecutablePath is incorrect: the destructor for the unique_ptr calls delete on the raw pointer underneath when its scope ends at the end of the while loop. Meanwhile, that pointer leaked to std::string * path.",-0.1,-0.1,negative
impala,6132,comment_1,"Also, you might be able to fix it by calling instead of The former will clear the raw pointer value out of the {{unique_ptr}}, which then cannot call {{delete}}, thus not leaking the memory.",code_debt,low_quality_code,"Tue, 31 Oct 2017 05:44:54 +0000","Tue, 31 Oct 2017 21:18:44 +0000","Tue, 31 Oct 2017 21:18:44 +0000",56030,"Also, you might be able to fix it by calling unitque_ptr::release instead of unique_ptr::get. The former will clear the raw pointer value out of the unique_ptr, which then cannot call delete, thus not leaking the memory.",0.688,0.344,neutral
impala,636,comment_0,"When we process topic deletion, we just remove an entry from the backend list without removing the entry from backend_map_. We should remove the entry from backend_map_ if the list is empty.",code_debt,low_quality_code,"Thu, 24 Oct 2013 02:56:18 +0000","Sun, 20 Dec 2015 00:05:08 +0000","Fri, 15 Nov 2013 23:12:07 +0000",1973749,"When we process topic deletion, we just remove an entry from the backend list without removing the entry from backend_map_. We should remove the entry from backend_map_ if the list is empty.",-0.1,-0.1,neutral
impala,6613,description,"Now that KRPC is on by default, we should rename the environment variable to reflect that.",code_debt,low_quality_code,"Tue, 6 Mar 2018 21:35:21 +0000","Mon, 19 Mar 2018 22:29:42 +0000","Mon, 19 Mar 2018 22:29:42 +0000",1126461,"Now that KRPC is on by default, we should rename the environment variable to reflect that.",-0.5,-0.5,neutral
impala,6666,description,"This is a performance optimization for spill-to-disk when encryption is enabled. If running with an OpenSSL version with AES-GCM support on a CPU that supports the CLMUL instruction, this faster encryption mode will be used.",code_debt,slow_algorithm,"Wed, 14 Mar 2018 20:32:08 +0000","Thu, 5 Apr 2018 22:12:46 +0000","Thu, 5 Apr 2018 22:12:46 +0000",1906838,"This is a performance optimization for spill-to-disk when encryption is enabled. If running with an OpenSSL version with AES-GCM support on a CPU that supports the CLMUL instruction, this faster encryption mode will be used.",0.2,0.2,neutral
impala,6694,description,"It appears that the buffer pool statistics of exec node is sometimes misaligned in the profile. For instance, the aggregation node's buffer pool appears after the exchange node below: cc'ing",code_debt,low_quality_code,"Sat, 17 Mar 2018 01:47:10 +0000","Sat, 31 Mar 2018 23:35:04 +0000","Thu, 29 Mar 2018 22:30:41 +0000",1111411,"It appears that the buffer pool statistics of exec node is sometimes misaligned in the profile. For instance, the aggregation node's buffer pool appears after the exchange node below: cc'ing tarmstrong@cloudera.com], mmokhtar",0,0,neutral
impala,6709,summary,Simplify tests that copy local files to tables,code_debt,complex_code,"Tue, 20 Mar 2018 14:39:45 +0000","Mon, 27 Aug 2018 13:53:59 +0000","Mon, 27 Aug 2018 13:53:59 +0000",13821254,Simplify tests that copy local files to tables,0,0,neutral
impala,6817,summary,Clean up Impala privilege model,code_debt,low_quality_code,"Fri, 6 Apr 2018 03:14:31 +0000","Tue, 10 Apr 2018 17:48:45 +0000","Tue, 10 Apr 2018 14:02:53 +0000",384502,Clean up Impala privilege model,0.4,0.4,neutral
impala,6850,summary,Print the actual error message to the console when Sentry fails,code_debt,low_quality_code,"Fri, 13 Apr 2018 18:43:27 +0000","Thu, 19 Apr 2018 18:12:10 +0000","Mon, 16 Apr 2018 01:26:50 +0000",197003,Print the actual error message to the console when Sentry fails,-0.1416666667,-0.1416666667,negative
impala,710,summary,Cleanup log spew from Planner changes,code_debt,low_quality_code,"Sun, 15 Dec 2013 07:15:13 +0000","Wed, 18 Dec 2013 18:24:35 +0000","Wed, 18 Dec 2013 18:24:35 +0000",299362,Cleanup log spew from Planner changes,0,0,neutral
impala,7682,description,"The method public Set<String> groups, Set<String> users, ActiveRoleSet roleSet) in AuthorizationPolicy needs to be synchronized.",code_debt,multi-thread_correctness,"Tue, 9 Oct 2018 18:24:48 +0000","Fri, 12 Oct 2018 17:11:38 +0000","Thu, 11 Oct 2018 01:40:18 +0000",112530,"The method public Set<String> listPrivileges(Set<String> groups, Set<String> users, ActiveRoleSet roleSet) in AuthorizationPolicy needs to be synchronized.",0,0,neutral
impala,7748,description,"With IMPALA-110, we can now support multiple count distinct directly, and the appx_count_distinct query option is no longer needed. Users who want the perf improvement from it can always just use the ndv() function directly in their sql. We'll mark this option as deprecated in the docs starting from 3.1. Removing it can be targeted for 4.0",code_debt,dead_code,"Tue, 23 Oct 2018 20:30:01 +0000","Thu, 25 Oct 2018 22:05:47 +0000","Thu, 25 Oct 2018 22:05:47 +0000",178546,"With IMPALA-110, we can now support multiple count distinct directly, and the appx_count_distinct query option is no longer needed. Users who want the perf improvement from it can always just use the ndv() function directly in their sql. We'll mark this option as deprecated in the docs starting from 3.1. Removing it can be targeted for 4.0",0.19275,0.19275,neutral
impala,7808,description,"The analysis steps in {{SelectStmt}} and {{AnalysisContext}} are large and cumbersome. There is ample evidence in the literature that simpler, smaller functions are easier to understand and debug than larger, more complex functions. This ticket requests breaking up the large functions in these two cases into smaller, easier-understood units in preparation for tracking down issues related to missing rewrites of the {{WHERE}} and {{GROUP BY}} clauses. One might argue that large functions perform better by eliminating unnecessary function calls. However, the planner is not performance sensitive, and the dozen extra calls that this change introduce will not change performance given the thousands of calls already made. Experience has shown that the JIT compiler in the JVM actually does a better job optimizing smaller functions, and gives up when functions get to large. So, by creating smaller functions, we may actually allow the JIT compiler to generate better code. And, this refactoring is in support of a possible outcome that the planner can handle rewrites without making multiple passes through the analyzer: that savings will far outweigh the few extra calls this change introduces.",code_debt,complex_code,"Mon, 5 Nov 2018 22:32:04 +0000","Fri, 1 Mar 2019 17:54:10 +0000","Fri, 1 Mar 2019 17:54:09 +0000",10005725,"The analysis steps in SelectStmt and AnalysisContext are large and cumbersome. There is ample evidence in the literature that simpler, smaller functions are easier to understand and debug than larger, more complex functions. This ticket requests breaking up the large functions in these two cases into smaller, easier-understood units in preparation for tracking down issues related to missing rewrites of the WHERE and GROUP BY clauses. One might argue that large functions perform better by eliminating unnecessary function calls. However, the planner is not performance sensitive, and the dozen extra calls that this change introduce will not change performance given the thousands of calls already made. Experience has shown that the JIT compiler in the JVM actually does a better job optimizing smaller functions, and gives up when functions get to large. So, by creating smaller functions, we may actually allow the JIT compiler to generate better code. And, this refactoring is in support of a possible outcome that the planner can handle rewrites without making multiple passes through the analyzer: that savings will far outweigh the few extra calls this change introduces.",0.088,0.088,neutral
impala,7808,summary,Refactor SelectStmt analyzer for easier debugging,code_debt,low_quality_code,"Mon, 5 Nov 2018 22:32:04 +0000","Fri, 1 Mar 2019 17:54:10 +0000","Fri, 1 Mar 2019 17:54:09 +0000",10005725,Refactor SelectStmt analyzer for easier debugging,0,0,neutral
impala,7841,description,"IMPALA-7808 started the process of refactoring the Analyzer code for easier debugging. It did so by grouping {{SelectStmt}} code into a nested class, which then allowed breaking up a large function into smaller chunks. This ticket continues that process with two changes: * Follow-on refactoring of {{SelectStmt}} to make some of the newly-created functions simpler. (The first change tried to keep code unchanged as much as possible.) * Apply the same technique to the {{QueryStmt}} base class and to its other subclasses.",code_debt,low_quality_code,"Thu, 8 Nov 2018 22:56:23 +0000","Fri, 1 Mar 2019 18:16:00 +0000","Fri, 1 Mar 2019 18:16:00 +0000",9746377,"IMPALA-7808 started the process of refactoring the Analyzer code for easier debugging. It did so by grouping SelectStmt code into a nested class, which then allowed breaking up a large function into smaller chunks. This ticket continues that process with two changes: Follow-on refactoring of SelectStmt to make some of the newly-created functions simpler. (The first change tried to keep code unchanged as much as possible.) Apply the same technique to the QueryStmt base class and to its other subclasses.",0,0,neutral
impala,7841,summary,"Refactor QueryStmt, other analysis code for easier debugging",code_debt,low_quality_code,"Thu, 8 Nov 2018 22:56:23 +0000","Fri, 1 Mar 2019 18:16:00 +0000","Fri, 1 Mar 2019 18:16:00 +0000",9746377,"Refactor QueryStmt, other analysis code for easier debugging",0,0,neutral
impala,7869,description,suggested reorganising the file to be easier to read on Compile times are also an issue - this file is the longest pole in the Impala compilation at the moment.,code_debt,low_quality_code,"Mon, 19 Nov 2018 16:57:08 +0000","Tue, 27 Nov 2018 02:01:56 +0000","Tue, 27 Nov 2018 02:01:56 +0000",637488,csringhofer suggested reorganising the file to be easier to read on https://gerrit.cloudera.org/#/c/8319/ Compile times are also an issue - this file is the longest pole in the Impala compilation at the moment.,0,0,negative
impala,8073,description,The error in the log (attached) appears to be a connection to the HMS error. Some initial googling suggested that it might be the server closing the connection because of hitting a connection limit.  could you take a look and see if you have any ideas. I wonder if we're leaking HMS connections in this test somehow?,code_debt,low_quality_code,"Fri, 11 Jan 2019 21:16:14 +0000","Thu, 14 Mar 2019 14:34:58 +0000","Wed, 23 Jan 2019 04:58:59 +0000",978165,The error in the log (attached) appears to be a connection to the HMS error. Some initial googling suggested that it might be the server closing the connection because of hitting a connection limit. fredyw could you take a look and see if you have any ideas. I wonder if we're leaking HMS connections in this test somehow?,0.10825,0.10825,neutral
impala,8146,comment_3,"I think we should stage this differently - first make it possible to do everything without make_impala.sh, have an interim period where the script is still present so any downstream users have a chance to migrate their tools, then remove the old scripts.",code_debt,low_quality_code,"Wed, 30 Jan 2019 10:44:43 +0000","Tue, 5 Feb 2019 10:00:46 +0000","Tue, 5 Feb 2019 10:00:46 +0000",515763,"I think we should stage this differently - first make it possible to do everything without make_impala.sh, have an interim period where the script is still present so any downstream users have a chance to migrate their tools, then remove the old scripts.",0.17025,0.17025,neutral
impala,844,description,"We idiomatically do this for Thrift RPCs: Thrift can throw as well (see e.g. {{recv_*}} for any method). We don't catch it, and therefore can abort on the rare occasion it gets thrown. Instead we should catch {{TException}}.",code_debt,low_quality_code,"Fri, 28 Feb 2014 00:19:53 +0000","Wed, 5 Mar 2014 23:24:00 +0000","Wed, 5 Mar 2014 23:24:00 +0000",515047,"We idiomatically do this for Thrift RPCs: Thrift can throw TApplicationException as well (see e.g. recv_* for any method). We don't catch it, and therefore can abort on the rare occasion it gets thrown. Instead we should catch TException.",0.2103333333,0.2103333333,neutral
impala,8485,description,Running the command *git grep authz-policy* produces the following output: ** authz-policy.ini = % WAREHOUSE These references to the *authz-policy.ini* should be cleaned up as the authorization policy file feature is deprecated as of *IMPALA-7918.*,code_debt,dead_code,"Thu, 2 May 2019 22:39:43 +0000","Fri, 3 May 2019 21:26:14 +0000","Fri, 3 May 2019 20:31:51 +0000",78728,"Running the command git grep authz-policy produces the following output: ** bin/create-test-configuration.sh:generate_config authz-policy.ini.template authz-policy.ini fe/.gitignore:src/test/resources/authz-policy.ini tests/authorization/test_authorization.py:AUTH_POLICY_FILE = ""%s/authz-policy.ini"" % WAREHOUSE These references to the authz-policy.ini should be cleaned up as the authorization policy file feature is deprecated as of IMPALA-7918.",0,0,neutral
impala,8485,summary,References to deprecated feature authorization policy file need to be removed,code_debt,dead_code,"Thu, 2 May 2019 22:39:43 +0000","Fri, 3 May 2019 21:26:14 +0000","Fri, 3 May 2019 20:31:51 +0000",78728,References to deprecated feature authorization policy file need to be removed,0,0,negative
impala,8578,description,"metrics.h and other metric headers are included a lot of places and there is a lot of code in the header that has very few callers. It appears to be pulled into several hundred compilation units, increasing the compile time of each of those and forcing recompilation when the headers are changed. Some ideas: * Move function implementations to .cc files. E.g. ToJson() and ToPrometheus() don't need to be inlined. * Move MetricGroup to its own file * Try to see if we can use forward declarations in more places to avoid including it.",code_debt,low_quality_code,"Wed, 22 May 2019 20:28:03 +0000","Wed, 5 Jun 2019 05:38:32 +0000","Wed, 5 Jun 2019 05:38:32 +0000",1156229,"metrics.h and other metric headers are included a lot of places and there is a lot of code in the header that has very few callers. It appears to be pulled into several hundred compilation units, increasing the compile time of each of those and forcing recompilation when the headers are changed. Some ideas: Move function implementations to .cc files. E.g. ToJson() and ToPrometheus() don't need to be inlined. Move MetricGroup to its own file Try to see if we can use forward declarations in more places to avoid including it.",-0.02857142857,-0.02857142857,neutral
impala,8605,description,"Following on from IMPALA-8538 and IMPALA-1653, it would be nice to clean up some of the session management logic and add some more tests for the HS2 APIs - we have some testing gaps and a few of the invariants are unclear about what operations are allowed.",code_debt,low_quality_code,"Fri, 31 May 2019 00:07:22 +0000","Tue, 11 Jun 2019 23:03:47 +0000","Tue, 11 Jun 2019 23:03:47 +0000",1032985,"Following on from IMPALA-8538 and IMPALA-1653, it would be nice to clean up some of the session management logic and add some more tests for the HS2 APIs - we have some testing gaps and a few of the invariants are unclear about what operations are allowed.",0.3416666667,0.3416666667,neutral
impala,8605,summary,Clean up connection/session management,code_debt,low_quality_code,"Fri, 31 May 2019 00:07:22 +0000","Tue, 11 Jun 2019 23:03:47 +0000","Tue, 11 Jun 2019 23:03:47 +0000",1032985,Clean up connection/session management,0.4,0.4,neutral
impala,8626,description,"I noticed that the parameterized JDBC tests are passing on the dockerised cluster, which shouldn't be possible because IMPALA-8623 isn't done. The connection strings look identical in both cases: I was looking at related code and saw some misuse of == vs equals() for string comparison here But I don't think that explains what I'm seeing above.",code_debt,low_quality_code,"Wed, 5 Jun 2019 19:19:28 +0000","Tue, 18 Jun 2019 18:54:48 +0000","Tue, 18 Jun 2019 18:54:48 +0000",1121720,"I noticed that the parameterized JDBC tests are passing on the dockerised cluster, which shouldn't be possible because IMPALA-8623 isn't done. https://jenkins.impala.io/job/ubuntu-16.04-dockerised-tests/453/testReport/org.apache.impala.service/JdbcTest/ The connection strings look identical in both cases: I was looking at related code and saw some misuse of == vs equals() for string comparison here https://github.com/apache/impala/blob/master/fe/src/test/java/org/apache/impala/testutil/ImpalaJdbcClient.java#L172 But I don't think that explains what I'm seeing above.",-0.4375,-0.4375,negative
impala,8884,description,It would be useful for debugging I/O performance problems if we had histogram stats for the time taken for various operations so that we could see if there were slow operations on a particular disk (e.g. because of disk failure) or from a particular remote filesystem.,code_debt,low_quality_code,"Thu, 22 Aug 2019 22:09:12 +0000","Wed, 4 Mar 2020 03:10:43 +0000","Thu, 17 Oct 2019 00:52:46 +0000",4761814,It would be useful for debugging I/O performance problems if we had histogram stats for the time taken for various operations so that we could see if there were slow operations on a particular disk (e.g. because of disk failure) or from a particular remote filesystem.,-0.1,-0.1,neutral
impala,902,description,"We use YARN's to check if a user has access to a resource pool. If the user is not on the remote system, (Hadoop) will write a scary looking message and stack to the log. This is not actually a functional problem, but this spew may appear in impalad logs and should be hidden. e.g. when I connect to a remote impalad where I'm not a user on the local system, our logs will show:",code_debt,low_quality_code,"Wed, 19 Mar 2014 17:31:08 +0000","Sun, 20 Dec 2015 00:05:14 +0000","Thu, 5 Jun 2014 18:04:32 +0000",6741204,"We use YARN's AllocationConfiguration.hasAccess() to check if a user has access to a resource pool. If the user is not on the remote system, (Hadoop) ShellBasedUnixGroupsMapping will write a scary looking message and stack to the log. This is not actually a functional problem, but this spew may appear in impalad logs and should be hidden. e.g. when I connect to a remote impalad where I'm not a user on the local system, our logs will show:",-0.05625,-0.045,neutral
impala,902,summary,Hide scary log spew warning from Hadoop when checking resource pool access,code_debt,low_quality_code,"Wed, 19 Mar 2014 17:31:08 +0000","Sun, 20 Dec 2015 00:05:14 +0000","Thu, 5 Jun 2014 18:04:32 +0000",6741204,Hide scary log spew warning from Hadoop when checking resource pool access,-0.475,-0.475,negative
impala,92,description,"I'm running the following two queries. The only difference between them is I'm using ""LIKE"" in one case and ""="" in another, though there is no ""%"" in the LIKE, so the effect is the same. I was surprised to see approximately a 10x difference in performance between them. I'm running I've attached the two query profiles. The basic difference is in the execution rate: Obviously I've fixed my query.",code_debt,slow_algorithm,"Sun, 24 Feb 2013 00:48:45 +0000","Thu, 16 Mar 2017 04:03:32 +0000","Fri, 1 Mar 2013 22:09:39 +0000",508854,"I'm running the following two queries. The only difference between them is I'm using ""LIKE"" in one case and ""="" in another, though there is no ""%"" in the LIKE, so the effect is the same. I was surprised to see approximately a 10x difference in performance between them. I'm running I've attached the two query profiles. The basic difference is in the execution rate: Obviously I've fixed my query.",0,0,neutral
impala,92,summary,Significant performance difference between LIKE = 'x' AND = 'x',code_debt,slow_algorithm,"Sun, 24 Feb 2013 00:48:45 +0000","Thu, 16 Mar 2017 04:03:32 +0000","Fri, 1 Mar 2013 22:09:39 +0000",508854,Significant performance difference between LIKE = 'x' AND = 'x',0.2,0.2,neutral
impala,9373,comment_0,"Notes so far: * In many cases it recommends including internal headers instead of the public-facing header * It gets confused by ""using"" statements in headers, e.g. it thinks is needed for references to ""string"" * The recommendations are mostly pretty good, but there were various small misfires, e.g. recommendations that didn't work or match our coding standards",code_debt,low_quality_code,"Tue, 11 Feb 2020 17:04:39 +0000","Wed, 25 Mar 2020 16:36:22 +0000","Wed, 25 Mar 2020 16:36:22 +0000",3713503,"Notes so far: In many cases it recommends including internal headers instead of the public-facing header It gets confused by ""using"" statements in headers, e.g. it thinks gutil/strings/substitute.h is needed for references to ""string"" The recommendations are mostly pretty good, but there were various small misfires, e.g. recommendations that didn't work or match our coding standards",0.0752,0.06266666667,neutral
impala,946,comment_0,"I think this is a duplicate of IMPALA-428. The time is being spent loading the table metadata from the Hive Metastore. I believe most of the time is spent loading column stats, but I need to re-run experiments to confirm.",code_debt,slow_algorithm,"Fri, 11 Apr 2014 22:34:38 +0000","Fri, 11 Apr 2014 23:43:29 +0000","Fri, 11 Apr 2014 22:55:24 +0000",1246,"I think this is a duplicate of IMPALA-428. The time is being spent loading the table metadata from the Hive Metastore. I believe most of the time is spent loading column stats, but I need to re-run experiments to confirm.",0,0,neutral
impala,946,description,"I ran a simple experiment to test the performance of various FE steps and noticed that loading the table metadata takes a ""long"" time, roughly linear in the number of columns. I performed the following series of actions for every data point: I ran this test for various N and the table below shows the total query time as reported by the Impala shell (I ran with impala-shell.py -f): A simple script to generate the SQL as above is attached.",code_debt,slow_algorithm,"Fri, 11 Apr 2014 22:34:38 +0000","Fri, 11 Apr 2014 23:43:29 +0000","Fri, 11 Apr 2014 22:55:24 +0000",1246,"I ran a simple experiment to test the performance of various FE steps and noticed that loading the table metadata takes a ""long"" time, roughly linear in the number of columns. I performed the following series of actions for every data point: I ran this test for various N and the table below shows the total query time as reported by the Impala shell (I ran with impala-shell.py -f): A simple script to generate the SQL as above is attached.",0,0,neutral
impala,946,summary,Loading the metadata of wide tables takes a long time (roughly linear in the number of columns).,code_debt,slow_algorithm,"Fri, 11 Apr 2014 22:34:38 +0000","Fri, 11 Apr 2014 23:43:29 +0000","Fri, 11 Apr 2014 22:55:24 +0000",1246,Loading the metadata of wide tables takes a long time (roughly linear in the number of columns).,0,0,neutral
impala,9543,description,"Reduce duplicate code in thrift CMakeLists.txt. And if in future, we change hive to version 4 or higher. This can adapt automatically.",code_debt,duplicated_code,"Mon, 23 Mar 2020 09:09:12 +0000","Thu, 9 Apr 2020 02:36:53 +0000","Thu, 9 Apr 2020 02:03:24 +0000",1443252,"Reduce duplicate code in thrift CMakeLists.txt. And if in future, we change hive to version 4 or higher. This can adapt automatically.",0,0,neutral
impala,9543,summary,Reduce duplicate code in thrift CMakeLists.txt,code_debt,duplicated_code,"Mon, 23 Mar 2020 09:09:12 +0000","Thu, 9 Apr 2020 02:36:53 +0000","Thu, 9 Apr 2020 02:03:24 +0000",1443252,Reduce duplicate code in thrift CMakeLists.txt,0,0,neutral
impala,3099,comment_0,I think we should hold off on this one - the {{rpc_pool}} is going to be mostly superseded by async rpcs when we switch to KRPC.,defect_debt,uncorrected_known_defects,"Sun, 28 Feb 2016 22:35:19 +0000","Tue, 30 Apr 2019 20:01:22 +0000","Thu, 12 Jan 2017 20:33:47 +0000",27554308,I think we should hold off on this one - the rpc_pool is going to be mostly superseded by async rpcs when we switch to KRPC.,0,0,neutral
impala,101,description,"The query log currently contains the query but not the current database. This makes it very hard to figure out what was run, particularly in our cluster setup where the db encodes the scale factor. We should just add this to the query log.",design_debt,non-optimal_design,"Fri, 1 Mar 2013 21:52:53 +0000","Thu, 11 Apr 2013 19:24:36 +0000","Thu, 11 Apr 2013 19:24:35 +0000",3533502,"The query log currently contains the query but not the current database. This makes it very hard to figure out what was run, particularly in our cluster setup where the db encodes the scale factor. We should just add this to the query log.",-0.06666666667,-0.06666666667,negative
impala,1118,comment_2,Fixing this would make query gen testing easier.,design_debt,non-optimal_design,"Mon, 28 Jul 2014 21:00:54 +0000","Thu, 14 May 2015 22:33:54 +0000","Wed, 24 Sep 2014 15:49:46 +0000",4992532,Fixing this would make query gen testing easier.,0,0,neutral
impala,137,description,"As an example: (Build version: Impala v0.7 (06b3f21) built on Wed Mar 13 16:54:46 PDT 2013) [localhost:21000] Query: select * from alltypes limit 10 ERROR: Analysis exception (in select * from alltypes limit 10) Caused by: Unknown table: 'alltypes' ... 2 more The error here is ""Unknown table: 'alltypes'"" but we dump all this stuff that is not user friendly. This might be useful for development but we should have the default not show the stack traces.",design_debt,non-optimal_design,"Thu, 14 Mar 2013 20:47:02 +0000","Mon, 28 Dec 2015 17:43:33 +0000","Thu, 14 Mar 2013 20:49:26 +0000",144,"As an example: (Build version: Impala v0.7 (06b3f21) built on Wed Mar 13 16:54:46 PDT 2013) [localhost:21000] > select * from alltypes limit 10; Query: select * from alltypes limit 10 ERROR: com.cloudera.impala.common.AnalysisException: Analysis exception (in select * from alltypes limit 10) at com.cloudera.impala.analysis.AnalysisContext.analyze(AnalysisContext.java:177) at com.cloudera.impala.service.Frontend.createExecRequest(Frontend.java:287) at com.cloudera.impala.service.JniFrontend.createExecRequest(JniFrontend.java:106) Caused by: com.cloudera.impala.common.AnalysisException: Unknown table: 'alltypes' at com.cloudera.impala.analysis.Analyzer.registerBaseTableRef(Analyzer.java:178) at com.cloudera.impala.analysis.BaseTableRef.analyze(BaseTableRef.java:51) at com.cloudera.impala.analysis.SelectStmt.analyze(SelectStmt.java:115) at com.cloudera.impala.analysis.AnalysisContext.analyze(AnalysisContext.java:174) ... 2 more The error here is ""Unknown table: 'alltypes'"" but we dump all this stuff that is not user friendly. This might be useful for development but we should have the default not show the stack traces.",-0.026,0.007544654088,neutral
impala,1382,description,We currently preallocate the null tuple indicator bitstring in each tuple stream assuming that very few tuples will be NULL. That can lead to lots of wasted space in the buffer if there are many NULL tuples in the stream. A possible solution is to use a slotted page (buffer) with NULL indicators growing from the end of the page (buffer).,design_debt,non-optimal_design,"Fri, 10 Oct 2014 18:29:51 +0000","Sat, 5 Aug 2017 03:21:22 +0000","Sat, 5 Aug 2017 03:21:22 +0000",88937491,We currently preallocate the null tuple indicator bitstring in each tuple stream assuming that very few tuples will be NULL. That can lead to lots of wasted space in the buffer if there are many NULL tuples in the stream. A possible solution is to use a slotted page (buffer) with NULL indicators growing from the end of the page (buffer).,-0.4,-0.4,neutral
impala,1382,summary,Wasted space in in presence of many NULL tuples,design_debt,non-optimal_design,"Fri, 10 Oct 2014 18:29:51 +0000","Sat, 5 Aug 2017 03:21:22 +0000","Sat, 5 Aug 2017 03:21:22 +0000",88937491,Wasted space in buffered-tuple-stream in presence of many NULL tuples,-0.4,-0.4,negative
impala,1430,description,"Currently codegen is disabled for the entire aggregation operator if a single aggregate function can't be codegen'd. We should address this by making it so all aggregate functions can be codegen'd, including UDAs. For UDAs in .so's, the codegen'd function will call into the UDA library. This also affects aggregation operator on timestamp. This perf hit can be especially bad for COMPUTE STATS which is heavily CPU bound on the aggregation and because there is no easy way to exclude the TIMESTAMP columns when computing the column stats (i.e., there is no simple workaround). Even if the portions involving TIMESTAMP cannot be codegen'd it would still be worthwhile to come up with a workaround where codegen for the other types is still enabled. *Workaround* If you are experiencing very slow COMPUTE STATS due to this issue, then you may be able to temporarily ALTER the TIMESTAMP columns to STRING or INT type before running COMPUTE STATS. After the command completed, the columns can be altered back to TIMESTAMP. Note the workaround is only apply to text data, not parquet data. parquet require compatibles data type. TIMESTAMP is INT96, it's not compatible with STRING or BIGINT.",design_debt,non-optimal_design,"Tue, 28 Oct 2014 22:43:15 +0000","Thu, 30 Aug 2018 18:46:14 +0000","Wed, 15 Feb 2017 06:06:13 +0000",72602578,"Currently codegen is disabled for the entire aggregation operator if a single aggregate function can't be codegen'd. We should address this by making it so all aggregate functions can be codegen'd, including UDAs. For UDAs in .so's, the codegen'd function will call into the UDA library. This also affects aggregation operator on timestamp. This perf hit can be especially bad for COMPUTE STATS which is heavily CPU bound on the aggregation and because there is no easy way to exclude the TIMESTAMP columns when computing the column stats (i.e., there is no simple workaround). Even if the portions involving TIMESTAMP cannot be codegen'd it would still be worthwhile to come up with a workaround where codegen for the other types is still enabled. Workaround If you are experiencing very slow COMPUTE STATS due to this issue, then you may be able to temporarily ALTER the TIMESTAMP columns to STRING or INT type before running COMPUTE STATS. After the command completed, the columns can be altered back to TIMESTAMP. Note the workaround is only apply to text data, not parquet data. parquet require compatibles data type. TIMESTAMP is INT96, it's not compatible with STRING or BIGINT.",0.02594444444,0.02594444444,negative
impala,1487,description,"It would be useful to have a query option to return more detailed error information to the shell/client. For example, including dlerror strings and stack traces. We don't do this by default because in many cases it makes errors needlessly verbose and confusing.",design_debt,non-optimal_design,"Tue, 18 Nov 2014 23:21:04 +0000","Sat, 3 Nov 2018 00:25:58 +0000","Sat, 3 Nov 2018 00:25:58 +0000",124851894,"It would be useful to have a query option to return more detailed error information to the shell/client. For example, including dlerror strings and stack traces. We don't do this by default because in many cases it makes errors needlessly verbose and confusing.",0.04311111111,0.04311111111,neutral
impala,148,description,"There exists documentation that explains the steps to configure short-circuit reads however, several occurrences of performance issues directly related to this misconfiguration have come up. To help avoid situations where the system is running in sub-optimal mode, a validation check could be put in place that would make the startup of impalad fail if the configuration settings are not correct. This would help prevent people from running unknowingly in a degraded mode.",design_debt,non-optimal_design,"Tue, 19 Mar 2013 00:37:18 +0000","Sun, 20 Dec 2015 00:04:59 +0000","Fri, 29 Mar 2013 05:27:43 +0000",881425,"There exists documentation that explains the steps to configure short-circuit reads https://ccp.cloudera.com/display/IMPALA10BETADOC/Configuring+Impala+for+Performance however, several occurrences of performance issues directly related to this misconfiguration have come up. To help avoid situations where the system is running in sub-optimal mode, a validation check could be put in place that would make the startup of impalad fail if the configuration settings are not correct. This would help prevent people from running unknowingly in a degraded mode.",-0.02291666667,-0.02291666667,neutral
impala,1493,comment_1,"It looks like almost all boost timestamp functions can throw. We can either try catch each of those or do it at a higher level up, e.g. scalar fn call. Putting it higher up is a bit tricky since we codegen scalar fn call.",design_debt,non-optimal_design,"Wed, 19 Nov 2014 02:25:09 +0000","Fri, 21 Nov 2014 19:27:26 +0000","Fri, 21 Nov 2014 19:27:26 +0000",234137,"It looks like almost all boost timestamp functions can throw. We can either try catch each of those or do it at a higher level up, e.g. scalar fn call. Putting it higher up is a bit tricky since we codegen scalar fn call.",0.2333333333,0.2333333333,neutral
impala,153,description,"All of our logging and the content in the runtime profile uses fragment/query ids. It's currently not very easy to map those to host names. This makes diagnosing issues unnecessarily difficult. e.g. The log says fragment instance foo failed, we don't have an easy way to know which node to go to for the log/more details.",design_debt,non-optimal_design,"Tue, 19 Mar 2013 18:42:31 +0000","Mon, 8 Apr 2013 04:57:20 +0000","Mon, 8 Apr 2013 04:57:20 +0000",1678489,"All of our logging and the content in the runtime profile uses fragment/query ids. It's currently not very easy to map those to host names. This makes diagnosing issues unnecessarily difficult. e.g. The log says fragment instance foo failed, we don't have an easy way to know which node to go to for the log/more details.",0.085,0.085,negative
impala,1598,description,"Some error conditions cause a high volume of output that gets sent to the users, particularly if each backend produces several messages that are aggregated by the coordinator. An example is the Parquet error message when a file spans several blocks: you get one warning per file, all of which are communicating roughly the same thing. Unfortunately we can't just do simple string-matching to de-duplicate the messages, because they're all slightly different (filename changes). We should add an error code to each user-facing message, and use that as the unique key with which to aggregate the messages when there are lots of them.",design_debt,non-optimal_design,"Thu, 11 Dec 2014 00:57:34 +0000","Sun, 1 Mar 2015 05:29:32 +0000","Sun, 1 Mar 2015 05:29:32 +0000",6928318,"Some error conditions cause a high volume of output that gets sent to the users, particularly if each backend produces several messages that are aggregated by the coordinator. An example is the Parquet error message when a file spans several blocks: you get one warning per file, all of which are communicating roughly the same thing. Unfortunately we can't just do simple string-matching to de-duplicate the messages, because they're all slightly different (filename changes). We should add an error code to each user-facing message, and use that as the unique key with which to aggregate the messages when there are lots of them.",-0.375,-0.375,negative
impala,1618,comment_1,"I'm going to close this, this has been implemented in IMPALA-8819. The fix is specific to result spooling = true), but I think that is okay.",design_debt,non-optimal_design,"Wed, 17 Dec 2014 21:44:47 +0000","Thu, 14 May 2020 17:44:56 +0000","Thu, 29 Aug 2019 20:19:54 +0000",148257307,"I'm going to close this, this has been implemented inIMPALA-8819. The fix is specific to result spooling (spool_query_results = true), but I think that is okay.",0.34625,0.34625,neutral
impala,1651,description,"Currently, CREATE TABLE LIKE 'blindly' copies the source table, including the table's and partitions' parameter maps. The parameter maps may contain information about caching directives that do not apply to the new table. I can see two acceptable behaviors (choose one) if the source table is cached: 1. the new table is also cached; we need to issue new caching directives and store them in the params of the new table as appropriate 2. the new table is not cached and must be cached explicitly",design_debt,non-optimal_design,"Fri, 9 Jan 2015 18:05:43 +0000","Wed, 4 Jan 2017 23:58:08 +0000","Thu, 28 Jan 2016 14:04:28 +0000",33163125,"Currently, CREATE TABLE LIKE 'blindly' copies the source table, including the table's and partitions' parameter maps. The parameter maps may contain information about caching directives that do not apply to the new table. I can see two acceptable behaviors (choose one) if the source table is cached: 1. the new table is also cached; we need to issue new caching directives and store them in the params of the new table as appropriate 2. the new table is not cached and must be cached explicitly",0.1812,0.1812,neutral
impala,1691,comment_1,We should get rid of the cached {{Partition}} object inside {{HdfsPartition}}. I'm pretty sure we can reconstruct it from the partition itself when needed (i.e. when sending updates to the HMS).,design_debt,non-optimal_design,"Fri, 23 Jan 2015 21:27:05 +0000","Tue, 3 Mar 2015 17:42:45 +0000","Tue, 3 Mar 2015 17:42:45 +0000",3356140,We should get rid of the cached Partition object inside HdfsPartition. I'm pretty sure we can reconstruct it from the partition itself when needed (i.e. when sending updates to the HMS).,-0.181,-0.181,neutral
impala,1691,comment_2,I just looked at a catalogd instance with a large table loaded in a profiler. This seems to bear out the idea that the arrays of {{FieldSchema}} in the take a lot of memory. take up about 1.1GB of heap even with a relatively small catalog.,design_debt,non-optimal_design,"Fri, 23 Jan 2015 21:27:05 +0000","Tue, 3 Mar 2015 17:42:45 +0000","Tue, 3 Mar 2015 17:42:45 +0000",3356140,I just looked at a catalogd instance with a large table loaded in a profiler. This seems to bear out the idea that the arrays of FieldSchema in the StorageDescriptor take a lot of memory. StorageDescriptors take up about 1.1GB of heap even with a relatively small catalog.,0,0,neutral
impala,1691,description,"It used to be the case that the entire Catalogd's memory usage won't go over 1~2GB. However, a single table with 1000 columns and 8000 partitions used up 1.3GB of memory in the catalogd. The attached script created such a table. To see the memory usage, start catalogd without background loading. Observe the initial memory usage. Then run ""describe"" to load the table and observe the memory usage again.",design_debt,non-optimal_design,"Fri, 23 Jan 2015 21:27:05 +0000","Tue, 3 Mar 2015 17:42:45 +0000","Tue, 3 Mar 2015 17:42:45 +0000",3356140,"It used to be the case that the entire Catalogd's memory usage won't go over 1~2GB. However, a single table with 1000 columns and 8000 partitions used up 1.3GB of memory in the catalogd. The attached script created such a table. To see the memory usage, start catalogd without background loading. Observe the initial memory usage. Then run ""describe"" to load the table and observe the memory usage again.",-0.08333333333,-0.08333333333,neutral
impala,1691,summary,Excessive Memory Usage in Catalogd (without stats),design_debt,non-optimal_design,"Fri, 23 Jan 2015 21:27:05 +0000","Tue, 3 Mar 2015 17:42:45 +0000","Tue, 3 Mar 2015 17:42:45 +0000",3356140,Excessive Memory Usage in Catalogd (without stats),0,0,neutral
impala,1697,comment_0,"With our current metadata/loading infrastructure this seemingly simple change is actually tricky to implement. Since the initial catalog update only contains the names of all databases and tables, but not their types implementing SHOW TABLES would force us to load all the table metadata from the catalogd for those tables. Since table metadata loading is rather expensive, the SHOW TABLES command could have a very high latency which is probably incomprehensible to users. As an alternative, we could display the type as ""UNKNOWN"" for tables that have not yet been loaded, but again, this seems incomprehensible to users. Needless to say, we should consider this use case when making architectural changes to the catalog/metadata infra.",design_debt,non-optimal_design,"Mon, 26 Jan 2015 16:45:21 +0000","Fri, 11 Aug 2017 21:55:58 +0000","Fri, 11 Aug 2017 21:55:58 +0000",80197837,"With our current metadata/loading infrastructure this seemingly simple change is actually tricky to implement. Since the initial catalog update only contains the names of all databases and tables, but not their types implementing SHOW TABLES would force us to load all the table metadata from the catalogd for those tables. Since table metadata loading is rather expensive, the SHOW TABLES command could have a very high latency which is probably incomprehensible to users. As an alternative, we could display the type as ""UNKNOWN"" for tables that have not yet been loaded, but again, this seems incomprehensible to users. Needless to say, we should consider this use case when making architectural changes to the catalog/metadata infra.",-0.38,-0.38,negative
impala,187,comment_0,The way joins are performed right now in Impala (broadcast of right-hand side input) is a limitation that will be addressed by adding more join strategies (and having the planner chose between them). It's not a good idea to address that via additional syntax that otherwise doesn't really make sense.,design_debt,non-optimal_design,"Wed, 3 Apr 2013 00:06:53 +0000","Wed, 3 Apr 2013 00:13:59 +0000","Wed, 3 Apr 2013 00:13:59 +0000",426,The way joins are performed right now in Impala (broadcast of right-hand side input) is a limitation that will be addressed by adding more join strategies (and having the planner chose between them). It's not a good idea to address that via additional syntax that otherwise doesn't really make sense.,-0.2326,-0.2326,negative
impala,1927,description,"I have a CSV table with 14 columns, about 44 million rows. I found that with SELECT * and certain conditions in the WHERE clause, the shell would seem to output all the results (I would see the ++ line at the bottom of the result table) but then would hang and never return to the impala-shell prompt. If I SELECT COUNT(*) with the same WHERE clause, it works. If I SELECT DISTINCT <one of the columnsIf I do a CTAS into a Parquet table with SELECT * from the original table (no WHERE clause), it works. However, if I do the same query via 'impala-shell -q' or with delimited results via 'impala-shell -B -q', it hangs the same way. When I hit Ctrl-C, the resulting message is: ^C Cancelling Query Failed to reconnect and close: ERROR: Cancelled Cancelling Query which is different than what I saw when cancelling other SELECT or INSERT statements that were still in progress. This seems like the query has finished but the shell doesn't close it properly. The equivalent COUNT(*) and DISTINCT queries finish in 1-2 seconds. I let the SELECT * run for 3 minutes or more and it stays stuck. It's possible there is some anomaly somewhere in the CSV files, but like I say I can CTAS the whole contents of the table. It's only outputting in the shell that stalls. Schema, stalled query, actual data, profiles, logs all available on the Cloudera network for diagnosis. (Ping me for the location.)",design_debt,non-optimal_design,"Tue, 31 Mar 2015 06:10:55 +0000","Wed, 12 Oct 2016 05:52:09 +0000","Wed, 12 Oct 2016 05:52:09 +0000",48469274,"I have a CSV table with 14 columns, about 44 million rows. I found that with SELECT * and certain conditions in the WHERE clause, the shell would seem to output all the results (I would see the ------ line at the bottom of the result table) but then would hang and never return to the impala-shell prompt. If I SELECT COUNT with the same WHERE clause, it works. If I SELECT DISTINCT <one of the columns> with the same where clause, it works. If I do a CTAS into a Parquet table with SELECT * from the original table (no WHERE clause), it works. However, if I do the same query via 'impala-shell -q' or with delimited results via 'impala-shell -B -q', it hangs the same way. When I hit Ctrl-C, the resulting message is: ^C Cancelling Query Failed to reconnect and close: ERROR: Cancelled Cancelling Query which is different than what I saw when cancelling other SELECT or INSERT statements that were still in progress. This seems like the query has finished but the shell doesn't close it properly. The equivalent COUNT and DISTINCT queries finish in 1-2 seconds. I let the SELECT * run for 3 minutes or more and it stays stuck. It's possible there is some anomaly somewhere in the CSV files, but like I say I can CTAS the whole contents of the table. It's only outputting in the shell that stalls. Schema, stalled query, actual data, profiles, logs all available on the Cloudera network for diagnosis. (Ping me for the location.)",0.1357282051,0.1709857143,neutral
impala,1934,description,"When LDAP authentication is enabled in Impala, impala-shell always prompts for a password when trying to connect to an Impala daemon, which makes it very hard to use impala-shell in a shell script (where to store the password?) Please make impala-shell support reading password from a command line parameter or from a password file, just like what beeline does.",design_debt,non-optimal_design,"Thu, 2 Apr 2015 00:08:08 +0000","Thu, 7 Apr 2016 17:40:54 +0000","Wed, 20 Jan 2016 01:22:12 +0000",25319644,"When LDAP authentication is enabled in Impala, impala-shell always prompts for a password when trying to connect to an Impala daemon, which makes it very hard to use impala-shell in a shell script (where to store the password?) Please make impala-shell support reading password from a command line parameter or from a password file, just like what beeline does.",-0.025,-0.025,negative
impala,1963,comment_2,"For my current project, I only really need it to handle the Z properly. Anything else (other timezones, full support) would be a bonus. In terms of Tableau/Impala usability - this problem will come up for anyone with a format that doesn't convert automatically, so the more coverage we get the better. However, in terms of release schedules, I would love to have time zones or at least Zulu time working sooner.",design_debt,non-optimal_design,"Wed, 22 Apr 2015 15:03:34 +0000","Fri, 5 Jun 2015 04:47:07 +0000","Fri, 5 Jun 2015 04:47:07 +0000",3764613,"For my current project, I only really need it to handle the Z properly. Anything else (other timezones, full support) would be a bonus. In terms of Tableau/Impala usability - this problem will come up for anyone with a format that doesn't convert automatically, so the more coverage we get the better. However, in terms of release schedules, I would love to have time zones or at least Zulu time working sooner.",0.27125,0.27125,neutral
impala,2068,description,"It is currently difficult for clients connected via load balancers to know which coordinator they're connected to. The TOpenSessionResp has a map of configuration settings which are returned, and we can return the connected coordinator in that map. Clients such as Hue can use that information to find the coordinator and thus the debug web UI pages for submitted queries over that session.",design_debt,non-optimal_design,"Mon, 15 Jun 2015 18:06:02 +0000","Fri, 19 Jun 2015 18:44:56 +0000","Fri, 19 Jun 2015 18:44:56 +0000",347934,"It is currently difficult for clients connected via load balancers to know which coordinator they're connected to. The TOpenSessionResp has a map of configuration settings which are returned, and we can return the connected coordinator in that map. Clients such as Hue can use that information to find the coordinator and thus the debug web UI pages for submitted queries over that session.",-0.06666666667,-0.06666666667,neutral
impala,2076,description,"*Problem Statement* The ExecSummary does not report the time spent on the network. The time reported on the EXCHANGE node is misleading and does not count network time. This makes it impossible to identify the performance bottleneck. *Cause* The ExecSummary reports the non-waiting time spent in the EXCHANGE as the network time. Obviously, the active time spent in the EXCHANGE is zero (or close to zero). It should be replaced by the corresponding Data Stream Sender total time instead. *Workaround* Examine the corresponding time in the Data Stream Sender instead.",design_debt,non-optimal_design,"Thu, 18 Jun 2015 02:08:59 +0000","Thu, 28 Apr 2016 23:57:23 +0000","Thu, 21 Apr 2016 18:24:19 +0000",26669720,"Problem Statement The ExecSummary does not report the time spent on the network. The time reported on the EXCHANGE node is misleading and does not count network time. This makes it impossible to identify the performance bottleneck. Cause The ExecSummary reports the non-waiting time spent in the EXCHANGE as the network time. Obviously, the active time spent in the EXCHANGE is zero (or close to zero). It should be replaced by the corresponding Data Stream Sender total time instead. Workaround Examine the corresponding time in the Data Stream Sender instead.",-0.1357142857,-0.1357142857,negative
impala,211,comment_2,"I think it is useful to log it once, but logging it for each impalad every 1/2 second seems excessive.",design_debt,non-optimal_design,"Tue, 9 Apr 2013 16:16:36 +0000","Thu, 11 Apr 2013 06:36:45 +0000","Thu, 11 Apr 2013 06:36:45 +0000",138009,"I think it is useful to log it once, but logging it for each impalad every 1/2 second seems excessive.",0.5,0.5,neutral
impala,2128,description,The current partitioned HJ and Agg implementation does not check what's the size of the hash table that it is going to be created. There are cases where the hash table of a partition to be needed to be larger than 1GB. In that case due to IMPALA-1619 we have to fail the query (for more info look the work-around IMPALA-2065). Instead of failing the query we may try to repartition that large partition in an effort to create HTs smaller than 1GB. For example in,design_debt,non-optimal_design,"Tue, 7 Jul 2015 22:25:38 +0000","Wed, 4 Jan 2017 23:58:07 +0000","Fri, 10 Jul 2015 20:09:41 +0000",251043,The current partitioned HJ and Agg implementation does not check what's the size of the hash table that it is going to be created. There are cases where the hash table of a partition to be needed to be larger than 1GB. In that case due to IMPALA-1619 we have to fail the query (for more info look the work-around IMPALA-2065). Instead of failing the query we may try to repartition that large partition in an effort to create HTs smaller than 1GB. For example in PartitionedHashJoinNode::Partition::BuildHashTableInternal,-0.06,0.06,neutral
impala,2174,description,"Whenever we serialize a row batch, even a row batch with 0 materialized slots, we always allocate an array of tuple_offsets per tuple. That means that there is a serialization overhead of 4B per tuple (per row). Currently we do not consider this overhead when we calculate the and consequently the avgRowSize_ which is used for example when we decide which input to We should take into account this overhead. Such a change may affect plans of queries with small avgRowSize_ or multiple tuples (joins).",design_debt,non-optimal_design,"Mon, 3 Aug 2015 20:54:01 +0000","Sat, 6 Feb 2016 20:11:30 +0000","Sat, 6 Feb 2016 20:11:20 +0000",16154239,"Whenever we serialize a row batch, even a row batch with 0 materialized slots, we always allocate an array of tuple_offsets per tuple. That means that there is a serialization overhead of 4B per tuple (per row). Currently we do not consider this overhead when we calculate the TupleDescriptor::avgSerializedSize_ and consequently the avgRowSize_ which is used for example when we decide which input to broadcast/distribute. We should take into account this overhead. Such a change may affect plans of queries with small avgRowSize_ or multiple tuples (joins).",0,0,neutral
impala,2208,comment_1,"Up until now Impala does not use the min/max metadata of Parquet so we don't need to backport it. But it is in our roadmap to exploit them, should that happen we need to be very careful and consider PARQUET-251.",design_debt,non-optimal_design,"Fri, 14 Aug 2015 06:04:56 +0000","Sun, 20 Dec 2015 00:05:33 +0000","Fri, 14 Aug 2015 16:19:11 +0000",36855,"Up until now Impala does not use the min/max metadata of Parquet so we don't need to backport it. But it is in our roadmap to exploit them, should that happen we need to be very careful and consider PARQUET-251.",0,0,neutral
impala,2212,description,"When beeline is accessing the impala, the output is misaligned.",design_debt,non-optimal_design,"Mon, 17 Aug 2015 17:21:51 +0000","Tue, 18 Aug 2015 17:11:21 +0000","Tue, 18 Aug 2015 17:11:21 +0000",85770,"When beeline is accessing the impala, the output is misaligned.",0,0,neutral
impala,2212,summary,Beeline output doesn't align properly when accessing impala,design_debt,non-optimal_design,"Mon, 17 Aug 2015 17:21:51 +0000","Tue, 18 Aug 2015 17:11:21 +0000","Tue, 18 Aug 2015 17:11:21 +0000",85770,Beeline output doesn't align properly when accessing impala,0,0,negative
impala,2295,description,doesn't deep copy collections. This leaves the destination Tuple with pointers into potentially invalid memory.,design_debt,non-optimal_design,"Thu, 3 Sep 2015 20:40:00 +0000","Wed, 9 Sep 2015 02:48:07 +0000","Wed, 9 Sep 2015 02:48:07 +0000",454087,BufferedTupleStream::DeepCopyInternal doesn't deep copy collections. This leaves the destination Tuple with pointers into potentially invalid memory.,0,0.3,negative
impala,242,description,"After cancelling a query in impala-shell with Control-C, the cancelled query remains in the executing queries section at the top of the debug UI web page and subsequently started queries end up sorted below it, which is contrary to the desired sorting order by start time and inconvenient when webex-ing with limit screen real estate as it makes it harder to see the latest test query status without scrolling to get past these cancelled queries.",design_debt,non-optimal_design,"Fri, 12 Apr 2013 22:58:53 +0000","Sun, 20 Dec 2015 00:05:00 +0000","Tue, 23 Apr 2013 01:54:50 +0000",874557,"After cancelling a query in impala-shell with Control-C, the cancelled query remains in the executing queries section at the top of the impalad:25000/queries debug UI web page and subsequently started queries end up sorted below it, which is contrary to the desired sorting order by start time and inconvenient when webex-ing with limit screen real estate as it makes it harder to see the latest test query status without scrolling to get past these cancelled queries.",-0.025,-0.025,negative
impala,2435,description,"While I was reviewing Dimitris's IMPALA-2369 patch, I realized that in AddBatch() we call Run::Init() without checking the return status of it. That means that Dimitris's patch probably won't do much good :) At Instead of creating a different patch, I will let Dimitris add it to his IMPALA-2369 patch.",design_debt,non-optimal_design,"Sat, 26 Sep 2015 05:44:45 +0000","Sun, 20 Dec 2015 00:05:37 +0000","Tue, 29 Sep 2015 19:09:03 +0000",307458,"While I was reviewing Dimitris's IMPALA-2369 patch, I realized that in AddBatch() we call Run::Init() without checking the return status of it. That means that Dimitris's patch probably won't do much good At https://github.com/cloudera/Impala/blob/cdh5-trunk/be/src/runtime/sorter.cc#L1001 Instead of creating a different patch, I will let Dimitris add it to his IMPALA-2369 patch.",0.2,0,negative
impala,2632,description,"Currently impalad needs external LLVM IR files to be compiled separately from the binary and distributed with the binary. This slows down builds, make it easy for the code to get out of sync in dev environments, and complicates distribution. Instead, we could simply link the IR data into the binary file, so impalad is more self-contained. These IR files are also loaded from disk by codegen for each fragment. Linking the IR into the binary would avoid this step, saving some overhead. Could also address IMPALA-799 (.bc file extension instead of .ll extension) at same time.",design_debt,non-optimal_design,"Wed, 4 Nov 2015 17:35:04 +0000","Fri, 13 Nov 2015 21:44:20 +0000","Fri, 13 Nov 2015 21:44:20 +0000",792556,"Currently impalad needs external LLVM IR files to be compiled separately from the binary and distributed with the binary. This slows down builds, make it easy for the code to get out of sync in dev environments, and complicates distribution. Instead, we could simply link the IR data into the binary file, so impalad is more self-contained. These IR files are also loaded from disk by codegen for each fragment. Linking the IR into the binary would avoid this step, saving some overhead. Could also address IMPALA-799 (.bc file extension instead of .ll extension) at same time.",0.03958333333,0.03958333333,neutral
impala,2657,comment_2,"Turns out CM already computes this based on the sampling counters already in the profile. The ""MemUsage"" profile timeseries counter keeps a max of 64 samples, downsampling after 64 are collected to make room for more. While this isn't really a perfect calculation of memory accrual, it's probably more than sufficient. We can reopen this if we find we need something more sophisticated.",design_debt,non-optimal_design,"Tue, 10 Nov 2015 05:11:10 +0000","Wed, 4 Jan 2017 23:58:13 +0000","Fri, 5 Feb 2016 20:18:59 +0000",7571269,"Turns out CM already computes this based on the sampling counters already in the profile. The ""MemUsage"" profile timeseries counter keeps a max of 64 samples, downsampling after 64 are collected to make room for more. While this isn't really a perfect calculation of memory accrual, it's probably more than sufficient. We can reopen this if we find we need something more sophisticated.",0.151125,0.151125,neutral
impala,2659,comment_1,"I did some further digging. It seems to be okay to relocate the driver as long as the companion files and {{cacerts.pem}} are relocated to the same directory as well. If is missing, you get curious error messages and you need to use the {{LD_PRELOAD}} hack. A better error message in this case ({{File SimbaImpalaODBC.did could not be found}}) would have helped a lot.",design_debt,non-optimal_design,"Tue, 10 Nov 2015 13:37:00 +0000","Mon, 12 Jun 2017 20:55:09 +0000","Mon, 12 Jun 2017 20:55:09 +0000",50138289,"I did some further digging. It seems to be okay to relocate the driver as long as the companion files SimbaImpalaODBC.did and cacerts.pem are relocated to the same directory as well. If SimbaImpalaODBC.did is missing, you get curious error messages and you need to use the LD_PRELOAD hack. A better error message in this case (File SimbaImpalaODBC.did could not be found) would have helped a lot.",0.3613333333,0.271,neutral
impala,2707,summary,Add FindOrInsert method to hash table to avoid unnecessary probe in aggregation,design_debt,non-optimal_design,"Tue, 24 Nov 2015 18:00:29 +0000","Wed, 9 Dec 2015 00:19:33 +0000","Mon, 7 Dec 2015 19:34:23 +0000",1128834,Add FindOrInsert method to hash table to avoid unnecessary probe in aggregation,-0.2,-0.2,neutral
impala,290,description,"A user ran into a problem when they tried to use LZO compressed RC file. Instead of displaying a clear ""Not Yet error, the query failed with an Unknown Codec message. It would be good to improve this.",design_debt,non-optimal_design,"Fri, 19 Apr 2013 20:10:38 +0000","Wed, 24 Apr 2013 23:13:35 +0000","Wed, 24 Apr 2013 23:13:35 +0000",442977,"A user ran into a problem when they tried to use LZO compressed RC file. Instead of displaying a clear ""Not Yet Supported/Implemented"" error, the query failed with an Unknown Codec message. It would be good to improve this.",-0.07066666667,0.01822222222,negative
impala,290,summary,Improve error message when trying to use LZO compression on file formats other than text,design_debt,non-optimal_design,"Fri, 19 Apr 2013 20:10:38 +0000","Wed, 24 Apr 2013 23:13:35 +0000","Wed, 24 Apr 2013 23:13:35 +0000",442977,Improve error message when trying to use LZO compression on file formats other than text,-0.1666666667,-0.1666666667,neutral
impala,2911,description,"Query contains functions like ""rand(), if()"" could crash Impala. *Workaround:* turning logging down or off should fix it",design_debt,non-optimal_design,"Fri, 29 Jan 2016 18:46:54 +0000","Fri, 29 Jan 2016 19:01:47 +0000","Fri, 29 Jan 2016 18:49:15 +0000",141,"Query contains functions like ""rand(), if()"" could crash Impala. Workaround: turning logging down or off should fix it",-0.173,-0.346,negative
impala,3008,description,"The filter routing table printed by the coordinator can be slightly improved: # Don't print it if there's no global filtering happening # Use the {{TablePrinter}} class to format it neatly # Print if filters are broadcast, or if they are partition only.",design_debt,non-optimal_design,"Tue, 16 Feb 2016 22:03:29 +0000","Fri, 19 Feb 2016 22:10:48 +0000","Fri, 19 Feb 2016 19:49:16 +0000",251147,"The filter routing table printed by the coordinator can be slightly improved: Don't print it if there's no global filtering happening Use the TablePrinter class to format it neatly Print if filters are broadcast, or if they are partition only.",0.4,0.4,neutral
impala,3077,description,"Because of a quirk in the runtime filters implementation, we currently have to disable them when a join spills. The only reason this is necessary is that the filters are constructed as part of the hash table build. But there is no reason that we need to construct the filters at that point: we could instead construct the filters when doing initial processing of the build input, at which point we see all build-side input rows regardless of whether they are spilled or not. This might actually perform better since it would move some of the CPU work from the CPU-intensive hash table build to the less CPU-intensive construction of the input stream.",design_debt,non-optimal_design,"Wed, 24 Feb 2016 22:16:19 +0000","Wed, 4 Jan 2017 23:58:12 +0000","Wed, 20 Apr 2016 05:51:17 +0000",4779298,"Because of a quirk in the runtime filters implementation, we currently have to disable them when a join spills. The only reason this is necessary is that the filters are constructed as part of the hash table build. But there is no reason that we need to construct the filters at that point: we could instead construct the filters when doing initial processing of the build input, at which point we see all build-side input rows regardless of whether they are spilled or not. This might actually perform better since it would move some of the CPU work from the CPU-intensive hash table build to the less CPU-intensive construction of the input stream.",0.0625,0.0625,neutral
impala,3099,description,"makes one RPC for every fragment instance, serially. There are no dependencies between fragment instances, so we should use to issue the cancellation requests.",design_debt,non-optimal_design,"Sun, 28 Feb 2016 22:35:19 +0000","Tue, 30 Apr 2019 20:01:22 +0000","Thu, 12 Jan 2017 20:33:47 +0000",27554308,"Coordinator::CancelRemoteFragments() makes one RPC for every fragment instance, serially. There are no dependencies between fragment instances, so we should use ExecEnv::rpc_pool() to issue the cancellation requests.",0,0,neutral
impala,3099,summary,should use a thread pool,design_debt,non-optimal_design,"Sun, 28 Feb 2016 22:35:19 +0000","Tue, 30 Apr 2019 20:01:22 +0000","Thu, 12 Jan 2017 20:33:47 +0000",27554308,Coordinator::CancelRemoteFragments should use a thread pool,0,0,neutral
impala,3144,description,"Currently our INSERT logic assumes that the table we're writing to lives only on one filesystem. However, the table could have different partitions on different filesystems. Adjust the logic in so that this becomes possible.",design_debt,non-optimal_design,"Fri, 4 Mar 2016 21:03:09 +0000","Tue, 3 May 2016 16:45:33 +0000","Tue, 3 May 2016 16:45:33 +0000",5168544,"Currently our INSERT logic assumes that the table we're writing to lives only on one filesystem. However, the table could have different partitions on different filesystems. Adjust the logic in Coordinator::FinalizeSuccessfulInsert() so that this becomes possible.",0,0,neutral
impala,321,description,In the summary node in the profile the info strings for start / end times are specified with second granularity. It would be nice to have them specify milliseconds.,design_debt,non-optimal_design,"Sat, 27 Apr 2013 00:28:49 +0000","Sun, 20 Dec 2015 00:05:02 +0000","Tue, 11 Jun 2013 01:46:59 +0000",3892690,In the summary node in the profile the info strings for start / end times are specified with second granularity. It would be nice to have them specify milliseconds.,0.4125,0.4125,neutral
impala,322,description,"It would be nice to get able to get information about errors the query encountered, like corruptions, missing block metadata, integer overflows, and anything you guys think would be interesting, in the profile so it's all centralized.",design_debt,non-optimal_design,"Sat, 27 Apr 2013 00:35:02 +0000","Sun, 20 Dec 2015 00:05:02 +0000","Tue, 7 May 2013 17:52:04 +0000",926222,"It would be nice to get able to get information about errors the query encountered, like corruptions, missing block metadata, integer overflows, and anything you guys think would be interesting, in the profile so it's all centralized.",0.1521666667,0.1521666667,neutral
impala,324,description,"It's occurred to me that, without a current global query view, situations will likely occur where a runaway query sent to another query planner goes unnoticed in administration but users may complain of poor cluster performance and aren't able to see why unless they somehow know to specifically go to the other query planner node which that runaway query was submitted to. One could suggest the best practice of only having clients connect to one node to submit queries and keep that effectively the only query planner in the cluster but it doesn't prevent the problem if they fail to heed such advice, which some users almost certainly will. Also some more advanced admins may load balance the query planners for HA or load distribution. I propose that there is, if not already, a max query time that a query planner will allow a query to run before self terminating the query across all impalad nodes. This will prevent the runaway query from continuously affecting performance and leaving a hard to trace problem. Given that Impala queries should execute in seconds, or minutes at the worst, maybe 300 or 600 seconds would be a reasonable default max query execution time (wall time). This would need to be configurable by the admin/user both globally for the node's query planner default via --switch/flags file and also per session override eg. in impala-shell some kind of ""set max_query_time="" command. Even if the global query view implementation I've asked for happens down the road, this max query time would also alleviate administrator burden of having to kill runaway user queries manually.",design_debt,non-optimal_design,"Mon, 29 Apr 2013 11:21:20 +0000","Sun, 20 Dec 2015 00:05:02 +0000","Mon, 13 Oct 2014 23:23:24 +0000",46008124,"It's occurred to me that, without a current global query view, situations will likely occur where a runaway query sent to another query planner goes unnoticed in administration but users may complain of poor cluster performance and aren't able to see why unless they somehow know to specifically go to the other query planner node which that runaway query was submitted to. One could suggest the best practice of only having clients connect to one node to submit queries and keep that effectively the only query planner in the cluster but it doesn't prevent the problem if they fail to heed such advice, which some users almost certainly will. Also some more advanced admins may load balance the query planners for HA or load distribution. I propose that there is, if not already, a max query time that a query planner will allow a query to run before self terminating the query across all impalad nodes. This will prevent the runaway query from continuously affecting performance and leaving a hard to trace problem. Given that Impala queries should execute in seconds, or minutes at the worst, maybe 300 or 600 seconds would be a reasonable default max query execution time (wall time). This would need to be configurable by the admin/user both globally for the node's query planner default via --switch/flags file and also per session override eg. in impala-shell some kind of ""set max_query_time="" command. Even if the global query view implementation I've asked for happens down the road, this max query time would also alleviate administrator burden of having to kill runaway user queries manually.",-0.01592916667,-0.01592916667,neutral
impala,3329,description,"I believe the {{impalad}} log rotation policy is causing logs to rotate out that we don't have access to. {{impalad}} defaults to keeping 10 log files in the log directory. This means if {{impalad}} restarts 11 times, the first logs will be rotated out. Many of the custom cluster tests restart {{impalad}}, though. This means {{impalad}} logs in the custom_cluster logs subdirectory are missing. To prove this is happening as a result of the log rotation policy, and it's not our inability to properly collect logs, I ran locally, and to get timestamps to show more about when tests were being run, I used this: (0) This gets us timestamps when there otherwise wouldn't be any. Here's the time range for our custom cluster tests: The earliest timetstamp log files are for 20:58. Contrast this to the end-to-end tests' log directory, which only has a handful of logs and span the 2 hours it took to run the test. The quick fix is to set the custom cluster tests to run with a limitless log rotation policy. The more detailed fix is to probably plumb things through such that we don't ever rotate logs. This is safe in Jenkins since we deleted the logs directory before every run (in code): we won't have a workspace accumulating logs. (0) Yup, you need {{gawk}} for {{strftime}} which {{mawk}}, the default {{awk}} on at least Ubuntu, doesn't have.",design_debt,non-optimal_design,"Mon, 11 Apr 2016 17:13:16 +0000","Wed, 27 Apr 2016 18:56:24 +0000","Wed, 27 Apr 2016 14:22:15 +0000",1372139,"I believe the impalad log rotation policy is causing logs to rotate out that we don't have access to. impalad defaults to keeping 10 log files in the log directory. This means if impalad restarts 11 times, the first logs will be rotated out. Many of the custom cluster tests restart impalad, though. This means impalad logs in the custom_cluster logs subdirectory are missing. To prove this is happening as a result of the log rotation policy, and it's not our inability to properly collect logs, I ran run-all-tests.sh locally, and to get timestamps to show more about when tests were being run, I used this: (0) This gets us timestamps when there otherwise wouldn't be any. Here's the time range for our custom cluster tests: The earliest timetstamp log files are for 20:58. Contrast this to the end-to-end tests' log directory, which only has a handful of logs and span the 2 hours it took to run the test. The quick fix is to set the custom cluster tests to run with a limitless log rotation policy. The more detailed fix is to probably plumb things through run-all-tests.sh such that we don't ever rotate logs. This is safe in Jenkins since we deleted the logs directory before every run (in code): we won't have a workspace accumulating logs. (0) Yup, you need gawk for strftime which mawk, the default awk on at least Ubuntu, doesn't have.",0.03958333333,0.03392857143,neutral
impala,3548,description,"Currently, the FE generates a number of runtime filters and assigns them to plan nodes without taking the value of RUNTIME_FILTER_MODE into account. In the backend, the filters are removed from the exec nodes based on the target node types (local or remote) and the value of the RUNTIME_FILTER_MODE option. This may cause some confusion to users because they may see runtime filters in the output of explain that aren't applied when the query is executed. This operation should be performed in the FE.",design_debt,non-optimal_design,"Mon, 16 May 2016 23:48:01 +0000","Sun, 29 Oct 2017 08:08:27 +0000","Sun, 29 Oct 2017 08:08:27 +0000",45822026,"Currently, the FE generates a number of runtime filters and assigns them to plan nodes without taking the value of RUNTIME_FILTER_MODE into account. In the backend, the filters are removed from the exec nodes based on the target node types (local or remote) and the value of the RUNTIME_FILTER_MODE option. This may cause some confusion to users because they may see runtime filters in the output of explain that aren't applied when the query is executed. This operation should be performed in the FE.",-0.09375,-0.09375,neutral
impala,3652,comment_1,"The required fix is for Reset() to take a RowBatch as an argument, to which memory referenced by previously returned rows can be returned. The logic for Reset() attaching memory needs to be similar to the handling of ReachedLimit(), e.g. this example in NLJNode::GetNext():",design_debt,non-optimal_design,"Tue, 31 May 2016 20:46:57 +0000","Mon, 19 Nov 2018 10:45:14 +0000","Wed, 7 Nov 2018 23:06:04 +0000",76904347,"The required fix is for Reset() to take a RowBatch as an argument, to which memory referenced by previously returned rows can be returned. The logic for Reset() attaching memory needs to be similar to the handling of ReachedLimit(), e.g. this example in NLJNode::GetNext():",-0.2,-0.2,neutral
impala,3652,description,"There is a tricky corner case in our resource transfer model with subplans and limits. The problem is that the limit in the subplan may mean that the exec node is reset before it has returned its full output. The resource transfer logic generally attaches resources to batches at specific points in the output, e.g. end of partition, end of block, so it's possible that batches returned before the Reset() may reference resources that have not yet been transferred. It's unclear if we test this scenario consistently or if it's always handled correctly. One example is this query, reported in IMPALA-5456:",design_debt,non-optimal_design,"Tue, 31 May 2016 20:46:57 +0000","Mon, 19 Nov 2018 10:45:14 +0000","Wed, 7 Nov 2018 23:06:04 +0000",76904347,"There is a tricky corner case in our resource transfer model with subplans and limits. The problem is that the limit in the subplan may mean that the exec node is reset before it has returned its full output. The resource transfer logic generally attaches resources to batches at specific points in the output, e.g. end of partition, end of block, so it's possible that batches returned before the Reset() may reference resources that have not yet been transferred. It's unclear if we test this scenario consistently or if it's always handled correctly. One example is this query, reported in IMPALA-5456:",-0.09833333333,-0.09833333333,neutral
impala,3671,description,"The immediate motivation for this is to enable better testing of graceful failures when spilling is disabled (e.g. IMPALA-3670). Currently we only have control over this via startup options, so we have to implement these as custom cluster tests, but there's no reason in principle we need to start a fresh cluster. The idea would be to add a query option 'scratch_limit' or similar, that limits the amount of scratch directory space that can be used. This would be useful to prevent runaway queries or to prevent queries from spilling when that is not desired.",design_debt,non-optimal_design,"Fri, 3 Jun 2016 21:31:45 +0000","Thu, 15 Dec 2016 00:36:24 +0000","Sat, 24 Sep 2016 04:53:45 +0000",9703320,"The immediate motivation for this is to enable better testing of graceful failures when spilling is disabled (e.g. IMPALA-3670). Currently we only have control over this via startup options, so we have to implement these as custom cluster tests, but there's no reason in principle we need to start a fresh cluster. The idea would be to add a query option 'scratch_limit' or similar, that limits the amount of scratch directory space that can be used. This would be useful to prevent runaway queries or to prevent queries from spilling when that is not desired.",-0.05375,-0.05375,neutral
impala,3828,description,"Today the planner creates left deep trees where joins are ordered based on selectivity. The proposal is to add a rule in the planner that traverses the plan after and flip the build and probe sides if the cardinality suggests so, which produces a bushy plan. This optimization should improve queries against normalized schemas with selective joins and multiple fact tables, more ""efficient"" runtime filters will be created as a result of the plan change The query below generates a left deep plan, where are a bush plan should be created query Plan",design_debt,non-optimal_design,"Wed, 6 Jul 2016 00:36:19 +0000","Fri, 19 Aug 2016 06:10:19 +0000","Fri, 19 Aug 2016 06:10:19 +0000",3821640,"Today the planner creates left deep trees where joins are ordered based on selectivity. The proposal is to add a rule in the planner that traverses the plan after createSingleNodePlan and flip the build and probe sides if the cardinality suggests so, which produces a bushy plan. This optimization should improve queries against normalized schemas with selective joins and multiple fact tables, more ""efficient"" runtime filters will be created as a result of the plan change The query below generates a left deep plan, where are a bush plan should be created query Plan",0.1333333333,0.1333333333,neutral
impala,3859,description,"is designed to log the data around a particular scanner parsing error. However, that log output may include sensitive table data, and should be avoided, particularly for uncompressed text formats.",design_debt,non-optimal_design,"Thu, 14 Jul 2016 00:23:13 +0000","Thu, 28 Sep 2017 15:50:33 +0000","Thu, 25 Aug 2016 16:25:24 +0000",3686531,"HdfsScanNode::LogRowParseError() is designed to log the data around a particular scanner parsing error. However, that log output may include sensitive table data, and should be avoided, particularly for uncompressed text formats.",-0.3,-0.3,neutral
impala,4162,description,"I noticed that during metadata loading or after running ""invalidate metadata"" is that there is an extensive amount of CPU spent and memory allocated When I checked the NameNode log I found thousands of entries with the error message below By default is false. When I enabled the warning and INFO messages stopped and metadata loading after ""invalidate metadata"" was 5-10% faster. This is the call stack for the exception Query timeline with ACLs enabled Query timeline with ACLs disable",design_debt,non-optimal_design,"Mon, 19 Sep 2016 18:07:45 +0000","Tue, 13 Jun 2017 05:28:26 +0000","Tue, 13 Jun 2017 05:27:17 +0000",23023172,"I noticed that during metadata loading or after running ""invalidate metadata"" is that there is an extensive amount of CPU spent and memory allocated When I checked the NameNode log I found thousands of entries with the error message below By default dfs.namenode.acls.enabled is false. When I enabled dfs.namenode.acls.enabled the warning and INFO messages stopped and metadata loading after ""invalidate metadata"" was 5-10% faster. This is the call stack for the exception Query timeline with ACLs enabled Query timeline with ACLs disable",-0.2555555556,-0.1166666667,negative
impala,4173,comment_0,"CHAR doesn't currently work properly, and it's very likely that in the future CHAR will be implemented as VARCHAR/STRING under the covers (with the appropriate blank padding). In that case, we still want to know the actual average length, excluding trailing spaces (because those won't be materialized during join and agg processing).",design_debt,non-optimal_design,"Wed, 21 Sep 2016 05:33:26 +0000","Thu, 19 Jan 2017 22:51:32 +0000","Thu, 19 Jan 2017 22:51:05 +0000",10430259,"CHAR doesn't currently work properly, and it's very likely that in the future CHAR will be implemented as VARCHAR/STRING under the covers (with the appropriate blank padding). In that case, we still want to know the actual average length, excluding trailing spaces (because those won't be materialized during join and agg processing).",0.08433333333,0.08433333333,negative
impala,4182,description,"It would be nice to have a function that returned the name of the current coordinator so that users could locate the webui that has their running queries. Problem statement: As more and more big installations use a load balancer between impala and users/hue, there is a need for users to be able to get to the query profiles for debugging. But there are a few problems with the current approaches: # Many users will not have access to CM # Profile; in shell does not help Hue users, and often the profile output is truncated on screen (via putty) so users have to exit the shell and pipe to file # Users cannot find the coordinator for the webui if they connect through a load balancer and do not have CM access",design_debt,non-optimal_design,"Thu, 22 Sep 2016 15:02:29 +0000","Wed, 28 Sep 2016 16:26:09 +0000","Thu, 22 Sep 2016 16:25:41 +0000",4992,"It would be nice to have a function that returned the name of the current coordinator so that users could locate the webui that has their running queries. Problem statement: As more and more big installations use a load balancer between impala and users/hue, there is a need for users to be able to get to the query profiles for debugging. But there are a few problems with the current approaches: Many users will not have access to CM Profile; in shell does not help Hue users, and often the profile output is truncated on screen (via putty) so users have to exit the shell and pipe to file Users cannot find the coordinator for the webui if they connect through a load balancer and do not have CM access",0.1958888889,0.1958888889,neutral
impala,4231,comment_2,"There are a few things that changed with the codegen'd code in the patch that may be relevant. * A couple of hash functions that were previously shared between the build and probe codegen are generated separately. The final IR should be the same since they're copied during inlining, but there may be some redundant optimisation done before inlining happens. * The function signature for AppendRow() changed so that it returned a status",design_debt,non-optimal_design,"Fri, 30 Sep 2016 18:19:07 +0000","Mon, 17 Oct 2016 00:30:56 +0000","Mon, 17 Oct 2016 00:30:56 +0000",1404709,"There are a few things that changed with the codegen'd code in the patch that may be relevant. A couple of hash functions that were previously shared between the build and probe codegen are generated separately. The final IR should be the same since they're copied during inlining, but there may be some redundant optimisation done before inlining happens. The function signature for AppendRow() changed so that it returned a status",0.05,0.05,neutral
impala,4320,description,We should switch Impala to build using the gold linker by default to speed up builds and improve the experience of new developers. We already had a discussion on the dev@ mailing list where people seemed to agree with the idea.,design_debt,non-optimal_design,"Wed, 19 Oct 2016 00:18:33 +0000","Thu, 20 Oct 2016 15:08:58 +0000","Thu, 20 Oct 2016 15:08:58 +0000",139825,We should switch Impala to build using the gold linker by default to speed up builds and improve the experience of new developers. We already had a discussion on the dev@ mailing list where people seemed to agree with the idea.,0.15,0.15,positive
impala,4485,description,"Currently table metadata changes are locked at the table level for DML. For tables with partitions, it would help performance if concurrent inserts if locking was based at the partition level based on changes to the partition(s) affected, instead of locking the entire table.",design_debt,non-optimal_design,"Mon, 14 Nov 2016 22:59:37 +0000","Wed, 25 Jan 2017 23:19:00 +0000","Wed, 25 Jan 2017 23:19:00 +0000",6221963,"Currently table metadata changes are locked at the table level for DML. For tables with partitions, it would help performance if concurrent inserts if locking was based at the partition level based on changes to the partition(s) affected, instead of locking the entire table.",0.05,0.05,neutral
impala,4612,description,I'm seeing a lot of error messages in the log when running test_udf.py along the lines of: I haven't seen any adverse effects of these messages but it's adding a lot of noise.,design_debt,non-optimal_design,"Wed, 7 Dec 2016 02:11:31 +0000","Thu, 19 Jan 2017 21:13:25 +0000","Thu, 19 Jan 2017 21:13:25 +0000",3783714,I'm seeing a lot of error messages in the log when running test_udf.py along the lines of: I haven't seen any adverse effects of these messages but it's adding a lot of noise.,0.05,0.05,negative
impala,4639,description,"Some tests are unable to run on remote clusters because of test setup or infrastructure reasons, and not because of product failure. We should be able to selectively skip tests that can't be set up properly to run against a remote cluster.",design_debt,non-optimal_design,"Fri, 9 Dec 2016 23:22:22 +0000","Tue, 29 Aug 2017 16:19:52 +0000","Tue, 29 Aug 2017 16:19:52 +0000",22697850,"Some tests are unable to run on remote clusters because of test setup or infrastructure reasons, and not because of product failure. We should be able to selectively skip tests that can't be set up properly to run against a remote cluster.",0.144,0.144,negative
impala,4671,description,"Kudu's {{ServicePool}} uses Kudu's {{Thread}} class to service RPC requests. While this works well, the threads it creates aren't monitored by Impala's {{ThreadMgr}} subsystem. Eventually we'd like to standardise on one thread class between projects, but for now it would be useful to reimplement {{ServicePool}} in terms of our thread class. The reactor threads and acceptor threads will still be Kudu threads, but those are less likely to do substantial work, so having them missing from monitoring isn't a big problem.",design_debt,non-optimal_design,"Thu, 15 Dec 2016 17:51:19 +0000","Thu, 14 Dec 2017 23:50:20 +0000","Fri, 8 Dec 2017 11:36:12 +0000",30908693,"Kudu's ServicePool uses Kudu's Thread class to service RPC requests. While this works well, the threads it creates aren't monitored by Impala's ThreadMgr subsystem. Eventually we'd like to standardise on one thread class between projects, but for now it would be useful to reimplement ServicePool in terms of our thread class. The reactor threads and acceptor threads will still be Kudu threads, but those are less likely to do substantial work, so having them missing from monitoring isn't a big problem.",0.3155833333,0.3155833333,neutral
impala,4728,description,"Currently Impala uses lazy evaluation for expressions. This can result in a performance overhead when using or reusing expressions in things like a window function order by vs having the expression materialized as a projection from the underlying relation, especially if the expression is used in multiple places.",design_debt,non-optimal_design,"Thu, 5 Jan 2017 01:33:09 +0000","Thu, 27 Apr 2017 18:22:58 +0000","Thu, 27 Apr 2017 18:22:58 +0000",9737389,"Currently Impala uses lazy evaluation for expressions. This can result in a performance overhead when using or reusing expressions in things like a window function order by vs having the expression materialized as a projection from the underlying relation, especially if the expression is used in multiple places.",-0.1,-0.1,negative
impala,4801,comment_1,"Yeah I figured out what this is. RuntimeProfile does some cleanup in it's destructor, which includes unregistering some callbacks that periodically touch MemTrackers. So if the MemTracker gets destroyed before the profile, we have a problem. Ideally we should move that out of the RuntimeProfile destructor, but we can fix the immediate problem by fixing the lifetime of the MemTracker.",design_debt,non-optimal_design,"Mon, 23 Jan 2017 18:58:36 +0000","Thu, 26 Jan 2017 01:33:15 +0000","Thu, 26 Jan 2017 01:33:15 +0000",196479,"Yeah I figured out what this is. RuntimeProfile does some cleanup in it's destructor, which includes unregistering some callbacks that periodically touch MemTrackers. So if the MemTracker gets destroyed before the profile, we have a problem. Ideally we should move that out of the RuntimeProfile destructor, but we can fix the immediate problem by fixing the lifetime of the MemTracker.",-0.175,-0.175,neutral
impala,4831,description,"If a client unpins some pages, then calls it can leave too many dirty unpinned pages in memory. Similarly, a client can cause mischief by calling AllocateFrom() and ReleaseTo() or by creating child",design_debt,non-optimal_design,"Thu, 26 Jan 2017 22:35:25 +0000","Thu, 16 Mar 2017 03:36:53 +0000","Thu, 16 Mar 2017 03:36:53 +0000",4165288,"If a client unpins some pages, then calls ReservationTracker::DecreaseReservation(), it can leave too many dirty unpinned pages in memory. Similarly, a client can cause mischief by calling AllocateFrom() and ReleaseTo() or by creating child ReservationTrackers.",-0.25,-0.1,negative
impala,4833,comment_0,There's some relevant code in the Scheduler and Coordinator. The Scheduler does a pass over all fragment instances to determine the set of hosts here: The coordinator also does something similar: It would maybe be helpful if the scheduler produced backend_params_map_ itself to make it easier to compute the per-host aggregates.,design_debt,non-optimal_design,"Fri, 27 Jan 2017 17:33:58 +0000","Mon, 14 Aug 2017 16:14:41 +0000","Mon, 14 Aug 2017 16:14:41 +0000",17188843,There's some relevant code in the Scheduler and Coordinator. The Scheduler does a pass over all fragment instances to determine the set of hosts here: The coordinator also does something similar: It would maybe be helpful if the scheduler produced backend_params_map_ itself to make it easier to compute the per-host aggregates.,0.3125,0.3125,neutral
impala,4833,description,"Following on from IMPALA-3748, we should consider combining the resource estimates with the schedule to accurate compute the minimum buffer requirement per daemon. Different daemons may have different resource requirements because not every fragment runs on every daemon (e.g. unpartitioned fragments, fragments with fewer scan ranges than daemons).",design_debt,non-optimal_design,"Fri, 27 Jan 2017 17:33:58 +0000","Mon, 14 Aug 2017 16:14:41 +0000","Mon, 14 Aug 2017 16:14:41 +0000",17188843,"Following on from IMPALA-3748, we should consider combining the resource estimates with the schedule to accurate compute the minimum buffer requirement per daemon. Different daemons may have different resource requirements because not every fragment runs on every daemon (e.g. unpartitioned fragments, fragments with fewer scan ranges than daemons).",0.375,0.375,neutral
impala,4862,description,"In the following example the way the peak resource estimate is computed from per-node estimates is wrong. It should be 476.41MB, because the scan node is Open()ed in the backend *while* the concurrent join builds are executing. Another example is this one, where in the backend the aggregations can execute concurrently with the join builds The behaviour for unions also is not accurate - branches of unions within the same fragment are execute serially, but anything below an exchanges is executed concurrently.",design_debt,non-optimal_design,"Wed, 1 Feb 2017 22:28:32 +0000","Wed, 12 Jul 2017 04:20:23 +0000","Wed, 12 Jul 2017 04:20:23 +0000",13845111,"In the following example the way the peak resource estimate is computed from per-node estimates is wrong. It should be 476.41MB, because the scan node is Open()ed in the backend while the concurrent join builds are executing. Another example is this one, where in the backend the aggregations can execute concurrently with the join builds The behaviour for unions also is not accurate - branches of unions within the same fragment are execute serially, but anything below an exchanges is executed concurrently.",-0.2638888889,-0.2638888889,negative
impala,4862,summary,Planner's peak resource estimates do not accurately reflect the behaviour of joins and unions in the backend,design_debt,non-optimal_design,"Wed, 1 Feb 2017 22:28:32 +0000","Wed, 12 Jul 2017 04:20:23 +0000","Wed, 12 Jul 2017 04:20:23 +0000",13845111,Planner's peak resource estimates do not accurately reflect the behaviour of joins and unions in the backend,0,0,negative
impala,4871,description,"Catalog when fails(or passes) to load block metadata of specific table files, throws following messages in logs. I0128 01:54:33.537742 22702 HdfsTable.java:345] load block md for table-x file 000066_0 I0128 01:54:59.373677 22702 Cancelled while waiting for datanode x.y.z.215:50020: I0128 01:54:59.373868 22702 Cancelled while waiting for datanode x.y.z.148:50020: These logs will be more useful if the entire hierarchy of file location is logged including all levels of partitions. Since just file name like ""000066_0"" can be present for thousands of partitions under same table, it is difficult to isolate the problematic file with a just above log entry.",design_debt,non-optimal_design,"Thu, 2 Feb 2017 18:47:59 +0000","Tue, 20 Jun 2017 04:41:18 +0000","Tue, 20 Jun 2017 04:41:18 +0000",11872399,"Catalog when fails(or passes) to load block metadata of specific table files, throws following messages in logs. I0128 01:54:33.537742 22702 HdfsTable.java:345] load block md for table-x file 000066_0 I0128 01:54:59.373677 22702 BlockStorageLocationUtil.java:167] Cancelled while waiting for datanode x.y.z.215:50020: java.util.concurrent.CancellationException I0128 01:54:59.373868 22702 BlockStorageLocationUtil.java:167] Cancelled while waiting for datanode x.y.z.148:50020: java.util.concurrent.CancellationException These logs will be more useful if the entire hierarchy of file location is logged including all levels of partitions. Since just file name like ""000066_0"" can be present for thousands of partitions under same table, it is difficult to isolate the problematic file with a just above log entry.",0.06325,0.1023333333,neutral
impala,4871,summary,"Log entry like ""loading block md for 'table-x' file 'file-1'"" needs more file details logging",design_debt,non-optimal_design,"Thu, 2 Feb 2017 18:47:59 +0000","Tue, 20 Jun 2017 04:41:18 +0000","Tue, 20 Jun 2017 04:41:18 +0000",11872399,"Log entry like  ""loading block md for 'table-x' file 'file-1'"" needs more file details logging",-0.2,-0.2,neutral
impala,4933,description,"Thrift currently initializes OpenSSL lazily, when the first connection is created. However, the OpenSSL initialization must happen before the Kudu library is used (because Kudu will not be doing the initialization itself, and it will check that the OpenSSL initialization already occurred), so thrift needs to do the initialization on startup.",design_debt,non-optimal_design,"Tue, 14 Feb 2017 23:50:38 +0000","Fri, 26 May 2017 21:18:49 +0000","Tue, 21 Feb 2017 00:01:59 +0000",519081,"Thrift currently initializes OpenSSL lazily, when the first connection is created. However, the OpenSSL initialization must happen before the Kudu library is used (because Kudu will not be doing the initialization itself, and it will check that the OpenSSL initialization already occurred), so thrift needs to do the initialization on startup.",0.25,0.25,neutral
impala,4967,comment_0,"For refresh or query after invalidate most of the time is usually spent fetching block metadata and partitions from HMS, around 10-15% of the overall time is in serializing the table. The bigger benefit from partition wise refresh is that memory overhead is reduced.",design_debt,non-optimal_design,"Wed, 22 Feb 2017 20:41:44 +0000","Fri, 11 Aug 2017 21:59:54 +0000","Fri, 11 Aug 2017 21:59:54 +0000",14692690,"For refresh or query after invalidate most of the time is usually spent fetching block metadata and partitions from HMS, around 10-15% of the overall time is in serializing the table. The bigger benefit from partition wise refresh is that memory overhead is reduced.",0.497,0.497,neutral
impala,4967,comment_1,"Thanks for that info, I'm less concerned about serialization costs and more about network. On larger clusters the Catalog Service will need to ship large amounts of metadata to every node on the cluster, which is a single point of congestion. If you have less nodes or fast network you wouldn't run into that as a problem in your testing. The CS becomes a single hot spot especially on busy large clusters.",design_debt,non-optimal_design,"Wed, 22 Feb 2017 20:41:44 +0000","Fri, 11 Aug 2017 21:59:54 +0000","Fri, 11 Aug 2017 21:59:54 +0000",14692690,"Thanks for that info, I'm less concerned about serialization costs and more about network. On larger clusters the Catalog Service will need to ship large amounts of metadata to every node on the cluster, which is a single point of congestion. If you have less nodes or fast network you wouldn't run into that as a problem in your testing. The CS becomes a single hot spot especially on busy large clusters.",-0.1,-0.1,neutral
impala,4967,description,"When running REFRESH table partition (x=1) the metadata distribution contains all partitions, not just the single partition that was refreshed. This creates additional load and delays refreshes. To solve metadata updates might have to be serialized (assign unique #, and apply in order), but this behavior would help the timeliness of data and reduce load on the cluster.",design_debt,non-optimal_design,"Wed, 22 Feb 2017 20:41:44 +0000","Fri, 11 Aug 2017 21:59:54 +0000","Fri, 11 Aug 2017 21:59:54 +0000",14692690,"When running REFRESH table partition (x=1) the metadata distribution contains all partitions, not just the single partition that was refreshed. This creates additional load and delays refreshes. To solve metadata updates might have to be serialized (assign unique #, and apply in order), but this behavior would help the timeliness of data and reduce load on the cluster.",0.3,0.3,neutral
impala,4970,description,"Although we retain the histogram of fragment instance startup latencies, we don't record the identity of the most expensive instance, or the host it runs on. This would be helpful in diagnosing slow query start-up times.",design_debt,non-optimal_design,"Wed, 22 Feb 2017 23:52:56 +0000","Tue, 19 Jun 2018 21:22:02 +0000","Tue, 19 Jun 2018 21:22:02 +0000",41635746,"Although we retain the histogram of fragment instance startup latencies, we don't record the identity of the most expensive instance, or the host it runs on. This would be helpful in diagnosing slow query start-up times.",0.3125,0.3125,neutral
impala,5002,description,"As identified in , it is not easy to know how (i.e. build flags) a toolchain build was produced. The build scripts are versioned in the native-toolchain repo, but that is not easily associated with generated toolchain builds. We should have some way to determine this information later. The simplest way to handle this may be to store the native-toolchain githash in the produced toolchain build.",design_debt,non-optimal_design,"Mon, 27 Feb 2017 22:08:12 +0000","Wed, 1 Mar 2017 22:25:34 +0000","Wed, 1 Mar 2017 22:25:34 +0000",173842,"As identified in https://gerrit.cloudera.org/#/c/6165/ , it is not easy to know how (i.e. build flags) a toolchain build was produced. The build scripts are versioned in the native-toolchain repo, but that is not easily associated with generated toolchain builds. We should have some way to determine this information later. The simplest way to handle this may be to store the native-toolchain githash in the produced toolchain build.",-0.073,-0.073,neutral
impala,5084,comment_0,"As a temporary solution, it may be reasonable to just increase the default buffer size in the sorter to match the maximum row size, given that the sorter's buffer requirements are more modest than the joins and aggs, and there are typically fewer sorts in a plan.",design_debt,non-optimal_design,"Thu, 16 Mar 2017 22:49:31 +0000","Sat, 15 Apr 2017 00:12:16 +0000","Sat, 15 Apr 2017 00:12:16 +0000",2510565,"As a temporary solution, it may be reasonable to just increase the default buffer size in the sorter to match the maximum row size, given that the sorter's buffer requirements are more modest than the joins and aggs, and there are typically fewer sorts in a plan.",0.2134,0.2134,neutral
impala,5084,comment_1,This wouldn't offer enough benefit to offset the complexity. We currently only need 6 regular-sized buffers to execute a spilling sort with var-len data. To support rows larger than the regular page size we'd still need 4 max-sized buffers and 2 regular-sized buffers (since we need to keep a read and write buffer in memory at the same time). A bigger memory reduction could be achieved by different means. E.g. packing fixed and variable-length data in merged runs into the same buffers.,design_debt,non-optimal_design,"Thu, 16 Mar 2017 22:49:31 +0000","Sat, 15 Apr 2017 00:12:16 +0000","Sat, 15 Apr 2017 00:12:16 +0000",2510565,This wouldn't offer enough benefit to offset the complexity. We currently only need 6 regular-sized buffers to execute a spilling sort with var-len data. To support rows larger than the regular page size we'd still need 4 max-sized buffers and 2 regular-sized buffers (since we need to keep a read and write buffer in memory at the same time). A bigger memory reduction could be achieved by different means. E.g. packing fixed and variable-length data in merged runs into the same buffers.,0.03056666667,0.03056666667,neutral
impala,5150,description,"When doing concurrency testing as part of the competitive benchmarking I noticed that it is very difficult to saturate all CPUs @100% Below is a snapshot from htop during a concurrency run, state below closely mimics the steady state, note that CPUs 41-60 are less busy compared to 1-20. Then I ran the command below which dumps the threads and processor associated with each, reference. for i in $(pgrep impalad); do ps -mo -p $i;done From the man page for ps : The output showed that a large number of threads are running on core 61, not surprisingly the 1K threads are all thrift-server threads, so I am wondering if this is skewing the kernel's ability to evenly distribute the threads across the cores or something. I did a followup experiment using by profiling different core ranges on the system : Run 80 concurrent queries dominated by shuffle exchange Profile cores 01-20 to foo_01-20 Profile cores 41-60 to foo_41-60 Results showed that : Cores 01-20 had 50% more instructions retired Cores 01-20 show significantly more contention on pthread_cond_wait, and __lll_lock_wait Skew is dominated by DataStreamSender ScannerThread(s) also show significant skew",design_debt,non-optimal_design,"Fri, 31 Mar 2017 22:54:47 +0000","Thu, 25 May 2017 15:05:25 +0000","Thu, 25 May 2017 15:05:25 +0000",4723838,"When doing concurrency testing as part of the competitive benchmarking I noticed that it is very difficult to saturate all CPUs @100% Below is a snapshot from htop during a concurrency run, state below closely mimics the steady state, note that CPUs 41-60 are less busy compared to 1-20. Then I ran the command below which dumps the threads and processor associated with each, reference. for i in $(pgrep impalad); do ps -mo pid,tid,fname,user,psr -p $i;done From the man page for ps : The output showed that a large number of threads are running on core 61, not surprisingly the 1K threads are all thrift-server threads, so I am wondering if this is skewing the kernel's ability to evenly distribute the threads across the cores or something. I did a followup experiment using by profiling different core ranges on the system : Run 80 concurrent queries dominated by shuffle exchange Profile cores 01-20 to foo_01-20 Profile cores 41-60 to foo_41-60 Results showed that : Cores 01-20 had 50% more instructions retired Cores 01-20 show significantly more contention on pthread_cond_wait, base::internal::SpinLockDelay and __lll_lock_wait Skew is dominated by DataStreamSender ScannerThread(s) also show significant skew",0.125,0.05,neutral
impala,5612,description,"The degree of inter-node parallelism for a join is determined by its left input, so when inverting a join the planner should be mindful of how the inversion affects parallelism. For example, the left join input may have been reduced by joining with several dimension tables so much that it becomes smaller than the right hand-side (another small dimension table). By inverting such a join the degree of parallelism may be reduced to one or very few nodes, based on how many nodes the right-hand size is executed on.",design_debt,non-optimal_design,"Fri, 30 Jun 2017 23:04:29 +0000","Fri, 18 Sep 2020 23:03:43 +0000","Tue, 22 Aug 2017 20:17:36 +0000",4569187,"The degree of inter-node parallelism for a join is determined by its left input, so when inverting a join the planner should be mindful of how the inversion affects parallelism. For example, the left join input may have been reduced by joining with several dimension tables so much that it becomes smaller than the right hand-side (another small dimension table). By inverting such a join the degree of parallelism may be reduced to one or very few nodes, based on how many nodes the right-hand size is executed on.",0.2408333333,0.2408333333,neutral
impala,5612,summary,Join inversion should avoid reducing the degree of parallelism,design_debt,non-optimal_design,"Fri, 30 Jun 2017 23:04:29 +0000","Fri, 18 Sep 2020 23:03:43 +0000","Tue, 22 Aug 2017 20:17:36 +0000",4569187,Join inversion should avoid reducing the degree of parallelism,0,0,neutral
impala,5618,comment_1,yeah I *think* that could defer construction of the boost::function until the non-templated slow path function is called. I'm thinking though that the performance is very hard to reason about and it may be better as a matter of policy to avoid lambdas with captured variables in perf-critical code.,design_debt,non-optimal_design,"Thu, 6 Jul 2017 00:51:03 +0000","Fri, 7 Jul 2017 18:02:02 +0000","Fri, 7 Jul 2017 18:02:02 +0000",148259,jbapple yeah I think that could defer construction of the boost::function until the non-templated slow path function is called. I'm thinking though that the performance is very hard to reason about and it may be better as a matter of policy to avoid lambdas with captured variables in perf-critical code.,0.1133333333,0.1133333333,neutral
impala,563,description,"Catalog will fail to initialize when there's a metastore connection issues (mostly misconfiguration). Impala logs the exception, but no one really reads the log. When a user (or admin) connects to Impala, they see an empty database and wouldn't know what wrong. To improve the experience, Impala should return an exception for all queries submitted (except invalidate metadata) if the catalog wasn't initialized. The exception should also contain the original exception message from the failed metastore connection.",design_debt,non-optimal_design,"Tue, 27 Aug 2013 18:23:13 +0000","Sun, 20 Dec 2015 00:05:07 +0000","Thu, 10 Oct 2013 19:30:13 +0000",3805620,"Catalog will fail to initialize when there's a metastore connection issues (mostly misconfiguration). Impala logs the exception, but no one really reads the log. When a user (or admin) connects to Impala, they see an empty database and wouldn't know what wrong. To improve the experience, Impala should return an exception for all queries submitted (except invalidate metadata) if the catalog wasn't initialized. The exception should also contain the original exception message from the failed metastore connection.",-0.075,-0.075,negative
impala,563,summary,Improve error message on querying an uninitialized Catalog,design_debt,non-optimal_design,"Tue, 27 Aug 2013 18:23:13 +0000","Sun, 20 Dec 2015 00:05:07 +0000","Thu, 10 Oct 2013 19:30:13 +0000",3805620,Improve error message on querying an uninitialized Catalog,0,0,negative
impala,5923,comment_0,"As far as I understand, you want the unexpected information(i.e. opid) will not be printed. If so, the line 132 is removed and then refine the logging message. I think it's too simple. Please leave more detailed description if there is any missing.",design_debt,non-optimal_design,"Tue, 12 Sep 2017 18:59:28 +0000","Mon, 16 Oct 2017 16:32:39 +0000","Mon, 16 Oct 2017 16:32:09 +0000",2928761,"lv As far as I understand, you want the unexpected information(i.e. opid) will not be printed. If so, the line 132 is removed and then refine the logging message. I think it's too simple. Please leave more detailed description if there is any missing.",0.1140625,0.1140625,neutral
impala,5963,description,"The Catalog log prints the information below which doesn't clearly show what tables are being loaded, how long is the queue and how far in the queue is a particular table.",design_debt,non-optimal_design,"Wed, 20 Sep 2017 18:27:53 +0000","Fri, 2 Nov 2018 22:36:29 +0000","Fri, 2 Nov 2018 22:36:29 +0000",35266116,"The Catalog log prints the information below which doesn't clearly show what tables are being loaded, how long is the queue and how far in the queue is a particular table.",0,0,neutral
impala,5997,description,"Currently if we shutdown an impalad, queries running on it will all fail. This is bad in a BI system integrating with Impala as its query engine. If we perform maintenance on the impala cluster, we hope users are not aware of it. For example, to decomission impalads in a rack to retire old machines, we hope these impalads not accept new PlanFragments and shutdown when their existing work is done. provides such a way since 0.128: * Presto workers can be instructed to shutdown by submiting a PUT request to /v1/info/state with the body ""SHUTTING_DOWN"". Once instructed to shutdown, the worker will no longer receive new tasks, and will exit once all existing tasks have completed. Hope we can provide a way to do so.",design_debt,non-optimal_design,"Fri, 29 Sep 2017 13:58:13 +0000","Mon, 9 Oct 2017 23:17:09 +0000","Mon, 9 Oct 2017 23:17:09 +0000",897536,"Currently if we shutdown an impalad, queries running on it will all fail. This is bad in a BI system integrating with Impala as its query engine. If we perform maintenance on the impala cluster, we hope users are not aware of it. For example, to decomission impalads in a rack to retire old machines, we hope these impalads not accept new PlanFragments and shutdown when their existing work is done. Presto provides such a way since 0.128: https://prestodb.io/docs/current/release/release-0.128.html Presto workers can be instructed to shutdown by submiting a PUT request to /v1/info/state with the body ""SHUTTING_DOWN"". Once instructed to shutdown, the worker will no longer receive new tasks, and will exit once all existing tasks have completed. Hope we can provide a way to do so.",-0.1188333333,-0.1188333333,negative
impala,5997,summary,Provide a way to gracefully shutdown an Impala Daemon,design_debt,non-optimal_design,"Fri, 29 Sep 2017 13:58:13 +0000","Mon, 9 Oct 2017 23:17:09 +0000","Mon, 9 Oct 2017 23:17:09 +0000",897536,Provide a way to gracefully shutdown an Impala Daemon,0,0,neutral
impala,6030,description,Since we introduced the we've forgotten to disable the coordinator specific thread pools on nodes that have only the executor role.,design_debt,non-optimal_design,"Mon, 9 Oct 2017 23:44:59 +0000","Thu, 12 Oct 2017 02:09:10 +0000","Thu, 12 Oct 2017 02:09:10 +0000",181451,"Since we introduced the FLAGS_is_coordinator, we've forgotten to disable the coordinator specific thread pools on nodes that have only the executor role.",-0.2,-0.2,negative
impala,605,comment_0,"0 means default and default value is 1k. Impala already adjust the hbase_caching according to the estimated stats. But if there are a few outliner that are HUGE, then the hbase_caching value will still be too big.",design_debt,non-optimal_design,"Thu, 26 Sep 2013 00:20:37 +0000","Wed, 9 May 2018 18:29:49 +0000","Wed, 9 May 2018 18:29:49 +0000",145735752,"0 means default and default value is 1k. Impala already adjust the hbase_caching according to the estimated stats. But if there are a few outliner that are HUGE, then the hbase_caching value will still be too big.",0.08016666667,0.08016666667,neutral
impala,6223,description,"Impala shell can throw a lexer error if it encounters a malformed ""with"" query. This happens because we use shlex to parse the input query to determine if its a DML and it can throw if the input doesn't have balanced quotes. A simple shlex repro of that is as follows, Fix: Either catch the exception and handle it gracefully or have a better way to figure out the query type, using a SQL parser (more involved). This query also repros it:",design_debt,non-optimal_design,"Mon, 20 Nov 2017 21:46:22 +0000","Tue, 26 Mar 2019 01:50:19 +0000","Mon, 9 Jul 2018 20:04:20 +0000",19952278,"Impala shell can throw a lexer error if it encounters a malformed ""with"" query. This happens because we use shlex to parse the input query to determine if its a DML and it can throw if the input doesn't have balanced quotes. A simple shlex repro of that is as follows, Fix: Either catch the exception and handle it gracefully or have a better way to figure out the query type, using a SQL parser (more involved). This query also repros it:",-0.025,-0.025,neutral
impala,6223,summary,Gracefully handle malformed 'with' queries in impala-shell,design_debt,non-optimal_design,"Mon, 20 Nov 2017 21:46:22 +0000","Tue, 26 Mar 2019 01:50:19 +0000","Mon, 9 Jul 2018 20:04:20 +0000",19952278,Gracefully handle malformed 'with' queries in impala-shell,0,0,positive
impala,6285,summary,Avoid printing the stack as part of DoTransmitDataRpc as it leads to burning lots of kernel CPU,design_debt,non-optimal_design,"Wed, 6 Dec 2017 20:47:37 +0000","Fri, 8 Dec 2017 08:07:54 +0000","Fri, 8 Dec 2017 08:07:54 +0000",127217,Avoid printing the stack as part of DoTransmitDataRpc as it leads to burning lots of kernel CPU,-0.2,-0.2,negative
impala,6442,description,"has an error message ""File $0 has invalid file metadata at file offset $1."" However, the value reported as ""file offset"" is an offset from the _end_ of the file, not from its _beginning_. This is very misleading, since without explicitly stating that the offset is from the end, it is usually understood to be counted from the beginning. Additionally, although the function name is clearly about a ""footer"", two comments explicitly mention processing the ""header"". This falsely suggests that metadata is at the beginning of the file, when in reality it is at the end.",design_debt,non-optimal_design,"Thu, 25 Jan 2018 14:02:45 +0000","Thu, 13 Sep 2018 20:27:19 +0000","Thu, 13 Sep 2018 20:27:19 +0000",19981474,"HdfsParquetScanner::ProcessFooter has an error message ""File $0 has invalid file metadata at file offset $1."" However, the value reported as ""file offset"" is an offset from theend of the file, not from its beginning. This is very misleading, since without explicitly stating that the offset is from the end, it is usually understood to be counted from the beginning. Additionally, although the function name is clearly about a ""footer"", two comments explicitly mention processing the ""header"". This falsely suggests that metadata is at the beginning of the file, when in reality it is at the end.",-0.31,-0.21,negative
impala,6442,summary,Misleading file offset reporting in error messages,design_debt,non-optimal_design,"Thu, 25 Jan 2018 14:02:45 +0000","Thu, 13 Sep 2018 20:27:19 +0000","Thu, 13 Sep 2018 20:27:19 +0000",19981474,Misleading file offset reporting in error messages,-0.575,-0.575,negative
impala,661,description,"Explain plan is an effective tool for tuning query. However, it doesn't give much inside into the (cpu) cost of expression evaluation. For complex predicates, it'll greatly affect the runtime of the query. For example, complex regex is very costly to evaluate. Right now, if a complex join predicate cause the join to slow down, it's not very easy to tell. If explain plan can annotate the cost of predicate evaluation (roughly), then it can guide our user to identify and tune the predicates.",design_debt,non-optimal_design,"Tue, 12 Nov 2013 21:25:53 +0000","Mon, 16 Sep 2019 22:40:22 +0000","Mon, 16 Sep 2019 22:40:22 +0000",184382069,"Explain plan is an effective tool for tuning query. However, it doesn't give much inside into the (cpu) cost of expression evaluation. For complex predicates, it'll greatly affect the runtime of the query. For example, complex regex is very costly to evaluate. Right now, if a complex join predicate cause the join to slow down, it's not very easy to tell. If explain plan can annotate the cost of predicate evaluation (roughly), then it can guide our user to identify and tune the predicates.",0.01143333333,0.01143333333,negative
impala,6709,description,"tests/query_test/ contains several tests that use ""hdfs dfs -copyFromLocal"" to fill tables with files that cannot be created with Impala. The current way of doing this looks like ""copy paste"" and adds unnecessary complexity to the test code. The common pattern of ""create a table, copy 1-2 files to it"" could be covered by a single function, or could be moved to the .test files, which support SHELL sections. for an example of SHELL section, see",design_debt,non-optimal_design,"Tue, 20 Mar 2018 14:39:45 +0000","Mon, 27 Aug 2018 13:53:59 +0000","Mon, 27 Aug 2018 13:53:59 +0000",13821254,"tests/query_test/ contains several tests that use ""hdfs dfs -copyFromLocal"" to fill tables with files that cannot be created with Impala. The current way of doing this looks like ""copy paste"" and adds unnecessary complexity to the test code. The common pattern of ""create a table, copy 1-2 files to it"" could be covered by a single function, or could be moved to the .test files, which support SHELL sections. for an example of SHELL section, see https://github.com/apache/impala/blob/8dde41e802e3566d07e2db7b2bf5cd76030ab3d3/testdata/workloads/functional-query/queries/QueryTest/parquet-resolution-by-name.test#L75",-0.045,-0.045,negative
impala,6806,description,"Take 2 certificate files: cert.pem and truststore.pem cert.pem has 2 certificates in it: A cert for that node (with CN=""hostname"", and signed by And the intermediate CA cert (with and signed by truststore.pem has 1 certificate in it: A cert which is the root CA (with self-signed) This format of certificates don't seem to verify on the OpenSSL command line but works with Thrift. This also doesn't work with KRPC. Workaround for this issue w/ KRPC turned on: If we move the second certificate from cert.pem into truststore.pem, then this seems to work. We'll need to dig into whether this is a PEM file format issue, or a KRPC issue. But the above workaround should unblock us for now.",design_debt,non-optimal_design,"Wed, 4 Apr 2018 21:59:37 +0000","Fri, 6 Apr 2018 16:39:47 +0000","Fri, 6 Apr 2018 05:35:26 +0000",113749,"Take 2 certificate files: cert.pem and truststore.pem cert.pem has 2 certificates in it: A cert for that node (with CN=""hostname"", and signed by CN=CertToolkitIntCA) And the intermediate CA cert (with CN=CertToolkitIntCA, and signed by CN=CertToolkitRootCA) truststore.pem has 1 certificate in it: A cert which is the root CA (with CN=CertToolkitRootCA, self-signed) This format of certificates don't seem to verify on the OpenSSL command line but works with Thrift. This also doesn't work with KRPC. Workaround for this issue w/ KRPC turned on: If we move the second certificate from cert.pem (CN=CertToolkitIntCA) into truststore.pem, then this seems to work. We'll need to dig into whether this is a PEM file format issue, or a KRPC issue. But the above workaround should unblock us for now.",0.1113636364,0.1113636364,neutral
impala,6847,description,"A scenario we've seen play out a couple of times is this. 1. An Impala admin sets up memory-based admission control but sets the default query mem_limit to 0. This means that admission control will use memory estimates instead of mem_limit. Typically admins want some protection from large queries consuming excessive memory but can't or don't want to set a single mem_limit because the workload is unknown or unpredictable. This configuration has caveats and we recommend against it, but this happens and often works well enough as long as the workload is comprised of relatively simple queries. The caveats include: * There is no enforcement that a query stays within the memory estimate. This means that a query can fail or force other queries to fail. * Memory estimates are often inaccurate (this is unavoidable since they depend on cardinality estimates, which are commonly off by 10x even with state-of-the-art query planners). This means that runnable queries may be impossible to admit without setting a mem_limit. 2. Something changes about the workload, e.g. a new query is added, stats are computed, data size changes, we make an otherwise-innocuous planner change. Then we run into the second problem above and one or more queries cannot be executed, e.g. ""Rejected query from pool root.foo: request memory needed 1234.56 GB is greater than pool max mem resources 1000.00 GB. Use the MEM_LIMIT query option to indicate how much memory is required per node. The total memory needed is the per-node MEM_LIMIT times the number of nodes executing the query. See the Admission Control documentation for more information. "" The last problem is the problem this JIRA is intending to work around. The preferred solutions, which work most of the time, are: # Configure a default query memory limit for the pool # Set a mem_limit for the query # Disable memory-based admission control and do admission control based on num_queries, which is simpler and easier to understand. It would however, be useful to have a third fallback in cases where the first two options are difficult or impossible to apply. The basic requirement is to allow an administrator to configure a resource pool such that queries with very high estimates can run. We probably need enough flexibility that they can run concurrently with other smaller queries (i.e. the big query shouldn't take over the whole pool) and ideally we would also have a mem_limit applied to the query so that we're still protected from runaway memory consumption. The long-term solution to this is IMPALA-6460. This is a short-term workaround that can tide users over until we have a comprehensive solution.",design_debt,non-optimal_design,"Fri, 13 Apr 2018 16:43:12 +0000","Thu, 19 Apr 2018 23:51:17 +0000","Wed, 18 Apr 2018 16:25:25 +0000",430933,"A scenario we've seen play out a couple of times is this. 1. An Impala admin sets up memory-based admission control but sets the default query mem_limit to 0. This means that admission control will use memory estimates instead of mem_limit. Typically admins want some protection from large queries consuming excessive memory but can't or don't want to set a single mem_limit because the workload is unknown or unpredictable. This configuration has caveats and we recommend against it, but this happens and often works well enough as long as the workload is comprised of relatively simple queries. The caveats include: There is no enforcement that a query stays within the memory estimate. This means that a query can fail or force other queries to fail. Memory estimates are often inaccurate (this is unavoidable since they depend on cardinality estimates, which are commonly off by 10x even with state-of-the-art query planners). This means that runnable queries may be impossible to admit without setting a mem_limit. 2. Something changes about the workload, e.g. a new query is added, stats are computed, data size changes, we make an otherwise-innocuous planner change. Then we run into the second problem above and one or more queries cannot be executed, e.g. ""Rejected query from pool root.foo: request memory needed 1234.56 GB is greater than pool max mem resources 1000.00 GB. Use the MEM_LIMIT query option to indicate how much memory is required per node. The total memory needed is the per-node MEM_LIMIT times the number of nodes executing the query. See the Admission Control documentation for more information. "" The last problem is the problem this JIRA is intending to work around. The preferred solutions, which work most of the time, are: Configure a default query memory limit for the pool Set a mem_limit for the query Disable memory-based admission control and do admission control based on num_queries, which is simpler and easier to understand. It would however, be useful to have a third fallback in cases where the first two options are difficult or impossible to apply. The basic requirement is to allow an administrator to configure a resource pool such that queries with very high estimates can run. We probably need enough flexibility that they can run concurrently with other smaller queries (i.e. the big query shouldn't take over the whole pool) and ideally we would also have a mem_limit applied to the query so that we're still protected from runaway memory consumption. The long-term solution to this is IMPALA-6460. This is a short-term workaround that can tide users over until we have a comprehensive solution.",0.02963888889,0.02963888889,neutral
impala,6858,description,"If query profiles (or structured plans) had table metadata information like types, testing Impala on a pre-existing query profile would be easier, since we could automate generating the table scaffolding.",design_debt,non-optimal_design,"Mon, 16 Apr 2018 18:13:50 +0000","Mon, 16 Apr 2018 23:14:45 +0000","Mon, 16 Apr 2018 23:14:45 +0000",18055,"If query profiles (or structured plans) had table metadata information like types, testing Impala on a pre-existing query profile would be easier, since we could automate generating the table scaffolding.",0,0,neutral
impala,6993,description,"We should use Status::Expected() here, just like it's used above. The stack trace isn't interesting and the error is expected once we get down this path. (I'm not sure why we don't just use {{exec_status}} though, but presumably the prefix was added for a reason).",design_debt,non-optimal_design,"Tue, 8 May 2018 19:12:39 +0000","Wed, 9 May 2018 16:33:47 +0000","Wed, 9 May 2018 16:33:47 +0000",76868,"We should use Status::Expected() here, just like it's used above. The stack trace isn't interesting and the error is expected once we get down this path. (I'm not sure why we don't just use exec_status though, but presumably the prefix was added for a reason).",0.03244444444,0.03244444444,negative
impala,7161,comment_0,"Additional issue: Suppose a user sets JAVA_HOME in their environment like they are supposed to. Suppose they also have JAVA_HOME in like writes. The two can get out of sync. The JAVA would be from the environment JAVA_HOME, but after setting JAVA, value for JAVA_HOME would overwrite the environment variable and JAVA_HOME would be that value. These could point to two different places.",design_debt,non-optimal_design,"Mon, 11 Jun 2018 23:50:00 +0000","Fri, 22 Feb 2019 03:42:37 +0000","Fri, 22 Jun 2018 18:46:27 +0000",932187,"Additional issue: Suppose a user sets JAVA_HOME in their environment like they are supposed to. Suppose they also have JAVA_HOME in bin/impala-config-local.sh like bin/bootstrap_system.sh writes. The two can get out of sync. The JAVA would be from the environment JAVA_HOME, but after setting JAVA, bin/impala-config-local.sh's value for JAVA_HOME would overwrite the environment variable and JAVA_HOME would be that value. These could point to two different places.",0.1,0.0625,neutral
impala,7161,description,"installs the Java SDK and sets JAVA_HOME in the current shell. It also adds a command to the to export JAVA_HOME there. This doesn't do the job. tests for JAVA_HOME at the very start of the script, before it has sourced So, the user doesn't have a way of developing over the long term without manually setting up JAVA_HOME. also doesn't detect the system JAVA_HOME. For Ubuntu 16.04, this is fairly simple and if a developer has their system JDK set up appropriately, it would make sense to use it. For example:",design_debt,non-optimal_design,"Mon, 11 Jun 2018 23:50:00 +0000","Fri, 22 Feb 2019 03:42:37 +0000","Fri, 22 Jun 2018 18:46:27 +0000",932187,"bin/bootstrap_system.sh installs the Java SDK and sets JAVA_HOME in the current shell. It also adds a command to the bin/impala-config-local.sh to export JAVA_HOME there. This doesn't do the job. bin/impala-config.sh tests for JAVA_HOME at the very start of the script, before it has sourced bin/impala-config-local.sh. So, the user doesn't have a way of developing over the long term without manually setting up JAVA_HOME. bin/impala-config.sh also doesn't detect the system JAVA_HOME. For Ubuntu 16.04, this is fairly simple and if a developer has their system JDK set up appropriately, it would make sense to use it. For example:",0.05714285714,0.03076923077,neutral
impala,7161,summary,Bootstrap's handling of JAVA_HOME needs improvement,design_debt,non-optimal_design,"Mon, 11 Jun 2018 23:50:00 +0000","Fri, 22 Feb 2019 03:42:37 +0000","Fri, 22 Jun 2018 18:46:27 +0000",932187,Bootstrap's handling of JAVA_HOME needs improvement,0.542,0.542,neutral
impala,7205,description,"Currently we respond with CANCELLED only when hitting EOS. It seems a bit more robust to always respond with CANCELLED whenever query execution has terminated. That way, if the cancel RPC from the coordinator to a backend fails, the backend will still cancel if it can send status back to the coordinator later on. Without this fix, the query can hang and/or finstances can continue running (until the query is closed, at which point the response to this RPC will be an error).",design_debt,non-optimal_design,"Mon, 25 Jun 2018 16:30:08 +0000","Sat, 13 Apr 2019 02:53:10 +0000","Fri, 29 Jun 2018 00:25:22 +0000",287714,"Currently we respond with CANCELLED only when hitting EOS. It seems a bit more robust to always respond with CANCELLED whenever query execution has terminated. That way, if the cancel RPC from the coordinator to a backend fails, the backend will still cancel if it can send status back to the coordinator later on. Without this fix, the query can hang and/or finstances can continue running (until the query is closed, at which point the response to this RPC will be an error).",-0.0324375,-0.0324375,neutral
impala,7234,comment_0,"Should this function actually take into account the total byte sizes or counts of ranges or files? In recently looking at this code I couldn't quite make sense of the logic. For example, if we have 10 partitions that are text, each containing one file, and one partition which is Parquet, containing 100 files, maybe it makes more sense to estimate scan range memory usage based on Parquet instead of text?",design_debt,non-optimal_design,"Mon, 2 Jul 2018 20:22:07 +0000","Wed, 1 Aug 2018 17:30:06 +0000","Wed, 1 Aug 2018 17:30:06 +0000",2581679,"Should this function actually take into account the total byte sizes or counts of ranges or files? In recently looking at this code I couldn't quite make sense of the logic. For example, if we have 10 partitions that are text, each containing one file, and one partition which is Parquet, containing 100 files, maybe it makes more sense to estimate scan range memory usage based on Parquet instead of text?",0,0,neutral
impala,7234,description,"The getMajorityFormat method of the FeCatalogUtils currently returns non-deterministic results when its argument is a list of partitions where there is no numerical majority in terms of the number of instances. The result is determined by the order in which the partitions are added to the HashMap. We need more deterministic results which also considers the memory requirement among different types of partitions. Ideally, this function should return the format with higher memory requirements in case of a tie.",design_debt,non-optimal_design,"Mon, 2 Jul 2018 20:22:07 +0000","Wed, 1 Aug 2018 17:30:06 +0000","Wed, 1 Aug 2018 17:30:06 +0000",2581679,"ThegetMajorityFormat method of the FeCatalogUtils currently returns non-deterministic results when its argument is a list of partitions where there is no numerical majority in terms of the number of instances. The result is determined by the order in which the partitions are added to the HashMap. We need more deterministic results which also considers the memory requirement among different types of partitions. Ideally, this function should return the format with higher memory requirements in case of a tie.",0.1,0.1,neutral
impala,7349,description,"We should add admission control support for intelligently choosing how much memory to give a query, based on the memory estimate. Currently we require that you set a single mem_limit per pool. Instead it would be better if we allowed configuring min/max guardrails within which admission control chose an amount of memory that will allow good performance. Initially, I think mem_limit will be the same for all backends. Eventually it could make sense to have different limits per backend depending on the fragments.",design_debt,non-optimal_design,"Wed, 25 Jul 2018 22:37:32 +0000","Mon, 29 Jul 2019 10:47:15 +0000","Mon, 8 Oct 2018 20:28:24 +0000",6472252,"We should add admission control support for intelligently choosing how much memory to give a query, based on the memory estimate. Currently we require that you set a single mem_limit per pool. Instead it would be better if we allowed configuring min/max guardrails within which admission control chose an amount of memory that will allow good performance. Initially, I think mem_limit will be the same for all backends. Eventually it could make sense to have different limits per backend depending on the fragments.",0.1384,0.1384,neutral
impala,7350,description,"For IMPALA-7349, we will be relying more on memory estimates. This is an umbrella JIRA to track improvements to memory estimates where the current estimates are way off and result in over- or under- admission. over-admission is probably the more significant concern.",design_debt,non-optimal_design,"Wed, 25 Jul 2018 22:43:34 +0000","Fri, 4 Dec 2020 16:20:56 +0000","Mon, 23 Sep 2019 21:29:06 +0000",36715532,"For IMPALA-7349, we will be relying more on memory estimates. This is an umbrella JIRA to track improvements to memory estimates where the current estimates are way off and result in over- or under- admission. over-admission is probably the more significant concern.",0.2333333333,0.2333333333,neutral
impala,7388,description,"The THROW_IF_ERROR macros all use a locally scoped variable ""status"". If they're called with a pattern like: then the status variable inside the macro ends up being assigned to itself instead of the outer-scope variable. This makes it not throw or return, which is quite surprising.",design_debt,non-optimal_design,"Thu, 2 Aug 2018 08:16:43 +0000","Tue, 14 Aug 2018 04:39:08 +0000","Tue, 14 Aug 2018 04:15:03 +0000",1022300,"The THROW_IF_ERROR macros all use a locally scoped variable ""status"". If they're called with a pattern like:  then the status variable inside the macro ends up being assigned to itself instead of the outer-scope variable. This makes it not throw or return, which is quite surprising.",0.03333333333,0.03333333333,neutral
impala,7388,summary,JNI THROW_IF_ERROR macros use local scope variables which likely conflict,design_debt,non-optimal_design,"Thu, 2 Aug 2018 08:16:43 +0000","Tue, 14 Aug 2018 04:39:08 +0000","Tue, 14 Aug 2018 04:15:03 +0000",1022300,JNI THROW_IF_ERROR macros use local scope variables which likely conflict,-0.4,-0.4,negative
impala,7400,description,"""Impala has no DELETE statement."" and ""Impala has no UPDATE statement. "" are not totally true - Impala has those statements but only for Kudu tables. ""For example, Impala does not support natural joins or anti-joins,"" - Impala does support Anti-joins via NOT IN/NOT EXISTS or even explicitly like: ""Within queries, Impala requires query aliases for any subqueries:"" - this is only true for subqueries used as inline views in the FROM clause. E.g. the following works: "" Impala .. requires the CROSS JOIN operator for Cartesian products."" - untrue, this works: ""Have you run the COMPUTE STATS statement on each table involved in join queries"". This isn't specific to queries with joins, although may have more impact. We recommend that users run COMPUTE STATS on all tables. ""A CREATE TABLE statement with no PARTITIONED BY clause stores all the data files in the same physical location,"" - unpartitioned tables with multiple files can have files residing in different locations (and there are already 3 replicas per file by default, so the statement is a little misleading even if there's a single file). I think the latest statement about ""Have you partitioned at the right granularity so that there is enough data in each partition to parallelize the work for each query?"" is also misleading for the same reason. ""The INSERT ... VALUES syntax is suitable for setting up toy tables with a few rows for functional testing, but because each such statement creates a separate tiny file in HDFS"". This advice only applies to HDFS, this should work fine for Kudu tables although the INSERT statements are not particularly fast. ""The number of expressions allowed in an Impala query might be smaller than for some other database systems, causing failures for very complicated queries"" - this doesn't seem right - I don't know why the queries would fail. Also the codegen time isn't really specific to expressions or where clauses. There seems to be a point buried in there, but maybe it's just essentially that ""Complex queries may have high codegen time""",design_debt,non-optimal_design,"Mon, 6 Aug 2018 16:34:24 +0000","Wed, 8 Aug 2018 21:07:52 +0000","Wed, 8 Aug 2018 20:43:20 +0000",187736,"""Impala has no DELETE statement."" and ""Impala has no UPDATE statement. "" are not totally true - Impala has those statements but only for Kudu tables. ""For example, Impala does not support natural joins or anti-joins,"" - Impala does support Anti-joins via NOT IN/NOT EXISTS or even explicitly like: ""Within queries, Impala requires query aliases for any subqueries:"" - this is only true for subqueries used as inline views in the FROM clause. E.g. the following works: "" Impala .. requires the CROSS JOIN operator for Cartesian products."" - untrue, this works: ""Have you run the COMPUTE STATS statement on each table involved in join queries"". This isn't specific to queries with joins, although may have more impact. We recommend that users run COMPUTE STATS on all tables. ""A CREATE TABLE statement with no PARTITIONED BY clause stores all the data files in the same physical location,"" - unpartitioned tables with multiple files can have files residing in different locations (and there are already 3 replicas per file by default, so the statement is a little misleading even if there's a single file). I think the latest statement about ""Have you partitioned at the right granularity so that there is enough data in each partition to parallelize the work for each query?"" is also misleading for the same reason. ""The INSERT ... VALUES syntax is suitable for setting up toy tables with a few rows for functional testing, but because each such statement creates a separate tiny file in HDFS"". This advice only applies to HDFS, this should work fine for Kudu tables although the INSERT statements are not particularly fast. ""The number of expressions allowed in an Impala query might be smaller than for some other database systems, causing failures for very complicated queries"" - this doesn't seem right - I don't know why the queries would fail. Also the codegen time isn't really specific to expressions or where clauses. There seems to be a point buried in there, but maybe it's just essentially that ""Complex queries may have high codegen time""",0.008328125,0.008328125,negative
impala,7406,description,"Currently the file descriptors stored in the catalogd memory for each partition use a FlatBuffer to reduce the number of separate objects on the Java heap. However, the FlatBuffer objects internally each store a ByteBuffer and int position, so each object takes 32 bytes on its own. The ByteBuffer takes 56 bytes since it stores various references, endianness, limit, mark, position, etc. This amounts to about 88 bytes overhead on top of the actual underlying flatbuf byte array which is typically around 100 bytes for a single-block file. So, we're have about a 1:1 ratio of memory overhead and a 2:1 ratio of object count overhead for each partition. If we simply stored the byte[] array and constructed wrappers on demand, we'd save 88 bytes and 2 objects per partition. The downside is that we'd need to do short-lived ByteBuffer allocations at access time, and based on some benchmarking I did, they don't get escape-analyzed out. So, it's not a super clear win, but still worth considering.",design_debt,non-optimal_design,"Tue, 7 Aug 2018 21:14:26 +0000","Fri, 17 Aug 2018 23:19:40 +0000","Fri, 17 Aug 2018 18:42:29 +0000",854883,"Currently the file descriptors stored in the catalogd memory for each partition use a FlatBuffer to reduce the number of separate objects on the Java heap. However, the FlatBuffer objects internally each store a ByteBuffer and int position, so each object takes 32 bytes on its own. The ByteBuffer takes 56 bytes since it stores various references, endianness, limit, mark, position, etc. This amounts to about 88 bytes overhead on top of the actual underlying flatbuf byte array which is typically around 100 bytes for a single-block file. So, we're have about a 1:1 ratio of memory overhead and a 2:1 ratio of object count overhead for each partition. If we simply stored the byte[] array and constructed wrappers on demand, we'd save 88 bytes and 2 objects per partition. The downside is that we'd need to do short-lived ByteBuffer allocations at access time, and based on some benchmarking I did, they don't get escape-analyzed out. So, it's not a super clear win, but still worth considering.",-0.14865625,-0.14865625,neutral
impala,74,comment_0,"Just to let everyone know what happened here: -nn_port and -nn are still around, but if -nn is not specified (and it defaults to blank), Impala will try to read the value of {{fs.defaultFS}} (and if that doesn't exist, from the frontend which has loaded a Hadoop configuration. We will consider removing -nn and -nn_port if this proves to be robust enough. This change will be in Impala 0.6.",design_debt,non-optimal_design,"Wed, 20 Feb 2013 00:16:40 +0000","Sun, 20 Dec 2015 00:04:57 +0000","Wed, 20 Feb 2013 02:18:11 +0000",7291,"Just to let everyone know what happened here: -nn_port and -nn are still around, but if -nn is not specified (and it defaults to blank), Impala will try to read the value of fs.defaultFS (and if that doesn't exist, fs.default.name) from the frontend which has loaded a Hadoop configuration. We will consider removing -nn and -nn_port if this proves to be robust enough. This change will be in Impala 0.6.",0.18275,0.0385,neutral
impala,74,description,"Impala hdfs-fs-cache values for namenode address and namenode port should be read from Impala's core-site.xml file rather than input as command line args. If these args are not specified when impalad starts, then it defaults to localhost:20500 which can be confusing. Definition: 20500, ""namenode port""); ""localhost"", ""namenode host"");",design_debt,non-optimal_design,"Wed, 20 Feb 2013 00:16:40 +0000","Sun, 20 Dec 2015 00:04:57 +0000","Wed, 20 Feb 2013 02:18:11 +0000",7291,"Impala hdfs-fs-cache values for namenode address and namenode port should be read from Impala's core-site.xml file rather than input as command line args. If these args are not specified when impalad starts, then it defaults to localhost:20500 which can be confusing. Definition: ./be/src/runtime/hdfs-fs-cache.cc:DEFINE_int32(nn_port, 20500, ""namenode port""); ./be/src/runtime/hdfs-fs-cache.cc:DEFINE_string(nn, ""localhost"", ""namenode host"");",-0.10925,0.095375,neutral
impala,7640,description,"Currently, when I execute ALTER TABLE RENAME on a managed Kudu table it will not rename the underlying Kudu table. Because of IMPALA-5654 it becomes nearly impossible to rename the underlying Kudu table, which is confusing and makes the Kudu tables harder to identify and manage.",design_debt,non-optimal_design,"Thu, 27 Sep 2018 20:56:51 +0000","Sun, 7 Apr 2019 20:21:06 +0000","Sun, 7 Apr 2019 20:21:06 +0000",16586655,"Currently, when I execute ALTER TABLE RENAME on a managed Kudu table it will not rename the underlying Kudu table. Because ofIMPALA-5654 it becomes nearly impossible to rename the underlying Kudu table, which is confusing and makes the Kudu tables harder toidentify and manage.",-0.2185,-0.2185,negative
impala,7688,summary,Spurious error messages when updating owner privileges,design_debt,non-optimal_design,"Wed, 10 Oct 2018 15:29:53 +0000","Fri, 12 Oct 2018 17:11:54 +0000","Thu, 11 Oct 2018 16:56:16 +0000",91583,Spurious error messages when updating owner privileges,-0.5335,-0.5335,negative
impala,7902,description,"The {{NumericLiteral}} class is a leaf node in the Impala FE AST. In order to address a number of pending issues, we must adjust this class to improve its semantics and fix a number of bugs. See the linked JIRA tickets for the issues that this roll-up ticket addresses. The issues are combined because they are tightly related; makes more sense to offer them as a single patch than as a series of disjoint patches.",design_debt,non-optimal_design,"Tue, 27 Nov 2018 23:48:34 +0000","Thu, 14 Mar 2019 14:41:33 +0000","Fri, 1 Mar 2019 17:48:02 +0000",8099968,"The NumericLiteral class is a leaf node in the Impala FE AST. In order to address a number of pending issues, we must adjust this class to improve its semantics and fix a number of bugs. See the linked JIRA tickets for the issues that this roll-up ticket addresses. The issues are combined because they are tightly related; makes more sense to offer them as a single patch than as a series of disjoint patches.",0.1166666667,0.1166666667,neutral
impala,7987,comment_0,"A major obstacle here that I'm running into is that ""localhost"" appears in many places in configuration files. Finding those and replacing them with the gateway IP of the docker network isn't sufficient to solve the problem without reloading data, because ""localhost"" makes its way into table definitions and other places (probably sentry?).",design_debt,non-optimal_design,"Fri, 14 Dec 2018 23:21:15 +0000","Wed, 23 Jan 2019 18:53:45 +0000","Wed, 23 Jan 2019 18:53:45 +0000",3439950,"A major obstacle here that I'm running into is that ""localhost"" appears in many places in configuration files. Finding those and replacing them with the gateway IP of the docker network isn't sufficient to solve the problem without reloading data, because ""localhost"" makes its way into table definitions and other places (probably sentry?).",-0.2708333333,-0.2708333333,negative
impala,8005,description,"Currently, we use the same hash seed for partitioning exchanges at the sender. For a table with skew in distribution in the shuffling keys, multiple queries using the same shuffling keys for exchanges will end up hashing to the same destination fragments running on particular host and potentially overloading that host. We should consider using the query id or other query specific information to seed the hashing function to randomize the destinations for different queries. Thanks to  for pointing this problem out.",design_debt,non-optimal_design,"Wed, 19 Dec 2018 19:46:27 +0000","Thu, 28 May 2020 16:17:52 +0000","Thu, 28 May 2020 16:17:52 +0000",45433885,"Currently, we use the same hash seed for partitioning exchanges at the sender. For a table with skew in distribution in the shuffling keys, multiple queries using the same shuffling keys for exchanges will end up hashing to the same destination fragments running on particular host and potentially overloading that host. We should consider using the query id or other query specific information to seed the hashing function to randomize the destinations for different queries. Thanks to tlipcon for pointing this problem out.",0,0,neutral
impala,8176,description,"With the initial commit for the unified backend test executable commited (IMPALA-8071), it should be straight-forward to convert another set of tests to use the unified executable. Any test with a simple main() function (such as IMPALA_TEST_MAIN()) should require only modest changes to convert. Command to find such tests: git grep IMPALA_TEST_MAIN It looks like there are dozens of such tests:",design_debt,non-optimal_design,"Fri, 8 Feb 2019 19:12:36 +0000","Fri, 19 Jul 2019 22:51:10 +0000","Fri, 19 Jul 2019 22:51:10 +0000",13923514,"With the initial commit for the unified backend test executable commited (IMPALA-8071), it should be straight-forward to convert another set of tests to use the unified executable. Any test with a simple main() function (such as IMPALA_TEST_MAIN()) should require only modest changes to convert. Command to find such tests: git grep IMPALA_TEST_MAIN It looks like there are dozens of such tests:",0.1138888889,0.1138888889,neutral
impala,8176,summary,Convert tests with trivial main() functions to use a unified executable,design_debt,non-optimal_design,"Fri, 8 Feb 2019 19:12:36 +0000","Fri, 19 Jul 2019 22:51:10 +0000","Fri, 19 Jul 2019 22:51:10 +0000",13923514,Convert tests with trivial main() functions to use a unified executable,0.4125,0.4125,neutral
impala,8181,description,"A recent change added plan node cardinality to the DESCRIBE output using a ""metric"" format: 123.46M instead of 123456789. This makes the output easier to read. Also, since all numbers are estimate, having seven or eight digits of precision is not very helpful. This ticket requests that the same formatting be used for the other places where the DESCRIBE output shows row counts: table stats, extrapolated row count, partition row counts, and so on.",design_debt,non-optimal_design,"Mon, 11 Feb 2019 17:14:05 +0000","Thu, 14 Mar 2019 23:58:34 +0000","Thu, 14 Mar 2019 23:58:34 +0000",2702669,"A recent change added plan node cardinality to the DESCRIBE output using a ""metric"" format: 123.46M instead of 123456789. This makes the output easier to read. Also, since all numbers are estimate, having seven or eight digits of precision is not very helpful. This ticket requests that the same formatting be used for the other places where the DESCRIBE output shows row counts: table stats, extrapolated row count, partition row counts, and so on.",0,0,neutral
impala,8187,description,"It's possible for UDF authors to get into a pickle by exporting symbols from UDF/UDA shared objects unintentionally. E.g. if using boost headers, which result in weak symbols in the .so, which then get merged with the (incompatible) symbols in the impalad binary. We should really be keeping symbols hidden by default then exporting the entry points explicitly in the UDF/UDA code",design_debt,non-optimal_design,"Mon, 11 Feb 2019 22:53:06 +0000","Thu, 14 Feb 2019 23:37:41 +0000","Thu, 14 Feb 2019 23:37:41 +0000",261875,"It's possible for UDF authors to get into a pickle by exporting symbols from UDF/UDA shared objects unintentionally. E.g. if using boost headers, which result in weak symbols in the .so, which then get merged with the (incompatible) symbols in the impalad binary. We should really be keeping symbols hidden by default then exporting the entry points explicitly in the UDF/UDA code",-0.1,-0.1,negative
impala,8473,comment_0,"Can you define ""ImpalaPostExecHook "" as an interface instead an abstract class? I don't see benefits of defining it as abstract class. On the other hand, defining it as interface allows the implementation to extends from another base class. Thanks! Instead of Can you define",design_debt,non-optimal_design,"Tue, 30 Apr 2019 18:58:16 +0000","Wed, 29 May 2019 21:54:40 +0000","Wed, 29 May 2019 21:54:40 +0000",2516184,"radford-nguyen Can you define ""ImpalaPostExecHook "" as an interface instead an abstract class? I don't see benefits of defining it as abstract class. On the other hand, defining it as interface allows the implementation to extends from another base class. Thanks! Instead of Can you define",0.04,0.04,negative
impala,8632,description,"In case of {{INSERT_EVENTS}} if Impala inserts into a table it causes a refresh to the underlying table/partition. This could be unnecessary when there is only one Impala cluster in the system. The existing self-event detection framework cannot identify such events because they are not sending HMS objects like tables and partitions to the HMS. Instead in case of {{INSERT_EVENT}} HMS API only asks for a table name or partition value to fire a insert event on it. We can detect a self-event in such cases if the HMS API to fire a listener event is improved to return the event id. This would be used by EventProcessor to ignore the event when it is fetched later in the next polling cycle. In order to support this, we will need to make a change to Hive as well so that the enhanced API can be used.",design_debt,non-optimal_design,"Thu, 6 Jun 2019 23:59:00 +0000","Thu, 9 Apr 2020 17:50:20 +0000","Thu, 9 Apr 2020 17:50:20 +0000",26589080,"In case of INSERT_EVENTS if Impala inserts into a table it causes a refresh to the underlying table/partition. This could be unnecessary when there is only one Impala cluster in the system. The existing self-event detection framework cannot identify such events because they are not sending HMS objects like tables and partitions to the HMS. Instead in case of INSERT_EVENT HMS API only asks for a table name or partition value to fire a insert event on it. We can detect a self-event in such cases if the HMS API to fire a listener event is improved to return the event id. This would be used by EventProcessor to ignore the event when it is fetched later in the next polling cycle. In order to support this, we will need to make a change to Hive as well so that the enhanced API can be used.",-0.01280952381,-0.01280952381,neutral
impala,8656,description,"Impala's current interaction with clients is pulled-based: it relies on clients to fetch results to trigger the generation of more result row batches until all the result rows have been produced. If a client issues a query without fetching all the results, the query fragments will continue to consume the resources until the query hits is cancelled and unregistered for whatever reasons. This is undesirable as resources are held up by misbehaving clients and other queries may wait for extended period of time in admission control due to this. The high level idea for this JIRA is for Impala to have a mode in which result sets of queries are eagerly fetched and spooled somewhere (preferably some persistent storage). In this way, the cluster's resources are freed up once all result rows have been fetched and stored in the spooling location. Incoming client fetches can be returned from this spooled locations. cc'ing , , ,",design_debt,non-optimal_design,"Wed, 12 Jun 2019 02:22:07 +0000","Wed, 3 Mar 2021 00:38:51 +0000","Tue, 8 Oct 2019 14:57:48 +0000",10240541,"Impala's current interaction with clients is pulled-based: it relies on clients to fetch results to trigger the generation of more result row batches until all the result rows have been produced. If a client issues a query without fetching all the results, the query fragments will continue to consume the resources until the query hits is cancelled and unregistered for whatever reasons. This is undesirable as resources are held up by misbehaving clients and other queries may wait for extended period of time in admission control due to this. The high level idea for this JIRA is for Impala to have a mode in which result sets of queries are eagerly fetched and spooled somewhere (preferably some persistent storage). In this way, the cluster's resources are freed up once all result rows have been fetched and stored in the spooling location. Incoming client fetches can be returned from this spooled locations. cc'ing stakiar, twm378, joemcdonnell, lv",-0.2,-0.2,neutral
impala,8892,description,Our docker images lack a bunch of tools that help to implement health checks and debugging issues.,design_debt,non-optimal_design,"Mon, 26 Aug 2019 23:25:38 +0000","Thu, 29 Aug 2019 19:56:42 +0000","Thu, 29 Aug 2019 19:55:10 +0000",246572,Our docker images lack a bunch of tools that help to implement health checks and debugging issues.,0,0,negative
impala,8912,comment_0,"The second call is required since the limit_ may be updated, which may cut the cardinality. I think what we should avoid is sampling twice on hbase.",design_debt,non-optimal_design,"Fri, 30 Aug 2019 12:39:19 +0000","Thu, 5 Sep 2019 00:59:26 +0000","Thu, 5 Sep 2019 00:59:26 +0000",476407,"The second call is required since the limit_ may be updated, which may cut the cardinality. I think what we should avoid is sampling twice on hbase.",-0.2,-0.2,neutral
impala,8912,description,"For simple queries on HBase tables that has HBaseScanNode as the root of the SingleNodePlan, will be called twice. Stacktrace for the first call: Stacktrace for the second call: Codes of the second call: Logs for a simple query on an old version of Impala: Such kind of queries are usually point queries and are always expected to return fast. is heavy since it requires RPCs to HBase. We should avoid calling it twice.",design_debt,non-optimal_design,"Fri, 30 Aug 2019 12:39:19 +0000","Thu, 5 Sep 2019 00:59:26 +0000","Thu, 5 Sep 2019 00:59:26 +0000",476407,"For simple queries on HBase tables that has HBaseScanNode as the root of the SingleNodePlan, HBaseScanNode#computeStats will be called twice. Stacktrace for the first call: Stacktrace for the second call: Codes of the second call: Logs for a simple query on an old version of Impala: Such kind of queries are usually point queries and are always expected to return fast. HBaseScanNode#computeStats is heavy since it requires RPCs to HBase. We should avoid calling it twice.",-0.05,-0.05,neutral
impala,8912,summary,Avoid calling computeStats twice on HBaseScanNode,design_debt,non-optimal_design,"Fri, 30 Aug 2019 12:39:19 +0000","Thu, 5 Sep 2019 00:59:26 +0000","Thu, 5 Sep 2019 00:59:26 +0000",476407,Avoid calling computeStats twice on HBaseScanNode,-0.2,-0.2,neutral
impala,8935,comment_2,"So the use-case you're talking about, accessing the minicluster webui in a dev environment from another machine, I think is difficult to make work in all cases. Rather than using ip addresses, we could specify hostnames for these links, but that's not guaranteed to always work - I think its common for people to develop on machines that don't have DNS-resolvable hostnames (eg see IMPALA-8917). And its always going to be the case that the machine's hostname resolves to 127.0.0.1 locally due to I don't think this is too big of a deal - it should work in any sort of real, non-development environment, and of course its possible in a dev environment to access the other webuis by manually specifying the right host:port instead of clicking the link, as its always been. One option if you really want this to work in a dev environment is to specify the flag to some public IP on minicluster startup. This flag is currently broken, but I have a patch out to get it working:",design_debt,non-optimal_design,"Tue, 10 Sep 2019 18:56:04 +0000","Thu, 19 Sep 2019 20:17:48 +0000","Thu, 19 Sep 2019 20:17:48 +0000",782504,"So the use-case you're talking about, accessing the minicluster webui in a dev environment from another machine, I think is difficult to make work in all cases. Rather than using ip addresses, we could specify hostnames for these links, but that's not guaranteed to always work - I think its common for people to develop on machines that don't have DNS-resolvable hostnames (eg see IMPALA-8917). And its always going to be the case that the machine's hostname resolves to 127.0.0.1 locally due to https://github.com/apache/impala/blob/master/bin/bootstrap_system.sh#L362 I don't think this is too big of a deal - it should work in any sort of real, non-development environment, and of course its possible in a dev environment to access the other webuis by manually specifying the right host:port instead of clicking the link, as its always been. One option if you really want this to work in a dev environment is to specify the --webserver_interface flag to some public IP on minicluster startup. This flag is currently broken, but I have a patch out to get it working: https://gerrit.cloudera.org/#/c/14266/",0.06908,0.06908,neutral
impala,9146,description,"Since broadcast based hash joins are often chosen, we sometimes see very large tables being broadcast, with sizes that are larger than the destination executor's total memory. This could potentially happen if the cluster membership is not accurately known and the planner's cost computation of the broadcastCost vs partitionCost happens to favor the broadcast distribution. This causes spilling and severely affects performance. Although the DistributedPlanner does a mem_limit check before picking broadcast, the mem_limit is not an accurate reflection since it is assigned during admission control (See Given this scenario, as a safety check it is better to have to an explicit configurable limit for the size of the broadcast input and set it to a reasonable default. The 'reasonable' default can be chosen based on analysis of existing benchmark queries and representative workloads where Impala is currently used.",design_debt,non-optimal_design,"Tue, 12 Nov 2019 00:33:43 +0000","Tue, 10 Dec 2019 01:35:11 +0000","Tue, 10 Dec 2019 01:35:11 +0000",2422888,"Since broadcast based hash joins are often chosen, we sometimes see very large tables being broadcast, with sizes that are larger than the destination executor's total memory. This could potentially happen if the cluster membership is not accurately known and the planner's cost computation of the broadcastCost vs partitionCost happens to favor the broadcast distribution. This causes spilling and severely affects performance. Although the DistributedPlanner does a mem_limit check before picking broadcast, the mem_limit is not an accurate reflection since it is assigned during admission control (See IMPALA-988). Given this scenario, as a safety check it is better to have to an explicit configurable limit for the size of the broadcast input and set it to a reasonable default. The 'reasonable' default can be chosen based on analysis of existing benchmark queries and representative workloads where Impala is currently used.",-0.1444,-0.2176,neutral
impala,9265,description,"If toolchain Kudu provided Java artifacts, then toolchain Kudu could be used standalone without needing a separate Maven repository for Kudu artifacts. This is nice, because it would allow us to use toolchain Kudu for both USE_CDP_HIVE=true and USE_CDP_HIVE=false without any Java artifact version conflicts. Having only a single version of Kudu to build against would simplify Kudu/Impala integration projects. To do this, the native toolchain (which may be a misnomer) would need to push Kudu artifacts to a repository. One option is for it to create a local filesystem repository and then include it in the kudu tarball produced.",design_debt,non-optimal_design,"Thu, 26 Dec 2019 21:14:10 +0000","Mon, 10 Feb 2020 21:07:29 +0000","Tue, 28 Jan 2020 16:20:58 +0000",2833608,"If toolchain Kudu provided Java artifacts, then toolchain Kudu could be used standalone without needing a separate Maven repository for Kudu artifacts. This is nice, because it would allow us to use toolchain Kudu for both USE_CDP_HIVE=true and USE_CDP_HIVE=false without any Java artifact version conflicts. Having only a single version of Kudu to build against would simplify Kudu/Impala integration projects. To do this, the native toolchain (which may be a misnomer) would need to push Kudu artifacts to a repository. One option is for it to create a local filesystem repository and then include it in the kudu tarball produced.",0.07192,0.07192,positive
impala,9467,summary,Impala Doc: Improve Impala shell usability by enabling live_progress in the interactive mode,design_debt,non-optimal_design,"Thu, 5 Mar 2020 21:31:32 +0000","Tue, 24 Mar 2020 17:57:28 +0000","Tue, 24 Mar 2020 17:57:28 +0000",1628756,Impala Doc: Improve Impala shell usability by enabling live_progress in the interactive mode,0.48125,0.48125,neutral
impala,9530,description,"In some cases pre-aggregations can balloon up and consume lots of memory, forcing the merge aggregation to spill. This is often OK as long as the preaggregation is reducing the input sufficiently, since it reduces the amount of data shuffled over the network. However in some cases it's preferable to be more conservative with memory and just cap the size of the preaggregation to prevent it ballooning too much. It would be useful to add a query option to directly limit the memory consumption of the preaggregations.",design_debt,non-optimal_design,"Tue, 17 Mar 2020 15:42:06 +0000","Wed, 20 May 2020 16:46:41 +0000","Fri, 20 Mar 2020 17:37:31 +0000",266125,"In some cases pre-aggregations can balloon up and consume lots of memory, forcing the merge aggregation to spill. This is often OK as long as the preaggregation is reducing the input sufficiently, since it reduces the amount of data shuffled over the network. However in some cases it's preferable to be more conservative with memory and just cap the size of the preaggregation to prevent it ballooning too much. It would be useful to add a query option to directly limit the memory consumption of the preaggregations.",0.3895833333,0.3895833333,neutral
impala,9560,description,"When working on the Impala 3.4 release, we changed the version on branch-3.4.0 from 3.4.0-SNAPSHOT to 3.4.0-RELEASE. now fails with the following error: The output is expecting a cardinality of 17.91K, but instead the cardinality is 17.90K. The RELEASE version has one character fewer than the SNAPSHOT version. The version gets embedded in parquet files, so the parquet file is slightly smaller than before. The test is estimating cardinality by looking at the size of the parquet file. Apparently, this is right on the edge. This test should tolerate this difference.",design_debt,non-optimal_design,"Fri, 27 Mar 2020 17:19:32 +0000","Sat, 28 Mar 2020 21:37:45 +0000","Fri, 27 Mar 2020 23:31:42 +0000",22330,"When working on the Impala 3.4 release, we changed the version on branch-3.4.0 from 3.4.0-SNAPSHOT to 3.4.0-RELEASE. metadata/test_stats_extrapolation.py::TestStatsExtrapolation::test_stats_extrapolation() now fails with the following error: The output is expecting a cardinality of 17.91K, but instead the cardinality is 17.90K. The RELEASE version has one character fewer than the SNAPSHOT version. The version gets embedded in parquet files, so the parquet file is slightly smaller than before. The test is estimating cardinality by looking at the size of the parquet file. Apparently, this is right on the edge. This test should tolerate this difference.",0.090875,0.08077777778,neutral
impala,979,description,If there is an error starting the statestore subscriber BE (heartbeat server) an Impala service should abort. This helps make it easier to diagnose issues such as port conflicts. This currently doesn't happen because we ignore the Status returned by the heartbeat_server_- From,design_debt,non-optimal_design,"Sun, 4 May 2014 22:05:08 +0000","Sun, 20 Dec 2015 00:05:15 +0000","Wed, 7 May 2014 21:51:35 +0000",258387,If there is an error starting the statestore subscriber BE (heartbeat server) an Impala service should abort. This helps make it easier to diagnose issues such as port conflicts. This currently doesn't happen because we ignore the Status returned by the heartbeat_server_->Start() call: From state-store-subscriber.cc:,-0.2,-0.15,neutral
impala,1587,comment_1,The WITH REPLICATION syntax will be in the next doc refresh for the appropriate release(s).,documentation_debt,outdated_documentation,"Tue, 9 Dec 2014 22:07:22 +0000","Thu, 25 Jun 2015 23:35:52 +0000","Tue, 27 Jan 2015 04:32:33 +0000",4170311,The WITH REPLICATION syntax will be in the next doc refresh for the appropriate release(s).,0.75,0.75,neutral
impala,2076,comment_0,The DataStreamSender's total time includes a lot of things that don't necessarily contribute to network bottlenecks: # Time taken to serialize batches # Time taken waiting for concurrent RPCs to finish (in random case) # Time taken blocked waiting for RPC response when receiver hasn't yet arrived (after IMPALA-1599) What precisely is it that the EXCHANGE node's time should measure? Should it just be processing time + time spent in {{read()}} calls in the underlying socket? Is this what you mean by 'active' time? I think we could add instrumentation to Thrift to measure the latter (by subclassing {{TSocket}}).,documentation_debt,low_quality_documentation,"Thu, 18 Jun 2015 02:08:59 +0000","Thu, 28 Apr 2016 23:57:23 +0000","Thu, 21 Apr 2016 18:24:19 +0000",26669720,The DataStreamSender's total time includes a lot of things that don't necessarily contribute to network bottlenecks: Time taken to serialize batches Time taken waiting for concurrent RPCs to finish (in random case) Time taken blocked waiting for RPC response when receiver hasn't yet arrived (after IMPALA-1599) What precisely is it that the EXCHANGE node's time should measure? Should it just be processing time + time spent in read() calls in the underlying socket? Is this what you mean by 'active' time? I think we could add instrumentation to Thrift to measure the latter (by subclassing TSocket).,0.03333333333,0.03333333333,neutral
impala,3403,description,"Currently, most of the verbiage around installation concerns the Cloudera Manager paths for packages or parcels. Probably there will be some new details in the context of an ASF release. Perhaps some of this info can be delegated to build instructions, README types of files, ancillary to the main docs containing the SQL Ref and such. That might be easier for maintenance where there are fast-changing version numbers, package names, etc. (Just an idea.)",documentation_debt,low_quality_documentation,"Fri, 22 Apr 2016 20:11:54 +0000","Fri, 16 Jun 2017 19:01:56 +0000","Tue, 4 Apr 2017 20:49:54 +0000",29983080,"Currently, most of the verbiage around installation concerns the Cloudera Manager paths for packages or parcels. Probably there will be some new details in the context of an ASF release. Perhaps some of this info can be delegated to build instructions, README types of files, ancillary to the main docs containing the SQL Ref and such. That might be easier for maintenance where there are fast-changing version numbers, package names, etc. (Just an idea.)",0.0625,0.0625,neutral
impala,3403,summary,Rework Impala installation instructions to be generic,documentation_debt,low_quality_documentation,"Fri, 22 Apr 2016 20:11:54 +0000","Fri, 16 Jun 2017 19:01:56 +0000","Tue, 4 Apr 2017 20:49:54 +0000",29983080,Rework Impala installation instructions to be generic,0,0,neutral
impala,3671,comment_1,- Docs work required.,documentation_debt,outdated_documentation,"Fri, 3 Jun 2016 21:31:45 +0000","Thu, 15 Dec 2016 00:36:24 +0000","Sat, 24 Sep 2016 04:53:45 +0000",9703320,jrussell - Docs work required.,0.6,0.6,neutral
impala,4009,description,"We cannot distribute Oracle binaries with Impala, but it will be useful to explain to someone how to get Oracle set up (server, client) so that he may use it as a reference db for the query generator.",documentation_debt,low_quality_documentation,"Tue, 23 Aug 2016 17:45:31 +0000","Thu, 25 Aug 2016 15:40:08 +0000","Thu, 25 Aug 2016 15:40:08 +0000",165277,"We cannot distribute Oracle binaries with Impala, but it will be useful to explain to someone how to get Oracle set up (server, client) so that he may use it as a reference db for the query generator.",0.5,0.5,neutral
impala,4245,comment_0,The links do not exist in the Apache Impala docs. Will address where applicable.,documentation_debt,outdated_documentation,"Tue, 4 Oct 2016 10:10:27 +0000","Fri, 25 May 2018 20:55:56 +0000","Fri, 25 May 2018 20:55:56 +0000",51705929,The links do not exist in the Apache Impala docs. Will address where applicable.,0.25,0.25,neutral
impala,4245,description,"The doc page for Impala UDFs contains multiple links to the sample UDF repo published by Cloudera. However, these links are inconsistent, they point to multiple locations. In the order of occurrence: 1. Just above section (below the code block) there is a link to This is the master head; subject to change. 2. The last paragraph of section points to the UDF sample repo at this is probably the intended location. 3. Typo: The second half of section contains links to the files uda-sample.h and uda-sample.cc; here the links are reversed compared to the file names (possibly a copy-paste mixup). The links point to the UDF sample repo, which I assume is the right location. 4. The last line of the same section contains a link to Cloudera's internal github. This probably wanted to be 5. Section again points to teh sample UDF repo which I assume is the correct location.",documentation_debt,low_quality_documentation,"Tue, 4 Oct 2016 10:10:27 +0000","Fri, 25 May 2018 20:55:56 +0000","Fri, 25 May 2018 20:55:56 +0000",51705929,"The doc page for Impala UDFs (http://www.cloudera.com/documentation/enterprise/latest/topics/impala_udf.html#udf_building) contains multiple links to the sample UDF repo published by Cloudera. However, these links are inconsistent, they point to multiple locations. In the order of occurrence: 1. Just above section http://www.cloudera.com/documentation/enterprise/latest/topics/impala_udf.html#udf_runtime (below the code block) there is a link to https://github.com/cloudera/impala/tree/master/be/src/udf_samples This is the master head; subject to change. 2. The last paragraph of section http://www.cloudera.com/documentation/enterprise/latest/topics/impala_udf.html#udf_demo_env points to the UDF sample repo at https://github.com/cloudera/impala-udf-samples; this is probably the intended location. 3. Typo: The second half of section http://www.cloudera.com/documentation/enterprise/latest/topics/impala_udf.html#udafs contains links to the files uda-sample.h and uda-sample.cc; here the links are reversed compared to the file names (possibly a copy-paste mixup). The links point to the UDF sample repo, which I assume is the right location. 4. The last line of the same section contains a link to Cloudera's internal github. This probably wanted to be https://github.com/cloudera/Impala/blob/cdh5-trunk/be/src/testutil/test-udas.cc 5. Section http://www.cloudera.com/documentation/enterprise/latest/topics/impala_udf.html#udf_tutorial again points to teh sample UDF repo (https://github.com/cloudera/impala-udf-samples), which I assume is the correct location.",0.1034666667,0.1034666667,neutral
impala,4245,summary,UDF docs refer to inconsistent GitHubs for samples,documentation_debt,low_quality_documentation,"Tue, 4 Oct 2016 10:10:27 +0000","Fri, 25 May 2018 20:55:56 +0000","Fri, 25 May 2018 20:55:56 +0000",51705929,UDF docs refer to inconsistent GitHubs for samples,0,0,neutral
impala,4778,description,The issue in IMPALA-1972 doesn't appear to be actively worked on and should be noted in the documentation unless and until it is fixed.,documentation_debt,low_quality_documentation,"Wed, 18 Jan 2017 05:51:21 +0000","Wed, 7 Jun 2017 14:41:30 +0000","Tue, 23 May 2017 22:23:47 +0000",10859546,The issue in IMPALA-1972 doesn't appear to be actively worked on and should be noted in the documentation unless and until it is fixed.,-0.875,-0.875,negative
impala,4956,description,"[This refers to the old mnemonics of EXPLAIN_LEVEL, when {{0}} meant {{normal}} and {{1}} meant {{verbose}}. It should be updated to [the new  - I'm assigning this to you thinking you may know best who can take this on. Please let me know if you want me to find someone else.",documentation_debt,outdated_documentation,"Mon, 20 Feb 2017 21:33:55 +0000","Fri, 23 Jun 2017 23:06:23 +0000","Fri, 23 Jun 2017 23:06:23 +0000",10632748,"This page refers to the old mnemonics of EXPLAIN_LEVEL, when 0 meant normal and 1 meant verbose. It should be updated to the new mnemonics. jrussell - I'm assigning this to you thinking you may know best who can take this on. Please let me know if you want me to find someone else.",0.2958333333,0.221875,neutral
impala,4956,summary,Page on EXPLAIN plans refers to outdated mnemonics,documentation_debt,outdated_documentation,"Mon, 20 Feb 2017 21:33:55 +0000","Fri, 23 Jun 2017 23:06:23 +0000","Fri, 23 Jun 2017 23:06:23 +0000",10632748,Page on EXPLAIN plans refers to outdated mnemonics,0,0,neutral
impala,5636,description,"The Parquet writer always adds the BIT_PACKED and RLE encodings even though (I'm pretty sure) we never write out the BIT_PACKED encoding for rep or def levels. The BIT_PACKED encoding is deprecated according to the Parquet specification: and it is not clear that Impala can even read it correctly: IMPALA-3006 One way of seeing that Impala claims to need the BIT_PACKED encoding is to write a parquet file with Impala then inspect it with parquet-tool. BIT_PACKED is never written by Impala's parquet writer - the definition levels are always written using an RleEncoder and reported as Encoding::RLE. Weirdly, the repetition levels are reported as ""BIT_PACKED"", but this encoding has no effect since we don't actually write out repetition levels.",documentation_debt,outdated_documentation,"Sun, 9 Jul 2017 21:08:28 +0000","Thu, 17 Aug 2017 00:23:43 +0000","Mon, 31 Jul 2017 17:37:57 +0000",1888169,"The Parquet writer always adds the BIT_PACKED and RLE encodings even though (I'm pretty sure) we never write out the BIT_PACKED encoding for rep or def levels. The BIT_PACKED encoding is deprecated according to the Parquet specification: https://github.com/Parquet/parquet-format/blob/master/Encodings.md and it is not clear that Impala can even read it correctly: IMPALA-3006 One way of seeing that Impala claims to need the BIT_PACKED encoding is to write a parquet file with Impala then inspect it with parquet-tool. BIT_PACKED is never written by Impala's parquet writer - the definition levels are always written using an RleEncoder and reported as Encoding::RLE. Weirdly, the repetition levels are reported as ""BIT_PACKED"", but this encoding has no effect since we don't actually write out repetition levels.",0.05,0.05,negative
impala,5763,comment_0,"Maybe we should just improve the documentation and discourage setting it to that value? This behaviour actually kind of makes sense to me, since the flag is controlling a trade-off between the cost of flushing log files and the delay. Setting it to 0 means to me ""I don't care about how expensive it is, just minimise the delay."" I'm sure we could optimise it and get the same frequency of flushing with less cost, but it seems a really niche optimisation. It does call sleep(0) so will yield to another thread if one is running so it won't eat up a full core on a busy system.",documentation_debt,low_quality_documentation,"Thu, 3 Aug 2017 23:48:56 +0000","Fri, 22 Feb 2019 17:34:53 +0000","Fri, 22 Jun 2018 01:41:03 +0000",27827527,"Maybe we should just improve the documentation and discourage setting it to that value? This behaviour actually kind of makes sense to me, since the flag is controlling a trade-off between the cost of flushing log files and the delay. Setting it to 0 means to me ""I don't care about how expensive it is, just minimise the delay."" I'm sure we could optimise it and get the same frequency of flushing with less cost, but it seems a really niche optimisation. It does call sleep(0) so will yield to another thread if one is running so it won't eat up a full core on a busy system.",-0.0933,-0.0933,neutral
impala,5763,comment_1,Making it more clear in the docs sounds good to me.  - What do you think?,documentation_debt,low_quality_documentation,"Thu, 3 Aug 2017 23:48:56 +0000","Fri, 22 Feb 2019 17:34:53 +0000","Fri, 22 Jun 2018 01:41:03 +0000",27827527,Making it more clear in the docs sounds good to me. jrussell - What do you think?,0.776,0.388,positive
impala,6408,comment_0,"I have found another minor insert hint related issue in ""To use a hint to influence the join order, put the hint keyword /* +SHUFFLE */ or /* +NOSHUFFLE */ (including the square brackets) after the PARTITION clause, immediately before the SELECT keyword."" I do not think that any join order is influenced by SHUFFLE in inserts, and there is also an inconsistency in hint style: "" /* +NOSHUFFLE */ (including the square brackets)""",documentation_debt,low_quality_documentation,"Wed, 17 Jan 2018 15:48:26 +0000","Wed, 13 Jun 2018 15:48:46 +0000","Tue, 12 Jun 2018 15:39:57 +0000",12613891,"I have found anotherminor insert hint related issue in topics/impala_parquet.html#parquet_etl: ""To use a hint to influence the join order, put the hint keyword /* +SHUFFLE / or / +NOSHUFFLE */ (including the square brackets) after the PARTITION clause, immediately before the SELECT keyword."" I do not think that any join order is influenced by SHUFFLE in inserts, and there is also an inconsistency in hint style: "" /* +NOSHUFFLE */ (including the square brackets)""",0.122,0.08133333333,negative
impala,6408,description,"The change in IMPALA-3930 states that if only one partition is written (because all partitioning columns are constant or the target table is not partitioned), then the ""shuffle"" hint leads to a plan where all rows are merged at the coordinator where the table sink is executed. The documentation of the ""shuffle"" hint does not mention this behavior.",documentation_debt,low_quality_documentation,"Wed, 17 Jan 2018 15:48:26 +0000","Wed, 13 Jun 2018 15:48:46 +0000","Tue, 12 Jun 2018 15:39:57 +0000",12613891,"The change in IMPALA-3930 states that if only one partition is written (because all partitioning columns are constant or the target table is not partitioned), then the ""shuffle""hintleads to a plan where all rows are merged at the coordinator where the table sink is executed. The documentation of the ""shuffle"" hint does not mention this behavior.",-0.3125,-0.3125,neutral
impala,6408,summary,"[DOCS] Description of ""shuffle"" hint does not mention changes in IMPALA-3930",documentation_debt,low_quality_documentation,"Wed, 17 Jan 2018 15:48:26 +0000","Wed, 13 Jun 2018 15:48:46 +0000","Tue, 12 Jun 2018 15:39:57 +0000",12613891,"[DOCS] Description of ""shuffle"" hint does not mention changes in IMPALA-3930",0,0,neutral
impala,6623,comment_0,The existing documentation: should be changed to: And for rtrim:,documentation_debt,low_quality_documentation,"Wed, 7 Mar 2018 23:04:59 +0000","Fri, 13 Apr 2018 22:59:54 +0000","Wed, 11 Apr 2018 22:08:43 +0000",3020624,The existing documentation: should be changed to: And for rtrim:,0,0,neutral
impala,6817,comment_1,Is there any user-facing feature change with this story? That needs to be documented?,documentation_debt,outdated_documentation,"Fri, 6 Apr 2018 03:14:31 +0000","Tue, 10 Apr 2018 17:48:45 +0000","Tue, 10 Apr 2018 14:02:53 +0000",384502,fredyw Is there any user-facing feature change with this story? That needs to be documented?,0,0,neutral
impala,6817,comment_2,Not for this. This story is just for code clean-up. It doesn't affect the behavior. We do need a documentation for this: As soon as we're done with we need to start documenting it.,documentation_debt,outdated_documentation,"Fri, 6 Apr 2018 03:14:31 +0000","Tue, 10 Apr 2018 17:48:45 +0000","Tue, 10 Apr 2018 14:02:53 +0000",384502,"Not for this. Thisstory is just for code clean-up. It doesn't affect the behavior. We do need a documentation for this: https://issues.apache.org/jira/browse/IMPALA-6648.As soon as we're done with https://issues.apache.org/jira/browse/IMPALA-6804,we need to start documenting it.",0.1,0.08,neutral
impala,7171,description,"On the page: at the end of the section: ""Impala DML Support for Kudu Tables (INSERT, UPDATE, DELETE, UPSERT)"", we should add text like: Starting from Impala 2.9, Impala will automatically add a partition and sort step to INSERTs before sending the rows to Kudu. Since Kudu partitions and sorts rows on write, pre-partitioning and sorting takes some of the load off of Kudu, and helps ensure that large INSERTs complete without timing out, but it may slow down the end-to-end performance of the INSERT. Starting from Impala 2.10, the hints ""/* */"" may be used to turn this pre-partitioning and sorting off. Additionally, since sorting may consume a lot of memory, users should consider setting a ""mem_limit"" for these queries.",documentation_debt,outdated_documentation,"Thu, 14 Jun 2018 18:06:25 +0000","Thu, 21 Feb 2019 05:00:44 +0000","Mon, 18 Jun 2018 23:55:57 +0000",366572,"On the page: http://impala.apache.org/docs/build3x/html/topics/impala_kudu.html, at the end of the section: ""Impala DML Support for Kudu Tables (INSERT, UPDATE, DELETE, UPSERT)"", we should add text like: Starting from Impala 2.9, Impala will automatically add a partition and sort step to INSERTs before sending the rows to Kudu. Since Kudu partitions and sorts rows on write, pre-partitioning and sorting takes some of the load off of Kudu, and helps ensure that large INSERTs complete without timing out, but it may slow down the end-to-end performance of the INSERT. Starting from Impala 2.10, the hints ""/* +noshuffle,noclustered */"" may be used to turn this pre-partitioning and sorting off. Additionally, since sorting may consume a lot of memory, users should consider setting a ""mem_limit"" for these queries.",-0.1243333333,-0.1243333333,neutral
impala,7434,summary,Impala 2.12 Doc: Kudu's kinit does not support auth_to_local rules with Heimdal kerberos,documentation_debt,outdated_documentation,"Mon, 13 Aug 2018 21:59:53 +0000","Sat, 8 Dec 2018 00:48:48 +0000","Sat, 8 Dec 2018 00:48:48 +0000",10032535,Impala 2.12 Doc: Kudu's kinit does not support auth_to_local rules with Heimdal kerberos,-0.4,-0.4,neutral
impala,7715,description,"Consider the documentation page [Impala Conditional Multiple functions have ambiguous descriptions. For example: The above is confusing, it essentially means: ""Returns true if a Boolean expression is false or not."" This obviously means the function always returns false, which is not accurate. Reword to say: ""Returns true if the expression is false. Returns false if the expression is true or NULL."" Other ambiguous descriptions: Better: ""Returns true if an expression is true. Returns false if the expression is false or NULL."" Others: Better: ""Returns true if the expression is non-null, tase if the expression is null. Same as {{expression IS NOT NULL}}."" Better: ""Returns true if the expression is NULL, false otherwise. Same as {{expression IS NULL}}""",documentation_debt,low_quality_documentation,"Tue, 16 Oct 2018 20:11:47 +0000","Tue, 23 Oct 2018 18:12:47 +0000","Wed, 17 Oct 2018 19:19:46 +0000",83279,"Consider the documentation page Impala Conditional Functions. Multiple functions have ambiguous descriptions. For example: isfalse(boolean) Purpose: Tests if a Boolean expression is false or not. Returns true if so. The above is confusing, it essentially means: ""Returns true if a Boolean expression is false or not."" This obviously means the function always returns false, which is not accurate. Reword to say: ""Returns true if the expression is false. Returns false if the expression is true or NULL."" Other ambiguous descriptions: istrue(boolean) Purpose: Tests if a Boolean expression is true or not. Returns true if so. Better: ""Returns true if an expression is true. Returns false if the expression is false or NULL."" Others: nonnullvalue(expression) Purpose: Tests if an expression (of any type) is NULL or not. Returns false if so. Better: ""Returns true if the expression is non-null, tase if the expression is null. Same as expression IS NOT NULL."" nullvalue(expression) Purpose: Tests if an expression (of any type) is NULL or not. Returns true if so. Better: ""Returns true if the expression is NULL, false otherwise. Same as expression IS NULL""",0.09571212121,0.1472416667,negative
impala,7861,description,"When it was originally submitted to Hadoop, the ABFS driver disabled TLS when using the URI scheme ""abfs"", and enabled TLS when using the URI scheme ""abfss"". This was reflected in the documentation originally submitted for ABFS: We should update that to reflect that TLS is enabled with either URI scheme unless you disable TLS in configuration by setting in the configuration.",documentation_debt,outdated_documentation,"Fri, 16 Nov 2018 11:20:17 +0000","Sat, 8 Dec 2018 00:49:36 +0000","Sat, 8 Dec 2018 00:49:36 +0000",1862959,"When it was originally submitted to Hadoop, the ABFS driver disabled TLS when using the URI scheme ""abfs"", and enabled TLS when using the URI scheme ""abfss"". This was reflected in the documentation originally submitted for ABFS: https://github.com/apache/impala/commit/5baa289fd039d339c63d5c475645e69fe0c6b8df. We should update that to reflect that TLS is enabled with either URI scheme unless you disable TLS in configuration by setting fs.azure.always.use.https=false in the configuration.",0,0.04285714286,neutral
impala,8146,description,These scripts are thin wrappers around make_impala.sh and don't add much value. We should remove them. I think there are a couple of references in the wiki that we can update to show alternative invocations.,documentation_debt,outdated_documentation,"Wed, 30 Jan 2019 10:44:43 +0000","Tue, 5 Feb 2019 10:00:46 +0000","Tue, 5 Feb 2019 10:00:46 +0000",515763,These scripts are thin wrappers around make_impala.sh and don't add much value. We should remove them. I think there are a couple of references in the wiki that we can update to show alternative invocations.,0,0,negative
impala,8429,description,The Query Option' page needs an update to reflect the changes in IMPALA-5120.,documentation_debt,outdated_documentation,"Wed, 17 Apr 2019 10:44:41 +0000","Mon, 12 Aug 2019 19:35:18 +0000","Mon, 12 Aug 2019 19:33:57 +0000",10140556,The 'DEFAULT_JOIN_DISTRIBUTION_MODE Query Option' page needs an update to reflect the changes in IMPALA-5120.,0,-0.15,neutral
impala,8855,description,"The documentation only mentions the values clause in the context of an INSERT statement. It can actually be used anywhere that a SELECT statement could be used, e.g. this is a valid query:",documentation_debt,low_quality_documentation,"Mon, 12 Aug 2019 20:35:28 +0000","Wed, 13 Nov 2019 22:10:11 +0000","Wed, 13 Nov 2019 19:43:16 +0000",8032068,"The documentation only mentions the values clause in the context of an INSERT statement. https://impala.apache.org/docs/build/html/topics/impala_insert.html It can actually be used anywhere that a SELECT statement could be used, e.g. this is a valid query:",0.344,0.344,neutral
impala,8855,summary,Impala docs do not mention all places VALUES clause can be used,documentation_debt,low_quality_documentation,"Mon, 12 Aug 2019 20:35:28 +0000","Wed, 13 Nov 2019 22:10:11 +0000","Wed, 13 Nov 2019 19:43:16 +0000",8032068,Impala docs do not mention all places VALUES clause can be used,0,0,neutral
impala,8945,description,"Reported by  The Impala docs entry for the IS DISTINCT FROM operator states: The <= But this expression is not equivalent to A <= (A IS NULL AND B IS NULL) OR ((A IS NOT NULL AND B IS NOT NULL) AND (A = B)) This expression should replace the existing incorrect expression. Another expression that is equivalent to A <= if(A IS NULL OR B IS NULL, A IS NULL AND B IS NULL, A = B) This one is a bit easier to follow. If you use this one in the docs, just replace the following line with: The <=> operator can use a hash join, while the if expression cannot.",documentation_debt,low_quality_documentation,"Sat, 14 Sep 2019 01:45:48 +0000","Tue, 17 Sep 2019 17:53:47 +0000","Mon, 16 Sep 2019 21:03:21 +0000",242253,"Reported by icook The Impala docs entry for the IS DISTINCT FROM operator states: The <=> operator, used like an equality operator in a join query, is more efficient than the equivalent clause: A = B OR (A IS NULL AND B IS NULL). The <=> operator can use a hash join, while the OR expression cannot. But this expression is not equivalent to A <=> B. See the attached screenshot demonstrating their non-equivalence. An expression that is equivalent to A <=> B is this: (A IS NULL AND B IS NULL) OR ((A IS NOT NULL AND B IS NOT NULL) AND (A = B)) This expression should replace the existing incorrect expression. Another expression that is equivalent to A <=> B is: if(A IS NULL OR B IS NULL, A IS NULL AND B IS NULL, A = B) This one is a bit easier to follow. If you use this one in the docs, just replace the following line with: The <=> operator can use a hash join, while the if expression cannot.",0.06666666667,0.1,neutral
impala,8945,summary,Impala Doc: Incorrect Claim of Equivalence in Impala Docs,documentation_debt,low_quality_documentation,"Sat, 14 Sep 2019 01:45:48 +0000","Tue, 17 Sep 2019 17:53:47 +0000","Mon, 16 Sep 2019 21:03:21 +0000",242253,Impala Doc: Incorrect Claim of Equivalence in Impala Docs,0,0,neutral
impala,9013,description,Review Hive implementation to see if anything special needs to be done for DML. The Hive column masking design doc does not reflect the current code.,documentation_debt,outdated_documentation,"Fri, 4 Oct 2019 23:40:08 +0000","Wed, 15 Jan 2020 00:22:05 +0000","Wed, 15 Jan 2020 00:22:05 +0000",8815317,Review Hive implementation to see if anything special needs to be done for DML. The Hive column masking design doc does not reflect the current code.,0,0,negative
impala,9431,description,"IMPALA-8549 added support for scanning deflate text files. The docs however still have following: ""Deflate Not supported for text files."" Update the docs to reflect that scanning deflate text files are supported.",documentation_debt,outdated_documentation,"Wed, 26 Feb 2020 18:27:16 +0000","Sat, 7 Mar 2020 16:23:44 +0000","Sat, 7 Mar 2020 16:23:44 +0000",856588,"IMPALA-8549 added support for scanning deflate text files. The docs however still have following: ""Deflate Not supported for text files."" Update the docs to reflect that scanning deflate text files are supported.",0.2468333333,0.2468333333,neutral
impala,9443,description,"The {{SHOW TABLE STATS}} output for HDFS tables is outdated on some doc sites or abbreviated with ellipsis. Additionally, some other tables can be broken as well, for example {{SHOW FILES IN}}. I am aware of the following pages: - -",documentation_debt,outdated_documentation,"Mon, 2 Mar 2020 15:49:36 +0000","Fri, 27 Mar 2020 23:39:16 +0000","Fri, 27 Mar 2020 23:39:15 +0000",2188179,"The SHOW TABLE STATS output for HDFS tables is outdated on some doc sites or abbreviated with ellipsis. Additionally, some other tables can be broken as well, for example SHOW FILES IN. I am aware of the following pages: https://impala.apache.org/docs/build/html/topics/impala_show.html https://impala.apache.org/docs/build/html/topics/impala_perf_stats.html",0.2595,0.2595,negative
impala,9467,description,"We enable shell option live_progress in interactive mode by default. As for in the non-interactive mode, live reporting is not supported. Impala-shell will disable live_progress if the mode is detected. Need to update the doc to reflect the changes.",documentation_debt,outdated_documentation,"Thu, 5 Mar 2020 21:31:32 +0000","Tue, 24 Mar 2020 17:57:28 +0000","Tue, 24 Mar 2020 17:57:28 +0000",1628756,"https://gerrit.cloudera.org/#/c/15219/ We enable shell option live_progress in interactive mode by default. As for in the non-interactive mode, live reporting is not supported. Impala-shell will disable live_progress if the mode is detected. Need to update the doc to reflect the changes.",0.03333333333,0.03333333333,neutral
impala,991,comment_2,"It means I will have to state an exception in the docs though. You can cast BOOLEAN to other numeric types but not DECIMAL: [localhost:21000] | cast(true as int) | | 1 | [localhost:21000] | cast(true as float) | | 1 | [localhost:21000] ERROR: AnalysisException: Invalid type cast of TRUE from BOOLEAN to DECIMAL(9,0)",documentation_debt,low_quality_documentation,"Fri, 9 May 2014 18:10:23 +0000","Tue, 2 Sep 2014 20:51:33 +0000","Wed, 30 Jul 2014 21:21:18 +0000",7096255,"It means I will have to state an exception in the docs though. You can cast BOOLEAN to other numeric types but not DECIMAL: [localhost:21000] > select cast(true as int); ------------------- ------------------- ------------------- [localhost:21000] > select cast(true as float); --------------------- --------------------- --------------------- [localhost:21000] > select cast(true as decimal); ERROR: AnalysisException: Invalid type cast of TRUE from BOOLEAN to DECIMAL(9,0)",0.021625,0.089625,neutral
impala,991,comment_3,I am using this opportunity to fill in some holes in the doc discussion of casting to/from BOOLEAN. BOOLEAN -BOOLEAN <-> TIMESTAMP treats false as the epoch date (1970-01-01 00:00:00) and true as 1 second later (1970-01-01 00:00:01).,documentation_debt,low_quality_documentation,"Fri, 9 May 2014 18:10:23 +0000","Tue, 2 Sep 2014 20:51:33 +0000","Wed, 30 Jul 2014 21:21:18 +0000",7096255,I am using this opportunity to fill in some holes in the doc discussion of casting to/from BOOLEAN. BOOLEAN -> numeric gives 1 or 0 for true or false. BOOLEAN <-> TIMESTAMP treats false as the epoch date (1970-01-01 00:00:00) and true as 1 second later (1970-01-01 00:00:01).,0.29325,0.2576666667,neutral
impala,2290,summary,BTrim() with non constant second argument is not thread safe.,requirement_debt,non-functional_requirements_not_fully_satisfied,"Thu, 3 Sep 2015 05:33:42 +0000","Wed, 9 Sep 2015 04:18:36 +0000","Wed, 9 Sep 2015 04:18:36 +0000",513894,BTrim() with non constant second argument is not thread safe.,-0.25,-0.25,negative
impala,4423,description,"Queries with several AND-ed EXISTS subqueries in the WHERE clause may produce incorrect results if some of the subqueries can be evaluated at query compile time. Repro with wrong plan: Same query as above but flipping the order of subqueries gives the correct plan: The underlying problem is that we substitute out the subqueries with constant literals using an but the Subquery.equals() function is not implemented properly, so the second subquery is replaced with whatever boolean literal corresponds to the first subquery.",requirement_debt,requirement_partially_implemented,"Wed, 2 Nov 2016 22:55:43 +0000","Tue, 15 Nov 2016 13:06:00 +0000","Fri, 4 Nov 2016 01:21:24 +0000",95141,"Queries with several AND-ed EXISTS subqueries in the WHERE clause may produce incorrect results if some of the subqueries can be evaluated at query compile time. Repro with wrong plan: Same query as above but flipping the order of subqueries gives the correct plan: The underlying problem is that we substitute out the subqueries with constant literals using an ExprSubstitutionMap, but the Subquery.equals() function is not implemented properly, so the second subquery is replaced with whatever boolean literal corresponds to the first subquery.",0.025,0.025,negative
impala,5130,summary,is not thread-safe,requirement_debt,non-functional_requirements_not_fully_satisfied,"Tue, 28 Mar 2017 18:59:35 +0000","Thu, 30 Mar 2017 00:21:05 +0000","Thu, 30 Mar 2017 00:21:05 +0000",105690,MemTracker::EnableReservationReporting() is not thread-safe,-0.25,-0.25,negative
impala,5489,description,"In IMPALA-4000 we added basic authorization support for Kudu tables, but it had several limitations: * Only the ALL privilege level can be granted to Kudu tables. (Finer-grained levels such as only SELECT or only INSERT are not supported.) * Column level permissions on Kudu tables are not supported. * Only users with ALL privileges on SERVER may create external Kudu tables. It looks like we could make the following work: * Allow column-level permissions * Allow fine grained privileges SELECT and INSERT for those statement types. However, would require ALL because Sentry doesn't have fine grained privilege actions for those types yet (work is planned though). So Impala can do this work, probably without much effort, but the question is whether or not it makes sense to implement this short-term solution in the context of the mid-to-longer term Kudu, Sentry, and Impala authorization plans. Kudu is currently figuring out what their authorization story will look like. Sentry is also poised for some large upcoming changes.",requirement_debt,requirement_partially_implemented,"Mon, 12 Jun 2017 21:53:42 +0000","Thu, 28 Sep 2017 22:26:54 +0000","Wed, 26 Jul 2017 13:23:42 +0000",3771000,"In IMPALA-4000 we added basic authorization support for Kudu tables, but it had several limitations: Only the ALL privilege level can be granted to Kudu tables. (Finer-grained levels such as only SELECT or only INSERT are not supported.) Column level permissions on Kudu tables are not supported. Only users with ALL privileges on SERVER may create external Kudu tables. It looks like we could make the following work: Allow column-level permissions Allow fine grained privileges SELECT and INSERT for those statement types. However, DELETE/UPDATE/UPSERT would require ALL because Sentry doesn't have fine grained privilege actions for those types yet (work is planned though). So Impala can do this work, probably without much effort, but the question is whether or not it makes sense to implement this short-term solution in the context of the mid-to-longer term Kudu, Sentry, and Impala authorization plans. Kudu is currently figuring out what their authorization story will look like. Sentry is also poised for some large upcoming changes.",0.05128888889,0.05128888889,neutral
impala,7682,summary,AuthorizationPolicy is not thread-safe,requirement_debt,non-functional_requirements_not_fully_satisfied,"Tue, 9 Oct 2018 18:24:48 +0000","Fri, 12 Oct 2018 17:11:38 +0000","Thu, 11 Oct 2018 01:40:18 +0000",112530,AuthorizationPolicy is not thread-safe,-0.25,-0.25,negative
impala,8771,description,"We currently don't support column stats for complex typed columns (ingored in `compute stats` statements). However running queries against those columns throws the missing col stats warning which is confusing. We could probably skip the warnings if we detect the missing stats are for complex typed columns, until we support them.",requirement_debt,requirement_partially_implemented,"Thu, 18 Jul 2019 21:38:54 +0000","Wed, 7 Aug 2019 14:23:13 +0000","Wed, 7 Aug 2019 14:23:13 +0000",1701859,"We currently don't support column stats for complex typed columns (ingored in `compute stats` statements). However running queries against those columns throws the missing col stats warning which is confusing.   We could probably skip the warnings if we detect the missing stats are for complex typed columns, until we support them.",-0.3596666667,-0.3596666667,negative
impala,1022,description,"rows_read < rows_in_file) { We should detect the case where doesn't actually equal the number of rows in the file. If abort_on_error is true, this should fail the query, otherwise we should log something via scan_node_-Such handling did not exist before. Need to decide whether we will read at most as many rows as the metadata or continue reading until there are no more rows in the file. We will need to add tests with parquet files whose metadata is not correct.",test_debt,lack_of_tests,"Fri, 30 May 2014 23:53:36 +0000","Sun, 20 Dec 2015 00:05:16 +0000","Tue, 10 Jun 2014 15:01:53 +0000",918497,"be/src/exec/hdfs-parquet-scanner.cc:736: rows_read < rows_in_file) { We should detect the case where file_metadata_.num_row doesn't actually equal the number of rows in the file. If abort_on_error is true, this should fail the query, otherwise we should log something via scan_node_>runtime_state()>LogError(). Such handling did not exist before. Need to decide whether we will read at most as many rows as the metadata or continue reading until there are no more rows in the file. We will need to add tests with parquet files whose metadata is not correct.",-0.2376666667,-0.1358095238,neutral
impala,1147,description,The view compatibility tests are disabled and need to be updated to run against Hive .13.,test_debt,lack_of_tests,"Mon, 11 Aug 2014 17:00:18 +0000","Sun, 20 Dec 2015 00:05:18 +0000","Tue, 2 Sep 2014 22:29:33 +0000",1920555,The view compatibility tests are disabled and need to be updated to run against Hive .13.,0,0,negative
impala,1147,summary,View compatibility tests need to be updated to run against Hive .13,test_debt,lack_of_tests,"Mon, 11 Aug 2014 17:00:18 +0000","Sun, 20 Dec 2015 00:05:18 +0000","Tue, 2 Sep 2014 22:29:33 +0000",1920555,View compatibility tests need to be updated to run against Hive .13,0,0,neutral
impala,1290,description,If an AnalyticEvalNode is the root of a plan tree it should have results ready when Open() returns so that clients do not time out once the query is in a ready state and expect to be able to fetch results. This only happens for unpartitioned analytic evaluation. Should test this in,test_debt,lack_of_tests,"Sat, 20 Sep 2014 23:24:53 +0000","Wed, 1 Oct 2014 14:59:30 +0000","Wed, 1 Oct 2014 14:59:30 +0000",920077,If an AnalyticEvalNode is the root of a plan tree it should have results ready when Open() returns so that clients do not time out once the query is in a ready state and expect to be able to fetch results. This only happens for unpartitioned analytic evaluation. Should test this in test_rows_availability.py,-0.07644444444,-0.05733333333,neutral
impala,1774,comment_2,"This is fixed in however, I'm waiting for us to rebase on the latest Hive bits in order to commit a test for it",test_debt,lack_of_tests,"Sat, 14 Feb 2015 06:25:02 +0000","Wed, 8 Apr 2015 09:04:31 +0000","Wed, 8 Apr 2015 09:04:31 +0000",4588769,"This is fixed in https://github.com/cloudera/Impala/commit/b7d41e57ba380a31429a5d0fdda4ee8723385349, however, I'm waiting for us to rebase on the latest Hive bits in order to commit a test for it (http://gerrit.cloudera.org:8080/#/c/151/).",0.4,0.4,neutral
impala,2178,comment_1,- that patch doesn't have query tests. Is there a simple repro query that we can add?,test_debt,lack_of_tests,"Wed, 5 Aug 2015 20:27:04 +0000","Fri, 9 Oct 2015 22:03:38 +0000","Fri, 28 Aug 2015 18:24:58 +0000",1979874,skye - that patch doesn't have query tests. Is there a simple repro query that we can add?,0,0,neutral
impala,2386,comment_0,"Isilon stress was run manually. Separate issue, IMPALA-2387 to document what was done. Future Isilon test coverage is to be tracked in a test plan.",test_debt,lack_of_tests,"Wed, 23 Sep 2015 00:55:37 +0000","Fri, 7 Oct 2016 17:17:06 +0000","Fri, 7 Oct 2016 17:17:06 +0000",32890889,"Isilon stress was run manually. Separate issue, IMPALA-2387 to document what was done. Future Isilon test coverage is to be tracked in a test plan.",-0.06666666667,-0.06666666667,neutral
impala,2386,description,We need to run the Isilon stress test on the RC.,test_debt,lack_of_tests,"Wed, 23 Sep 2015 00:55:37 +0000","Fri, 7 Oct 2016 17:17:06 +0000","Fri, 7 Oct 2016 17:17:06 +0000",32890889,We need to run the Isilon stress test on the RC.,-0.2,-0.2,neutral
impala,2724,comment_0,"Dan, you seem to be the last person who touched this test. It appears the flakiness of the test had to do with the general non-determinism of impala memory usage. I believe that's the same case for IMPALA-2728.",test_debt,flaky_test,"Sat, 28 Nov 2015 21:29:22 +0000","Tue, 15 Dec 2015 21:39:21 +0000","Tue, 15 Dec 2015 21:39:21 +0000",1469399,"Dan, you seem to be the last person who touched this test. It appears the flakiness of the test had to do with the general non-determinism of impala memory usage. I believe that's the same case for IMPALA-2728.",0,0,neutral
impala,3017,description,I saw a private build where an impalad appears to get OOM-killed while executing the below tests. I believe what is probably happening is that a bunch of large allocations happen at the same time.,test_debt,expensive_tests,"Wed, 17 Feb 2016 21:04:11 +0000","Thu, 18 Feb 2016 17:42:59 +0000","Thu, 18 Feb 2016 17:42:59 +0000",74328,I saw a private build where an impalad appears to get OOM-killed while executing the below tests. I believe what is probably happening is that a bunch of large allocations happen at the same time.,-0.275,-0.275,neutral
impala,3352,comment_0,"I've seen this before where ""hosts=2"" leads to a different plan. Maybe the tests should check that the number of hosts is as expected.",test_debt,low_coverage,"Thu, 14 Apr 2016 03:42:37 +0000","Mon, 1 May 2017 21:49:34 +0000","Mon, 1 May 2017 21:49:34 +0000",33070017,"I've seen this before where ""hosts=2"" leads to a different plan. Maybe the tests should check that the number of hosts is as expected.",0,0,neutral
impala,3718,description,The functional test coverage for Impala on Kudu can be significantly improved by doing the following: * Run TPC-H and TPC-DS against Kudu tables * Add an alltypes test table that covers all the supported Kudu data types and expand existing functional tests to use this table * Add more complex queries in the planner tests that query both Kudu and Hdfs tables * Add more complex expressions (e.g. conditional functions) in the SET portion of UPDATE statements in functional query tests * Improve the test coverage for using and computing stats in Kudu tables,test_debt,low_coverage,"Fri, 10 Jun 2016 17:34:15 +0000","Tue, 11 Oct 2016 03:31:11 +0000","Mon, 10 Oct 2016 17:30:58 +0000",10540603,The functional test coverage for Impala on Kudu can be significantly improved by doing the following: Run TPC-H and TPC-DS against Kudu tables Add an alltypes test table that covers all the supported Kudu data types and expand existing functional tests to use this table Add more complex queries in the planner tests that query both Kudu and Hdfs tables Add more complex expressions (e.g. conditional functions) in the SET portion of UPDATE statements in functional query tests Improve the test coverage for using and computing stats in Kudu tables,0.35,0.35,neutral
impala,4145,comment_0,This is leftover from a previous test plan. We still want to do this kind of testing but need to rescope it.,test_debt,lack_of_tests,"Sat, 17 Sep 2016 03:15:12 +0000","Wed, 11 Jul 2018 16:54:44 +0000","Wed, 11 Jul 2018 16:54:44 +0000",57245972,This is leftover from a previous test plan. We still want to do this kind of testing but need to rescope it.,0.12025,0.12025,neutral
impala,4387,description,Haven't seen this before; may be a flaky test.,test_debt,flaky_test,"Thu, 27 Oct 2016 16:20:42 +0000","Tue, 15 Nov 2016 13:05:59 +0000","Sun, 30 Oct 2016 21:11:26 +0000",276644,Haven't seen this before; may be a flaky test.,0,0,neutral
impala,5084,description,See IMPALA-3208 for the context. Sorter::Run changes: * We can use a similar approach to that used for BufferedTupleStream as described in IMPALA-5085 Testing: Needs end-to-end tests exercising all operators with large operators,test_debt,low_coverage,"Thu, 16 Mar 2017 22:49:31 +0000","Sat, 15 Apr 2017 00:12:16 +0000","Sat, 15 Apr 2017 00:12:16 +0000",2510565,See IMPALA-3208 for the context. Sorter::Run changes: We can use a similar approach to that used for BufferedTupleStream as described in IMPALA-5085 Testing: Needs end-to-end tests exercising all operators with large operators,0,0,neutral
impala,5341,description,"In our planner tests we want to ignore minor differences in reported file sizes to avoid flaky tests, see IMPALA-2565. However, the introduced filter also matches ""row-size="" which we do not want to filter. Relevant code is in TestUtils.java:",test_debt,flaky_test,"Fri, 19 May 2017 04:36:45 +0000","Thu, 16 Nov 2017 10:27:23 +0000","Thu, 16 Nov 2017 10:27:23 +0000",15659438,"In our planner tests we want to ignore minor differences in reported file sizes to avoid flaky tests, see IMPALA-2565. However, the introduced filter also matches ""row-size="" which we do not want to filter. Relevant code is in TestUtils.java:",-0.06666666667,-0.06666666667,neutral
impala,5499,description,I suspect this is a rare flaky test failure. Filing a JIRA so that we can see if it reoccurs. This test failed and everything else passed. It looked like for some reason the port was occupied and the process then crashed on teardown. Info log: Error log:,test_debt,flaky_test,"Tue, 13 Jun 2017 17:36:12 +0000","Fri, 8 Jun 2018 06:33:00 +0000","Tue, 17 Apr 2018 17:15:30 +0000",26609958,I suspect this is a rare flaky test failure. Filing a JIRA so that we can see if it reoccurs. This test failed and everything else passed. It looked like for some reason the port was occupied and the process then crashed on teardown. Info log: Error log:,-0.22,-0.22,negative
impala,5525,description,"TestScannersFuzzing currently tests compressed parquet but does not test uncompressed parquet. Uncompressed parquet would help with test coverage because there's more potential to corrupt the actual data in interesting ways, not just the headers. To do this we'd probably need to set do a create table as select to write out some parquet data, then do the fuzz testing on that.",test_debt,low_coverage,"Fri, 16 Jun 2017 17:25:34 +0000","Mon, 9 Oct 2017 18:29:06 +0000","Mon, 9 Oct 2017 18:29:06 +0000",9939812,"TestScannersFuzzing currently tests compressed parquet but does not test uncompressed parquet. Uncompressed parquet would help with test coverage because there's more potential to corrupt the actual data in interesting ways, not just the headers. To do this we'd probably need to set COMPRESSION_CODEC=none, do a create table as select to write out some parquet data, then do the fuzz testing on that.",0.2515555556,0.2515555556,neutral
impala,5535,description,"Make sure that we have test coverage for large null-aware anti joins. * The streams start off unpinned, so can spill even when we don't go down the normal spilling path. I think my current iteration of the BufferPool patch doesn't test NAAJ when spilling is enabled",test_debt,low_coverage,"Mon, 19 Jun 2017 22:44:24 +0000","Tue, 8 Aug 2017 23:53:00 +0000","Tue, 8 Aug 2017 23:53:00 +0000",4324116,"Make sure that we have test coverage for large null-aware anti joins. The streams start off unpinned, so can spill even when we don't go down the normal spilling path. I think my current iteration of the BufferPool patch doesn't test NAAJ when spilling is enabled",0.02583333333,0.02583333333,neutral
impala,5535,summary,Ensure test coverage for spilling and no-spilling cases,test_debt,low_coverage,"Mon, 19 Jun 2017 22:44:24 +0000","Tue, 8 Aug 2017 23:53:00 +0000","Tue, 8 Aug 2017 23:53:00 +0000",4324116,Ensure Null-aware-anti-join test coverage for spilling and no-spilling cases,0.2,0.19075,neutral
impala,5636,comment_0,"I think just replacing 2 occurrences of with Encoding::RLE makes sense. I've manually tested with parquet-tools and it seems to work. Since parquet-tools is not in our distribution, I'm not sure how to write a test. Any suggestion?",test_debt,lack_of_tests,"Sun, 9 Jul 2017 21:08:28 +0000","Thu, 17 Aug 2017 00:23:43 +0000","Mon, 31 Jul 2017 17:37:57 +0000",1888169,"I think just replacing 2 occurrences of Encoding::BIT_PACKED with Encoding::RLE makes sense. I've manually tested with parquet-tools and it seems to work. Since parquet-tools is not in our distribution, I'm not sure how to write a test. Any suggestion?",0.145375,0.145375,neutral
impala,5640,description,There's a comment in the code saying that it was disabled because of IMPALA-424. This test coverage seems useful - we should add it back,test_debt,low_coverage,"Mon, 10 Jul 2017 21:20:30 +0000","Wed, 12 Jul 2017 00:38:22 +0000","Wed, 12 Jul 2017 00:38:22 +0000",98272,There's a comment in the code saying that it was disabled because of IMPALA-424. This test coverage seems useful - we should add it back,0.25,0.25,neutral
impala,5640,summary,Enable test coverage for Parquet gzip inserts was disabled,test_debt,low_coverage,"Mon, 10 Jul 2017 21:20:30 +0000","Wed, 12 Jul 2017 00:38:22 +0000","Wed, 12 Jul 2017 00:38:22 +0000",98272,Enable test coverage for Parquet gzip inserts was disabled,0.5,0.5,neutral
impala,5688,description,"Two tests ({{LongReverse}} and the base64 tests in run their tests over all lengths from 0..{{some length}}. Both take several minutes to complete. This adds a lot of runtime for not much more confidence. If instead we pick a set of 'interesting' (including powers-of-two, prime numbers, edge-cases) lengths, we can get a similar amount of confidence while significantly reducing the runtime of expr-test.",test_debt,expensive_tests,"Thu, 20 Jul 2017 17:54:43 +0000","Sat, 22 Jul 2017 21:12:03 +0000","Sat, 22 Jul 2017 21:12:03 +0000",184640,"Two tests (LongReverse and the base64 tests in StringFunctions) run their tests over all lengths from 0..some length. Both take several minutes to complete. This adds a lot of runtime for not much more confidence. If instead we pick a set of 'interesting' (including powers-of-two, prime numbers, edge-cases) lengths, we can get a similar amount of confidence while significantly reducing the runtime of expr-test.",0.003125,0.003125,neutral
impala,5688,summary,Speed up a couple of heavy-hitting expr-tests,test_debt,expensive_tests,"Thu, 20 Jul 2017 17:54:43 +0000","Sat, 22 Jul 2017 21:12:03 +0000","Sat, 22 Jul 2017 21:12:03 +0000",184640,Speed up a couple of heavy-hitting expr-tests,0,0,neutral
impala,5732,comment_1,"This Jira is old, has little information, and there have been several recent JIRAs filed for similar flakiness in this test, so there's not really any value to keeping this around.",test_debt,flaky_test,"Thu, 27 Jul 2017 15:16:15 +0000","Tue, 22 May 2018 18:28:25 +0000","Thu, 25 Jan 2018 23:23:45 +0000",15754050,"This Jira is old, has little information, and there have been several recent JIRAs filed for similar flakiness in this test, so there's not really any value to keeping this around.",0,0,negative
impala,5732,description,"failed on an exhaustive integration jenkins run. I'm not sure if the test is flaky (is it safe to check {{row_regex: .*Rows rejected: 2.43K .*}} - will that be deterministic?) or if there was some other problem, e.g. missing stats and the plan had the wrong join order.",test_debt,flaky_test,"Thu, 27 Jul 2017 15:16:15 +0000","Tue, 22 May 2018 18:28:25 +0000","Thu, 25 Jan 2018 23:23:45 +0000",15754050,"TestRuntimeRowFilters::test_row_filters failed on an exhaustive integration jenkins run. I'm not sure if the test is flaky (is it safe to check row_regex: .Rows rejected: 2.43K . - will that be deterministic?) or if there was some other problem, e.g. missing stats and the plan had the wrong join order.",-0.1225,-0.153125,negative
impala,5732,summary,flakiness on exhaustive run,test_debt,flaky_test,"Thu, 27 Jul 2017 15:16:15 +0000","Tue, 22 May 2018 18:28:25 +0000","Thu, 25 Jan 2018 23:23:45 +0000",15754050,TestRuntimeRowFilters flakiness on exhaustive run,0.5,0.5,neutral
impala,5779,description,We do not have any end-to-end tests where we attempt to spill buffers larger than the I/O size. I have tested manually but we need some automated testing.,test_debt,lack_of_tests,"Wed, 9 Aug 2017 00:07:13 +0000","Thu, 24 Aug 2017 15:18:48 +0000","Thu, 24 Aug 2017 15:18:48 +0000",1350695,We do not have any end-to-end tests where we attempt to spill buffers larger than the I/O size. I have tested manually but we need some automated testing.,0.2815,0.2815,neutral
impala,5780,description,We do not have any end-to-end test coverage with = true. We should add basic tests for the success and OOM cases where querying a table with no stats with = true.,test_debt,low_coverage,"Wed, 9 Aug 2017 01:04:41 +0000","Thu, 24 Aug 2017 15:18:41 +0000","Thu, 24 Aug 2017 15:18:41 +0000",1347240,We do not have any end-to-end test coverage with disable_unsafe_spills = true. We should add basic tests for the success and OOM cases where querying a table with no stats with disable_unsafe_spills = true.,-0.32975,0.08833333333,neutral
impala,5780,summary,Add missing test coverage for,test_debt,low_coverage,"Wed, 9 Aug 2017 01:04:41 +0000","Thu, 24 Aug 2017 15:18:41 +0000","Thu, 24 Aug 2017 15:18:41 +0000",1347240,Add missing test coverage for disable_unsafe_spills,-0.4,-0.5125,negative
impala,6106,summary,test_tpcds_q53 extremely flaky because of decimal_v2 not being reset,test_debt,flaky_test,"Wed, 25 Oct 2017 20:41:34 +0000","Wed, 15 Nov 2017 01:32:44 +0000","Thu, 26 Oct 2017 01:51:00 +0000",18566,test_tpcds_q53 extremely flaky because of decimal_v2 not being reset,0,0,negative
impala,6601,description,"ASAN fails with in during  - Im assigning this to you thinking you might have an idea whats going on here; feel free to find another person or assign back to me if you're swamped. Ive seen this happen in a private Jenkins run. Please ping me if you would like access to the build artifacts. I saw this in builds that also had issues during the e2e tests, so I'm not sure whether this is flaky or reproducibly broken.",test_debt,flaky_test,"Sat, 3 Mar 2018 01:55:02 +0000","Tue, 6 Mar 2018 17:34:44 +0000","Tue, 6 Mar 2018 17:34:44 +0000",315582,"ASAN fails with memcpy-param-overlap in {{impala::RawValue::Write during RowBatchSerializeTest.RowBatchLZ4Success: joemcdonnell - Im assigning this to you thinking you might have an idea whats going on here; feel free to find another person or assign back to me if you're swamped. Ive seen this happen in a private Jenkins run. Please ping me if you would like access to the build artifacts. I saw this in builds that also had issues during the e2e tests, so I'm not sure whether this is flaky or reproducibly broken.",0.125,0.07,negative
impala,6937,comment_2,As expected a subsequent build passed so I'm marking this one as flaky.,test_debt,flaky_test,"Thu, 26 Apr 2018 01:03:00 +0000","Mon, 4 Jun 2018 16:00:07 +0000","Mon, 4 Jun 2018 16:00:06 +0000",3423426,As expected a subsequent build passed so I'm marking this one as flaky.,0,0,neutral
impala,72,comment_0,Hm - we have a test that tests almost exactly this case: We had better check that the test is running as expected.,test_debt,lack_of_tests,"Tue, 19 Feb 2013 22:18:38 +0000","Mon, 4 Mar 2013 17:27:09 +0000","Mon, 4 Mar 2013 17:27:09 +0000",1105711,Hm - we have a test that tests almost exactly this case: We had better check that the test is running as expected.,0.5,0.5,neutral
impala,8168,description,"Currently we enable Sentry HDFS sync when running on S3 which sometimes can make the authorization tests on S3 flaky. Since HDFS sync is not supported on S3, we should update our build script to only enable Sentry HDFS sync on HDFS and not on S3.",test_debt,flaky_test,"Wed, 6 Feb 2019 17:57:04 +0000","Thu, 14 Mar 2019 14:04:21 +0000","Fri, 22 Feb 2019 00:12:41 +0000",1318537,"Currently we enable Sentry HDFS sync when running on S3 which sometimes can make the authorization tests on S3 flaky. Since HDFS sync is not supported on S3, we should update our build script to only enable Sentry HDFS sync on HDFS and not on S3.",0.3875,0.3875,neutral
impala,8248,description,We have authorization tests that are specific to Sentry and authorization tests that can be applicable to any authorization provider. We need to re-organize the authorization tests to easily differentiate between Sentry-specific tests vs generic authorization tests. This also will improve test coverage for Ranger.,test_debt,low_coverage,"Tue, 26 Feb 2019 06:59:59 +0000","Tue, 28 May 2019 10:26:13 +0000","Fri, 24 May 2019 20:39:06 +0000",7565947,We have authorization tests that are specific to Sentry and authorization tests that can be applicable to any authorization provider. We need to re-organize the authorization tests to easily differentiate between Sentry-specific tests vs generic authorization tests. This also will improve test coverage for Ranger.,0.3,0.3,neutral
impala,8248,summary,Improve Ranger test coverage,test_debt,low_coverage,"Tue, 26 Feb 2019 06:59:59 +0000","Tue, 28 May 2019 10:26:13 +0000","Fri, 24 May 2019 20:39:06 +0000",7565947,Improve Ranger test coverage,0.4,0.4,neutral
impala,8534,comment_0,"I'm not going to get to this right away, although I'd like to circle back. If you want to turn it on to get some additional testing, feel free to pick it up - I can provide any pointers if you need them.",test_debt,low_coverage,"Fri, 10 May 2019 15:39:22 +0000","Tue, 6 Aug 2019 20:45:17 +0000","Tue, 6 Aug 2019 20:45:17 +0000",7621555,"kwho I'm not going to get to this right away, although I'd like to circle back. If you want to turn it on to get some additional testing, feel free to pick it up - I can provide any pointers if you need them.",-0.1635,-0.1635,neutral
impala,8857,description,"I believe this is a flaky tests, since there's no attempt to pass the timestamp from the kudu client that did the insert to the impala client that's doing the reading.",test_debt,flaky_test,"Tue, 13 Aug 2019 00:17:21 +0000","Thu, 9 Apr 2020 16:53:41 +0000","Thu, 9 Apr 2020 16:53:40 +0000",20795779,"I believe this is a flaky tests, since there's no attempt to pass the timestamp from the kudu client that did the insert to the impala client that's doing the reading.",0,0,negative
impala,9209,summary,is flaky,test_debt,flaky_test,"Mon, 2 Dec 2019 20:08:51 +0000","Tue, 10 Dec 2019 22:29:41 +0000","Tue, 10 Dec 2019 22:29:40 +0000",699649,TestRPCException.test_end_data_stream_error is flaky,0,-0.2,negative
thrift,1141,comment_0,"patch did not apply cleanly.. fixed that, the wrong libthrift.jar file location and updated the changelog. => committed",architecture_debt,violation_of_modularity,"Tue, 12 Apr 2011 10:41:32 +0000","Tue, 1 Nov 2011 02:53:46 +0000","Tue, 12 Apr 2011 20:37:07 +0000",35735,"patch did not apply cleanly.. fixed that, the wrong libthrift.jar file location and updated the changelog. => committed",0.08333333333,0.08333333333,negative
thrift,1217,comment_1,"I really like to have Windows port within next Thrift release. The suggestion by [David and to use APR on Windows and do minimal changes on the current implementation is a key factor to bring this up and running. However, this patch depends on pthreads for windows and as mentioned on the dev mailing list or within Jira = Is APR feasible for your? Do you really need thet libevent version? Most GNU/Linux distro still provide 1.4.x and I would really like to keep capability to build and use thrift without lot of extra versions of libraries not within a standard distribution.",architecture_debt,using_obsolete_technology,"Thu, 23 Jun 2011 20:39:21 +0000","Thu, 1 Sep 2011 17:41:04 +0000","Fri, 8 Jul 2011 12:45:13 +0000",1267552,"I really like to have Windows port within next Thrift release. The suggestion by David Reiss and me to use APR on Windows and do minimal changes on the current implementation is a key factor to bring this up and running. However, this patch depends on pthreads for windows and as mentioned on the dev mailing list or within Jira => it seems that this is not compatible with the apache license. Is APR feasible for your? Do you really need thet libevent version? Most GNU/Linux distro still provide 1.4.x and I would really like to keep capability to build and use thrift without lot of extra versions of libraries not within a standard distribution.",0.2676666667,0.1401428571,positive
thrift,191,comment_4,"I like the map declaration, but I think that the MetaData class should live outside of the generated code. It's not actually a class that will vary by structure, so it should just live in c.f.thrift.",architecture_debt,violation_of_modularity,"Tue, 4 Nov 2008 03:21:09 +0000","Tue, 1 Nov 2011 02:54:08 +0000","Mon, 5 Jan 2009 21:16:54 +0000",5421345,"I like the map declaration, but I think that the MetaData class should live outside of the generated code. It's not actually a class that will vary by structure, so it should just live in c.f.thrift.",0.2,0.2,neutral
thrift,1953,comment_3,"Overall, looks good. A few more comments: - Tests have moved from ./test/csharp/ to ./lib/csharp/test - The C# generator has a warning about an unused variable. Can you please delete that variable? - I don't think that we should be building the MVC projects by default. For example, Mono does not have support for MVC3, so I couldn't compile those elements. Overall, looks good though.",architecture_debt,violation_of_modularity,"Wed, 1 May 2013 16:23:43 +0000","Tue, 28 May 2013 02:25:27 +0000","Sat, 18 May 2013 13:35:51 +0000",1458728,"Overall, looks good. A few more comments: Tests have moved from ./test/csharp/ to ./lib/csharp/test The C# generator has a warning about an unused variable. Can you please delete that variable? I don't think that we should be building the MVC projects by default. For example, Mono does not have support for MVC3, so I couldn't compile those elements. Overall, looks good though.",0.2205625,0.2205625,neutral
thrift,1953,comment_4,"hi, Carl Yeksigian 1. Tests for MVC have moved to ./lib/csharp/test 2. I don't find any unused variable in my patch, could you please show me where it is? 3. I agree with your point, by default, we should not build the MVC projects. so I create a solution file ""thrift.mvc.sln"", but this will increase the maintenance effort. please check the attachment patch file.",architecture_debt,violation_of_modularity,"Wed, 1 May 2013 16:23:43 +0000","Tue, 28 May 2013 02:25:27 +0000","Sat, 18 May 2013 13:35:51 +0000",1458728,"hi, Carl Yeksigian 1. Tests for MVC have moved to ./lib/csharp/test 2. I don't find any unused variable in my patch, could you please show me where it is? 3. I agree with your point, by default, we should not build the MVC projects. so I create a solution file ""thrift.mvc.sln"", but this will increase the maintenance effort. please check the attachment patch file.",0.065,0.065,neutral
thrift,1990,comment_1,"Ah ha, turns out Ubuntu's thrift package is v 0.8, which is quite outdated and doesn't include exception support.",architecture_debt,using_obsolete_technology,"Mon, 3 Jun 2013 05:20:56 +0000","Sat, 20 Feb 2021 15:28:26 +0000","Fri, 7 Jun 2013 05:23:52 +0000",345776,"Ah ha, turns out Ubuntu's thrift package is v 0.8, which is quite outdated and doesn't include exception support.",0,0,negative
thrift,2124,comment_1,after moving test to subdir to allow for distclean to run missing following in tutorials folder csharp/tutorial.sln d/async_client.d d/client.d d/server.d erl/client.erl erl/client.sh erl/json_client.erl erl/README erl/server.erl erl/server.sh gen-html/index.html gen-html/style.css go/server.crt go/server.key hs/HaskellClient.hs hs/HaskellServer.hs java/build.xml js/build.xml js/src/Httpd.java js/tutorial.html ocaml/CalcClient.ml ocaml/CalcServer.ml ocaml/_oasis ocaml/README perl/PerlClient.pl perl/PerlServer.pl php/PhpClient.php php/PhpServer.php php/runserver.py shared.thrift tutorial.thrift,architecture_debt,violation_of_modularity,"Thu, 15 Aug 2013 21:41:34 +0000","Fri, 16 Aug 2013 01:55:51 +0000","Fri, 16 Aug 2013 01:23:00 +0000",13286,after moving test to subdir to allow for distclean to run missing following in tutorials folder csharp/CsharpClient/CsharpClient.cs csharp/CsharpClient/CsharpClient.csproj csharp/CsharpClient/Properties/AssemblyInfo.cs csharp/CsharpServer/CsharpServer.cs csharp/CsharpServer/CsharpServer.csproj csharp/CsharpServer/Properties/AssemblyInfo.cs csharp/tutorial.sln d/async_client.d d/client.d d/server.d delphi/DelphiClient/DelphiClient.dpr delphi/DelphiClient/DelphiClient.dproj delphi/DelphiServer/DelphiServer.dpr delphi/DelphiServer/DelphiServer.dproj delphi/Tutorial.groupproj erl/client.erl erl/client.sh erl/json_client.erl erl/README erl/server.erl erl/server.sh gen-html/index.html gen-html/shared.html gen-html/style.css gen-html/tutorial.html go/server.crt go/server.key hs/HaskellClient.hs hs/HaskellServer.hs hs/ThriftTutorial.cabal java/build.xml java/src/CalculatorHandler.java java/src/JavaClient.java java/src/JavaServer.java js/build.xml js/src/Httpd.java js/tutorial.html ocaml/CalcClient.ml ocaml/CalcServer.ml ocaml/_oasis ocaml/README perl/PerlClient.pl perl/PerlServer.pl php/PhpClient.php php/PhpServer.php php/runserver.py py.twisted/PythonServer.tac shared.thrift tutorial.thrift,0.05517241379,0.0375,neutral
thrift,228,description,"Following exception is raised by the read_message_begin method"" Missing version identifier from `receive_message' Comparing the implementation of the method in the Ruby class to its python counterpart TBinaryProtocol shows that the Ruby method is quite outdated. I have changed the method to be: def read_message_begin version = read_i32 if(version <0) if (version & VERSION_MASK != VERSION_1) raise 'Missing version identifier') end type = version & 0000000ff name = read_string seqid = read_i32 else name = type = read_byte seqid = read_i32 end [name, type, seqid] end This does not raise an exception on the strict read condition in the else clause as is raised by the Python version but can be easily added to.",architecture_debt,using_obsolete_technology,"Sun, 7 Dec 2008 20:13:15 +0000","Tue, 1 Nov 2011 02:51:49 +0000","Mon, 16 Mar 2009 04:38:52 +0000",8497537,"Following exception is raised by the read_message_begin method"" /usr/local/lib/site_ruby/1.8/thrift/protocol/binaryprotocol.rb:82:in 'read_message_begin': Missing version identifier (Thrift::ProtocolException) from /usr/local/lib/site_ruby/1.8/thrift/client.rb:26:in `receive_message' Comparing the implementation of the method in the Ruby class Thrift::BinaryProtocol to its python counterpart TBinaryProtocol shows that the Ruby method is quite outdated. I have changed the method to be: def read_message_begin version = read_i32 if(version <0) if (version & VERSION_MASK != VERSION_1) raise ProtocolException.new(ProtocolException::BAD_VERSION, 'Missing version identifier') end type = version & 0000000ff name = read_string seqid = read_i32 else name = trans.readAll(version) type = read_byte seqid = read_i32 end [name, type, seqid] end This does not raise an exception on the strict read condition in the else clause as is raised by the Python version but can be easily added to.",-0.2666666667,-0.05714285714,negative
thrift,228,summary,Ruby version of binaryprotocol.rb has an outdated version of read_message_begin,architecture_debt,using_obsolete_technology,"Sun, 7 Dec 2008 20:13:15 +0000","Tue, 1 Nov 2011 02:51:49 +0000","Mon, 16 Mar 2009 04:38:52 +0000",8497537,Ruby version of binaryprotocol.rb has an outdated version of read_message_begin,0,0,negative
thrift,3864,comment_0,"If large block allocation or garbage collection is not efficient in golang, that sounds like a language issue. One is not supposed to have to think about heap or stack in go, from what I understand (I am by no means an expert). The proposed solution sounds like it would create a cyclic dependency.",architecture_debt,violation_of_modularity,"Mon, 27 Jun 2016 14:23:22 +0000","Thu, 3 Jan 2019 14:33:44 +0000","Thu, 3 Jan 2019 14:33:44 +0000",79488622,"If large block allocation or garbage collection is not efficient in golang, that sounds like a language issue. One is not supposed to have to think about heap or stack in go, from what I understand (I am by no means an expert). The proposed solution sounds like it would create a cyclic dependency.",-0.2291666667,-0.2291666667,negative
thrift,4043,summary,thrift perl debian package is placing files in the wrong place,architecture_debt,violation_of_modularity,"Fri, 27 Jan 2017 14:45:15 +0000","Thu, 14 Dec 2017 13:56:07 +0000","Thu, 9 Feb 2017 01:45:52 +0000",1076437,thrift perl debian package is placing files in the wrong place,-0.25,-0.25,negative
thrift,4048,comment_0,"As support for the older cocoa compiler and library have been removed (see THRIFT-4719), all of the issues in Jira related to that code have also been removed. For legacy cocoa support you can use version 0.12.0 - everyone is expected to move to swift if they want to use the next release of Thrift.",architecture_debt,using_obsolete_technology,"Sun, 29 Jan 2017 14:57:45 +0000","Thu, 10 Oct 2019 23:00:08 +0000","Mon, 14 Jan 2019 15:03:35 +0000",61776350,"As support for the older cocoa compiler and library have been removed (see THRIFT-4719), all of the issues in Jira related to that code have also been removed. For legacy cocoa support you can use version 0.12.0 - everyone is expected to move to swift if they want to use the next release of Thrift.",0.3666666667,0.3666666667,neutral
thrift,4069,description,"Currently our perl package module files contain multiple packages. We should break each package out to an individual file (or at least make sure everything is in the Thrift namespace) and properly version it. Package versioning was introduced in Perl 5.10 so: 1. Update the minimum required perl to 5.10. This is based on indicating that perl version object was added to perl in 5.10. 2. For each package use the {{perl MODULE VERSION}} perlmod syntax, where VERSION is {{v0.11.0}}. This is based on 3. Each module not under the Thrift namespace must be moved there TMessageType, TType). This will be a breaking change, but necessary for proper packaging of the library. Currently if you inspect the Perl PAUSE version metadata for Thrift's sub-modules only the 0.9.0 modules from gslin have version identities. For example if you look at Thrift and in the CPAN list of packages at you will see: There are some anomalies, for example packages defined in Thrift.pm come out at the top level namespace like: So technically if you do 'install I would expect you might get thrift. This is wrong and should be fixed. needs to be inside Thrift, not at the top level. Also we should pull in relevant changes from the patch in THRIFT-4059 around improving packaging. Also we should actually use TProtocolException and TTransportException instead of just TException everywhere.",architecture_debt,violation_of_modularity,"Sun, 5 Feb 2017 16:20:39 +0000","Thu, 14 Dec 2017 13:54:42 +0000","Thu, 30 Mar 2017 21:12:02 +0000",4596683,"Currently our perl package module files contain multiple packages. We should break each package out to an individual file (or at least make sure everything is in the Thrift namespace) and properly version it. Package versioning was introduced in Perl 5.10 so: 1. Update the minimum required perl to 5.10. This is based on http://search.cpan.org/~jpeacock/version-0.9917/lib/version.pod indicating that perl version object was added to perl in 5.10. 2. For each package use the perl MODULE VERSION perlmod syntax, where VERSION is v0.11.0. This is based on http://perldoc.perl.org/functions/package.html. 3. Each module not under the Thrift namespace must be moved there (TApplicationException, TMessageType, TType). This will be a breaking change, but necessary for proper packaging of the library. Currently if you inspect the Perl PAUSE version metadata for Thrift's sub-modules only the 0.9.0 modules from gslin have version identities. For example if you look at Thrift and Thrift::BinaryProtocol in the CPAN list of packages at http://www.cpan.org/modules/02packages.details.txt you will see: There are some anomalies, for example packages defined in Thrift.pm come out at the top level namespace like: So technically if you do 'install TApplicationException' I would expect you might get thrift. This is wrong and should be fixed. TApplicationException needs to be inside Thrift, not at the top level. Also we should pull in relevant changes from the patch in THRIFT-4059 around improving packaging. Also we should actually use TProtocolException and TTransportException instead of just TException everywhere.",-0.01560526316,-0.01560526316,neutral
thrift,4256,description,"Currently, Thrift.cabal has an exact dependency of vector==0.10.12.2, but this version is much, much older than what other packages depend on. This makes it necessary to enable ""allow-newer"", which effectively ignores the dependency, and then breaks when a package is uploaded to hackage, and prevents inclusion of thrift in a stack curated package set. If there's no particular reason for it (and I've been successfully compiling thrift with vector==0.12.0.2), could this dependency be set to a range, .e.g. >=0.12.0? I could then enter a request for thrift to be added to stack's curated package sets.",architecture_debt,using_obsolete_technology,"Wed, 19 Jul 2017 01:05:49 +0000","Thu, 12 Oct 2017 09:39:39 +0000","Wed, 19 Jul 2017 15:21:19 +0000",51330,"Currently, Thrift.cabal has an exact dependency of vector==0.10.12.2, but this version is much, much older than what other packages depend on. This makes it necessary to enable ""allow-newer"", which effectively ignores the dependency, and then breaks when a package is uploaded to hackage, and prevents inclusion of thrift in a stack curated package set. If there's no particular reason for it (and I've been successfully compiling thrift with vector==0.12.0.2), could this dependency be set to a range, .e.g. >=0.12.0? I could then enter a request for thrift to be added to stack's curated package sets.",-0.004777777778,-0.004777777778,neutral
thrift,4256,summary,Dependency on very old version of vector library,architecture_debt,using_obsolete_technology,"Wed, 19 Jul 2017 01:05:49 +0000","Thu, 12 Oct 2017 09:39:39 +0000","Wed, 19 Jul 2017 15:21:19 +0000",51330,Dependency on very old version of vector library,0,0,neutral
thrift,4321,description,"Current library is still using old project structure (project.json) which is slightly outdated and needs to be migrated to new MSBuild format. In addition to that, I'd like to have separate packages build for different .NET Standard versions starting from 1.4 (UWP) and up to 2.0 with full feature set.",architecture_debt,using_obsolete_technology,"Fri, 8 Sep 2017 09:34:56 +0000","Sat, 2 Feb 2019 13:47:58 +0000","Thu, 19 Oct 2017 13:10:30 +0000",3555334,"Current library is still using old project structure (project.json) which is slightly outdated and needs to be migrated to new MSBuild format. In addition to that, I'd like to have separate packages build for different .NET Standard versions starting from 1.4 (UWP) and up to 2.0 with full feature set.",0.07025,0.07025,neutral
thrift,4369,description,"ws@0.4.32 is very dangerous, please upgrade it.",architecture_debt,using_obsolete_technology,"Tue, 24 Oct 2017 18:17:34 +0000","Tue, 6 Mar 2018 21:04:38 +0000","Tue, 6 Mar 2018 21:04:00 +0000",11501186,"https://snyk.io/vuln/npm:ws:20160104, https://nodesecurity.io/advisories/67, https://snyk.io/vuln/npm:ws:20160624, https://nodesecurity.io/advisories/120, https://snyk.io/vuln/npm:ws:20160920 ws@0.4.32 is very dangerous, please upgrade it.",-0.004166666667,-0.004166666667,negative
thrift,4546,comment_3,"Part of the reason that the thrift.apache.org site is so woefully under-maintained is that it uses a content manage system from the last century. I have to recommend that we move all our development related content to markdown files within the GitHub repository so that we have better control over it. We should be able to teach GitHub and/or the CI build systems that if the only changes are to a documentation directly, not to do a build.",architecture_debt,using_obsolete_technology,"Fri, 6 Apr 2018 23:04:12 +0000","Fri, 28 Dec 2018 13:06:40 +0000","Fri, 28 Dec 2018 13:06:34 +0000",22946542,"Part of the reason that the thrift.apache.org site is so woefully under-maintained is that it uses a content manage system from the last century. I have to recommend that we move all our development related content to markdown files within the GitHub repository so that we have better control over it. We should be able to teach GitHub and/or the CI build systems that if the only changes are to a documentation directly, not to do a build.",0.2851333333,0.2851333333,negative
thrift,503,summary,"""make check""-enabled C++ tests should be in lib/cpp/test",architecture_debt,violation_of_modularity,"Thu, 14 May 2009 00:49:03 +0000","Thu, 2 May 2013 02:29:29 +0000","Tue, 31 Aug 2010 18:31:46 +0000",41017363,"""make check""-enabled C++ tests should be in lib/cpp/test",0,0,neutral
thrift,525,description,"The C# Library Solution fails to compile because of problems with the PreBuildEvent for the ThriftTest Project - Thrift.exe is called with the outdated -csharp instead of --gen csharp - All of the existing commands in the PreBuildEvent will fail if there's a space in any directory name in the directory tree (such as ""My Documents"" or ""Visual Studio 2008""). - The recursive forced rmdir command will match other directory trees that share a common root (e.g. c:\test\Work\ will be recursively removed if the project is located in c:\test\Work for Thrift\test\csharp) I have attached a patch that replaces the PreBuildEvent in the ThriftTest project file to fix these problems and work with directory trees that contain spaces. As the thrift.exe compiler does NOT accept paths with spaces when surrounded by quotes, my script generates MSDOS 8.3 pathnames to pass to the thrift compiler and it appears to work just fine. This has only been tested on my machine with VS.net pro 2008 and a current thrift checkout",architecture_debt,using_obsolete_technology,"Tue, 23 Jun 2009 16:18:25 +0000","Tue, 1 Nov 2011 02:51:46 +0000","Thu, 2 Jul 2009 20:28:57 +0000",792632,"The C# Library Solution (thrift\lib\csharp\src\Thrift.sln) fails to compile because of problems with the PreBuildEvent for the ThriftTest Project (thrift\test\csharp\ThriftTest\ThriftTest.csproj). Thrift.exe is called with the outdated -csharp instead of --gen csharp All of the existing commands in the PreBuildEvent will fail if there's a space in any directory name in the directory tree (such as ""My Documents"" or ""Visual Studio 2008""). The recursive forced rmdir command will match other directory trees that share a common root (e.g. c:\test\Work\ will be recursively removed if the project is located in c:\test\Work for Thrift\test\csharp) I have attached a patch that replaces the PreBuildEvent in the ThriftTest project file to fix these problems and work with directory trees that contain spaces. As the thrift.exe compiler does NOT accept paths with spaces when surrounded by quotes, my script generates MSDOS 8.3 pathnames to pass to the thrift compiler and it appears to work just fine. This has only been tested on my machine with VS.net pro 2008 and a current thrift checkout",0.1303777778,0.03385185185,negative
thrift,568,description,"When building thrift python libs on ubuntu, it places the files into site-packages instead of dist-utils",architecture_debt,violation_of_modularity,"Fri, 21 Aug 2009 19:02:02 +0000","Mon, 29 Aug 2011 00:13:10 +0000","Mon, 29 Aug 2011 00:13:10 +0000",63695468,"When building thrift python libs on ubuntu, it places the files into site-packages instead of dist-utils",0,0,neutral
thrift,1048,summary,Thrift java library has redundant dependency on commons-lang.,build_debt,over-declared_dependencies,"Wed, 26 Jan 2011 15:18:48 +0000","Thu, 27 Jan 2011 09:44:43 +0000","Wed, 26 Jan 2011 23:41:15 +0000",30147,Thrift java library has redundant dependency on commons-lang.,0,0,negative
thrift,1440,description,"A listing of detectable policy problems in the thrift Debian packaging (in contrib/) can be found with a lintian run: I'll note some of them here for posterity. h3. thrift source: libthrift-dev on libthrift0 The libthrift-dev package should have a versioned dependency on libthrift0, i.e., in debian/control: h3. thrift source: build-depends You don't need the ""build-essential"" bit in Build-Depends. h3. thrift-compiler: Syntax is a bit off in debian/control for the Description fields; I'll attach a patch. h3. thrift-compiler: usr/bin/thrift You need a man page for /usr/bin/thrift. h3. python-thrift-dbg: python-thrift-dbg = The python-thrift-dbg package should be in Section: debug. h3. python-thrift-dbg: dir-in-usr-local usr/local/lib/ Debian packages shouldn't be shipping anything in /usr/local; that's supposed to be reserved for the local system admin. There isn't much reason for this anyway; the dirs being shipped by python-thrift-dbg here are empty. h3. libthrift-ruby: libthrift-ruby = The libthrift-ruby package should be in Section: ruby. Also, according to , it looks like Ruby packages are undergoing a name change in the current Debian testing suite. libthrift-ruby probably needs to become ruby-thrift and switch to using gem2deb. h3. libthrift-ruby: This will probably be addressed under THRIFT-1421. h3. libthrift0: libthrift-c-glib0 This is complaining because the package name of a library package should usually reflect the soname of the included library (see [chapter 5 of the Debian Library Packaging for more info). Something is fishy here, though. Did you intend to distribute the c-glib library as ""libthrift0""? If so, where is the cpp library supposed to go? I don't think I see it after a quick search through the packages. h3. php5-thrift: See the lintian explanation for detailed info. Basically, you need some extra Sauce to add a dependency to php5-thrift on a PHP with a compatible API version. h3. libthrift-java: libthrift-java = libthrift-java should be Section: java h3. libthrift-cil: libthrift-cil = libthrift-cil should be Section: cli-mono h3. libthrift-cil: Thrift.dll shouldn't have its executable bit set. h3. libthrift-perl: usr/usr/ Yeah, installing into /usr/usr/local/lib is kinda wacko. Ought to be /usr/lib. - And as a final note, a lot of the packaging here could be pretty greatly simplified and better future-proofed using the Debhelper 7 command sequencer (""{{dh}}"").",build_debt,under-declared_dependencies,"Mon, 28 Nov 2011 22:14:06 +0000","Thu, 20 Jun 2013 16:12:38 +0000","Thu, 20 Jun 2013 16:12:38 +0000",49226312,"A listing of detectable policy problems in the thrift Debian packaging (in contrib/) can be found with a lintian run: I'll note some of them here for posterity. thrift source: weak-library-dev-dependency libthrift-dev on libthrift0 The libthrift-dev package should have a versioned dependency on libthrift0, i.e., in debian/control: thrift source: build-depends-on-build-essential build-depends You don't need the ""build-essential"" bit in Build-Depends. thrift-compiler: description-contains-invalid-control-statement Syntax is a bit off in debian/control for the Description fields; I'll attach a patch. thrift-compiler: binary-without-manpage usr/bin/thrift You need a man page for /usr/bin/thrift. python-thrift-dbg: wrong-section-according-to-package-name python-thrift-dbg => debug The python-thrift-dbg package should be in Section: debug. python-thrift-dbg: dir-in-usr-local usr/local/lib/ Debian packages shouldn't be shipping anything in /usr/local; that's supposed to be reserved for the local system admin. There isn't much reason for this anyway; the dirs being shipped by python-thrift-dbg here are empty. libthrift-ruby: wrong-section-according-to-package-name libthrift-ruby => ruby The libthrift-ruby package should be in Section: ruby. Also, according to http://wiki.debian.org/Teams/Ruby/Packaging , it looks like Ruby packages are undergoing a name change in the current Debian testing suite. libthrift-ruby probably needs to become ruby-thrift and switch to using gem2deb. libthrift-ruby: empty-binary-package This will probably be addressed under THRIFT-1421. libthrift0: package-name-doesnt-match-sonames libthrift-c-glib0 This is complaining because the package name of a library package should usually reflect the soname of the included library (see chapter 5 of the Debian Library Packaging Guide for more info). Something is fishy here, though. Did you intend to distribute the c-glib library as ""libthrift0""? If so, where is the cpp library supposed to go? I don't think I see it after a quick search through the packages. php5-thrift: missing-dependency-on-phpapi See the lintian explanation for detailed info. Basically, you need some extra Sauce to add a dependency to php5-thrift on a PHP with a compatible API version. libthrift-java: wrong-section-according-to-package-name libthrift-java => java libthrift-java should be Section: java libthrift-cil: wrong-section-according-to-package-name libthrift-cil => cli-mono libthrift-cil should be Section: cli-mono libthrift-cil: executable-not-elf-or-script ./usr/lib/cli/thrift/Thrift.dll Thrift.dll shouldn't have its executable bit set. libthrift-perl: non-standard-dir-in-usr usr/usr/ Yeah, installing into /usr/usr/local/lib is kinda wacko. Ought to be /usr/lib. And as a final note, a lot of the packaging here could be pretty greatly simplified and better future-proofed using the Debhelper 7 command sequencer (""dh"").",0.06018518519,0.08583333333,neutral
thrift,1504,description,"The Cocoa generator imports various thrift files as global imports instead of local importss. This ends up requiring that the user alter their header search paths to include the directories the generated thrift code is in, or the user has to edit the generated files to remove this. e.g. #import <TProtocol.h#import <TProcessor.h vs. #import ""TProtocol.h"" #import #import ""TProtocolUtil.h"" #import ""TProcessor.h""",build_debt,build_others,"Wed, 25 Jan 2012 20:41:06 +0000","Fri, 27 Jan 2012 04:13:57 +0000","Fri, 27 Jan 2012 03:08:57 +0000",109671,"The Cocoa generator imports various thrift files as global imports instead of local importss. This ends up requiring that the user alter their header search paths to include the directories the generated thrift code is in, or the user has to edit the generated files to remove this. e.g. #import <TProtocol.h> #import <TApplicationException.h> #import <TProtocolUtil.h> #import <TProcessor.h> vs. #import ""TProtocol.h"" #import ""TApplicationException.h"" #import ""TProtocolUtil.h"" #import ""TProcessor.h""",0,0,neutral
thrift,1661,description,"In MacPorts we don't want packages to pick up dependencies unless explicitly instructed to, so it's useful to explicitly turn off Qt support otherwise it'll find Qt4 even though the package doesn't list it as a dependency. I chose --with-qt4 since Qt5 will be out soon and I don't know if this code is supported in Qt5.",build_debt,over-declared_dependencies,"Fri, 27 Jul 2012 01:31:50 +0000","Fri, 27 Jul 2012 16:40:07 +0000","Fri, 27 Jul 2012 16:02:54 +0000",52264,"In MacPorts we don't want packages to pick up dependencies unless explicitly instructed to, so it's useful to explicitly turn off Qt support otherwise it'll find Qt4 even though the package doesn't list it as a dependency. I chose --with-qt4 since Qt5 will be out soon and I don't know if this code is supported in Qt5.",-0.08333333333,-0.08333333333,neutral
thrift,1919,description,"I believe this is the same issue as THRIFT-1693, but with different versions. There's a direct dependency on both httpclient and httpcore version 4.1.3 in thriftlib, but httpclient:4.1.3 has a dependency on httpcore:4.1.4. Here are the dependencies Gradle shows me. Here's a copy of the dependency resolution section of my Gradle build in case anyone needs a short term workaround. The force option is what allows the direct dependency to be overridden to 4.1.4 (shown with a * in the above snippet).",build_debt,build_others,"Sun, 7 Apr 2013 03:43:29 +0000","Fri, 21 Jun 2013 20:55:45 +0000","Fri, 21 Jun 2013 19:37:56 +0000",6537267,"I believe this is the same issue as THRIFT-1693, but with different versions. There's a direct dependency on both httpclient and httpcore version 4.1.3 in thriftlib, but httpclient:4.1.3 has a dependency on httpcore:4.1.4. Here are the dependencies Gradle shows me. Here's a copy of the dependency resolution section of my Gradle build in case anyone needs a short term workaround. The force option is what allows the direct dependency to be overridden to 4.1.4 (shown with a * in the above snippet).",0.08,0.08,neutral
thrift,2150,comment_0,"I would prefer not to add #define WIN32_LEAN_AND_MEAN to an externally visible header file. This doesn't fix the Winsock / Winsock2 incompatibility in all the cases, as the including code could have pulled in Windows.h before pulling in any thrift headers. In addition, putting the #define in windows/config.h will likely break code that was relying on the non-lean_and_mean headers. It is possible to add #define WIN32_LEAN_AND_MEAN to thrift's .cpp files, but that doesn't get much of an improvement in build times.",build_debt,build_others,"Mon, 26 Aug 2013 12:29:13 +0000","Wed, 6 May 2015 13:19:11 +0000","Wed, 6 May 2015 13:19:10 +0000",53398197,"I would prefer not to add #define WIN32_LEAN_AND_MEAN to an externally visible header file. This doesn't fix the Winsock / Winsock2 incompatibility in all the cases, as the including code could have pulled in Windows.h before pulling in any thrift headers. In addition, putting the #define in windows/config.h will likely break code that was relying on the non-lean_and_mean headers. It is possible to add #define WIN32_LEAN_AND_MEAN to thrift's .cpp files, but that doesn't get much of an improvement in build times.",-0.1488571429,-0.1488571429,negative
thrift,2150,description,"As ""windows/config.h"" includes Winsock2.h, is it not best practise to define before it: This is in case someone includes Windows.h, which automatically includes the incompatible Winsock.h. It also reduces the size of the Win32 header files which helps compilation speeds :)",build_debt,build_others,"Mon, 26 Aug 2013 12:29:13 +0000","Wed, 6 May 2015 13:19:11 +0000","Wed, 6 May 2015 13:19:10 +0000",53398197,"As ""windows/config.h"" includes Winsock2.h, is it not best practise to define before it: This is in case someone includes Windows.h, which automatically includes the incompatible Winsock.h. It also reduces the size of the Win32 header files which helps compilation speeds",-0.095,-0.095,neutral
thrift,2292,comment_0,Can you please provide a list of the missing dependencies or errors you are encountering,build_debt,under-declared_dependencies,"Fri, 20 Dec 2013 09:53:21 +0000","Mon, 23 Dec 2013 16:12:40 +0000","Mon, 23 Dec 2013 16:12:40 +0000",281959,Can you please provide a list of the missing dependencies or errors you are encountering,-0.2,-0.2,neutral
thrift,2292,comment_2,"I already implement cordova hook that do this job for me on after platform add, and all is brilliant but you have weak link in your build system. In exec.js file your exec implementation fail and stop build due to buffer overflow. Error is ""Error: stdout maxBuffer exceeded"". All that should be done is adding maxBuffer: 800*1024 option to exec call, also 800 can be replaced on any other big number. How can i contribute with this and other fixes to cordova? I saw your repository on github and even forked it. So if i send you pull requests with fixes do you include them in next releases?",build_debt,build_others,"Fri, 20 Dec 2013 09:53:21 +0000","Mon, 23 Dec 2013 16:12:40 +0000","Mon, 23 Dec 2013 16:12:40 +0000",281959,"I already implement cordova hook that do this job for me on after platform add, and all is brilliant but you have weak link in your build system. In exec.js file your exec implementation fail and stop build due to buffer overflow. Error is ""Error: stdout maxBuffer exceeded"". All that should be done is adding maxBuffer: 800*1024 option to exec call, also 800 can be replaced on any other big number. How can i contribute with this and other fixes to cordova? I saw your repository on github and even forked it. So if i send you pull requests with fixes do you include them in next releases?",-0.0549375,-0.0549375,negative
thrift,2333,description,"As can be seen in this email message to the users list it is possible for some pre-required packages to be missing, but the traditional method for validating their existence in specfile using ""BuildRequires"" won't work for items that are not commonly available as RPMS. I think that in this user's case the missing bit is the bundler gem, which I have a locally built RPM for, but I have to assume that most folks do not and it is not commonly available in most repos (not that I saw as a Centos6.4 user anyway) The patch I am attaching just checks if ruby subpackage was not disabled at the (at the buildrpm level) and if the results of ./configure have determined that ""make install"" won't build the gem (and so the rpm build of the rubygem-thrift subpackage is bound to fail) abort the build with a clear message An alternative solution MAY be to just dynamically skip that package when it is detected that ./configure excluded ruby (I don't know off hand how to do that)",build_debt,build_others,"Sat, 25 Jan 2014 21:13:20 +0000","Wed, 16 Oct 2019 22:27:23 +0000","Thu, 20 Dec 2018 03:13:32 +0000",154591212,"As can be seen in this email message to the users list it is possible for some pre-required packages to be missing, but the traditional method for validating their existence in specfile using ""BuildRequires"" won't work for items that are not commonly available as RPMS. http://mail-archives.apache.org/mod_mbox/thrift-user/201401.mbox/%3CCACeqxwQrLr7ghoBmbabQpu7z_u0FHjh+vbYXQKwP8=ibR4bHKQ@mail.gmail.com%3E I think that in this user's case the missing bit is the bundler gem, which I have a locally built RPM for, but I have to assume that most folks do not and it is not commonly available in most repos (not that I saw as a Centos6.4 user anyway) The patch I am attaching just checks if ruby subpackage was not disabled at the (at the buildrpm level) and if the results of ./configure have determined that ""make install"" won't build the gem (and so the rpm build of the rubygem-thrift subpackage is bound to fail) abort the build with a clear message An alternative solution MAY be to just dynamically skip that package when it is detected that ./configure excluded ruby (I don't know off hand how to do that)",-0.07,-0.07,neutral
thrift,2759,description,The Trusty (Ubuntu 14.04) Vagrantfile in the current trunk does not build clean. Subtasks created for: 1. D not installed 2. Haskell does not compile 3. Go crashes on symlink creation 4. Python Twisted tests fail 5. CPP thrift/test/cpp does not make check due to missing boost packages,build_debt,build_others,"Fri, 3 Oct 2014 01:10:00 +0000","Wed, 5 Nov 2014 04:49:16 +0000","Thu, 9 Oct 2014 03:23:54 +0000",526434,The Trusty (Ubuntu 14.04) Vagrantfile in the current trunk does not build clean. Subtasks created for: 1. D not installed 2. Haskell does not compile 3. Go crashes on symlink creation 4. Python Twisted tests fail 5. CPP thrift/test/cpp does not make check due to missing boost packages,0.05357142857,0.05357142857,negative
thrift,3168,description,"1. THRIFT-2269 was not fixed properly. Sources are supposed to be deployed using a ""jar"" type. cannot resolve them otherwise and IDEs such as IntelliJ IDEA are unable to resolve or even manually attach sources with a ""src"" extension/type. 2. Between 0.9.1 and 0.9.2, the POM for libthrift was modified to remove a dependency on commons-lang3. As a result, the code generated by libthrift no longer builds as it misses this dependency. The dependency needs to be restored, or whatever reason resulted in its removal re-evaluated to take this requirement into account. Personally, I'd prefer if we could just drop this dependency entirely and use native Java language utilities such as `Objects` to solve the problems `commons-lang3` is trying to solve.",build_debt,under-declared_dependencies,"Wed, 20 May 2015 00:23:04 +0000","Tue, 21 Jul 2015 02:21:24 +0000","Tue, 23 Jun 2015 12:40:27 +0000",2981843,"1. THRIFT-2269 was not fixed properly. Sources are supposed to be deployed using a ""jar"" type. maven-dependency-plugin cannot resolve them otherwise and IDEs such as IntelliJ IDEA are unable to resolve or even manually attach sources with a ""src"" extension/type. 2. Between 0.9.1 and 0.9.2, the POM for libthrift was modified to remove a dependency on commons-lang3. As a result, the code generated by libthrift no longer builds as it misses this dependency. The dependency needs to be restored, or whatever reason resulted in its removal re-evaluated to take this requirement into account. Personally, I'd prefer if we could just drop this dependency entirely and use native Java language utilities such as `Objects` to solve the problems `commons-lang3` is trying to solve.",-0.008148148148,-0.008148148148,negative
thrift,3747,description,"As npm download is unstable, node.js build was supposed to be isolated into a separate job on Travis (with haskell which is unstable due to cabal download). In reality, node.js was built in ""Java Lua ..."" job too, so we've been having double chance of download failures.",build_debt,build_others,"Thu, 17 Mar 2016 17:57:18 +0000","Sat, 19 Mar 2016 16:45:38 +0000","Fri, 18 Mar 2016 06:40:40 +0000",45802,"As npm download is unstable, node.js build was supposed to be isolated into a separate job on Travis (with haskell which is unstable due to cabal download). In reality, node.js was built in ""Java Lua ..."" job too, so we've been having double chance of download failures.",-0.1333333333,-0.1333333333,negative
thrift,3747,summary,Duplicate node.js build on Travis-CI,build_debt,build_others,"Thu, 17 Mar 2016 17:57:18 +0000","Sat, 19 Mar 2016 16:45:38 +0000","Fri, 18 Mar 2016 06:40:40 +0000",45802,Duplicate node.js build on Travis-CI,0,0,neutral
thrift,3983,description,"Hi, libthrift is deployed with a ""pom"" packaging on maven central, see This is very wrong. ""pom"" means ""no jar, just a list of dependencies to be pulled transitively"". As a consequence, some build tools, such as sbt, don't pull the libthrift jar itself. The pom should be be using a ""jar"" packaging, or even no packaging at all as ""jar"" is the default value.",build_debt,build_others,"Wed, 23 Nov 2016 16:58:21 +0000","Thu, 27 Dec 2018 15:25:21 +0000","Wed, 24 Jan 2018 17:31:47 +0000",36894806,"Hi, libthrift is deployed with a ""pom"" packaging on maven central, see https://repo1.maven.org/maven2/org/apache/thrift/libthrift/0.9.3/libthrift-0.9.3.pom <groupId>org.apache.thrift</groupId> <artifactId>libthrift</artifactId> <version>0.9.3</version> <packaging>pom</packaging> <name>Apache Thrift</name> This is very wrong. ""pom"" means ""no jar, just a list of dependencies to be pulled transitively"". As a consequence, some build tools, such as sbt, don't pull the libthrift jar itself. The pom should be be using a ""jar"" packaging, or even no packaging at all as ""jar"" is the default value.",-0.1875,-0.125,negative
thrift,4616,comment_0,AIX is not an environment we can currently test/verify in our Continuous Integration environment. You will need to make changes like the one your already found in order to use it. I apologize that we cannot assist you further but we have no access to an AIX environment. Perhaps someone on the thrift user mailing list could be of more assistance? It sounds like we're missing headers in some places to cover what's needed. I would prefer to see changes to headers rather than an pre-include.,build_debt,build_others,"Thu, 9 Aug 2018 15:19:42 +0000","Thu, 9 Aug 2018 22:18:51 +0000","Thu, 9 Aug 2018 22:18:06 +0000",25104,AIX is not an environment we can currently test/verify in our Continuous Integration environment. You will need to make changes like the one your already found in order to use it. I apologize that we cannot assist you further but we have no access to an AIX environment. Perhaps someone on the thrift user mailing list could be of more assistance? It sounds like we're missing headers in some places to cover what's needed. I would prefer to see changes to headers rather than an pre-include.,-0.07291666667,-0.07291666667,negative
thrift,701,summary,Generated classes take up more space in jar than needed,build_debt,build_others,"Wed, 10 Feb 2010 23:42:07 +0000","Tue, 23 Mar 2010 05:39:48 +0000","Tue, 23 Mar 2010 05:39:48 +0000",3477461,Generated classes take up more space in jar than needed,0,0,neutral
thrift,841,description,"The following files are shipped in the binary release artifacts, (despite being removed in the clean target). This creates problems for packaging where it is typical to import to the binary releases into a vcs alongside the packaging artifacts. Also, the following is generated during a build, but not removed on a clean.",build_debt,build_others,"Sat, 7 Aug 2010 00:10:30 +0000","Thu, 10 Jul 2014 13:42:35 +0000","Thu, 10 Jul 2014 13:42:35 +0000",123859925,"The following files are shipped in the binary release artifacts, (despite being removed in the clean target). This creates problems for packaging where it is typical to import to the binary releases into a vcs alongside the packaging artifacts. Also, the following is generated during a build, but not removed on a clean.",-0.325,-0.325,neutral
thrift,841,summary,Build cruft,build_debt,build_others,"Sat, 7 Aug 2010 00:10:30 +0000","Thu, 10 Jul 2014 13:42:35 +0000","Thu, 10 Jul 2014 13:42:35 +0000",123859925,Build cruft,0,0,neutral
thrift,1003,description,"attached patch contains following changes: * Added Apache headers to c/h files * Use gtester for running tests. We don't need -wrapper script anymore * Use one-line macros G_DEFINE_TYPE instead of 15-line class definition * Keep formatting closer to glib-like style (one line class definition macroses/remove trailing spaces) Given changes are mostly fixing low hanging fruits. It does not change any logic/api. There are more chages needed, such as * using CLASS_TYPE_new functions instead of * stop using _set_property (aka reflection) in constructors * check more careful about _ref and _unref handling but this requires more careful refactoring so it will be later in a separate patch.",code_debt,low_quality_code,"Mon, 22 Nov 2010 19:56:47 +0000","Tue, 1 Nov 2011 02:51:41 +0000","Fri, 26 Nov 2010 10:18:37 +0000",310910,"attached patch contains following changes: Added Apache headers to c/h files Use gtester for running tests. We don't need -wrapper script anymore Use one-line macros G_DEFINE_TYPE instead of 15-line class definition Keep formatting closer to glib-like style (one line class definition macroses/remove trailing spaces) Given changes are mostly fixing low hanging fruits. It does not change any logic/api. There are more chages needed, such as using CLASS_TYPE_new functions instead of g_object_new(CLASS_TYPE) stop using _set_property (aka reflection) in constructors check more careful about _ref and _unref handling but this requires more careful refactoring so it will be later in a separate patch.",0.05,0.002375,neutral
thrift,1003,summary,Polishing c_glib code,code_debt,low_quality_code,"Mon, 22 Nov 2010 19:56:47 +0000","Tue, 1 Nov 2011 02:51:41 +0000","Fri, 26 Nov 2010 10:18:37 +0000",310910,Polishing c_glib code,0,0,neutral
thrift,1048,description,The only class from commons-lang used in Thrift is It is used in and in It can be replaced with like it is done in,code_debt,low_quality_code,"Wed, 26 Jan 2011 15:18:48 +0000","Thu, 27 Jan 2011 09:44:43 +0000","Wed, 26 Jan 2011 23:41:15 +0000",30147,The only class from commons-lang used in Thrift is org.apache.commons.lang.NotImplementedException. It is used in org.apache.thrift.transport.AutoExpandingBufferReadTransport and in org.apache.thrift.transport.AutoExpandingBufferWriteTransport It can be replaced with java.lang.UnsupportedOperationException like it is done in org.apache.thrift.transport.TMemoryInputTransport,0,0,neutral
thrift,1065,comment_1,remove some misplaced code (probably copied from another implementation?),code_debt,dead_code,"Thu, 17 Feb 2011 18:30:31 +0000","Wed, 9 Mar 2011 18:17:15 +0000","Tue, 22 Feb 2011 21:04:05 +0000",441214,remove some misplaced code (probably copied from another implementation?),-0.4,-0.4,neutral
thrift,1065,summary,Unexpected exceptions not proper handled on JS,code_debt,low_quality_code,"Thu, 17 Feb 2011 18:30:31 +0000","Wed, 9 Mar 2011 18:17:15 +0000","Tue, 22 Feb 2011 21:04:05 +0000",441214,Unexpected exceptions not proper handled on JS,-0.531,-0.531,negative
thrift,1100,description,"The python TSSLSocket.py module has TSSLSocket and TSSLServerSocket for outbound and inbound SSL connection wrapping. This ticket is for a patch that makes several improvements: * adds Apache license at top of file * for outbound sockets, SSL certificate validation is now performed by default ** but may be disabled with validate=False in the constructor ** instructs python's ssl library to perform CERT_REQUIRED validation of the certificate ** also checks to make sure the certificate's {{commonName}} matches the hostname we tried to connect to ** raises when the certificate fails validation - tested using google's www.gmail.com (doesnt match) versus mail.google.com (matched cert commonName) ** puts a copy of the peer certificate in self.peercert, regardless of validation status ** sets a public boolean self.is_valid member variable to indicate whether the certificate was validated or not * adds a configurable server certificate file, as a constructor argument {{certfile}} ** allows runtime changing of server cert with setCertfile() on the server, that changes the certfile used in subsequent ssl_wrap() calls ** exposes a class-level variable SSL_PROTOCOL to let the user select ssl.PROTOCOL_TLSv1 or other versions of SSL, instead of hard-coding TLSv1. Defaults to TLSv1 though. * removes unnecessary sys.path modification * adds lots of docstrings In a somewhat unrelated change, this patch changes two lines in TSocket.py where self.handle is compared to None using {{!=}} instead of: {{is not}}.",code_debt,low_quality_code,"Sat, 19 Mar 2011 23:35:33 +0000","Mon, 21 Mar 2011 18:00:02 +0000","Mon, 21 Mar 2011 18:00:02 +0000",152669,"The python TSSLSocket.py module has TSSLSocket and TSSLServerSocket for outbound and inbound SSL connection wrapping. This ticket is for a patch that makes several improvements: adds Apache license at top of file for outbound sockets, SSL certificate validation is now performed by default but may be disabled with validate=False in the constructor instructs python's ssl library to perform CERT_REQUIRED validation of the certificate also checks to make sure the certificate's commonName matches the hostname we tried to connect to raises TTransportExceptions when the certificate fails validation - tested using google's www.gmail.com (doesnt match) versus mail.google.com (matched cert commonName) puts a copy of the peer certificate in self.peercert, regardless of validation status sets a public boolean self.is_valid member variable to indicate whether the certificate was validated or not adds a configurable server certificate file, as a constructor argument certfile allows runtime changing of server cert with setCertfile() on the server, that changes the certfile used in subsequent ssl_wrap() calls exposes a class-level variable SSL_PROTOCOL to let the user select ssl.PROTOCOL_TLSv1 or other versions of SSL, instead of hard-coding TLSv1. Defaults to TLSv1 though. removes unnecessary sys.path modification adds lots of docstrings In a somewhat unrelated change, this patch changes two lines in TSocket.py where self.handle is compared to None using != instead of: is not.",0.03998958333,0.03998958333,neutral
thrift,1103,comment_3,"I updated the test suite to include running every valid combination of server, protocol and wrapping transports (both ssl and zlib). For python2.4, this is 30 combinations and runs in about 24 seconds. For python2.7, there is an extra server type (TProcessPool which uses the multiprocessing module) and the SSL transport (unavailable in py2.4), whichadds up to 66 combinations of tests, running in ~95 seconds. The 4 nested for-loops significantly expands the code test coverage. In addition to everything in the _v1 of this patch, the _v2 version also has: Updated test code: * added testing of TSSLServer, an alternate socket transport * added testing of TZlibTransport, a wrapping transport * added a self-signed cert in with a cautionary .readme to allow testing of the TSSLServerSocket (it needs a certificate file) * fixed -q (quiet) and -v (verbose) options to to lower and raise the verbosity Fixed two problems in and one enhancement: * fixed confusing parameters to both client and server constructors, removing the overly ornate \*args and \*\*kwargs which made the constructor behave poorly when used with just (host,port) as arguments. The constructors better match the TSocket and TServerSocket constructor parameters now. * fixed logic in TSSLServerSocket parameter checking, if validate=True and ca_certs=None, now it raises an exception like the docstring claims it should. * made TSSLServerSocket more robust on failed SSL handshake by closing socket connection and returning None from accept() call, which is better than terminating the entire server in some cases I will attach the _v2 patch in a moment.",code_debt,low_quality_code,"Mon, 21 Mar 2011 01:48:29 +0000","Tue, 22 Mar 2011 18:06:16 +0000","Tue, 22 Mar 2011 18:06:16 +0000",145067,"I updated the test suite to include running every valid combination of server, protocol and wrapping transports (both ssl and zlib). For python2.4, this is 30 combinations and runs in about 24 seconds. For python2.7, there is an extra server type (TProcessPool which uses the multiprocessing module) and the SSL transport (unavailable in py2.4), whichadds up to 66 combinations of tests, running in ~95 seconds. The 4 nested for-loops significantly expands the code test coverage. In addition to everything in the _v1 of this patch, the _v2 version also has: Updated test code: added testing of TSSLServer, an alternate socket transport added testing of TZlibTransport, a wrapping transport added a self-signed cert in test/py/test_cert.pem with a cautionary .readme to allow testing of the TSSLServerSocket (it needs a certificate file) fixed -q (quiet) and -v (verbose) options to RunClientServer/TestServer/TestClient to lower and raise the verbosity Fixed two problems in lib/py/src/transport/TSSLSocket.py and one enhancement: fixed confusing parameters to both client and server constructors, removing the overly ornate *args and **kwargs which made the constructor behave poorly when used with just (host,port) as arguments. The constructors better match the TSocket and TServerSocket constructor parameters now. fixed logic in TSSLServerSocket parameter checking, if validate=True and ca_certs=None, now it raises an exception like the docstring claims it should. made TSSLServerSocket more robust on failed SSL handshake by closing socket connection and returning None from accept() call, which is better than terminating the entire server in some cases I will attach the _v2 patch in a moment.",0.04605357143,0.03066666667,neutral
thrift,1121,comment_1,Bryan can you revert THRIFT-959? We noticed the same slowdown,code_debt,slow_algorithm,"Mon, 28 Mar 2011 18:06:04 +0000","Thu, 3 Jan 2019 04:32:08 +0000","Tue, 27 Sep 2011 17:08:01 +0000",15807717,Bryan can you revert THRIFT-959? We noticed the same slowdown,0,0,neutral
thrift,1121,comment_3,"I don't have any test case - the regression showed up in large scale performance tests of a distributed system. Just forwarding along some results for some folks who, at the time, were not permitted to participate in the JIRA.",code_debt,slow_algorithm,"Mon, 28 Mar 2011 18:06:04 +0000","Thu, 3 Jan 2019 04:32:08 +0000","Tue, 27 Sep 2011 17:08:01 +0000",15807717,"I don't have any test case - the regression showed up in large scale performance tests of a distributed system. Just forwarding along some results for some folks who, at the time, were not permitted to participate in the JIRA.",0,0,neutral
thrift,1121,description,"A user reports a 30% performance regression after upgrading some high-request-rate Java software from Thrift 0.3 to 0.6. After some inspection, it turns out that the changes for THRIFT-959 caused the slowdown. However, even after altering the code to use the TFramedTransport, performance was still only 80% of version 0.3. I believe the problem is that the TFramedTransport must read the length (unbuffered) before reading (only) one message. In one particular workload, sent with oneway streaming, the server is making many more system calls. It wasn't obvious how to compose a Transport that would add back the buffering using existing components. We created our own trivial TServerSocket that adds the socket buffering back. Performance is now back where it was with 0.3.",code_debt,slow_algorithm,"Mon, 28 Mar 2011 18:06:04 +0000","Thu, 3 Jan 2019 04:32:08 +0000","Tue, 27 Sep 2011 17:08:01 +0000",15807717,"A user reports a 30% performance regression after upgrading some high-request-rate Java software from Thrift 0.3 to 0.6. After some inspection, it turns out that the changes for THRIFT-959 caused the slowdown. However, even after altering the code to use the TFramedTransport, performance was still only 80% of version 0.3. I believe the problem is that the TFramedTransport must read the length (unbuffered) before reading (only) one message. In one particular workload, sent with oneway streaming, the server is making many more system calls. It wasn't obvious how to compose a Transport that would add back the buffering using existing components. We created our own trivial TServerSocket that adds the socket buffering back. Performance is now back where it was with 0.3.",0.041375,0.041375,negative
thrift,1121,summary,Java server performance regression in 0.6,code_debt,slow_algorithm,"Mon, 28 Mar 2011 18:06:04 +0000","Thu, 3 Jan 2019 04:32:08 +0000","Tue, 27 Sep 2011 17:08:01 +0000",15807717,Java server performance regression in 0.6,0,0,neutral
thrift,1135,comment_4,From what I can tell Pierre put together a pretty complete calculator tutorial in another patch which is now committed. I added some minor clean up in the attached patch: The predates the above work and should not be applied. I think 1135 can be safely closed.,code_debt,low_quality_code,"Fri, 8 Apr 2011 21:20:38 +0000","Wed, 12 Mar 2014 02:36:59 +0000","Wed, 12 Mar 2014 02:36:59 +0000",92294181,From what I can tell Pierre put together a pretty complete calculator tutorial in another patch which is now committed. I added some minor clean up in the attached patch: 0001-node.js-tutorial-cleanup.patch. The thrift-1135-nodejs-tutorial_1.patch predates the above work and should not be applied. I think 1135 can be safely closed.,0.35,0.2214285714,neutral
thrift,1174,comment_2,got this all working and published to the staging repo with no issues. Generates the pom and swc files based on the current java version. Artifacts uploaded where I'm cleaning up the and as3 build files to share common components and reduce code duplications. Will check the updates into trunk shortly,code_debt,duplicated_code,"Thu, 19 May 2011 19:12:52 +0000","Fri, 20 May 2011 19:37:34 +0000","Fri, 20 May 2011 19:05:14 +0000",85942,got this all working and published to the staging repo with no issues. Generates the pom and swc files based on the current java version. Artifacts uploaded where libthrift-as3-0.7.0-snapshot.pom libthrift-as3-0.7.0-snapshot.swc I'm cleaning up the java/fb303/javascript and as3 build files to share common components and reduce code duplications. Will check the updates into trunk shortly,0.2,0.08,positive
thrift,1176,description,Below I added the var qualifier on the declare field function. This is fairly trivial and fixes the global scope leak.,code_debt,low_quality_code,"Fri, 20 May 2011 23:04:02 +0000","Tue, 1 Nov 2011 02:53:52 +0000","Sun, 22 May 2011 10:03:03 +0000",125941,Below I added the var qualifier on the declare field function. This is fairly trivial and fixes the global scope leak.,-0.1,-0.1,positive
thrift,1176,summary,Thrift compiler global leakage js,code_debt,low_quality_code,"Fri, 20 May 2011 23:04:02 +0000","Tue, 1 Nov 2011 02:53:52 +0000","Sun, 22 May 2011 10:03:03 +0000",125941,Thrift compiler global leakage js,0,0,neutral
thrift,1199,description,"For example, in the following union it would be nice to be able to do something like {{boolean test = as an alternative to {{boolean test = ==",code_debt,low_quality_code,"Tue, 7 Jun 2011 20:29:48 +0000","Thu, 9 Jun 2011 22:33:47 +0000","Wed, 8 Jun 2011 17:47:12 +0000",76644,"For example, in the following union it would be nice to be able to do something like boolean test = myUnion.is_my_field1; as an alternative to boolean test = (myUnion.getSetField() == _Fields.MY_FIELD1);",0.7565,0.189125,positive
thrift,1202,comment_1,Could you reuse Xtruct and Xtruct2 instead of adding new definitions foo and foo2 to ThriftTest.thrift? and use something like: Thanks Roger,code_debt,low_quality_code,"Thu, 9 Jun 2011 13:32:10 +0000","Tue, 1 Nov 2011 02:52:15 +0000","Tue, 14 Jun 2011 19:38:39 +0000",453989,Could you reuse Xtruct and Xtruct2 instead of adding new definitions foo and foo2 to ThriftTest.thrift? and use something like: Thanks Roger,0.1333333333,0.1333333333,neutral
thrift,1202,comment_2,"should solve the problem with maps, at least for i32 and strings... Cheers! ps: I removed the old foo struct test, this is already tested by ThriftTest.Xtruct anyway",code_debt,dead_code,"Thu, 9 Jun 2011 13:32:10 +0000","Tue, 1 Nov 2011 02:52:15 +0000","Tue, 14 Jun 2011 19:38:39 +0000",453989,"THRIFT-1202-qunit-tests-java-test-server.patch should solve the problem with maps, at least for i32 and strings... Cheers! ps: I removed the old foo struct test, this is already tested by ThriftTest.Xtruct anyway",0.2098888889,0.1574166667,positive
thrift,1231,description,"TAsyncChannel.h includes TTransportUtils.h, but doesn't really need/use it.",code_debt,dead_code,"Tue, 5 Jul 2011 23:39:04 +0000","Fri, 8 Jul 2011 20:05:18 +0000","Fri, 8 Jul 2011 17:33:05 +0000",237241,"TAsyncChannel.h includes TTransportUtils.h, but doesn't really need/use it.",0,0,negative
thrift,1231,summary,Remove bogus include,code_debt,low_quality_code,"Tue, 5 Jul 2011 23:39:04 +0000","Fri, 8 Jul 2011 20:05:18 +0000","Fri, 8 Jul 2011 17:33:05 +0000",237241,Remove bogus include,-1,-1,negative
thrift,1241,description,Patch is based mainly on but some more improvements: namespace can be specified with dots '.' like for every other language for example: namespace php com.onego.thrift would generate in php file namespace com\onego\thrift to generate php files with namespaces use: thrift --gen php:namespace53 example.thrift,code_debt,low_quality_code,"Wed, 20 Jul 2011 15:24:17 +0000","Wed, 10 Aug 2011 18:27:52 +0000","Thu, 4 Aug 2011 23:00:01 +0000",1323344,"Patch is based mainly on https://issues.apache.org/jira/browse/THRIFT-777, but some more improvements: namespace can be specified with dots '.' like for every other language for example: namespace php com.onego.thrift would generate in php file namespace com\onego\thrift to generate php files with namespaces use: thrift --gen php:namespace53 example.thrift",0,0,neutral
thrift,1243,comment_4,"Included is a patch on 0.8 that adds more error handling to the libevent based server. It also fixes 2 minor bugs (handle failure of evhttp_bind_socket and prevent exceptions to reach libevent in As far as I can tell, the question of @diwaker is still valid, there is no good way currently to have the callback of to be passed the exact failure. However I don't think returning a bool helps, because libevent is the one asynchronously calling, so the error will have to be handled by the client *within* the callback. This callback will invariably throw a when a disconnect or a 404 happens. I added to this patch the ability to log such failures to stderr, but it would take more changes (compiler?) to actually pass the error to the client, so that a call will throw the correct error (and *not* EOF). Finally I removed all abort calls, trying to replace them by appropriate exceptions instead. Thanks!",code_debt,low_quality_code,"Sun, 24 Jul 2011 10:03:04 +0000","Tue, 1 Nov 2011 00:57:57 +0000","Sun, 11 Sep 2011 07:29:12 +0000",4224368,"Included is a patch on 0.8 that adds more error handling to the libevent based server. It also fixes 2 minor bugs (handle failure of evhttp_bind_socket and prevent exceptions to reach libevent in TEvhttpClientChannel::response). As far as I can tell, the question of @diwaker is still valid, there is no good way currently to have the callback of TEvhttpClientChannel::finish, to be passed the exact failure. However I don't think returning a bool helps, because libevent is the one asynchronously calling, so the error will have to be handled by the client within the callback. This callback will invariably throw a TTransportException::END_OF_FILE, when a disconnect or a 404 happens. I added to this patch the ability to log such failures to stderr, but it would take more changes (compiler?) to actually pass the error to the client, so that a call will throw the correct error (and not EOF). Finally I removed all abort calls, trying to replace them by appropriate exceptions instead. Thanks!",-0.04545833333,-0.04096296296,neutral
thrift,1275,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Thu, 4 Mar 2010 00:53:37 +0000 Subject: [PATCH 07/33] thrift: always prefix namespaces with "" ::"" Summary: Thrift always refers to namespaces using their full name. Therefore, it should prefix them with ""::"" to avoid accidentally matching a name defined in one of the current namespaces, rather than at the top-level. For example, if ServiceB is in namespace bar, and inherits from ServiceA in namespace foo, all code emitted for ServiceB now refers to ServiceA as ::foo::ServiceA instead of just foo::ServiceA. This allows the code to compile even if a namespace ::bar::foo also exists. An extra leading whitespace is also emitted, which is needed in cases when the name is used as the first template parameter, so that the emitted code contains ""< ::"" instead of ""<::"". Test Plan: jsong reported a build problem because of this name lookup error, and this change fixed his build. I also tested building [internal fb thing] and [internal fb thing], and they both built successfully. Tags: thrift  | 11 +++++++++-- 1 files changed, 9 insertions(+), 2 deletions(-)",code_debt,low_quality_code,"Thu, 18 Aug 2011 00:47:13 +0000","Fri, 19 Aug 2011 18:43:53 +0000","Fri, 19 Aug 2011 18:23:56 +0000",149803,"From d56203d414d23c7858a269e4aa547ee3164832fd Mon Sep 17 00:00:00 2001 From: Adam Simpkins <simpkins@fb.com> Date: Thu, 4 Mar 2010 00:53:37 +0000 Subject: [PATCH 07/33] thrift: always prefix namespaces with "" ::"" Summary: Thrift always refers to namespaces using their full name. Therefore, it should prefix them with ""::"" to avoid accidentally matching a name defined in one of the current namespaces, rather than at the top-level. For example, if ServiceB is in namespace bar, and inherits from ServiceA in namespace foo, all code emitted for ServiceB now refers to ServiceA as ::foo::ServiceA instead of just foo::ServiceA. This allows the code to compile even if a namespace ::bar::foo also exists. An extra leading whitespace is also emitted, which is needed in cases when the name is used as the first template parameter, so that the emitted code contains ""< ::"" instead of ""<::"". Test Plan: jsong reported a build problem because of this name lookup error, and this change fixed his build. I also tested building [internal fb thing] and [internal fb thing], and they both built successfully. Tags: thrift  compiler/cpp/src/generate/t_cpp_generator.cc | 11 +++++++++-- 1 files changed, 9 insertions, 2 deletions",-0.07966666667,-0.07081481481,neutral
thrift,1290,comment_0,"Hm, this patch doesn't apply cleanly for me.",code_debt,low_quality_code,"Wed, 24 Aug 2011 16:52:01 +0000","Thu, 25 Aug 2011 17:47:08 +0000","Thu, 25 Aug 2011 17:29:00 +0000",88619,"Hm, this patch doesn't apply cleanly for me.",0,0,negative
thrift,1290,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Tue, 6 Apr 2010 20:43:23 +0000 Subject: [PATCH 17/33] thrift: TNonblockingServer: clean up state in the destructor Summary: Implement the TNonblockingServer destructor, so that it closes the listen socket, destroys the event_base, and deletes any TConnections left in the connectionStack_. However, TNonblockingServer doesn't keep track of active TConnection objects, so those objects are still leaked. As part of this, I also changed the code to use event_init() rather than event_base_new(). This way we won't set the global event_base inside libevent, and we can be sure that no one else will be using it after the TNonblockingServer is destroyed. I grepped through all of [fb code base] to check for any other direct uses of event_set(), and didn't see any places that weren't also using event_base_set(). Therefore it seems like it should be safe to stop initializing the global event_base pointer. Test Plan: Tested with the test code in [a fb unittest], which creates, stops, and then deletes several Ran it under valgrind, and now it only complains about any active connections being leaked. Revert Plan: OK  | 4 ++-- 1 files changed, 2 insertions(+), 2 deletions(-)",code_debt,low_quality_code,"Wed, 24 Aug 2011 16:52:01 +0000","Thu, 25 Aug 2011 17:47:08 +0000","Thu, 25 Aug 2011 17:29:00 +0000",88619,"From cd9c1a10cb4df058fbdbed1b98a21a7a7470a28c Mon Sep 17 00:00:00 2001 From: Adam Simpkins <simpkins@fb.com> Date: Tue, 6 Apr 2010 20:43:23 +0000 Subject: [PATCH 17/33] thrift: TNonblockingServer: clean up state in the destructor Summary: Implement the TNonblockingServer destructor, so that it closes the listen socket, destroys the event_base, and deletes any TConnections left in the connectionStack_. However, TNonblockingServer doesn't keep track of active TConnection objects, so those objects are still leaked. As part of this, I also changed the code to use event_init() rather than event_base_new(). This way we won't set the global event_base inside libevent, and we can be sure that no one else will be using it after the TNonblockingServer is destroyed. I grepped through all of [fb code base] to check for any other direct uses of event_set(), and didn't see any places that weren't also using event_base_set(). Therefore it seems like it should be safe to stop initializing the global event_base pointer. Test Plan: Tested with the test code in [a fb unittest], which creates, stops, and then deletes several TNonblockingServers. Ran it under valgrind, and now it only complains about any active connections being leaked. Revert Plan: OK  lib/cpp/src/server/TNonblockingServer.cpp | 4 ++-- 1 files changed, 2 insertions, 2 deletions",0.13910625,0.1168416667,neutral
thrift,1290,summary,thrift: TNonblockingServer: clean up state in the,code_debt,low_quality_code,"Wed, 24 Aug 2011 16:52:01 +0000","Thu, 25 Aug 2011 17:47:08 +0000","Thu, 25 Aug 2011 17:29:00 +0000",88619," thrift: TNonblockingServer: clean up state in the
",0.4,0.4,neutral
thrift,1314,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Mon, 19 Apr 2010 19:09:16 +0000 Subject: [PATCH 25/33] thrift: add TProcessorFactory Summary: Adds a new TProcessorFactory class, and update all server classes to store a TProcessorFactory instead of a TProcessor. is called once for each newly accepted connection. This will allow a separate processor to be created for each connection, if desired. For now, everything always uses so all connections still end up using the same TProcessor. Some of the servers don't handle it well if getProcessor() throws an exception. TNonblockingServer may leak the fd and a TConnection object. TSimpleServer will exit and stop serving. However, this is no worse than the existing behavior if Test Plan: Ran the test code from [a fb unittest]. Revert Plan: OK Conflicts:  | 39 | 12 ++++++-- | 31 | 9 +++++- | 7 ++++- | 5 +++- 6 files changed, 89 insertions(+), 14 deletions(-)",code_debt,low_quality_code,"Wed, 31 Aug 2011 17:22:11 +0000","Thu, 1 Sep 2011 18:07:29 +0000","Thu, 1 Sep 2011 18:07:29 +0000",89118,"From a01b4ee026a6e0fa269af9f3f16684c4e312cd3c Mon Sep 17 00:00:00 2001 From: Adam Simpkins <simpkins@fb.com> Date: Mon, 19 Apr 2010 19:09:16 +0000 Subject: [PATCH 25/33] thrift: add TProcessorFactory Summary: Adds a new TProcessorFactory class, and update all server classes to store a TProcessorFactory instead of a TProcessor. TProcessorFactory::getProcessor() is called once for each newly accepted connection. This will allow a separate processor to be created for each connection, if desired. For now, everything always uses TSingletonProcessorFactory, so all connections still end up using the same TProcessor. Some of the servers don't handle it well if getProcessor() throws an exception. TNonblockingServer may leak the fd and a TConnection object. TSimpleServer will exit and stop serving. However, this is no worse than the existing behavior if eventHandler_->createContext() throws an exception. Test Plan: Ran the test code from [a fb unittest]. Revert Plan: OK Conflicts: lib/cpp/src/server/TSimpleServer.cpp  lib/cpp/src/TProcessor.h | 39 +++++++++++++++++++++++++++++ lib/cpp/src/server/TNonblockingServer.cpp | 12 ++++++-- lib/cpp/src/server/TServer.h | 31 +++++++++++++++++----- lib/cpp/src/server/TSimpleServer.cpp | 9 +++++- lib/cpp/src/server/TThreadPoolServer.cpp | 7 ++++- lib/cpp/src/server/TThreadedServer.cpp | 5 +++- 6 files changed, 89 insertions, 14 deletions",0.03822222222,0.02023529412,neutral
thrift,1349,summary,Remove unnecessary print outs,code_debt,low_quality_code,"Mon, 19 Sep 2011 05:07:55 +0000","Thu, 10 Jul 2014 13:42:24 +0000","Thu, 10 Jul 2014 13:42:24 +0000",88590869,Remove unnecessary print outs,0,0,negative
thrift,1393,description,"This is a very minor issue, but should be addressed nonetheless. The THttpClient class ensures the $uri_ property has a slash prefixed by appending one if needed in the constructor. However in THttpClient::read, there are 2 exceptions thrown where a slash is concatenated between the port and uri. This results in a superfluous slash in the TTransportException message. Example: ""THttpClient: Could not read 184549154 bytes from",code_debt,low_quality_code,"Tue, 18 Oct 2011 15:20:48 +0000","Tue, 18 Oct 2011 15:41:56 +0000","Tue, 18 Oct 2011 15:32:07 +0000",679,"This is a very minor issue, but should be addressed nonetheless. The THttpClient class ensures the $uri_ property has a slash prefixed by appending one if needed in the constructor. However in THttpClient::read, there are 2 exceptions thrown where a slash is concatenated between the port and uri. This results in a superfluous slash in the TTransportException message. Example: ""THttpClient: Could not read 184549154 bytes from xxx.yyy.com:80//randomService""",-0.14,-0.1,neutral
thrift,1393,summary,thrown from THttpClient contain superfluous slashes in the Exception message,code_debt,low_quality_code,"Tue, 18 Oct 2011 15:20:48 +0000","Tue, 18 Oct 2011 15:41:56 +0000","Tue, 18 Oct 2011 15:32:07 +0000",679,TTransportException's thrown from THttpClient contain superfluous slashes in the Exception message,-0.4,-0.4,negative
thrift,1452,comment_0,"Line 1514 of t_cpp_generator.cc in the function has an unused variable ttype, any reason for this or can it be removed? t_type *ttype =",code_debt,dead_code,"Wed, 7 Dec 2011 22:17:56 +0000","Fri, 9 Dec 2011 21:27:30 +0000","Thu, 8 Dec 2011 21:16:16 +0000",82700,"Line 1514 of t_cpp_generator.cc in the generate_struct_swap function has an unused variable ttype, any reason for this or can it be removed? t_type *ttype = get_true_type(tfield->get_type());",0,0.191,negative
thrift,1452,comment_1,Nope no reason it looks unused in our codebase as well.,code_debt,dead_code,"Wed, 7 Dec 2011 22:17:56 +0000","Fri, 9 Dec 2011 21:27:30 +0000","Thu, 8 Dec 2011 21:16:16 +0000",82700,Nope no reason it looks unused in our codebase as well.,0.631,0.631,negative
thrift,1480,description,"The python library files have some inconsistencies (different indent levels and docstring placement) and the pep8 linter produces dozens of warnings. There are also several places where tabs are used instead of spaces, which is not good. This patch addresses almost all of the pep8 issues with as little modification of the code as possible. This patch: * converts 3 instances of tabs into the correct number of spaces * removes unnecessary trailing semicolons and backslashes * changes None comparisons to be identity based, 'x != None' becomes 'x is not None' in a handful of places * removes unnecessary '== True' in one if statement * wraps lines at 80 characters and removes trailing whitespace * corrects a handful of grammar problems in docstrings (mostly to help with 80 char line wrap) * converts all the docstrings to use """""" (instead of ''' or "") and makes placement consistent * fixes pep8 warnings about missing spaces around operators, e.g. (a-b) becomes (a - b) * adjusts ordering of stdlib imports to be alphabetical (could be better still) * correct internal indent depths of methods when they switch from 2 to 4 spaces There's a mix of files that use 4-spaces for indentation, versus the majority which use 2-spaces for indentation. This patch doesn't change that. I wanted to get the code as pep8 clean as possible and touch as few lines as possible to get it there. The TType constants defined in Thrift.py have some nice vertical whitespace that isn't pep8-happy, but it looked too clean to touch so I left it unchanged. After this patch, the pep8 utility only reports two warnings: # ""indentation is not a multiple of four"" for most files (no biggie) # ""multiple spaces before operator"" in Thrift.py for the TTypes class constants The unit tests all pass with this patch.",code_debt,low_quality_code,"Sat, 31 Dec 2011 19:26:52 +0000","Tue, 3 Jan 2012 17:56:04 +0000","Tue, 3 Jan 2012 17:33:07 +0000",252375,"The python library files have some inconsistencies (different indent levels and docstring placement) and the pep8 linter produces dozens of warnings. There are also several places where tabs are used instead of spaces, which is not good. This patch addresses almost all of the pep8 issues with as little modification of the code as possible. This patch: converts 3 instances of tabs into the correct number of spaces removes unnecessary trailing semicolons and backslashes changes None comparisons to be identity based, 'x != None' becomes 'x is not None' in a handful of places removes unnecessary '== True' in one if statement wraps lines at 80 characters and removes trailing whitespace corrects a handful of grammar problems in docstrings (mostly to help with 80 char line wrap) converts all the docstrings to use """""" (instead of ''' or "") and makes placement consistent fixes pep8 warnings about missing spaces around operators, e.g. (a-b) becomes (a - b) adjusts ordering of stdlib imports to be alphabetical (could be better still) correct internal indent depths of methods when they switch from 2 to 4 spaces There's a mix of files that use 4-spaces for indentation, versus the majority which use 2-spaces for indentation. This patch doesn't change that. I wanted to get the code as pep8 clean as possible and touch as few lines as possible to get it there. The TType constants defined in Thrift.py have some nice vertical whitespace that isn't pep8-happy, but it looked too clean to touch so I left it unchanged. After this patch, the pep8 utility only reports two warnings: ""indentation is not a multiple of four"" for most files (no biggie) ""multiple spaces before operator"" in Thrift.py for the TTypes class constants The unit tests all pass with this patch.",0.03835227273,0.03835227273,negative
thrift,1480,summary,"python: remove tabs, adjust whitespace and address PEP8 warnings",code_debt,low_quality_code,"Sat, 31 Dec 2011 19:26:52 +0000","Tue, 3 Jan 2012 17:56:04 +0000","Tue, 3 Jan 2012 17:33:07 +0000",252375,"python: remove tabs, adjust whitespace and address PEP8 warnings",-0.6,-0.6,neutral
thrift,1504,summary,Cocoa Generator should use local file imports for base Thrift headers,code_debt,low_quality_code,"Wed, 25 Jan 2012 20:41:06 +0000","Fri, 27 Jan 2012 04:13:57 +0000","Fri, 27 Jan 2012 03:08:57 +0000",109671,Cocoa Generator should use local file imports for base Thrift headers,0,0,neutral
thrift,1583,description,The generated code has memory leaks if you create and destroy an object. I've fixed the main problem on this git repo on branch c_glib_0.8.x but commits can be moved to trunk easily. There are still some errors to control on generated code (there are comments in the code). I'm planning to fix in a short term if you validate the current patches. This git repository has also the patches sent on issues 1582 and 1578 that are also related:,code_debt,low_quality_code,"Fri, 20 Apr 2012 14:56:38 +0000","Fri, 11 May 2012 00:41:39 +0000","Sat, 28 Apr 2012 11:35:10 +0000",679112,The generated code has memory leaks if you create and destroy an object. I've fixed the main problem on this git repo on branch c_glib_0.8.x but commits can be moved to trunk easily. https://github.com/jcaden/thrift There are still some errors to control on generated code (there are comments in the code). I'm planning to fix in a short term if you validate the current patches. This git repository has also the patches sent on issues 1582 and 1578 that are also related: https://issues.apache.org/jira/browse/THRIFT-1582 https://issues.apache.org/jira/browse/THRIFT-1578,-0.10675,-0.10675,neutral
thrift,1583,summary,c_glib leaks memory,code_debt,low_quality_code,"Fri, 20 Apr 2012 14:56:38 +0000","Fri, 11 May 2012 00:41:39 +0000","Sat, 28 Apr 2012 11:35:10 +0000",679112,c_glib leaks memory,0,0,neutral
thrift,1624,comment_2,"Here's a theory: maybe t_struct::is_union_ is just an uninitialized variable? The t_struct constructor doesn't initialize it. thrifty.yy line 724 will initialize it for struct/union declarations, but I'm guessing that codepath is skipped for service declarations.",code_debt,low_quality_code,"Thu, 7 Jun 2012 17:29:11 +0000","Tue, 26 Jun 2012 16:45:58 +0000","Tue, 26 Jun 2012 16:45:58 +0000",1639007,"Here's a theory: maybe t_struct::is_union_ is just an uninitialized variable? The t_struct constructor doesn't initialize it. thrifty.yy line 724 will initialize it for struct/union declarations, but I'm guessing that codepath is skipped for service declarations.",0.14075,0.14075,neutral
thrift,1624,comment_4,"Hey Bryan, would you consider merging this patch? Nathaniel never replied to confirm that it fixed his issue, but fixing an uninitialized variable couldn't hurt.",code_debt,low_quality_code,"Thu, 7 Jun 2012 17:29:11 +0000","Tue, 26 Jun 2012 16:45:58 +0000","Tue, 26 Jun 2012 16:45:58 +0000",1639007,"Hey Bryan, would you consider merging this patch? Nathaniel never replied to confirm that it fixed his issue, but fixing an uninitialized variable couldn't hurt.",0.2855,0.2855,neutral
thrift,1688,description,The markup on the IDL page needs to be updated. It's pretty hard to read at the moment.,code_debt,low_quality_code,"Sat, 8 Sep 2012 14:46:41 +0000","Fri, 28 Sep 2012 04:02:30 +0000","Fri, 28 Sep 2012 04:02:30 +0000",1689349,The markup on the IDL page needs to be updated. It's pretty hard to read at the moment.,0,0,negative
thrift,1799,comment_2,"Cleaned up patch, no other changes.",code_debt,low_quality_code,"Fri, 21 Dec 2012 20:16:32 +0000","Sat, 8 Jun 2013 03:15:51 +0000","Sat, 8 Jun 2013 03:15:50 +0000",14540358,"Cleaned up patch, no other changes.",0,0,neutral
thrift,1800,comment_1,"Can you change test/DocTest.thrift to demonstrate the change that you made? I'm not able to get it to properly escape the characters (instead, it is adding additional bogus characters).",code_debt,low_quality_code,"Fri, 21 Dec 2012 21:45:26 +0000","Sat, 8 Jun 2013 03:15:43 +0000","Sat, 8 Jun 2013 03:15:43 +0000",14535017,"Can you change test/DocTest.thrift to demonstrate the change that you made? I'm not able to get it to properly escape the characters (instead, it is adding additional bogus characters).",-0.1653333333,-0.1653333333,negative
thrift,1813,comment_3,"I rebased Arvind's patch, removed the duplication between autogen_comment and autogen_summary, and added a date as I previously suggested. GitHub pull request also available at:",code_debt,duplicated_code,"Fri, 28 Dec 2012 15:28:08 +0000","Wed, 17 Jun 2015 08:01:22 +0000","Sat, 9 Nov 2013 18:43:06 +0000",27314098,"roger.meier I rebased Arvind's patch, removed the duplication between autogen_comment and autogen_summary, and added a date as I previously suggested. GitHub pull request also available at: https://github.com/apache/thrift/pull/63",0,0,neutral
thrift,1813,description,Why? # A lot of static analysis tools understand the annotation and treat those classes differently. # Setting the date field with the date & comments field with the version of the thrift compiler might prove to be valuable for tracing the origins of the generated file. The overheads should be minimal as the retention policy of the annotation is purely at a source level,code_debt,low_quality_code,"Fri, 28 Dec 2012 15:28:08 +0000","Wed, 17 Jun 2015 08:01:22 +0000","Sat, 9 Nov 2013 18:43:06 +0000",27314098,Why? A lot of static analysis tools understand the javax.annotation.Generated annotation and treat those classes differently. Setting the date field with the date & comments field with the version of the thrift compiler might prove to be valuable for tracing the origins of the generated file. The overheads should be minimal as the retention policy of the annotation is purely at a source level,-0.04675,-0.03116666667,neutral
thrift,1819,comment_1,I tested using GCC 4.6.3 on Ubuntu 12.04. I have a static analysis script which finds unused #includes; I doubt that I removed any referenced symbols.,code_debt,dead_code,"Sat, 5 Jan 2013 21:44:49 +0000","Thu, 17 Jan 2019 20:06:08 +0000","Thu, 17 Jan 2019 20:06:08 +0000",190333279,I tested using GCC 4.6.3 on Ubuntu 12.04. I have a static analysis script which finds unused #includes; I doubt that I removed any referenced symbols.,0.01275,0.01275,neutral
thrift,1819,summary,Thrift has many unused #includes,code_debt,low_quality_code,"Sat, 5 Jan 2013 21:44:49 +0000","Thu, 17 Jan 2019 20:06:08 +0000","Thu, 17 Jan 2019 20:06:08 +0000",190333279,Thrift has many unused #includes,0,0,neutral
thrift,1842,summary,Memory leak with Pipes,code_debt,low_quality_code,"Tue, 22 Jan 2013 23:11:05 +0000","Sat, 8 Jun 2013 03:15:41 +0000","Sat, 8 Jun 2013 03:15:41 +0000",11765076,Memory leak with Pipes,-0.2,-0.2,neutral
thrift,1873,description,"The binary protocol factory takes strict read/write flags as (optional) arguments, but did in fact never use them.",code_debt,dead_code,"Mon, 4 Mar 2013 22:56:22 +0000","Sat, 8 Jun 2013 03:15:40 +0000","Sat, 8 Jun 2013 03:15:40 +0000",8223558,"The binary protocol factory takes strict read/write flags as (optional) arguments, but did in fact never use them.",0,0,neutral
thrift,1883,comment_2,I'm withdrawing the request to fix this since I'm working on a full rewrite of this library (see,code_debt,low_quality_code,"Thu, 14 Mar 2013 07:41:50 +0000","Sat, 8 Jun 2013 18:35:53 +0000","Sat, 8 Jun 2013 18:35:53 +0000",7469643,I'm withdrawing the request to fix this since I'm working on a full rewrite of this library (see https://mail-archives.apache.org/mod_mbox/thrift-dev/201303.mbox/browser).,0.6,0.6,neutral
thrift,1886,description,The virtual method implementations and *_get/set_property functions are not part of the public API and should be static.,code_debt,low_quality_code,"Thu, 14 Mar 2013 08:19:25 +0000","Sat, 8 Jun 2013 18:35:46 +0000","Sat, 8 Jun 2013 18:35:46 +0000",7467381,The virtual method implementations and *_get/set_property functions are not part of the public API and should be static.,0.875,0.875,negative
thrift,1886,summary,Virtual method implementations and *_get/set_property functions should be static,code_debt,low_quality_code,"Thu, 14 Mar 2013 08:19:25 +0000","Sat, 8 Jun 2013 18:35:46 +0000","Sat, 8 Jun 2013 18:35:46 +0000",7467381,Virtual method implementations and *_get/set_property functions should be static,-0.875,-0.875,neutral
thrift,18,description,"gcc 4.2 shows a huge amount of warnings ""warning: deprecated conversion from string constant to 'char*'""",code_debt,low_quality_code,"Wed, 21 May 2008 14:50:42 +0000","Tue, 1 Nov 2011 02:54:27 +0000","Tue, 27 May 2008 02:07:45 +0000",472623,"gcc 4.2 shows a huge amount of warnings ""warning: deprecated conversion from string constant to 'char*'""",-0.2333333333,-0.2333333333,negative
thrift,18,summary,warning: deprecated conversion from string constant to 'char*',code_debt,low_quality_code,"Wed, 21 May 2008 14:50:42 +0000","Tue, 1 Nov 2011 02:54:27 +0000","Tue, 27 May 2008 02:07:45 +0000",472623,warning: deprecated conversion from string constant to 'char*',-0.6,-0.6,negative
thrift,191,comment_3,"Here is a first patch. For the time being the metadata structure only contains the field name and is an inner class just like Isset, but unlike Isset this one will be the same for all classes, so may be it should be placed somewhere else. Let me know your thoughts.",code_debt,low_quality_code,"Tue, 4 Nov 2008 03:21:09 +0000","Tue, 1 Nov 2011 02:54:08 +0000","Mon, 5 Jan 2009 21:16:54 +0000",5421345,"Here is a first patch. For the time being the metadata structure only contains the field name and is an inner class just like Isset, but unlike Isset this one will be the same for all classes, so may be it should be placed somewhere else. Let me know your thoughts.",-0.1666666667,-0.1666666667,neutral
thrift,1932,description,"The Compilation of thrift-0.9.0 ended with the following warnings: * QA Notice: Package triggers severe warnings which indicate that it * may exhibit random runtime failures. * warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing] * warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing] When looking into ... 711 = 712 713 if == 4) { 714 // 0 length event indicates padding 715 if (*((uint32_t == 0) { 716 // T_DEBUG_L(1, ""Got padding""); 717 718 continue; 719 } ... it becomes obvious that the four values casted into a pointer were previously read from the input stream by ::read() in line 661: 661 = ::read(fd_, readBuff_, readBuffSize_); Same issue here: 725 readState_.event_ = new eventInfo(); 726 While it is a two minute job fix this problem, the method looks so fragile that a clean rewrite appears to be more appropriate.",code_debt,low_quality_code,"Thu, 18 Apr 2013 16:10:01 +0000","Mon, 20 May 2013 03:13:13 +0000","Sun, 5 May 2013 21:38:30 +0000",1488509,"The Compilation of thrift-0.9.0 ended with the following warnings: QA Notice: Package triggers severe warnings which indicate that it may exhibit random runtime failures. src/thrift/transport/TFileTransport.cpp:715:56: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing] src/thrift/transport/TFileTransport.cpp:726:84: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing] When looking into src/thrift/transport/TFileTransport.cpp:715 ... 711 readState_.eventSizeBuff_[readState_.eventSizeBuffPos_++] = 712 readBuff_[readState_.bufferPtr_++]; 713 if (readState_.eventSizeBuffPos_ == 4) { 714 // 0 length event indicates padding 715 if (*((uint32_t *)(readState_.eventSizeBuff_)) == 0) { 716 // T_DEBUG_L(1, ""Got padding""); 717 readState_.resetState(readState_.lastDispatchPtr_); 718 continue; 719 } ... it becomes obvious that the four values casted into a pointer were previously read from the input stream by ::read() in line 661: 661 readState_.bufferLen_ = ::read(fd_, readBuff_, readBuffSize_); Same issue here: 725 readState_.event_ = new eventInfo(); 726 readState_.event_->eventSize_ = *((uint32_t*)(readState_.eventSizeBuff_)); 727 While it is a two minute job fix this problem, the method TFileTransport::readEvent() looks so fragile that a clean rewrite appears to be more appropriate.",-0.17,-0.08055555556,negative
thrift,1953,comment_1,Thanks for the Patch! Great idea to integrate Apache Thrift with ASP.NET MVC 3. What do you think about renaming according to the same naming as System.Web.Mvc? - Thrift.Server.MVC - - Could you merge into to help reducing maintenance effort? And please add the mvc option to the macro within thanks! ;-r,code_debt,low_quality_code,"Wed, 1 May 2013 16:23:43 +0000","Tue, 28 May 2013 02:25:27 +0000","Sat, 18 May 2013 13:35:51 +0000",1458728,Thanks for the Patch! Great idea to integrate Apache Thrift with ASP.NET MVC 3. What do you think about renaming according to the same naming as System.Web.Mvc? Thrift.Server.MVC > Thrift.Server.Mvc lib/csharp/src/Server/MVC/ > lib/csharp/src/Server/Mvc/ lib/csharp/src/Thrift.MVC3.csproj > lib/csharp/src/Thrift.Mvc.csproj Could you merge lib/csharp/src/Thrift.MVC3.sln into lib/csharp/src/Thrift.sln to help reducing maintenance effort? And please add the mvc option to the THRIFT_REGISTER_GENERATOR macro within compiler/cpp/src/generate/t_csharp_generator.cc thanks! ;-r,0.2060606061,0.1261904762,positive
thrift,1953,comment_2,"hi, Roger Meier, i've changed the code style as you wish, please check the attachment patch, thanks",code_debt,low_quality_code,"Wed, 1 May 2013 16:23:43 +0000","Tue, 28 May 2013 02:25:27 +0000","Sat, 18 May 2013 13:35:51 +0000",1458728,"hi, Roger Meier, i've changed the code style as you wish, please check the attachment patch, thanks",0.2666666667,0.2666666667,neutral
thrift,1953,comment_5,"Thanks! Here is the error that I got when making the compiler: I've attached a patch to remove the variable, rather than having more back and forth about that. I'd rather have them in a single solution, where we just don't build all of them.  : Do either of you have an opinion on this?",code_debt,dead_code,"Wed, 1 May 2013 16:23:43 +0000","Tue, 28 May 2013 02:25:27 +0000","Sat, 18 May 2013 13:35:51 +0000",1458728,"Thanks! Here is the error that I got when making the compiler: I've attached a patch to remove the variable, rather than having more back and forth about that. I'd rather have them in a single solution, where we just don't build all of them. jensg roger.meier: Do either of you have an opinion on this?",0.05,0.04,negative
thrift,1953,comment_6,"thanks, Carl Yeksigian I find the variable now. I upload a new patch to remove it.",code_debt,low_quality_code,"Wed, 1 May 2013 16:23:43 +0000","Tue, 28 May 2013 02:25:27 +0000","Sat, 18 May 2013 13:35:51 +0000",1458728,"thanks, Carl Yeksigian I find the variable now. I upload a new patch to remove it.",0.2,0.2,neutral
thrift,1982,description,"vsnprintf on Windows (visual) return -1 on failure instead of amount of bytes needed. so line like this: int need = STACK_BUF_SIZE, message, ap); won't work. Solution: ugly conditional compilation which will use microsoft specific functions... See patch for details.",code_debt,low_quality_code,"Wed, 29 May 2013 17:29:07 +0000","Wed, 5 Jun 2013 03:11:00 +0000","Tue, 4 Jun 2013 20:27:24 +0000",529097,"vsnprintf on Windows (visual) return -1 on failure instead of amount of bytes needed. so line like this: int need = vsnprintf(stack_buf, STACK_BUF_SIZE, message, ap); won't work. Solution: ugly conditional compilation which will use microsoft specific functions... See patch for details.",-0.4666666667,-0.4666666667,negative
thrift,1999,description,"""unused arguments"" warning. suppressed by (void) x; in patch",code_debt,low_quality_code,"Wed, 5 Jun 2013 15:30:17 +0000","Thu, 10 Jul 2014 13:42:22 +0000","Thu, 10 Jul 2014 13:42:22 +0000",34553525,"""unused arguments"" warning. suppressed by (void) x; in patch",-0.3,-0.3,negative
thrift,2006,comment_1,There should be an upper limit on the size of the string that is read for the RPC method name in the TBinaryProtocol message header. Does anybody know what the maximum length of a call name is? Does the compiler have an upper limit on string length for the name of a RPC method?,code_debt,low_quality_code,"Sat, 8 Jun 2013 08:18:10 +0000","Wed, 30 Sep 2015 20:18:37 +0000","Mon, 28 Sep 2015 13:34:36 +0000",72767786,There should be an upper limit on the size of the string that is read for the RPC method name in the TBinaryProtocol message header. Does anybody know what the maximum length of a call name is? Does the compiler have an upper limit on string length for the name of a RPC method?,0.2083333333,0.2083333333,neutral
thrift,2006,comment_3,"The code path is inside backwards compatibility (pre-versioned) header reads. By setting a string size limit in BinaryProtocol before serving, this will prevent a core. Use setStringSizeLimit in BinaryProtocol to set an upper limit on string reads. We could be more strict here, for example if someone was able to tell me what the maximum allowed size of a method call name was, we could hardcode the limit and prevent this without ""optional"" behavior. Nobody could provide this information, so I provided a way to achieve the desired behavior without any code changes.",code_debt,low_quality_code,"Sat, 8 Jun 2013 08:18:10 +0000","Wed, 30 Sep 2015 20:18:37 +0000","Mon, 28 Sep 2015 13:34:36 +0000",72767786,"The code path is inside backwards compatibility (pre-versioned) header reads. By setting a string size limit in BinaryProtocol before serving, this will prevent a core. Use setStringSizeLimit in BinaryProtocol to set an upper limit on string reads. We could be more strict here, for example if someone was able to tell me what the maximum allowed size of a method call name was, we could hardcode the limit and prevent this without ""optional"" behavior. Nobody could provide this information, so I provided a way to achieve the desired behavior without any code changes.",0.1142,0.1142,neutral
thrift,2006,summary,TBinaryProtocol message header call name length is not validated and can be used to core the server,code_debt,low_quality_code,"Sat, 8 Jun 2013 08:18:10 +0000","Wed, 30 Sep 2015 20:18:37 +0000","Mon, 28 Sep 2015 13:34:36 +0000",72767786,TBinaryProtocol message header call name length is not validated and can be used to core the server,-0.5,-0.5,negative
thrift,2017,description,"In file class t_program : public t_doc { 59 public: 60 path, std::string name) : 61 path_(path), 62 name_(name), 63 out_path_(""./""), 64 { 65 scope_ = new t_scope(); 66 } 67 68 path) : 69 path_(path), 70 out_path_(""./""), 71 { 72 name_ = program_name(path); 73 scope_ = new t_scope(); 74 } In Above code at line number 65 and 73 Resource leaks happens as 1. Allocating memory by calling ""new t_scope"". 2. Assigning: ""this-3. The constructor allocates field ""scope_"" of ""t_program"" but there is no destructor in the code. destructor should deallocate memroy for t_scope. which sometimes causes Resource Leak. Possible patch: There should be destructor to release the resources allocated dynamically. ~t_program() { delete scope_; scope_ = NULL; }",code_debt,low_quality_code,"Thu, 13 Jun 2013 12:45:44 +0000","Mon, 19 Aug 2013 00:31:54 +0000","Sun, 4 Aug 2013 12:21:09 +0000",4491325,"In file compiler/cpp/src/parse/t_program.h class t_program : public t_doc { 59 public: 60 t_program(std::string path, std::string name) : 61 path_(path), 62 name_(name), 63 out_path_(""./""), 64 out_path_is_absolute_(false) { 65 scope_ = new t_scope(); 66 } 67 68 t_program(std::string path) : 69 path_(path), 70 out_path_(""./""), 71 out_path_is_absolute_(false) { 72 name_ = program_name(path); 73 scope_ = new t_scope(); 74 } In Above code at line number 65 and 73 Resource leaks happens as 1. Allocating memory by calling ""new t_scope"". 2. Assigning: ""this->scope_"" = ""new t_scope"". 3. The constructor allocates field ""scope_"" of ""t_program"" but there is no destructor in the code. destructor should deallocate memroy for t_scope. which sometimes causes Resource Leak. Possible patch: There should be destructor to release the resources allocated dynamically. ~t_program() { delete scope_; scope_ = NULL; }",-0.01818181818,-0.04615384615,neutral
thrift,2017,summary,Resource Leak in thrift struct under,code_debt,low_quality_code,"Thu, 13 Jun 2013 12:45:44 +0000","Mon, 19 Aug 2013 00:31:54 +0000","Sun, 4 Aug 2013 12:21:09 +0000",4491325,Resource Leak in thrift struct under compiler/cpp/src/parse/t_program.h,-0.2,-0.1,neutral
thrift,2020,description,"A lot of windows specific files have been ""removed"" recently, but git clone still creates empty versions of those files.",code_debt,low_quality_code,"Thu, 13 Jun 2013 13:55:00 +0000","Fri, 12 Aug 2016 01:29:47 +0000","Fri, 8 Apr 2016 05:36:07 +0000",88962067,"A lot of windows specific files have been ""removed"" recently, but git clone still creates empty versions of those files.",0.0405,0.0405,neutral
thrift,2020,summary,Thrift library has some empty files that haven't really been deleted,code_debt,dead_code,"Thu, 13 Jun 2013 13:55:00 +0000","Fri, 12 Aug 2016 01:29:47 +0000","Fri, 8 Apr 2016 05:36:07 +0000",88962067,Thrift library has some empty files that haven't really been deleted,-0.2,-0.2,neutral
thrift,2021,comment_2,"+1 Applies after fixing EOL style (win=>*nix), looks good to me.",code_debt,low_quality_code,"Thu, 13 Jun 2013 13:59:34 +0000","Fri, 12 Aug 2016 01:29:52 +0000","Fri, 8 Apr 2016 05:36:08 +0000",88961794,"+1 Applies after fixing EOL style (win=>*nix), looks good to me.",0.788,0.788,positive
thrift,2021,description,"Currently, TBinaryProtocol reads into a temporary buffer, then copies the results into the resulting std::string. For large strings (like a megabyte), the copy can be a performance issue. The attached patch reads directly into the std::string instead.",code_debt,slow_algorithm,"Thu, 13 Jun 2013 13:59:34 +0000","Fri, 12 Aug 2016 01:29:52 +0000","Fri, 8 Apr 2016 05:36:08 +0000",88961794,"Currently, TBinaryProtocol reads into a temporary buffer, then copies the results into the resulting std::string. For large strings (like a megabyte), the copy can be a performance issue. The attached patch reads directly into the std::string instead.",0,0,neutral
thrift,2021,summary,Improve large binary protocol string performance,code_debt,slow_algorithm,"Thu, 13 Jun 2013 13:59:34 +0000","Fri, 12 Aug 2016 01:29:52 +0000","Fri, 8 Apr 2016 05:36:08 +0000",88961794,Improve large binary protocol string performance,0.4,0.4,neutral
thrift,2028,comment_4,"Another suggestion would be to eliminate the project-specific abstraction and require either boost or C++11, assuming either of them have all the primitives. This would simplify the code base quite a bit and remove the need to maintain this abstraction layer.",code_debt,complex_code,"Thu, 13 Jun 2013 14:42:57 +0000","Wed, 6 Jan 2016 02:06:04 +0000","Sat, 10 Oct 2015 23:22:11 +0000",73384754,"Another suggestion would be to eliminate the project-specific abstraction and require either boost or C++11, assuming either of them have all the primitives. This would simplify the code base quite a bit and remove the need to maintain this abstraction layer.",-0.3,-0.3,neutral
thrift,2028,description,"The current threading implementations have some minor annoyances: All do some amount of internal state tracking with an enum that ends up being overkill. All use weak_ptrs to help manage lifetimes, instead of the base class that was designed for exactly this purpose. All of the specific thread factories implement ""detached"" methods, but the base thread factory doesn't have virtual methods exposing the detached methods. The thread manager has an unused local. Adding a ""UniqueGuard"" class to Mutex.h, to give more flexible RAII management to locks. Currently no clients of this, but I have some patches that will eventually use this.",code_debt,low_quality_code,"Thu, 13 Jun 2013 14:42:57 +0000","Wed, 6 Jan 2016 02:06:04 +0000","Sat, 10 Oct 2015 23:22:11 +0000",73384754,"The current threading implementations have some minor annoyances: All do some amount of internal state tracking with an enum that ends up being overkill. All use weak_ptrs to help manage lifetimes, instead of the enable_shared_from_this base class that was designed for exactly this purpose. All of the specific thread factories implement ""detached"" methods, but the base thread factory doesn't have virtual methods exposing the detached methods. The thread manager has an unused local. Adding a ""UniqueGuard"" class to Mutex.h, to give more flexible RAII management to locks. Currently no clients of this, but I have some patches that will eventually use this.",0.009523809524,0.03452380952,negative
thrift,2028,summary,Cleanup threading headers / libraries,code_debt,low_quality_code,"Thu, 13 Jun 2013 14:42:57 +0000","Wed, 6 Jan 2016 02:06:04 +0000","Sat, 10 Oct 2015 23:22:11 +0000",73384754,Cleanup threading headers / libraries,0,0,neutral
thrift,2031,comment_0,"Ack... doesn't apply cleanly, because of other unsubmitted patches. I'll clean this up shortly.",code_debt,low_quality_code,"Fri, 14 Jun 2013 20:57:16 +0000","Fri, 12 Aug 2016 01:29:46 +0000","Fri, 8 Apr 2016 05:36:10 +0000",88850334,"Ack... doesn't apply cleanly, because of other unsubmitted patches. I'll clean this up shortly.",0.2,0.2,negative
thrift,2031,comment_1,"Seems outdated, the -1 has been replaced with in the meantime.",code_debt,low_quality_code,"Fri, 14 Jun 2013 20:57:16 +0000","Fri, 12 Aug 2016 01:29:46 +0000","Fri, 8 Apr 2016 05:36:10 +0000",88850334,"Seems outdated, the -1 has been replaced with THRIFT_INVALID_SOCKET in the meantime.",0,0,negative
thrift,2032,comment_0,Patch ] does also fix a unnecessary double assignment to result2 in AcceptImpl().,code_debt,low_quality_code,"Sat, 15 Jun 2013 01:16:56 +0000","Thu, 25 Jul 2013 03:40:04 +0000","Tue, 25 Jun 2013 20:23:29 +0000",932793,Patch [^THRIFT-2032-csharp-client-leaks-handles.patch ] does also fix a unnecessary double assignment to result2 in AcceptImpl().,0,0,neutral
thrift,2032,comment_3,"Exactly. So lets drop the idisposable from the iface. Dont want to put too much effort in here, just need to address the leak.",code_debt,low_quality_code,"Sat, 15 Jun 2013 01:16:56 +0000","Thu, 25 Jul 2013 03:40:04 +0000","Tue, 25 Jun 2013 20:23:29 +0000",932793,"Exactly. So lets drop the idisposable from the iface. Dont want to put too much effort in here, just need to address the leak.",-0.05555555556,-0.05555555556,negative
thrift,2116,summary,Cocoa generator improvements (service inheritance / server-side exception handling) and an implementation of the Thrift tutorial,code_debt,low_quality_code,"Sun, 11 Aug 2013 14:49:54 +0000","Thu, 10 Oct 2019 23:00:05 +0000","Mon, 14 Jan 2019 15:03:33 +0000",171245619,Cocoa generator improvements (service inheritance / server-side exception handling) and an implementation of the Thrift tutorial,0,0,neutral
thrift,211,comment_3,"Replaced tabs with spaces (oops). Fixed #1. Is this unified enough to you? It eliminates the double search. It is not super-unified, but unifying them more would be kind of gross. For 2, I did consider it, but who would forge a 'DOWN' message? Verifying it just requires more state. I think converting to eunit would be great.",code_debt,low_quality_code,"Thu, 20 Nov 2008 22:30:30 +0000","Tue, 1 Nov 2011 02:52:21 +0000","Thu, 4 Jun 2009 02:01:37 +0000",16860667,"Replaced tabs with spaces (oops). Fixed #1. Is this unified enough to you? It eliminates the double search. It is not super-unified, but unifying them more would be kind of gross. For 2, I did consider it, but who would forge a 'DOWN' message? Verifying it just requires more state. I think converting to eunit would be great.",0.076,0.076,neutral
thrift,211,comment_6,"OK. This is probably good enough to check in. I don't remember if erlang:monitor has a monopoly on DOWN messages, so hence there's some reason to at least add a guard clause (is_reference() on the monitor_ref) to make sure that it's in the right format.",code_debt,low_quality_code,"Thu, 20 Nov 2008 22:30:30 +0000","Tue, 1 Nov 2011 02:52:21 +0000","Thu, 4 Jun 2009 02:01:37 +0000",16860667,"OK. This is probably good enough to check in. I don't remember if erlang:monitor has a monopoly on DOWN messages, so hence there's some reason to at least add a guard clause (is_reference() on the monitor_ref) to make sure that it's in the right format.",0.7198333333,0.7198333333,neutral
thrift,211,description,"Add a client option that causes clients to monitor their creators and terminate when the creator dies. This makes it possible to prevent client leaks without linking, because the latter causes application code to be killed when a transport error occurs and exits are not trapped.",code_debt,low_quality_code,"Thu, 20 Nov 2008 22:30:30 +0000","Tue, 1 Nov 2011 02:52:21 +0000","Thu, 4 Jun 2009 02:01:37 +0000",16860667,"http://gitweb.thrift-rpc.org/?p=thrift.git;a=log;h=refs/heads/pri/dreiss/erl-tether;hb=HEAD Add a client option that causes clients to monitor their creators and terminate when the creator dies. This makes it possible to prevent client leaks without linking, because the latter causes application code to be killed when a transport error occurs and exits are not trapped.",-0.1,-0.1,neutral
thrift,2141,comment_0,"Hi  and other cocoa users, from my feelings we should follow the approach used in THRIFT-2204 as it is more in line with the other languages. On the other side, two SSL implementations for cocoa seems one too much, but I like the idea of improving the tutorial code by including the SSL feature, so I'd keep that part, updated to the solution used in THRIFT-2204. I'd like to know your opinion on that matter. Any other cocoa people reading this are invited to chime in as well. I'm not a cocoa user yet, so I may overlook sth. important. Thanks, JensG",code_debt,low_quality_code,"Wed, 21 Aug 2013 12:50:19 +0000","Thu, 10 Oct 2019 23:00:18 +0000","Mon, 14 Jan 2019 15:03:26 +0000",170388787,"Hi drobakowski and other cocoa users, from my feelings we should follow the approach used in THRIFT-2204 as it is more in line with the other languages. On the other side, two SSL implementations for cocoa seems one too much, but I like the idea of improving the tutorial code by including the SSL feature, so I'd keep that part, updated to the solution used in THRIFT-2204. I'd like to know your opinion on that matter. Any other cocoa people reading this are invited to chime in as well. I'm not a cocoa user yet, so I may overlook sth. important. Thanks, JensG",0.3258571429,0.3258571429,neutral
thrift,2143,comment_1,"Hi , I think the added line {{ENUM = 16}} in {{types.rb}} is incorrect. If you compare with any other language, there is no such entry for enums.",code_debt,low_quality_code,"Wed, 21 Aug 2013 23:35:10 +0000","Fri, 4 Dec 2015 18:49:55 +0000","Fri, 4 Dec 2015 18:49:55 +0000",72126885,"Hi thecarlhall, I think the added line ENUM = 16 in types.rb is incorrect. If you compare with any other language, there is no such entry for enums.",0,0,negative
thrift,2143,comment_4,"Hi , IIRC I added to know when an {{int}} value should be used to find the {{enum}} value in a list since the int value transmitted is the enum position. There's a good chance that I'll be back in the space soon, so I'll hopefully be able to provide more exception/error details if my description above doesn't help much.",code_debt,low_quality_code,"Wed, 21 Aug 2013 23:35:10 +0000","Fri, 4 Dec 2015 18:49:55 +0000","Fri, 4 Dec 2015 18:49:55 +0000",72126885,"Hi jensg, IIRC I added ::Thrift::Types::ENUM to know when an int value should be used to find the enum value in a list since the int value transmitted is the enum position. There's a good chance that I'll be back in the space soon, so I'll hopefully be able to provide more exception/error details if my description above doesn't help much.",0.122,0.122,positive
thrift,2227,summary,Thrift compiler generates spurious warnings with Xlint,code_debt,low_quality_code,"Mon, 14 Oct 2013 20:18:46 +0000","Tue, 19 Nov 2013 17:06:16 +0000","Mon, 11 Nov 2013 21:45:22 +0000",2424396,Thrift compiler generates spurious warnings with Xlint,-0.6335,-0.6335,negative
thrift,2246,comment_3,Some edge cases still produce warnings.,code_debt,low_quality_code,"Tue, 29 Oct 2013 10:12:32 +0000","Mon, 11 Nov 2013 20:41:33 +0000","Mon, 4 Nov 2013 23:52:38 +0000",567606,Some edge cases still produce warnings.,-0.1595,-0.1595,neutral
thrift,2246,description,"if enum is not set, ToString() returns a first enum value, but it should indicate that field is empty enum Distance { DISTANCE_1 = 0, DISTANCE_2 = 1, } struct RaceDetails { 1: optional Distance distance, } c#: new result: expected: )""",code_debt,low_quality_code,"Tue, 29 Oct 2013 10:12:32 +0000","Mon, 11 Nov 2013 20:41:33 +0000","Mon, 4 Nov 2013 23:52:38 +0000",567606,"if enum is not set, ToString() returns a first enum value, but it should indicate that field is empty enum Distance { DISTANCE_1 = 0, DISTANCE_2 = 1, } struct RaceDetails { 1: optional Distance distance, } c#: new RaceDetails().ToString() result: ""RaceDetails(Distance:DISTANCE_1)"" expected: ""RaceDetails(Distance: )""",-0.2,0.2,neutral
thrift,2255,comment_3,"Hello, I can give an update on this patch: This version does not work with exceptions. Additionally I think to remove the copyTo implementation. It was in the patch since the beginning, but up to now I haven't used the copy methods, so it does not seem to be usefull and should be removed.(I will update the patch)",code_debt,dead_code,"Thu, 7 Nov 2013 02:08:12 +0000","Thu, 10 Jul 2014 13:42:32 +0000","Thu, 10 Jul 2014 13:42:32 +0000",21209660,"Hello, I can give an update on this patch: https://issues.apache.org/jira/browse/THRIFT-1712 This version does not work with exceptions. Additionally I think to remove the copyTo implementation. It was in the patch since the beginning, but up to now I haven't used the copy methods, so it does not seem to be usefull and should be removed.(I will update the patch)",-0.15,-0.15,negative
thrift,2285,comment_6,"I'm re-opening this issue to get some attention. Please close it again, if it's not important enough or I'm breaking some workflow with this. The change done to PHP's library doesn't do anything: the variable {{$ESCAPE_CHARS}} is not used anywhere. Instead json_encode and json_decode built-in functions are used. Now JAVA can understand PHP and this is good enough for now, but ideally, the JSON should be the same whatever language's library has generated it.",code_debt,dead_code,"Tue, 10 Dec 2013 19:21:01 +0000","Sun, 23 Feb 2014 18:46:04 +0000","Sun, 23 Feb 2014 18:46:04 +0000",6477903,"I'm re-opening this issue to get some attention. Please close it again, if it's not important enough or I'm breaking some workflow with this. The change done to PHP's library doesn't do anything: the variable $ESCAPE_CHARS is not used anywhere. Instead json_encode and json_decode built-in functions are used. Now JAVA can understand PHP and this is good enough for now, but ideally, the JSON should be the same whatever language's library has generated it.",0.2933333333,0.2933333333,negative
thrift,2285,comment_7,"Hi , feel free to submit a patch cleaning up the code. I have only modified things that already existed. If the stuff is not used at all, it should be removed. And it should probably be a new ticket.",code_debt,dead_code,"Tue, 10 Dec 2013 19:21:01 +0000","Sun, 23 Feb 2014 18:46:04 +0000","Sun, 23 Feb 2014 18:46:04 +0000",6477903,"Hi lcf, feel free to submit a patch cleaning up the code. I have only modified things that already existed. If the stuff is not used at all, it should be removed. And it should probably be a new ticket.",0.05,0.05,neutral
thrift,2293,comment_4,"Hi, Thanks community for finding the fix for this. While testing and reviewing the patch, we noticed the createSSLContext() allows for the condition where both the keystore and truststore is set: if && { null); } For the case above where both the keystore and truststore parameters are set, the truststore file input stream should still leak (depend on GC) since it was not implicitly closed before the reference ""fin"" is reused for the keystore. We got around this by allocating a different file input stream reference for the keystore and truststore and closed both in the finally block. Best Regards, T.",code_debt,low_quality_code,"Fri, 20 Dec 2013 12:46:55 +0000","Fri, 11 Apr 2014 15:19:01 +0000","Thu, 26 Dec 2013 14:39:54 +0000",525179,"Hi, Thanks community for finding the fix for this. While testing and reviewing the patch, we noticed the createSSLContext() allows for the condition where both the keystore and truststore is set: if (params.isKeyStoreSet && params.isTrustStoreSet) { ctx.init(kmf.getKeyManagers(), tmf.getTrustManagers(), null); } For the case above where both the keystore and truststore parameters are set, the truststore file input stream should still leak (depend on GC) since it was not implicitly closed before the reference ""fin"" is reused for the keystore. We got around this by allocating a different file input stream reference for the keystore and truststore and closed both in the finally block. Best Regards, T.",0.278875,0.1239444444,neutral
thrift,2344,comment_1,"+1 ""--compiler-only"" is perhaps more readable? but it doesn't matter really Cheers!",code_debt,low_quality_code,"Mon, 3 Feb 2014 21:48:28 +0000","Sat, 22 Feb 2014 00:45:17 +0000","Sun, 9 Feb 2014 20:18:31 +0000",513003,"+1 ""--compiler-only"" is perhaps more readable? but it doesn't matter really Cheers!",-0.15,-0.15,positive
thrift,2344,comment_3,"should be --with-libs to match current convention, otherwise +1",code_debt,low_quality_code,"Mon, 3 Feb 2014 21:48:28 +0000","Sat, 22 Feb 2014 00:45:17 +0000","Sun, 9 Feb 2014 20:18:31 +0000",513003,"should be --with-libs to match current convention, otherwise +1",0,0,neutral
thrift,2375,comment_3,Hi Jens Thanks so much! Here is one more testcase where {{<a>}} tags are not processed correctly,code_debt,low_quality_code,"Sun, 23 Feb 2014 02:53:28 +0000","Tue, 1 Apr 2014 20:01:32 +0000","Tue, 11 Mar 2014 20:33:57 +0000",1446029,Hi Jens Thanks so much! Here is one more testcase where <a> tags are not processed correctly,-0.05,-0.05,negative
thrift,2404,summary,emit warning on (typically inefficient) list<byte>,code_debt,low_quality_code,"Fri, 14 Mar 2014 21:42:30 +0000","Tue, 1 Apr 2014 20:01:29 +0000","Sun, 16 Mar 2014 14:49:06 +0000",147996,emit warning on (typically inefficient) list<byte>,-0.675,-0.675,neutral
thrift,2405,comment_0,"This patch repairs the Node.js Multiplex server implementation. The full node.js make check now passes. As far as I can tell from walking the code history, the client side Multiplex code never worked for multiple servers. The node.js code base dumps several pages of jsHint warnings right now. The three files attached here are now jsHint clean, but there's still more to do. It would be great if we could require new JavaScript code to jsHint clean and include working tests prior to commit.",code_debt,low_quality_code,"Sun, 16 Mar 2014 08:53:57 +0000","Sun, 6 Jul 2014 04:01:01 +0000","Tue, 22 Apr 2014 13:44:51 +0000",3214254,"This patch repairs the Node.js Multiplex server implementation. The full node.js make check now passes. As far as I can tell from walking the code history, the client side Multiplex code never worked for multiple servers. The node.js code base dumps several pages of jsHint warnings right now. The three files attached here are now jsHint clean, but there's still more to do. It would be great if we could require new JavaScript code to jsHint clean and include working tests prior to commit.",0.07772222222,0.07772222222,neutral
thrift,241,comment_0,Sorry about the messed up formatting. Clean patch attached.,code_debt,low_quality_code,"Wed, 24 Dec 2008 16:53:54 +0000","Thu, 15 Jan 2009 22:15:34 +0000","Mon, 5 Jan 2009 20:19:05 +0000",1049111,Sorry about the messed up formatting. Clean patch attached.,-0.025,-0.025,negative
thrift,241,description,"Having the repr return the repr of __dict__ is confusing, because the object is not a dict and does not act (much) like one. 100% of the 3 python developers I have seen who are new to thrift were confused at first why the repr looked like a dict but obj[attributename] raised KeyError. Additionally, this violates the repr guideline that where possible eval(repr(obj)) == obj. Finally, specifying a __str__ equal to __repr__ is redundant, since str() will use __repr__ if no __str__ is given. Here is a patch: $ diff -u compiler/cpp/sr\  2008-12-24 16:36:54.000000000 +0000 +++ 2008-12-24 16:49:54.000000000 +0000 @@ -614,11 +614,10 @@ // Printing utilities so that on the command line thrift // structs look pretty like dictionaries out << - indent() << ""def __str__(self):"" << endl << - indent() << "" return str(self.__dict__)"" << endl << - endl << indent() << ""def __repr__(self):"" << endl << - indent() << "" return << endl << + indent() << "" L = ['%s=%r' % (key, value)"" << endl << + indent() << "" for key, value in << endl << + indent() << "" return '%s(%s)' % ', '.join(L))"" << endl << endl; // Equality and inequality methods that compare by value",code_debt,low_quality_code,"Wed, 24 Dec 2008 16:53:54 +0000","Thu, 15 Jan 2009 22:15:34 +0000","Mon, 5 Jan 2009 20:19:05 +0000",1049111,"Having the repr return the repr of _dict_ is confusing, because the object is not a dict and does not act (much) like one. 100% of the 3 python developers I have seen who are new to thrift were confused at first why the repr looked like a dict but obj[attributename] raised KeyError. Additionally, this violates the repr guideline that where possible eval(repr(obj)) == obj. Finally, specifying a _str_ equal to _repr_ is redundant, since str() will use _repr_ if no _str_ is given. Here is a patch: $ diff -u compiler/cpp/src/generate/t_py_generator.cc.old compiler/cpp/sr\ c/generate/t_py_generator.cc  compiler/cpp/src/generate/t_py_generator.cc.old 2008-12-24 16:36:54.000000000 +0000 +++ compiler/cpp/src/generate/t_py_generator.cc 2008-12-24 16:49:54.000000000 +0000 @@ -614,11 +614,10 @@ // Printing utilities so that on the command line thrift // structs look pretty like dictionaries out << indent() << ""def _str_(self):"" << endl << indent() << "" return str(self._dict_)"" << endl << endl << indent() << ""def _repr_(self):"" << endl << indent() << "" return repr(self._dict_)"" << endl << + indent() << "" L = ['%s=%r' % (key, value)"" << endl << + indent() << "" for key, value in self._dict_.iteritems()]"" << endl << + indent() << "" return '%s(%s)' % (self._class.name_, ', '.join(L))"" << endl << endl; // Equality and inequality methods that compare by value",-0.1427857143,-0.06108333333,negative
thrift,241,summary,Python __repr__ is confusing and does not eval to the object in question,code_debt,low_quality_code,"Wed, 24 Dec 2008 16:53:54 +0000","Thu, 15 Jan 2009 22:15:34 +0000","Mon, 5 Jan 2009 20:19:05 +0000",1049111,Python __repr__ is confusing and does not eval to the object in question,0.0625,0.0625,negative
thrift,2435,comment_1,"Proposed The idea is, to reference the enum types at these places always via full namespace to prevent ambiguities.",code_debt,low_quality_code,"Mon, 31 Mar 2014 08:43:58 +0000","Sat, 31 May 2014 21:09:58 +0000","Wed, 2 Apr 2014 21:22:58 +0000",218340,"Proposed patch. The idea is, to reference the enum types at these places always via full namespace to prevent ambiguities.",-0.2,-0.1,neutral
thrift,2511,comment_0,This patch adds the compact protocol to the Node.js lib. It includes various ThriftTest tests in This patch also repairs a map of maps bug in the node.js test driver and repairs all jshint warnings in There are however still many (valid) jshint warnings in thrift compiler generated node.js code. Will create a new issue for that.,code_debt,low_quality_code,"Sun, 4 May 2014 08:16:17 +0000","Sun, 6 Jul 2014 04:00:56 +0000","Sun, 11 May 2014 07:27:33 +0000",601876,This patch adds the compact protocol to the Node.js lib. It includes various ThriftTest tests in thirft/lib/nodejs/test/testAll.sh. This patch also repairs a map of maps bug in the node.js test driver and repairs all jshint warnings in thrift/lib/node.js/lib. There are however still many (valid) jshint warnings in thrift compiler generated node.js code. Will create a new issue for that.,-0.05105555556,-0.07595,neutral
thrift,255,comment_2,1) const std::string& for path 2) does specifying default params in just the .cpp file really work? seems like the compiler would have trouble understanding that. 3) Document using O_APPEND. Maybe allow overwriting as another option,code_debt,low_quality_code,"Sat, 10 Jan 2009 01:07:06 +0000","Tue, 1 Nov 2011 02:52:03 +0000","Thu, 26 Mar 2009 06:23:50 +0000",6499004,1) const std::string& for path 2) does specifying default params in just the .cpp file really work? seems like the compiler would have trouble understanding that. 3) Document using O_APPEND. Maybe allow overwriting as another option,-0.0524,-0.0524,neutral
thrift,2561,description,"When an enumerated type is translated to objective-c, the type is replaced with simply ""int"". This takes away half the use of an enumerated type as a type. For instance, it is both clearer and a more precise translation to see ""PersonID"" as a field type in a method than ""int"". However, the objective-c compiler does not typedef the enum and as a result essentially forgets the type that is declared in the thrift IDL.",code_debt,low_quality_code,"Fri, 30 May 2014 16:45:06 +0000","Tue, 5 Dec 2017 17:29:58 +0000","Thu, 19 Jan 2017 21:03:14 +0000",83391488,"When an enumerated type is translated to objective-c, the type is replaced with simply ""int"". This takes away half the use of an enumerated type as a type. For instance, it is both clearer and a more precise translation to see ""PersonID"" as a field type in a method than ""int"". However, the objective-c compiler does not typedef the enum and as a result essentially forgets the type that is declared in the thrift IDL.",0.20325,0.20325,neutral
thrift,2599,summary,Uncompileable Delphi code due to naming conflicts with IDL,code_debt,low_quality_code,"Thu, 3 Jul 2014 21:57:20 +0000","Sun, 6 Jul 2014 04:00:54 +0000","Thu, 3 Jul 2014 22:18:23 +0000",1263,Uncompileable Delphi code due to naming conflicts with IDL,-0.4,-0.4,negative
thrift,2605,comment_0,patch attached fixes both TSocket and TServerSocket also - removed mixed line endings in TSocket,code_debt,low_quality_code,"Mon, 7 Jul 2014 14:06:20 +0000","Thu, 10 Jul 2014 13:36:09 +0000","Tue, 8 Jul 2014 05:33:09 +0000",55609,patch attached fixes both TSocket and TServerSocket also - removed mixed line endings in TSocket,0,0,neutral
thrift,2607,comment_0,just removed unused field,code_debt,dead_code,"Mon, 7 Jul 2014 14:22:17 +0000","Thu, 10 Jul 2014 13:36:08 +0000","Mon, 7 Jul 2014 19:56:05 +0000",20028,just removed unused field,0,0,neutral
thrift,2609,comment_0,"field used - this field could be removed, but by using it code is more consistent",code_debt,dead_code,"Mon, 7 Jul 2014 14:32:58 +0000","Thu, 10 Jul 2014 13:36:10 +0000","Mon, 7 Jul 2014 20:03:38 +0000",19840,"field used - this field could be removed, but by using it code is more consistent",0,0,neutral
thrift,2609,summary,TFileTransport.h unused field warning (clang 3.4),code_debt,dead_code,"Mon, 7 Jul 2014 14:32:58 +0000","Thu, 10 Jul 2014 13:36:10 +0000","Mon, 7 Jul 2014 20:03:38 +0000",19840,TFileTransport.h unused field warning (clang 3.4),-0.3,-0.3,neutral
thrift,275,description,"There are a lot of weirdly named files in the Ruby Thrift library in order for backwards compatibility to work. I think this is of dubious value, and is really confusing. The deprecation code is also very complicated. I think we should just get rid of the deprecated stuff. We're pre-version 1.0, so better to get all the big changes in now.",code_debt,dead_code,"Sat, 17 Jan 2009 00:44:02 +0000","Tue, 1 Nov 2011 02:54:12 +0000","Tue, 24 Mar 2009 05:31:09 +0000",5719627,"There are a lot of weirdly named files in the Ruby Thrift library in order for backwards compatibility to work. I think this is of dubious value, and is really confusing. The deprecation code is also very complicated. I think we should just get rid of the backwards-compatibility deprecated stuff. We're pre-version 1.0, so better to get all the big changes in now.",-0.0037,-0.0037,negative
thrift,275,summary,Remove deprecated classes from Ruby library,code_debt,dead_code,"Sat, 17 Jan 2009 00:44:02 +0000","Tue, 1 Nov 2011 02:54:12 +0000","Tue, 24 Mar 2009 05:31:09 +0000",5719627,Remove deprecated classes from Ruby library,0,0,negative
thrift,2768,description,"Of course I had to make some content changes. The C# patch file is 354 KB, a lot of files are considered completely replaced. The Delphi patch file has ""only"" 282 KB. The ""content"" changes are (1) one indentation alignment in a batch file, and (2) one missing ASF header in a .cs file which I added. Anything else is just TABS, SPACEs and CRs. I'm going to commit that. For the records, here are the patch files.",code_debt,low_quality_code,"Fri, 3 Oct 2014 18:07:55 +0000","Sun, 9 Nov 2014 01:36:40 +0000","Thu, 9 Oct 2014 19:18:26 +0000",522631,"Of course I had to make some content changes. The C# patch file is 354 KB, a lot of files are considered completely replaced. The Delphi patch file has ""only"" 282 KB. The ""content"" changes are (1) one indentation alignment in a batch file, and (2) one missing ASF header in a .cs file which I added. Anything else is just TABS, SPACEs and CRs. I'm going to commit that. For the records, here are the patch files.",0.15625,0.15625,neutral
thrift,2768,summary,Whitespace fixups,code_debt,low_quality_code,"Fri, 3 Oct 2014 18:07:55 +0000","Sun, 9 Nov 2014 01:36:40 +0000","Thu, 9 Oct 2014 19:18:26 +0000",522631,Whitespace fixups,0,0,neutral
thrift,277,description,"It's really an abstract method, so not overriding it is an error. We should throw an error when someone calls it inappropriately. The same can probably be said for #write. #open and #open? might also want to change, though that's not quite as certain.",code_debt,low_quality_code,"Sat, 17 Jan 2009 00:48:32 +0000","Tue, 1 Nov 2011 02:53:49 +0000","Wed, 18 Mar 2009 02:41:15 +0000",5190763,"It's really an abstract method, so not overriding it is an error. We should throw an error when someone calls it inappropriately. The same can probably be said for #write. #open and #open? might also want to change, though that's not quite as certain.",-0.0625,-0.0625,negative
thrift,277,summary,Abstract Transport in Ruby #read method should throw,code_debt,low_quality_code,"Sat, 17 Jan 2009 00:48:32 +0000","Tue, 1 Nov 2011 02:53:49 +0000","Wed, 18 Mar 2009 02:41:15 +0000",5190763,Abstract Transport in Ruby #read method should throw NotImplementedException,0,0,neutral
thrift,278,description,"If you have an enum field, and its value isn't in the VALID_VALUES of your enum, the exception thrown says that it's not found, but it doesn't say what the value is.",code_debt,low_quality_code,"Tue, 20 Jan 2009 00:52:35 +0000","Tue, 1 Nov 2011 02:54:22 +0000","Wed, 18 Mar 2009 01:51:07 +0000",4928312,"If you have an enum field, and its value isn't in the VALID_VALUES of your enum, the exception thrown says that it's not found, but it doesn't say what the value is.",0,0,neutral
thrift,278,summary,#validate exceptions should contain the offending value,code_debt,low_quality_code,"Tue, 20 Jan 2009 00:52:35 +0000","Tue, 1 Nov 2011 02:54:22 +0000","Wed, 18 Mar 2009 01:51:07 +0000",4928312,#validate exceptions should contain the offending value,-0.4,-0.4,neutral
thrift,2791,description,"There is currently no way in a go server to use buffered sockets. Failing to do so decreases performance significantly in my tests. I added an option on TServerSocket to set the buffer size to use. This will default to 1024 bytes, but can be disabled if desired to get back to the original behavior by setting BufferSize to 0. Github pull request: Patch",code_debt,slow_algorithm,"Fri, 24 Oct 2014 16:50:39 +0000","Mon, 10 Nov 2014 23:14:30 +0000","Wed, 29 Oct 2014 17:58:42 +0000",436083,"There is currently no way in a go server to use buffered sockets. Failing to do so decreases performance significantly in my tests. I added an option on TServerSocket to set the buffer size to use. This will default to 1024 bytes, but can be disabled if desired to get back to the original behavior by setting BufferSize to 0. Github pull request: https://github.com/apache/thrift/pull/249 Patch https://github.com/apache/thrift/pull/249.patch",-0.19,-0.19,negative
thrift,2851,comment_1,"Ok I see the point now. I stumbled upon it again, because in THRIFT-2502 the Peek function for iostream_transport got commented out. There seem to be no support from GO to provide such a function to most of the transport. IMHO Only barely used memory_buffer could get a real implementation. So the question is if we should carry along the Peek() function even though there will be no real implementation for quiet some time. Currently the implementation doesn't seem to follow a strict line. E.g. the transport interface doesn't carry a Peek method and not all transport implements the Peek function. I vote for remove, as we gain nothing, and appreciate the differences of having different language platforms with different abilities.",code_debt,low_quality_code,"Mon, 24 Nov 2014 18:49:14 +0000","Mon, 8 Dec 2014 20:35:00 +0000","Tue, 25 Nov 2014 20:48:59 +0000",93585,"Ok I see the point now. I stumbled upon it again, because in THRIFT-2502 the Peek function for iostream_transport got commented out. There seem to be no support from GO to provide such a function to most of the transport. IMHO Only barely used memory_buffer could get a real implementation. So the question is if we should carry along the Peek() function even though there will be no real implementation for quiet some time. Currently the implementation doesn't seem to follow a strict line. E.g. the transport interface doesn't carry a Peek method and not all transport implements the Peek function. I vote for remove, as we gain nothing, and appreciate the differences of having different language platforms with different abilities.",0.03275,0.03275,negative
thrift,2851,comment_2,"After sleeping a night about it, I think you are right. In its current form Peek() is not only next to useless, but more important misleading to the less careful observer. So unless someone comes up with a better implementation, we should throw it out. Committed.",code_debt,low_quality_code,"Mon, 24 Nov 2014 18:49:14 +0000","Mon, 8 Dec 2014 20:35:00 +0000","Tue, 25 Nov 2014 20:48:59 +0000",93585,"After sleeping a night about it, I think you are right. In its current form Peek() is not only next to useless, but more important misleading to the less careful observer. So unless someone comes up with a better implementation, we should throw it out. Committed.",0.359875,0.359875,negative
thrift,2851,description,"I've been seeing this public Peek() function in the GO library for a while, but still cannot figure out any sense to it. If there are useless, we can remove them right? This PR removes the public Peek() from the implemented transports. All tests still pass. PR:",code_debt,dead_code,"Mon, 24 Nov 2014 18:49:14 +0000","Mon, 8 Dec 2014 20:35:00 +0000","Tue, 25 Nov 2014 20:48:59 +0000",93585,"I've been seeing this public Peek() function in the GO library for a while, but still cannot figure out any sense to it. If there are useless, we can remove them right? This PR removes the public Peek() from the implemented transports. All tests still pass. PR: https://github.com/apache/thrift/pull/283",0.147,0.147,negative
thrift,2851,summary,Remove strange public Peek() from Go transports,code_debt,low_quality_code,"Mon, 24 Nov 2014 18:49:14 +0000","Mon, 8 Dec 2014 20:35:00 +0000","Tue, 25 Nov 2014 20:48:59 +0000",93585,Remove strange public Peek() from Go transports,0.15,0.15,negative
thrift,2868,description,The Go client doesn't do proper error checking. E.g. it doesn't check whether the received method name is correct nor if the message type has the expected value. The following PR enhances the Go client error handling by the following: - Check if method name is correct - - Check if MessageType is thrift.REPLY or EXCEPTION - - Checking the sequence id is done before checking the message type Includes test cases for every error case.,code_debt,low_quality_code,"Mon, 1 Dec 2014 20:32:30 +0000","Mon, 29 Dec 2014 21:36:34 +0000","Mon, 8 Dec 2014 20:52:23 +0000",605993,The Go client doesn't do proper error checking. E.g. it doesn't check whether the received method name is correct nor if the message type has the expected value. The following PR enhances the Go client error handling by the following: Check if method name is correct -> if not return thrift.WRONG_METHOD_NAME Check if MessageType is thrift.REPLY or EXCEPTION -> if not return thrift.INVALID_MESSAGE_TYPE_EXCEPTION Checking the sequence id is done before checking the message type Includes test cases for every error case. https://github.com/apache/thrift/pull/297,-0.20675,-0.1538571429,negative
thrift,2874,description,string_buf_ and string_buf_size_ are never used. Removing these will also resolve THRIFT-2465.,code_debt,dead_code,"Wed, 3 Dec 2014 17:21:17 +0000","Mon, 26 Jan 2015 02:26:32 +0000","Thu, 4 Dec 2014 21:47:24 +0000",102367,string_buf_ and string_buf_size_ are never used. Removing these will also resolve THRIFT-2465.,0.2,0.2,neutral
thrift,2874,summary,"TBinaryProtocol member variable ""string_buf_"" is never used.",code_debt,dead_code,"Wed, 3 Dec 2014 17:21:17 +0000","Mon, 26 Jan 2015 02:26:32 +0000","Thu, 4 Dec 2014 21:47:24 +0000",102367,"TBinaryProtocol  member variable ""string_buf_"" is never used.",0,0,negative
thrift,2907,description,The {{ntohll}} macro is already defined in on Mac OS X.,code_debt,low_quality_code,"Fri, 19 Dec 2014 23:32:04 +0000","Mon, 26 Jan 2015 02:26:40 +0000","Sat, 20 Dec 2014 12:31:19 +0000",46755,The ntohll macro is already defined in /usr/include/sys/_endian.h on Mac OS X.,0.5,0.25,neutral
thrift,2907,summary,ntohll' macro redefined,code_debt,low_quality_code,"Fri, 19 Dec 2014 23:32:04 +0000","Mon, 26 Jan 2015 02:26:40 +0000","Sat, 20 Dec 2014 12:31:19 +0000",46755,ntohll' macro redefined,0.5,0.5,neutral
thrift,2969,comment_0,"These patches together represent a major refactoring of the nodejs library testing code, eliminating most of the duplication. The testAll.sh script now tests every possible implementation. There is room for parallelizing these tests for speed, but that can be the subject of another patch. I'm trying to move as fast as possible to improve the nodejs code. The sooner this and my other patches can be reviewed the faster I can contribute more patches before we hit 1.0.0.",code_debt,duplicated_code,"Sat, 31 Jan 2015 01:30:54 +0000","Tue, 21 Jul 2015 02:21:07 +0000","Mon, 16 Feb 2015 08:54:10 +0000",1408996,"These patches together represent a major refactoring of the nodejs library testing code, eliminating most of the duplication. The testAll.sh script now tests every possible implementation. There is room for parallelizing these tests for speed, but that can be the subject of another patch. I'm trying to move as fast as possible to improve the nodejs code. The sooner this and my other patches can be reviewed the faster I can contribute more patches before we hit 1.0.0.",-0.008333333333,-0.008333333333,positive
thrift,2969,comment_5,"Hey Andrew, Great patch. Big improvement. I had to remove a spurious require xtend from server.js but otherwise as submitted. -Randy",code_debt,low_quality_code,"Sat, 31 Jan 2015 01:30:54 +0000","Tue, 21 Jul 2015 02:21:07 +0000","Mon, 16 Feb 2015 08:54:10 +0000",1408996,"Hey Andrew, Great patch. Big improvement. I had to remove a spurious require xtend from server.js but otherwise as submitted. -Randy",0.23225,0.23225,positive
thrift,2972,description,It seems that I failed to escape a line ending in THRIFT-2910 fix. For some reason it doesn't break {{make check}} right now but only causes a bootstrap (or configure ?) warning and processor_test executable is missing because of this.,code_debt,low_quality_code,"Sat, 31 Jan 2015 16:21:16 +0000","Tue, 21 Jul 2015 02:21:08 +0000","Mon, 2 Feb 2015 21:05:26 +0000",189850,It seems that I failed to escape a line ending in THRIFT-2910 fix. For some reason it doesn't break make check right now but only causes a bootstrap (or configure ?) warning and processor_test executable is missing because of this.,-0.2506666667,-0.2506666667,negative
thrift,2972,summary,Missing backslash in,code_debt,low_quality_code,"Sat, 31 Jan 2015 16:21:16 +0000","Tue, 21 Jul 2015 02:21:08 +0000","Mon, 2 Feb 2015 21:05:26 +0000",189850,Missing backslash in lib/cpp/test/Makefile.am,-0.4,-0.2,neutral
thrift,298,comment_3,"Well, if you can believe it, I don't get what's going on here, either :). I think this code is a little too complicated. For now, I think I'll just apply your patch, and hopefully one day it won't be a problem, as I refactor my way through the Ruby libraries.",code_debt,complex_code,"Sat, 31 Jan 2009 09:36:34 +0000","Tue, 3 Feb 2009 01:17:25 +0000","Tue, 3 Feb 2009 00:32:30 +0000",226556,"Well, if you can believe it, I don't get what's going on here, either . I think this code is a little too complicated. For now, I think I'll just apply your patch, and hopefully one day it won't be a problem, as I refactor my way through the Ruby libraries.",0.3051666667,0.3436666667,negative
thrift,3040,comment_1,"Committed, thanks for the patch! Note, our Bower solution is a mess (cloning the entire thrift repo) and may change in the future. Also the src dir thrift.js may end up broken into multiple files, with thrift.js only being available in lib/js/dist after a grunt build. In the mean time this patch is a needed fix.",code_debt,complex_code,"Sun, 15 Mar 2015 11:27:22 +0000","Tue, 21 Jul 2015 02:21:18 +0000","Sun, 15 Mar 2015 15:41:09 +0000",15227,"Committed, thanks for the patch! Note, our Bower solution is a mess (cloning the entire thrift repo) and may change in the future. Also the src dir thrift.js may end up broken into multiple files, with thrift.js only being available in lib/js/dist after a grunt build. In the mean time this patch is a needed fix.",0.025,0.025,negative
thrift,3045,description,Java generated code is adding two suppress warnings as follows: The java_suppressions() seems to be added in 0.9.2 which is right. But there is a default getting added. One of it needs to be turned off.,code_debt,low_quality_code,"Wed, 18 Mar 2015 05:52:07 +0000","Fri, 5 Jun 2015 19:54:44 +0000","Fri, 5 Jun 2015 19:54:44 +0000",6876157,"Java generated code is adding two suppress warnings as follows: The java_suppressions() seems to be added in 0.9.2 which is right. But there is a default SuppressWarnings(""all"") getting added. One of it needs to be turned off.",-0.5121666667,-0.5121666667,negative
thrift,3047,description,"indent_down was called one extra time which gave us an indentation level of -1, then to combat this, we indented the server implementation an extra level. This appeared as if everything was fine unless you generated two services in one thrift file, in which case the indentation would get progressively worse.",code_debt,low_quality_code,"Thu, 19 Mar 2015 05:03:56 +0000","Thu, 30 Apr 2015 18:57:11 +0000","Thu, 16 Apr 2015 20:15:46 +0000",2473910,"indent_down was called one extra time which gave us an indentation level of -1, then to combat this, we indented the server implementation an extra level. This appeared as if everything was fine unless you generated two services in one thrift file, in which case the indentation would get progressively worse.",-0.27925,-0.27925,negative
thrift,3088,description,"Start TThreadPoolServer to server with as transportFactory. While using nc to test the specified port whether reachable, it will leak CLOSE_WAIT socket.That's because nc will close socket at once while successful connect TThreadPoolServer, but the server still try using sasl protocol to build an inputTransport which of course failed at once. However inputTransport is null which makes it can't close socket properly which lead to CLOSE_WAIT socket.",code_debt,low_quality_code,"Thu, 9 Apr 2015 07:18:24 +0000","Wed, 5 Apr 2017 17:17:46 +0000","Sun, 12 Apr 2015 15:47:31 +0000",289747,"Start TThreadPoolServer to server with TSaslServerTransport.Factory as transportFactory. While using nc to test the specified port whether reachable, it will leak CLOSE_WAIT socket.That's because nc will close socket at once while successful connect TThreadPoolServer, but the server still try using sasl protocol to build an inputTransport which of course failed at once. However inputTransport is null which makes it can't close socket properly which lead to CLOSE_WAIT socket.",0.09008333333,0.07206666667,neutral
thrift,3088,summary,TThreadPoolServer with Sasl auth may leak CLOSE_WAIT socket,code_debt,low_quality_code,"Thu, 9 Apr 2015 07:18:24 +0000","Wed, 5 Apr 2017 17:17:46 +0000","Sun, 12 Apr 2015 15:47:31 +0000",289747,TThreadPoolServer with Sasl auth may leak CLOSE_WAIT socket,-0.2,-0.2,neutral
thrift,3114,description,"Should prefix the field deserialization statements with ""local"" whenever the output is a temporary variable, for example this compiler output: _elem46 = iprot:readI32() should be changed to: local _elem46 = iprot:readI32().",code_debt,low_quality_code,"Sat, 25 Apr 2015 19:10:46 +0000","Sat, 20 Feb 2021 15:26:47 +0000","Sun, 26 Apr 2015 15:49:16 +0000",74310,"Should prefix the field deserialization statements with ""local"" whenever the output is a temporary variable, for example this compiler output: _elem46 = iprot:readI32() should be changed to: local _elem46 = iprot:readI32().",0,0,neutral
thrift,3114,summary,Using local temp variables to not pollute the global table,code_debt,low_quality_code,"Sat, 25 Apr 2015 19:10:46 +0000","Sat, 20 Feb 2021 15:26:47 +0000","Sun, 26 Apr 2015 15:49:16 +0000",74310,Using local temp variables to not pollute the global table,0.4,0.4,neutral
thrift,3140,description,"After running *make check* for JS or *ant test* in {{lib/js/test}}, you can always find several stack traces in I don't know if it's even a bug since it does not cause any test failures. It distracted me from solving real cause of failures for a while through. Cause: Threading issue. Synchronization is needed because all instances of MimeUtil2 seems to share single state.",code_debt,multi-thread_correctness,"Sun, 10 May 2015 15:01:09 +0000","Tue, 21 Jul 2015 02:21:11 +0000","Sun, 10 May 2015 16:31:11 +0000",5402,"After running make check for JS or ant test in lib/js/test, you can always find several ConcurrentModificationException stack traces in lib/js/test/build/log/unittest.log. I don't know if it's even a bug since it does not cause any test failures. It distracted me from solving real cause of failures for a while through. Cause: Threading issue. Synchronization is needed because all instances of MimeUtil2 seems to share single state. http://sourceforge.net/p/mime-util/bugs/33/",0.1,0.05714285714,negative
thrift,3197,description,"While creating ThreadPoolExecutor in TThreadPoolServer, keepAliveTime is hard coded as 60 sec. It should be",code_debt,low_quality_code,"Mon, 22 Jun 2015 11:14:41 +0000","Wed, 24 Jun 2015 14:14:41 +0000","Wed, 24 Jun 2015 13:21:16 +0000",180395,"While creating ThreadPoolExecutor in TThreadPoolServer, keepAliveTime is hard coded as 60 sec. It should be ""args.stopTimeoutVal""",-0.1,-0.06666666667,neutral
thrift,3197,summary,keepAliveTime is hard coded as 60 sec in TThreadPoolServer,code_debt,low_quality_code,"Mon, 22 Jun 2015 11:14:41 +0000","Wed, 24 Jun 2015 14:14:41 +0000","Wed, 24 Jun 2015 13:21:16 +0000",180395,keepAliveTime is hard coded as 60 sec in TThreadPoolServer,-0.2,-0.2,neutral
thrift,3226,summary,Fix TNamedPipeServer trapped in loop on accept,code_debt,low_quality_code,"Tue, 7 Jul 2015 12:39:56 +0000","Thu, 9 Jul 2015 07:18:53 +0000","Thu, 9 Jul 2015 07:18:53 +0000",153537,Fix TNamedPipeServer trapped in loop on accept,-0.1,-0.1,negative
thrift,3283,description,"When terminating the C (GLib) tutorial server with Ctrl-C, this message is output to the console: The server should instead exit quietly without reporting an issue.",code_debt,low_quality_code,"Fri, 31 Jul 2015 11:48:46 +0000","Tue, 25 Aug 2015 04:44:51 +0000","Fri, 31 Jul 2015 21:26:09 +0000",34643,"When terminating the C (GLib) tutorial server with Ctrl-C, this message is output to the console: The server should instead exit quietly without reporting an issue.",0.375,0.375,neutral
thrift,3283,summary,c_glib: Tutorial server always exits with warning,code_debt,low_quality_code,"Fri, 31 Jul 2015 11:48:46 +0000","Tue, 25 Aug 2015 04:44:51 +0000","Fri, 31 Jul 2015 21:26:09 +0000",34643,c_glib: Tutorial server always exits with warning,-0.6,-0.6,negative
thrift,3364,description,"Ruby JSON protocol uses pack('m') method to encode Base64 string. It seems that it inserts a ""\n"" character every 60 characters. You can refer to these pages for this behavior. This has been making it impossible to send long binary field data to other languages. I fixed this by using alternative encode method that is added in Ruby 1.9 (which should be OK). After the fix, I had to add Ruby namespace to to avoid name collision of ""Base64"" symbols that is used for new encode method and also as DebugProtoTest message name. I also removed extraneous double quote in encoded binary fields that resulted in invalid JSON.",code_debt,low_quality_code,"Thu, 1 Oct 2015 16:06:11 +0000","Fri, 18 Mar 2016 18:02:06 +0000","Fri, 18 Mar 2016 18:02:06 +0000",14608555,"Ruby JSON protocol uses pack('m') method to encode Base64 string. It seems that it inserts a ""\n"" character every 60 characters. You can refer to these pages for this behavior. http://stackoverflow.com/questions/2620975/strange-n-in-base64-encoded-string-in-ruby http://ruby-doc.org/stdlib-2.2.3/libdoc/base64/rdoc/Base64.html Line feeds are added to every 60 encoded characters. This has been making it impossible to send long binary field data to other languages. I fixed this by using alternative encode method that is added in Ruby 1.9 (which should be OK). After the fix, I had to add Ruby namespace to DebugProtoTest.thrift to avoid name collision of ""Base64"" symbols that is used for new encode method and also as DebugProtoTest message name. I also removed extraneous double quote in encoded binary fields that resulted in invalid JSON.",0.06428571429,0.05,negative
thrift,3391,description,The go test server printf()s bools with wrong formatting: while this is expected:,code_debt,low_quality_code,"Fri, 16 Oct 2015 19:32:55 +0000","Fri, 18 Mar 2016 18:02:04 +0000","Fri, 18 Mar 2016 18:02:04 +0000",13300149,The go test server printf()s bools with wrong formatting: while this is expected:,0.125,0.125,negative
thrift,3391,summary,Wrong bool formatting in test server,code_debt,low_quality_code,"Fri, 16 Oct 2015 19:32:55 +0000","Fri, 18 Mar 2016 18:02:04 +0000","Fri, 18 Mar 2016 18:02:04 +0000",13300149,Wrong bool formatting in test server,-0.25,-0.25,negative
thrift,3409,description,There's at least 3 problems in NodeJS binary field. # API are inconsistent across binary/comact/JSON protocols # compact/JSON wire format is imcompatible with other languages (JSON : THRIFT-3200) # size of compact wire format is 2x of the original binary I propose following changes to fix this # Change JSON protocol return value from string to Buffer (same as binary/compact) # Change JSON protocol wire format to Base64 (same as other languages) # Change compact protocol wire format to plain binary (same as other languages),code_debt,low_quality_code,"Tue, 3 Nov 2015 16:08:13 +0000","Tue, 18 Dec 2018 16:43:00 +0000","Fri, 18 Mar 2016 17:54:32 +0000",11756779,There's at least 3 problems in NodeJS binary field. API are inconsistent across binary/comact/JSON protocols compact/JSON wire format is imcompatible with other languages (JSON : THRIFT-3200) size of compact wire format is 2x of the original binary I propose following changes to fix this Change JSON protocol return value from string to Buffer (same as binary/compact) Change JSON protocol wire format to Base64 (same as other languages) Change compact protocol wire format to plain binary (same as other languages),-0.575,-0.575,negative
thrift,3415,comment_1,I'll do some scans with clang's iwyu and clean a little bit around in THeader* related includes,code_debt,low_quality_code,"Wed, 11 Nov 2015 16:04:45 +0000","Wed, 6 Jan 2016 02:06:10 +0000","Thu, 12 Nov 2015 15:39:03 +0000",84858,I'll do some scans with clang's iwyu (include-what-you-use) and clean a little bit around in THeader* related includes,0.4,0.4,neutral
thrift,3416,description,"While poking into the IDL syntax, I found that hidden gem: While thinking whether I should do sth about it, I came to the conclusion that the time ist right to ""_Get rid of this_"" since today probably really ""_everyone is using the new hotness_"", as the comments say. Opinions? If there are no objections, I'll provide a patch to convert the warnings into an error and remove the rest of it.",code_debt,low_quality_code,"Wed, 11 Nov 2015 20:42:20 +0000","Fri, 18 Mar 2016 17:54:27 +0000","Fri, 18 Mar 2016 17:54:27 +0000",11049127,"While poking into the IDL syntax, I found that hidden gem: While thinking whether I should do sth about it, I came to the conclusion that the time ist right to ""Get rid of this"" since today probably really ""everyone is using the new hotness"", as the comments say. Opinions? If there are no objections, I'll provide a patch to convert the warnings into an error and remove the rest of it.",0.01411111111,0.01411111111,neutral
thrift,3440,summary,Python make check takes too much time,code_debt,slow_algorithm,"Mon, 23 Nov 2015 10:00:28 +0000","Wed, 6 Jan 2016 02:06:12 +0000","Mon, 23 Nov 2015 14:54:38 +0000",17650,Python make check takes too much time,-0.5,-0.5,negative
thrift,3495,description,"h4. fixes * fix python mapmap test * fix problematic regex compilation in connection retry logic in cross test runner (was compiling already-compiled regex object) h4. enhancement * add some C++ edage-case tests (unicode, double values, empty collections) * flush C++ client stdout so that we have better diagnostics when it crashed/hanged * add go binary test * add ruby unicode string test",code_debt,low_quality_code,"Sun, 20 Dec 2015 12:52:23 +0000","Wed, 6 Jan 2016 02:06:10 +0000","Wed, 23 Dec 2015 17:48:48 +0000",276985,"fixes fix python mapmap test fix problematic regex compilation in connection retry logic in cross test runner (was compiling already-compiled regex object) enhancement add some C++ edage-case tests (unicode, double values, empty collections) flush C++ client stdout so that we have better diagnostics when it crashed/hanged add go binary test add ruby unicode string test",-0.079,0.1476,neutral
thrift,353,description,"service generator needs to capitalize service names, since services are ruby modules and ruby modules are constants.",code_debt,low_quality_code,"Wed, 4 Mar 2009 19:14:17 +0000","Thu, 5 Mar 2009 00:42:52 +0000","Wed, 4 Mar 2009 21:34:23 +0000",8406,"service generator needs to capitalize service names, since services are ruby modules and ruby modules are constants.",-0.5,-0.5,neutral
thrift,353,summary,"Service generation needs to make service name capitalized, since Ruby modules need to be constant.",code_debt,low_quality_code,"Wed, 4 Mar 2009 19:14:17 +0000","Thu, 5 Mar 2009 00:42:52 +0000","Wed, 4 Mar 2009 21:34:23 +0000",8406,"Service generation needs to make service name capitalized, since Ruby modules need to be constant.",0,0,neutral
thrift,3559,description,Changes made in THRIFT-3545 introduce and awkward semi-colons and extra new lines.,code_debt,low_quality_code,"Sun, 17 Jan 2016 18:50:16 +0000","Fri, 22 Apr 2016 19:50:59 +0000","Sat, 16 Apr 2016 21:55:56 +0000",7787140,Changes made in THRIFT-3545 introduce and awkward semi-colons and extra new lines.,-0.271,-0.271,neutral
thrift,3559,summary,Fix awkward extra semi-colons with Cocoa container literals,code_debt,low_quality_code,"Sun, 17 Jan 2016 18:50:16 +0000","Fri, 22 Apr 2016 19:50:59 +0000","Sat, 16 Apr 2016 21:55:56 +0000",7787140,Fix awkward extra semi-colons with Cocoa container literals,-0.271,-0.271,neutral
thrift,3572,comment_0,"I have a fix, though I think there must be a simpler one. In any case, it does amend the build; I'll make a pull request after testing with THRIFT-3573 to make sure nothing new pops up.",code_debt,complex_code,"Wed, 20 Jan 2016 20:42:02 +0000","Sat, 20 Feb 2021 15:27:01 +0000","Sat, 23 Jan 2016 16:28:45 +0000",244003,"I have a fix, though I think there must be a simpler one. In any case, it does amend the build; I'll make a pull request after testing with THRIFT-3573 to make sure nothing new pops up.",0.25,0.25,neutral
thrift,3596,description,"py coding_standards.md states it follows PEP8 but currently it does not do so at all. So a typical experience of a first-time (potential) py contributor would be to see red warnings all over the display and then has to either adjust their editor's settings or stop there. On the other hand, a huge downside of global reformat is git-blame experience but will be mostly mitigated by git blame -w in this case.",code_debt,low_quality_code,"Tue, 2 Feb 2016 16:55:43 +0000","Mon, 28 Jan 2019 03:17:53 +0000","Thu, 4 Feb 2016 05:31:43 +0000",131760,"py coding_standards.md states it follows PEP8 but currently it does not do so at all. So a typical experience of a first-time (potential) py contributor would be to see red warnings all over the display and then has to either adjust their editor's settings or stop there. On the other hand, a huge downside of global reformat is git-blame experience but will be mostly mitigated by git blame -w in this case.",-0.2053666667,-0.2053666667,negative
thrift,3596,summary,Better conformance to PEP8,code_debt,low_quality_code,"Tue, 2 Feb 2016 16:55:43 +0000","Mon, 28 Jan 2019 03:17:53 +0000","Thu, 4 Feb 2016 05:31:43 +0000",131760,Better conformance to PEP8,0.5,0.5,positive
thrift,3605,comment_2,"A few comments: - A few generators did not document all switches, some did not dcoument any switches at all. I fixed what I found. - The Python generator stood out form the crowd by being the least maintainable one. I tried hard to no break anything and to mimic the existing behaviour to the best of my knowledge, but it was a hard task. If, despite of my efforts, I still managed to overlooked some obscure edge case combination, please bear with me.",code_debt,complex_code,"Mon, 8 Feb 2016 21:53:44 +0000","Fri, 19 Feb 2016 02:17:59 +0000","Sun, 14 Feb 2016 10:11:28 +0000",476264,"A few comments: A few generators did not document all switches, some did not dcoument any switches at all. I fixed what I found. The Python generator stood out form the crowd by being the least maintainable one. I tried hard to no break anything and to mimic the existing behaviour to the best of my knowledge, but it was a hard task. If, despite of my efforts, I still managed to overlooked some obscure edge case combination, please bear with me.",-0.1355625,-0.10845,negative
thrift,3617,comment_1,"Does not have much to do with the ticket itself, but is there any specific reason why we should not sort that list alphabetically? Makes it much easier to compare things (like forgotten targets). $0,02 JensG",code_debt,low_quality_code,"Fri, 12 Feb 2016 15:35:40 +0000","Fri, 19 Feb 2016 02:17:39 +0000","Sat, 13 Feb 2016 14:26:49 +0000",82269,"Does not have much to do with the ticket itself, but is there any specific reason why we should not sort that list alphabetically? Makes it much easier to compare things (like forgotten targets). $0,02 JensG",-0.06666666667,-0.06666666667,neutral
thrift,3619,comment_3,"This workaround will break gtest on OSX with clang libc++. Using C\+\+11 should have already implied but manually setting this macro to {{0}} will just make gtest skip checking C\+\+11 and include {{tr1/tuple}} rather than {{tuple}}, which seems like a bug in gtest, as I see it. See {{tr1/tuple}} works for libstdc++ even on C\+\+11, but not for libc++. I think thrift should detect C\+\+11 and switch to the non-tr1-prefixed headers when available, just like what gtest does, and not depend on the odd behavior of",code_debt,low_quality_code,"Fri, 12 Feb 2016 18:54:01 +0000","Mon, 22 Feb 2016 13:17:21 +0000","Thu, 18 Feb 2016 22:04:34 +0000",529833,"This workaround will break gtest on OSX with clang libc++. Using C++11 should have already implied GTEST_USE_OWN_TR1_TUPLE=0, but manually setting this macro to 0 will just make gtest skip checking C++11 and include tr1/tuple rather than tuple, which seems like a bug in gtest, as I see it. See this. tr1/tuple works for libstdc++ even on C++11, but not for libc++. I think thrift should detect C++11 and switch to the non-tr1-prefixed headers when available, just like what gtest does, and not depend on the odd behavior of GTEST_USE_OWN_TR1_TUPLE.",0.375,0.3,negative
thrift,3634,description,Same as THRIFT-3615. It turned out that the problematic part of TSSLSocket originated from TSocket. I took this opportunity to remove the duplicate code.,code_debt,duplicated_code,"Sun, 14 Feb 2016 13:03:25 +0000","Fri, 19 Feb 2016 02:17:54 +0000","Wed, 17 Feb 2016 15:00:45 +0000",266240,Same as THRIFT-3615. It turned out that the problematic part of TSSLSocket originated from TSocket. I took this opportunity to remove the duplicate code.,0.1333333333,0.1333333333,negative
thrift,3634,summary,Fix Python TSocket resource leak on connection failure,code_debt,low_quality_code,"Sun, 14 Feb 2016 13:03:25 +0000","Fri, 19 Feb 2016 02:17:54 +0000","Wed, 17 Feb 2016 15:00:45 +0000",266240,Fix Python TSocket resource leak on connection failure,-0.3,-0.3,negative
thrift,3681,description,It was fragile against parallel make because it was erroneously invoking pub twice in a same directory.,code_debt,low_quality_code,"Fri, 26 Feb 2016 18:29:23 +0000","Sat, 27 Feb 2016 09:26:29 +0000","Sat, 27 Feb 2016 08:01:03 +0000",48700,It was fragile against parallel make because it was erroneously invoking pub twice in a same directory.,0,0,negative
thrift,372,comment_0,"How's this? I cleaned up some missed dead code related to deprecation, as well.",code_debt,dead_code,"Fri, 13 Mar 2009 23:44:08 +0000","Thu, 26 Mar 2009 18:42:05 +0000","Thu, 26 Mar 2009 18:42:05 +0000",1105077,"How's this? I cleaned up some missed dead code related to deprecation, as well.",-0.108625,-0.108625,neutral
thrift,3744,description,"The precision is lost when converting double to string. E.g: double PI = 3.1415926535897931; string value = format(""%.16g"", PI); The value will be '3.141592653589793' and last 1 is lost after format operation. But expected value should be Solution: string value = format(""%.17g"", PI);",code_debt,low_quality_code,"Tue, 15 Mar 2016 01:51:33 +0000","Thu, 17 Mar 2016 22:04:13 +0000","Wed, 16 Mar 2016 17:20:39 +0000",142146,"The precision is lost when converting double to string. E.g: double PI = 3.1415926535897931; string value = format(""%.16g"", PI); The value will be '3.141592653589793' and last 1 is lost after format operation. But expected value should be '3.1415926535897931'. Solution: string value = format(""%.17g"", PI);",-0.0775,-0.06458333333,neutral
thrift,3744,summary,The precision should be 17 (16 bits need after dot) after dot for double type.,code_debt,low_quality_code,"Tue, 15 Mar 2016 01:51:33 +0000","Thu, 17 Mar 2016 22:04:13 +0000","Wed, 16 Mar 2016 17:20:39 +0000",142146,The precision should be 17 (16 bits need after dot) after dot for double type.,0.625,0.625,neutral
thrift,37,summary,add some missing new lines to fprintfs,code_debt,low_quality_code,"Fri, 13 Jun 2008 18:04:39 +0000","Tue, 1 Nov 2011 02:54:16 +0000","Sat, 12 Jul 2008 04:54:25 +0000",2458186,add some missing new lines to fprintfs,-0.4,-0.4,neutral
thrift,3839,description,"I have found performance issue when tried to deserialize big thrift binary message with enabled thrift_protocol php extension. Messsage size was 10 mb and it took about 30 seconds to deserialize it. When i have done debug of php extension and php library i have found that issue is because small read buffer is used in TBufferedTransport and i cannot change it from method. So i have added parameter $buffer_size to function from php extension. And also this parameter i have added to method from php library. And i extended class by method putBack, so this class will be used for desearilization without TBufferedTransport warapper. After these changes it takes less than a second to deserizlize message with 10 mb size id read buffer 512 kb. Here is the pull request",code_debt,slow_algorithm,"Mon, 23 May 2016 08:57:37 +0000","Wed, 28 Sep 2016 13:28:46 +0000","Sun, 25 Sep 2016 17:39:30 +0000",10831313,"I have found performance issue when tried to deserialize big thrift binary message with enabled thrift_protocol php extension. Messsage size was 10 mb and it took about 30 seconds to deserialize it. When i have done debug of php extension and php library i have found that issue is because small read buffer is used in TBufferedTransport and i cannot change it from TBinarySerializer::deserialize method. So i have added parameter $buffer_size to function thrift_protocol_read_binary from php extension. And also this parameter i have added to method TBinarySerializer::deserialize from php library. And i extended class Thrift\Transport\TMemoryBuffer by method putBack, so this class will be used for desearilization without TBufferedTransport warapper. After these changes it takes less than a second to deserizlize message with 10 mb size id read buffer 512 kb. Here is the pull request https://github.com/apache/thrift/pull/1014",0.025,0.025,negative
thrift,3839,summary,Performance issue with big message deserialization using php extension,code_debt,slow_algorithm,"Mon, 23 May 2016 08:57:37 +0000","Wed, 28 Sep 2016 13:28:46 +0000","Sun, 25 Sep 2016 17:39:30 +0000",10831313,Performance issue with big message deserialization using php extension,0.2,0.2,negative
thrift,3864,comment_1,"Let me put it this way: From what I know the GC is Java is by no means the 100% perfect solution either. So if one knows about an yet unfixed issue in the underlying ecosystem, having at least a workaround at hand sounds not so bad to me. Whether it is Java or Go or anything else. So from my point of view iff we can have a * working solution, * that is more efficent in terms of throughput * and does not introduce other problems then (and only then) we should have a look at it. Now the big question: Is this the case?",code_debt,slow_algorithm,"Mon, 27 Jun 2016 14:23:22 +0000","Thu, 3 Jan 2019 14:33:44 +0000","Thu, 3 Jan 2019 14:33:44 +0000",79488622,"Let me put it this way: From what I know the GC is Java is by no means the 100% perfect solution either. So if one knows about an yet unfixed issue in the underlying ecosystem, having at least a workaround at hand sounds not so bad to me. Whether it is Java or Go or anything else. So from my point of view iff we can have a working solution, that is more efficent in terms of throughput and does not introduce other problems then (and only then) we should have a look at it. Now the big question: Is this the case?",0.03,0.03,neutral
thrift,3907,description,"Previously we were able to reuse Docker layers from prebuilt images pulled from docker hub. This has been reducing total build time by 3 hours out of 7~8 hours total. After Docker 1.10 or so, it is no longer possible and we tend to easily saturate entire Apache's 30 jobs on Travis-CI. Standard solution as of now is to use docker save/load. This typically requires automated file upload on CI to some external storage. Unfortunately it cannot be done with our current Travis-CI account settings. To workaround this, we can put Dockerfile itself to Docker image and see if it's modified after the prebuild time and skip fresh builds if unchanged.",code_debt,slow_algorithm,"Mon, 29 Aug 2016 02:33:58 +0000","Wed, 28 Sep 2016 13:28:51 +0000","Sun, 4 Sep 2016 12:27:01 +0000",553983,"Previously we were able to reuse Docker layers from prebuilt images pulled from docker hub. This has been reducing total build time by 3 hours out of 7~8 hours total. After Docker 1.10 or so, it is no longer possible and we tend to easily saturate entire Apache's 30 jobs on Travis-CI. Standard solution as of now is to use docker save/load. This typically requires automated file upload on CI to some external storage. Unfortunately it cannot be done with our current Travis-CI account settings. To workaround this, we can put Dockerfile itself to Docker image and see if it's modified after the prebuild time and skip fresh builds if unchanged.",0.06257142857,0.06257142857,neutral
thrift,3944,description,"There is a block of code in checkHandshake that attempts to set read/write memory bios to be nonblocking. This code doesn't do anything: Here's what this code looks like, and the problems: - creates a new memory BIO. Not sure why. - BIO_set_nbio() executes BIO_ctrl(..., BIO_C_SET_NBIO, ...). This errors out and return 0 because mem_ctrl does not have a case for BIO_C_SET_NBIO. See: - SSL_set_bio() sets the SSL* to use the memory BIOs. - SSL_set_fd() creates a socket BIO, sets the FD on it, and uses SSL_set_bio() to replace the memory BIOs. As far as I can tell, this block of code does nothing and will not change functionality. If there's a reason that it's there, it needs to be re-implemented.",code_debt,low_quality_code,"Thu, 6 Oct 2016 17:29:23 +0000","Thu, 14 Dec 2017 13:55:25 +0000","Sat, 1 Apr 2017 14:18:47 +0000",15281364,"There is a block of code in checkHandshake that attempts to set read/write memory bios to be nonblocking. This code doesn't do anything: https://github.com/apache/thrift/blob/master/lib/cpp/src/thrift/transport/TSSLSocket.cpp#L441 Here's what this code looks like, and the problems: BIO_new(BIO_s_mem()) creates a new memory BIO. Not sure why. BIO_set_nbio() executes BIO_ctrl(..., BIO_C_SET_NBIO, ...). This errors out and return 0 because mem_ctrl does not have a case for BIO_C_SET_NBIO. See: https://github.com/openssl/openssl/blob/6f0ac0e2f27d9240516edb9a23b7863e7ad02898/crypto/bio/bss_mem.c#L226 SSL_set_bio() sets the SSL* to use the memory BIOs. SSL_set_fd() creates a socket BIO, sets the FD on it, and uses SSL_set_bio() to replace the memory BIOs. As far as I can tell, this block of code does nothing and will not change functionality. If there's a reason that it's there, it needs to be re-implemented.",-0.1714285714,-0.1333333333,negative
thrift,3944,summary,TSSLSocket has dead code in checkHandshake,code_debt,dead_code,"Thu, 6 Oct 2016 17:29:23 +0000","Thu, 14 Dec 2017 13:55:25 +0000","Sat, 1 Apr 2017 14:18:47 +0000",15281364,TSSLSocket has dead code in checkHandshake,-0.6,-0.6,neutral
thrift,397,description,This is a holdover from the OCaml generator and is unnecessary in Haskell.,code_debt,dead_code,"Tue, 24 Mar 2009 01:14:55 +0000","Tue, 24 Mar 2009 14:47:58 +0000","Tue, 24 Mar 2009 14:47:58 +0000",48783,This is a holdover from the OCaml generator and is unnecessary in Haskell.,0,0,negative
thrift,397,summary,remove unnecessary redefinition of generate_program(),code_debt,dead_code,"Tue, 24 Mar 2009 01:14:55 +0000","Tue, 24 Mar 2009 14:47:58 +0000","Tue, 24 Mar 2009 14:47:58 +0000",48783,remove unnecessary redefinition of generate_program(),0,0,negative
thrift,39,comment_0,"I've looked this over pretty thoroughly, and I can't seem to figure out what the cause is. Only some of my structs' indentation is being dropped, while others are completely fine. There's no apparent connection between the ones that work and the ones that don't. For the moment, I'm stumped. If anyone else can think of a reason as to why indent() calls in the generator would be getting outright ignored, I'm all ears.",code_debt,low_quality_code,"Wed, 18 Jun 2008 23:50:20 +0000","Tue, 1 Nov 2011 02:54:21 +0000","Fri, 21 Nov 2008 20:25:27 +0000",13466107,"I've looked this over pretty thoroughly, and I can't seem to figure out what the cause is. Only some of my structs' indentation is being dropped, while others are completely fine. There's no apparent connection between the ones that work and the ones that don't. For the moment, I'm stumped. If anyone else can think of a reason as to why indent() calls in the generator would be getting outright ignored, I'm all ears.",-0.24,-0.24,negative
thrift,39,description,The code that gets generated when you run thrift -javabean has no indentation in it at all. This makes it a little challenging to read through it.,code_debt,low_quality_code,"Wed, 18 Jun 2008 23:50:20 +0000","Tue, 1 Nov 2011 02:54:21 +0000","Fri, 21 Nov 2008 20:25:27 +0000",13466107,The code that gets generated when you run thrift -javabean has no indentation in it at all. This makes it a little challenging to read through it.,0,0,negative
thrift,39,summary,javabean-style generated code is very poorly formatted,code_debt,low_quality_code,"Wed, 18 Jun 2008 23:50:20 +0000","Tue, 1 Nov 2011 02:54:21 +0000","Fri, 21 Nov 2008 20:25:27 +0000",13466107,javabean-style generated code is very poorly formatted,-0.563,-0.563,negative
thrift,4014,description,The meta data in AssemblyInfo.cs are inconsistent and do not reflect the ASF properly in all cases.,code_debt,low_quality_code,"Mon, 26 Dec 2016 10:32:00 +0000","Sat, 14 Jan 2017 09:04:50 +0000","Mon, 26 Dec 2016 10:44:10 +0000",730,The meta data in AssemblyInfo.cs are inconsistent and do not reflect the ASF properly in all cases.,0,0,negative
thrift,4030,summary,CI jobs do not reuse downloaded docker images,code_debt,low_quality_code,"Sat, 14 Jan 2017 07:06:27 +0000","Fri, 1 Feb 2019 05:29:49 +0000","Fri, 1 Feb 2019 05:29:49 +0000",64621402,CI jobs do not reuse downloaded docker images,0,0,neutral
thrift,4069,summary,"All perl packages should have proper namespace, version syntax, and use proper thrift exceptions",code_debt,low_quality_code,"Sun, 5 Feb 2017 16:20:39 +0000","Thu, 14 Dec 2017 13:54:42 +0000","Thu, 30 Mar 2017 21:12:02 +0000",4596683,"All perl packages should have proper namespace, version syntax, and use proper thrift exceptions",0.531,0.531,neutral
thrift,4078,description,"I believe this issue would warrant a 0.10.1 fix. It's quite frustrating. Inside there appears to be a debugging message which was left in. This pollutes the client application's console with ""Received 1"" messages. See Line 84 in the {{receiveBase}} method:",code_debt,dead_code,"Wed, 8 Feb 2017 21:40:51 +0000","Wed, 8 Feb 2017 21:48:58 +0000","Wed, 8 Feb 2017 21:47:16 +0000",385,"I believe this issue would warrant a 0.10.1 fix. It's quite frustrating. Inside lib/java/src/org/apache/thrift/TServiceClient.java, there appears to be a debugging message which was left in. This pollutes the client application's console with ""Received 1"" messages. See Line 84 in the receiveBase method:",-0.0525,-0.04375,negative
thrift,4129,description,"In THRIFT-2789 the error handling for connections where notify fails leaks a file descriptor. This was reported and fixed in a pull request without an Apache Jira entry: When failing to dispatch new connections to other IO threads other than the number 0, we returned these connections for reuse without closing them, so the corresponding fds were leaked forever. We should close these connections instead.",code_debt,low_quality_code,"Wed, 22 Mar 2017 18:52:30 +0000","Thu, 14 Dec 2017 13:56:10 +0000","Wed, 22 Mar 2017 19:11:01 +0000",1111,"In THRIFT-2789 the error handling for connections where notify fails leaks a file descriptor. This was reported and fixed in a pull request without an Apache Jira entry: https://github.com/apache/thrift/pull/1210/files When failing to dispatch new connections to other IO threads other than the number 0, we returned these connections for reuse without closing them, so the corresponding fds were leaked forever. We should close these connections instead.",-0.2333333333,-0.2333333333,negative
thrift,4129,summary,C++ TNonblockingServer fd leak when failing to dispatch new connections,code_debt,low_quality_code,"Wed, 22 Mar 2017 18:52:30 +0000","Thu, 14 Dec 2017 13:56:10 +0000","Wed, 22 Mar 2017 19:11:01 +0000",1111,C++ TNonblockingServer fd leak when failing to dispatch new connections,-0.3,-0.3,negative
thrift,4136,description,Similar to {{is_string()}} the {{is_binary()}} method should be virtual and implemented at {{t_type}}. This simplifies the code and reduces possibilities for making technically wrong casts.,code_debt,complex_code,"Sun, 26 Mar 2017 13:51:41 +0000","Fri, 22 Dec 2017 04:04:29 +0000","Sun, 26 Mar 2017 17:56:30 +0000",14689,Similar to is_string() the is_binary() method should be virtual and implemented at t_type. This simplifies the code and reduces possibilities for making technically wrong casts.,-0.125,-0.125,neutral
thrift,4136,summary,Align is_binary() method with is_string() to simplify those checks,code_debt,complex_code,"Sun, 26 Mar 2017 13:51:41 +0000","Fri, 22 Dec 2017 04:04:29 +0000","Sun, 26 Mar 2017 17:56:30 +0000",14689,Align is_binary() method with is_string() to simplify those checks,0,0,neutral
thrift,4231,description,"When a required (e.g. string) field is not set, the C# code may throw a non-Thrift nullptr exception, which at this point is a bit unexpected. Happened to me with TJSON but is in fact a problem of required fields not being checked properly in the generated struct.Write() code.",code_debt,low_quality_code,"Fri, 16 Jun 2017 12:22:03 +0000","Thu, 14 Dec 2017 13:54:50 +0000","Sat, 17 Jun 2017 16:06:15 +0000",99852,"When a required (e.g. string) field is not set, the C# code may throw a non-Thrift nullptr exception, which at this point is a bit unexpected. Happened to me with TJSON but is in fact a problem of required fields not being checked properly in the generated struct.Write() code.",-0.1333333333,-0.1333333333,negative
thrift,4231,summary,TJSONProtocol throws unexpected on null strings,code_debt,low_quality_code,"Fri, 16 Jun 2017 12:22:03 +0000","Thu, 14 Dec 2017 13:54:50 +0000","Sat, 17 Jun 2017 16:06:15 +0000",99852,TJSONProtocol throws unexpected non-Thrift-exception on null strings,0,0,negative
thrift,4245,description,"if p.transport.Write fails, p.buf will not be truncated, which leads to thrift client's memory increasing forever. Is it more reasonable to truncate p.buf when write to transport fails? here are my pull request, i'm new in github&jira, if more details are needed, please tell me, thx.",code_debt,low_quality_code,"Wed, 5 Jul 2017 13:54:36 +0000","Thu, 14 Dec 2017 13:55:48 +0000","Wed, 5 Jul 2017 20:23:18 +0000",23322,"https://github.com/apache/thrift/blob/master/lib/go/thrift/framed_transport.go#L143 if p.transport.Write fails, p.buf will not be truncated, which leads to thrift client's memory increasing forever. Is it more reasonable to truncate p.buf when write to transport fails? here are my pull request, https://github.com/apache/thrift/pull/1303 i'm new in github&jira, if more details are needed, please tell me, thx.",-0.00725,-0.00725,neutral
thrift,4316,description,The TByteBuffer read() method is using the wrong length variable inside the processing loop and may read more than it should.,code_debt,low_quality_code,"Wed, 6 Sep 2017 03:48:40 +0000","Thu, 14 Dec 2017 13:56:12 +0000","Wed, 6 Sep 2017 04:36:28 +0000",2868,The TByteBuffer read() method is using the wrong length variable inside the processing loop and may read more than it should.,-0.25,-0.25,negative
thrift,4316,summary,TByteBuffer.java will read too much data if a previous read returns fewer bytes than requested,code_debt,low_quality_code,"Wed, 6 Sep 2017 03:48:40 +0000","Thu, 14 Dec 2017 13:56:12 +0000","Wed, 6 Sep 2017 04:36:28 +0000",2868,TByteBuffer.java will read too much data if a previous read returns fewer bytes than requested,-0.25,-0.25,negative
thrift,4468,comment_0,"I don't see this patch as part of the Thrift library. The entire Console construct is basically only a helper that was introduced as part of the initial development. Since the model was the C# library, the original author introduced that class, very likely to reduce dependencies. The implementation is rather raw (read: shitty), and - in the case of the real console it is not safe against concurrent accesses. If asked, I would rather throw it out instead of polishing it and adding features. We are dealing with RPC here, not with providing cool console output stuff. Long story short: I'm against it. -1",code_debt,slow_algorithm,"Mon, 22 Jan 2018 14:29:43 +0000","Thu, 27 Dec 2018 15:24:53 +0000","Thu, 25 Jan 2018 23:16:49 +0000",290826,"I don't see this patch as part of the Thrift library. The entire Console construct is basically only a helper that was introduced as part of the initial development. Since the model was the C# library, the original author introduced that class, very likely to reduce dependencies. The implementation is rather raw (read: shitty), and - in the case of the real console it is not safe against concurrent accesses. If asked, I would rather throw it out instead of polishing it and adding features. We are dealing with RPC here, not with providing cool console output stuff. Long story short: I'm against it. -1",-0.02552083333,-0.02552083333,negative
thrift,4468,description,"In Delphi all methods that refer to VCL should do it only from main thread. But class TGUIConsole despite the name does not contain any synchronization methods. My suggestion is to rename this class to TStringsConsole, make method InternalWrite virtual and make new class TGUIConsole inherits from TStringsConsole",code_debt,multi-thread_correctness,"Mon, 22 Jan 2018 14:29:43 +0000","Thu, 27 Dec 2018 15:24:53 +0000","Thu, 25 Jan 2018 23:16:49 +0000",290826,"In Delphi all methodsthat refer to VCLshould do it only from main thread. But classTGUIConsoledespite the namedoes not contain any synchronization methods. My suggestion is to rename this class to TStringsConsole, make methodInternalWrite virtual and make new class TGUIConsoleinherits fromTStringsConsole",-0.25,-0.25,neutral
thrift,447,comment_0,"This patch restructures the code generator to rely on some abstract base classes for clients, processors, and process functions. It makes the generated code marginally smaller and a lot cleaner.",code_debt,complex_code,"Thu, 9 Apr 2009 17:38:54 +0000","Tue, 8 Feb 2011 17:26:55 +0000","Tue, 8 Feb 2011 17:26:55 +0000",57887281,"This patch restructures the code generator to rely on some abstract base classes for clients, processors, and process functions. It makes the generated code marginally smaller and a lot cleaner.",0.2,0.2,positive
thrift,447,description,"The Java generator currently uses the generator to create all of the contents of the myService.Client class, including boring stuff like the constructor and instance variables. It seems like we could just factor this common base stuff out into a BaseClient that lives in the library and simplify the generator accordingly.",code_debt,complex_code,"Thu, 9 Apr 2009 17:38:54 +0000","Tue, 8 Feb 2011 17:26:55 +0000","Tue, 8 Feb 2011 17:26:55 +0000",57887281,"The Java generator currently uses the generator to create all of the contents of the myService.Client class, including boring stuff like the constructor and instance variables. It seems like we could just factor this common base stuff out into a BaseClient that lives in the library and simplify the generator accordingly.",-0.2,-0.2,neutral
thrift,4485,description,"If a read or write operation on pipes reaches the set timeout, the read/write operation is not properly cancelled. However, the overlapped struct gets freed when leaving the method, which essentially leaves the pending read or write operation with an undefined pointer. Easily reproducible with buffered transport over pipes, a combination that does not work at all anyways. The workaround for both problems is to not use buffered transport with pipes (use framed instead), and some sane tinemouts (not too short).",code_debt,low_quality_code,"Thu, 1 Feb 2018 22:29:25 +0000","Thu, 27 Dec 2018 15:25:13 +0000","Fri, 2 Feb 2018 16:52:27 +0000",66182,"If a read or write operation on pipes reaches the set timeout, the read/write operation is not properly cancelled. However, the overlapped struct gets freed when leaving the method, which essentially leaves the pending read or write operation with an undefined pointer. Easily reproducible with buffered transport over pipes, a combination that does not work at all anyways. The workaround for both problems is to not use buffered transport with pipes (use framed instead), and some sane tinemouts (not too short).",-0.079625,-0.079625,neutral
thrift,4559,description,"Tested on both 0.11.0 and master. C++ Server, Python Client. SSL sockets. SSL works correctly and communication is successful, however when the client disconnects the server always prints the following message: {{Thrift: Tue Apr 17 15:43:36 2018 TConnectedClient died: SSL_read: error code: 0 (SSL_error_code = 5)}} {{Deeper diving shows that SSL_error_code 5 is SSL_ERROR_SYSCALL. Documentation says to check both errno and the SLL error stack, however upon inspection both return 0 (no error). I believe this message is printed incorrectly.}} Upon inspecting the code for handing SSL_read, it appears that reading is done in a while-loop, which if no error is found is broken out of. At some point a switch-case was added, but the single level of break statements remained, leaving non-errors to break out of the switch instead of the while. A potential fix can be seen here:",code_debt,low_quality_code,"Thu, 19 Apr 2018 11:42:12 +0000","Thu, 27 Dec 2018 15:24:59 +0000","Wed, 2 May 2018 17:40:05 +0000",1144673,"Tested on both 0.11.0 and master.  C++ Server, Python Client. SSL sockets. SSL works correctly and communication is successful, however when the client disconnects the server always prints the following message: Thrift: Tue Apr 17 15:43:36 2018 TConnectedClient died: SSL_read: error code: 0 (SSL_error_code = 5)  Deeper diving shows that SSL_error_code 5 is SSL_ERROR_SYSCALL. Documentation says to check both errno and the SLL error stack, however upon inspection both return 0 (no error). I believe this message is printed incorrectly.  Upon inspecting the code for handing SSL_read, it appears that reading is done in a while-loop, which if no error is found is broken out of. At some point a switch-case was added, but the single level of break statements remained, leaving non-errors to break out of the switch instead of the while.  A potential fix can be seen here: https://github.com/apache/thrift/pull/1549",0.08752777778,0.08752777778,neutral
thrift,4559,summary,TSSLServerSocket incorrectly prints errors,code_debt,low_quality_code,"Thu, 19 Apr 2018 11:42:12 +0000","Thu, 27 Dec 2018 15:24:59 +0000","Wed, 2 May 2018 17:40:05 +0000",1144673,TSSLServerSocket incorrectly prints errors,-0.4,-0.4,negative
thrift,4604,description,When using the Node.js libs in the browser (via browser.js) Int64 isn't exposed the same way as it is in a Node environment. This just adds Int64 to the exports in browser.js to make consuming the libs more consistent with how we do in an actual Node environment.,code_debt,low_quality_code,"Sun, 29 Jul 2018 16:25:24 +0000","Thu, 27 Dec 2018 15:25:01 +0000","Wed, 1 Aug 2018 12:59:47 +0000",246863,When using the Node.js libs in the browser (via browser.js) Int64 isn't exposed the same way as it is in a Node environment. This just adds Int64 to the exports in browser.js to make consuming the libs more consistent with how we do in an actual Node environment.,0.0275,0.0275,neutral
thrift,471,comment_0,"yeah, the python docs say that __repr__ should be used if there is no __str__ and every other case I have seen follows that rule but the exception printing apparently does not. +1",code_debt,low_quality_code,"Wed, 29 Apr 2009 21:12:16 +0000","Tue, 1 Nov 2011 02:52:11 +0000","Wed, 29 Apr 2009 23:35:22 +0000",8586,"yeah, the python docs say that _repr_ should be used if there is no _str_ and every other case I have seen follows that rule but the exception printing apparently does not. +1",0.1,0.1,neutral
thrift,471,description,"When the python generator makes an exception class since THRIFT-241, it does not include a __str__ method. This is problematic since raised exceptions don't include any useful information that might be part of the exception struct. Without __str__: With __str__: Clearly the latter is way more useful. Patch to follow if no one objects.",code_debt,low_quality_code,"Wed, 29 Apr 2009 21:12:16 +0000","Tue, 1 Nov 2011 02:52:11 +0000","Wed, 29 Apr 2009 23:35:22 +0000",8586,"When the python generator makes an exception class since THRIFT-241, it does not include a _str_ method. This is problematic since raised exceptions don't include any useful information that might be part of the exception struct. Without _str_: With _str_: Clearly the latter is way more useful. Patch to follow if no one objects.",-0.25,-0.25,negative
thrift,471,summary,Generated exceptions in Python should implement __str__,code_debt,low_quality_code,"Wed, 29 Apr 2009 21:12:16 +0000","Tue, 1 Nov 2011 02:52:11 +0000","Wed, 29 Apr 2009 23:35:22 +0000",8586,Generated exceptions in Python should implement __str__,0,0,neutral
thrift,4745,summary,"warning C4305: 'initializing' : truncation from '""__int64' to 'long'",code_debt,low_quality_code,"Sun, 20 Jan 2019 11:34:57 +0000","Wed, 16 Oct 2019 22:26:47 +0000","Fri, 1 Feb 2019 16:14:31 +0000",1053574,"warning C4305: 'initializing' : truncation from '""__int64' to 'long'",-0.6,-0.6,negative
thrift,4830,description,ITNOA I think it useful and have some performance benefits to generate to_string function for enum beside of operator<< overloading.,code_debt,slow_algorithm,"Wed, 20 Mar 2019 14:04:52 +0000","Wed, 16 Oct 2019 22:26:35 +0000","Tue, 2 Jul 2019 00:14:11 +0000",8935759,ITNOA I think it useful and have some performance benefits to generate to_string function for enum beside of operator<< overloading.,0.45,0.45,positive
thrift,4851,description,Remove all calls to and ensure that everything is sent through the logging system so that all logging goes to the same place.,code_debt,low_quality_code,"Wed, 17 Apr 2019 14:59:20 +0000","Wed, 16 Oct 2019 22:26:57 +0000","Fri, 3 May 2019 21:01:14 +0000",1404114,Remove all calls to printStackTrace() and ensure that everything is sent through the logging system so that all logging goes to the same place.,0.2,0.2,neutral
thrift,4857,description,"The {{TField}} hash code implementation is inconsistent with equals, which is a breaking bug. If you know what hash codes are and are familiar with the Java {{Object}} API, then you already know what I'm talking about. Basically Java _requires_ that, if you overriden {{hashCode()}} and {{equals()}}, then for any two objects that are equal they _must_ return the same hash code. The {{TField}} class API contract isn't clear about what is considered equality, but according to the {{TField.equals()}} implementation, fields are equal if and only if: * Both objects are a {{TField}} (I'm generalizing here; there's another subtle bug lurking with class checking, but that's another story). * The fields both have the same {{type}} and {{id}}. In other words, fields are equal _without regard to name_. And this follows the overall Thrift architecture, in which field names are little more than window dressing, and the IDs carry the semantics. Unfortunately includes the name in the hash code calculation! This completely breaks the {{Object}} contract. It makes the hash code inconsistent with equality. To put it another way, two fields {{foo}} and {{bar}} could have the same type and ID, and {{foo.equals(bar)}} would return {{true}}, but they would be given different hash codes!! This is completely forbidden, and means that with this bug you cannot use a {{TField}} as the key in a map, for example, or even reliably keep a {{Set<TField This is simply broken as per the Java {{Object}} API contract.",code_debt,low_quality_code,"Fri, 3 May 2019 16:47:14 +0000","Wed, 16 Oct 2019 22:26:37 +0000","Mon, 13 May 2019 20:54:17 +0000",878823,"The TField hash code implementation is inconsistent with equals, which is a breaking bug. If you know what hash codes are and are familiar with the Java Object API, then you already know what I'm talking about. Basically Java requires that, if you overriden hashCode() and equals(), then for any two objects that are equal they must return the same hash code. The TField class API contract isn't clear about what is considered equality, but according to the TField.equals() implementation, fields are equal if and only if: Both objects are a TField (I'm generalizing here; there's another subtle bug lurking with class checking, but that's another story). The fields both have the same type and id. In other words, fields are equal without regard to name. And this follows the overall Thrift architecture, in which field names are little more than window dressing, and the IDs carry the semantics. Unfortunately TField.hashCode() includes the name in the hash code calculation! This completely breaks the Object contract. It makes the hash code inconsistent with equality. To put it another way, two fields foo and bar could have the same type and ID, and foo.equals(bar) would return true, but they would be given different hash codes!! This is completely forbidden, and means that with this bug you cannot use a TField as the key in a map, for example, or even reliably keep a Set<TField> of fields! If you were to store foo and bar as keys in a map, for example, the different hash codes would put them in different buckets, even though they were considered equal, providing inconsistent and strange lookup results, depending on the name of the field you used to query with. This is simply broken as per the Java Object API contract.",-0.1351714286,-0.1120196078,negative
thrift,4857,summary,Java field hash code implementation inconsistent with equals.,code_debt,low_quality_code,"Fri, 3 May 2019 16:47:14 +0000","Wed, 16 Oct 2019 22:26:37 +0000","Mon, 13 May 2019 20:54:17 +0000",878823,Java field hash code implementation inconsistent with equals.,0,0,neutral
thrift,4863,description,"A failing call to WinHTTP that leads to an NULL handle should be properly reported with the original error code. The NULL handle is reported, but thats only a symptom, not the real problem.",code_debt,low_quality_code,"Thu, 9 May 2019 22:06:08 +0000","Wed, 16 Oct 2019 22:26:49 +0000","Thu, 9 May 2019 22:42:13 +0000",2165,"A failing call to WinHTTP that leads to an NULL handle should be properly reported with the original error code. The NULL handle is reported, but thats only a symptom, not the real problem.",0,0,negative
thrift,4863,summary,better indication of WinHTTP errors,code_debt,low_quality_code,"Thu, 9 May 2019 22:06:08 +0000","Wed, 16 Oct 2019 22:26:49 +0000","Thu, 9 May 2019 22:42:13 +0000",2165,better indication of WinHTTP errors,0.05,0.05,neutral
thrift,4886,description,"The WinHTTP transport method {{CreateRequest()}} needs more detailed error information. Otherwise it is pretty hard to guess at which step exactly the problem arises. Additionally, the method should wrap any into Furthermore, the error codes retrieved via GetLastError should be enriched by a textual representation of the error code.",code_debt,low_quality_code,"Wed, 12 Jun 2019 20:07:21 +0000","Wed, 16 Oct 2019 22:27:18 +0000","Thu, 27 Jun 2019 19:46:28 +0000",1294747,"The WinHTTP transport method CreateRequest() needs more detailed error information. Otherwise it is pretty hard to guess at which step exactly the problem arises. Additionally, the method should wrap any non-Thrift-Exception into TTransportException. Furthermore, the error codes retrieved via GetLastError should be enriched by a textual representation of the error code.",-0.1402777778,-0.1052083333,negative
thrift,48,comment_0,"Line 74 of the patch, please edit the comment to say ""We need remove the old unix socket if the file exists and nobody is listening on it."" Line 95 of the patch, I personally prefer leaving the parens here. Line 103 of the patch, please use ""is not None"" instead of ""!="" Other than those nitpicks, I'm fine with this. bmaurer, you wrote the original Python Unix-domain code. What do you think of it?",code_debt,low_quality_code,"Mon, 23 Jun 2008 12:56:07 +0000","Tue, 1 Nov 2011 02:54:21 +0000","Thu, 31 Jul 2008 20:15:25 +0000",3309558,"Line 74 of the patch, please edit the comment to say ""We need remove the old unix socket if the file exists and nobody is listening on it."" Line 95 of the patch, I personally prefer leaving the parens here. Line 103 of the patch, please use ""is not None"" instead of ""!="" Other than those nitpicks, I'm fine with this. bmaurer, you wrote the original Python Unix-domain code. What do you think of it?",0.1333333333,0.1333333333,neutral
thrift,48,description,"The patch adds the unix_socket parameter for TServerSocket. The patch fixes an error message, because it's confusing to see the message ""Could not connect to localhost:9090"" if you try to connect to /tmp/unix_test.",code_debt,low_quality_code,"Mon, 23 Jun 2008 12:56:07 +0000","Tue, 1 Nov 2011 02:54:21 +0000","Thu, 31 Jul 2008 20:15:25 +0000",3309558,"The patch adds the unix_socket parameter for TServerSocket. The patch fixes an error message, because it's confusing to see the message ""Could not connect to localhost:9090"" if you try to connect to /tmp/unix_test.",-0.20925,-0.20925,neutral
thrift,508,description,"If you define a Thrift service that mentions the same method more than once, the Python compiler generates code for the method more than once. Given that this is (almost?) certainly an error in the Thrift service specification, it should be flagged and, I think, an error should be raised. This will prevent people (like me) from becoming the victim seemingly mysterious Thrift service failures due to silly cut & paste errors when editing specification files.",code_debt,low_quality_code,"Thu, 14 May 2009 20:09:09 +0000","Tue, 1 Nov 2011 02:54:10 +0000","Thu, 7 Apr 2011 15:53:18 +0000",59859849,"If you define a Thrift service that mentions the same method more than once, the Python compiler generates code for the method more than once. Given that this is (almost?) certainly an error in the Thrift service specification, it should be flagged and, I think, an error should be raised. This will prevent people (like me) from becoming the victim seemingly mysterious Thrift service failures due to silly cut & paste errors when editing specification files.",-0.1423928571,-0.1423928571,neutral
thrift,544,comment_5,"I think the patch is correct, but the argument for correctness is very complicated because constants_[name] is a mutating operation that creates an entry if the ""name"" key is not present. I'd much prefer the condition be != constants.end()""",code_debt,complex_code,"Tue, 21 Jul 2009 15:00:39 +0000","Fri, 10 Sep 2010 17:04:55 +0000","Thu, 5 Aug 2010 23:24:18 +0000",32862219,"I think the patch is correct, but the argument for correctness is very complicated because constants_[name] is a mutating operation that creates an entry if the ""name"" key is not present. I'd much prefer the condition be ""constants_.find(name) != constants.end()""",0.1615,0.1292,negative
thrift,554,description,"There are two more issues affecting the functioning of a Perl service server: 1. Failure to prepend the Perl namespace to the exception name when checking the exception type from a eval'ed method call. 2. writeMessageEnd() should be present after a method call writes its result. I'm attaching a patch which addresses these issues, in addition to the following more minor changes: 1. Tried to make indentation and line breaks more consistent to ensure readability of the generated code. 2. Added a few best practice ideas to improve the code in minor ways. 3. Added a readAll() function to the as the one found in Thrift::Transport uses a while loop to consume the data, which results in a endless loop.",code_debt,low_quality_code,"Wed, 29 Jul 2009 22:42:16 +0000","Tue, 1 Nov 2011 02:53:50 +0000","Fri, 31 Jul 2009 01:32:00 +0000",96584,"There are two more issues affecting the functioning of a Perl service server: 1. Failure to prepend the Perl namespace to the exception name when checking the exception type from a eval'ed method call. 2. writeMessageEnd() should be present after a method call writes its result. I'm attaching a patch which addresses these issues, in addition to the following more minor changes: 1. Tried to make indentation and line breaks more consistent to ensure readability of the generated code. 2. Added a few best practice ideas to improve the code in minor ways. 3. Added a readAll() function to the Thrift::MemoryBuffer, as the one found in Thrift::Transport uses a while loop to consume the data, which results in a endless loop.",0.04375,0.04375,negative
thrift,554,summary,Perl improper namespace check for exception handling and writeMessageEnd missing on processor calls,code_debt,low_quality_code,"Wed, 29 Jul 2009 22:42:16 +0000","Tue, 1 Nov 2011 02:53:50 +0000","Fri, 31 Jul 2009 01:32:00 +0000",96584,Perl improper namespace check for exception handling and writeMessageEnd missing on processor calls,-0.3875,-0.3875,neutral
thrift,56,description,"When displaying the caller of a deprecated class/module, any thrift library code should be skipped. This really only affects (as it called each_field, which would trigger the deprecation). The given patch is actually 2 patches (suitable for use with git-am), one for this specific issue, the other fixes the specs to handle deprecation warnings a bit better. I bundled the second with this because the issue with the specs only shows up once the first patch is added.",code_debt,low_quality_code,"Tue, 24 Jun 2008 23:42:51 +0000","Thu, 19 Aug 2010 05:45:17 +0000","Thu, 26 Jun 2008 18:45:40 +0000",154969,"When displaying the caller of a deprecated class/module, any thrift library code should be skipped. This really only affects ThriftStruct#initialize (as it called each_field, which would trigger the deprecation). The given patch is actually 2 patches (suitable for use with git-am), one for this specific issue, the other fixes the specs to handle deprecation warnings a bit better. I bundled the second with this because the issue with the specs only shows up once the first patch is added.",-0.1116875,-0.1116875,negative
thrift,593,description,"Perl client library for Thrift is bit slow.This implementation can make 24.75 QPS. I wrote a I/O buffering patch for this implementation.This makes 147x faster the echo server. The patch is here: regards,",code_debt,slow_algorithm,"Mon, 28 Sep 2009 16:34:33 +0000","Tue, 29 Sep 2009 00:19:08 +0000","Tue, 29 Sep 2009 00:19:08 +0000",27875,"Perl client library for Thrift is bit slow.This implementation can make 24.75 QPS. I wrote a I/O buffering patch for this implementation.This makes 147x faster the echo server. The patch is here: http://gist.github.com/195539 regards,",0,0,neutral
thrift,597,comment_1,"One query: I want to use THttpServer in my python project but from initial performance review for a single client single request (No ThreadingMixIn required at this point), it shows that HttpServer is always 2 to 4 times slower than NonBlocking server in tutorial Calculator client server app and same was observed in my product testing as well. Is there a way to make the performance equal for THTTPServer as compared to NonBlocking? Heres the code for NonBlocking client and server in python and its performance on ubuntu. single machine.  In my project I observed that on client side, its the generated code send_<api Will really appreciate if someone can shed some light on how to improve performance for THttpServer in thrift 0.8.0.",code_debt,slow_algorithm,"Fri, 2 Oct 2009 17:12:03 +0000","Sun, 14 Oct 2012 06:49:13 +0000","Thu, 2 Sep 2010 15:15:44 +0000",28937021,"One query: I want to use THttpServer in my python project but from initial performance review for a single client single request (No ThreadingMixIn required at this point), it shows that HttpServer is always 2 to 4 times slower than NonBlocking server in tutorial Calculator client server app and same was observed in my product testing as well. Is there a way to make the performance equal for THTTPServer as compared to NonBlocking? Heres the code for NonBlocking client and server in python and its performance on ubuntu. single machine. ------------------ client: def non_blocking_server_client(): try: Make socket transport = TSocket.TSocket('localhost', 9090) Buffering is critical. Raw sockets are very slow transport = TTransport.TFramedTransport(transport) Wrap in a protocol protocol = TBinaryProtocol.TBinaryProtocol(transport) Create a client to use the protocol encoder client = Calculator.Client(protocol) Connect! transport.open() perform_ops(client) Close! transport.close() except Thrift.TException, tx: print '%s' % (tx.message) server: def non_blocking_server(): handler = CalculatorHandler() processor = Calculator.Processor(handler) transport = TSocket.TServerSocket(port=9090) server = TNonblockingServer.TNonblockingServer(processor, transport) print 'Starting the server...' server.serve() print 'done.' performance timings: $ ./PythonClient.py ping took 0.633 ms <================== ping() add took 0.395 ms 1+1=2 InvalidOperation: InvalidOperation(what=4, why='Cannot divide by 0') calculate took 0.399 ms 15-10=5 getStruct took 0.361 ms Check log: 5 $ ./PythonClient.py ping took 0.536 ms <================== ping() add took 0.362 ms 1+1=2 InvalidOperation: InvalidOperation(what=4, why='Cannot divide by 0') calculate took 0.403 ms 15-10=5 getStruct took 0.364 ms Check log: 5 ------------------ Heres the code for HttpServer client and server in python and its performance ------------------ client: def http_server_client(): try: path = ""http://%s:%s/"" % ('127.0.0.1', 9090) transport = THttpClient.THttpClient(uri_or_host=path) Wrap in a protocol protocol = TBinaryProtocol.TBinaryProtocol(transport) Create a client to use the protocol encoder client = Calculator.Client(protocol) Connect! transport.open() perform_ops(client) Close! transport.close() except Thrift.TException, tx: print '%s' % (tx.message) server: def http_server(): handler = CalculatorHandler() processor = Calculator.Processor(handler) pfactory = TBinaryProtocol.TBinaryProtocolFactory() server = THttpServer.THttpServer(processor, ('127.0.0.1', 9090), pfactory) print 'Starting the server...' server.serve() print 'done.' performance timings: $ ./PythonClient.py ping took 1.535 ms <================== ping() add took 0.972 ms 1+1=2 InvalidOperation: InvalidOperation(what=4, why='Cannot divide by 0') calculate took 0.929 ms 15-10=5 getStruct took 0.943 ms Check log: 5 $ ./PythonClient.py ping took 1.243 ms <================== ping() add took 0.944 ms 1+1=2 InvalidOperation: InvalidOperation(what=4, why='Cannot divide by 0') calculate took 0.930 ms 15-10=5 getStruct took 0.925 ms Check log: 5 ------------------ server side timings are pretty low. In my project I observed that on client side, its the generated code send_<api> takes most of the time. which internally calls httplib's getresponse which probably is waiting for something to come on wire? Will really appreciate if someone can shed some light on how to improve performance for THttpServer in thrift 0.8.0.",0.1631,0.00240625,neutral
thrift,597,description,"This class was originally meant for functional testing only, so performance wasn't a concern. But now I'm using it for load testing. :) Two patches here. The first enables buffered I/O. The second allows the http server class to be specified, which allows users to use the ThreadingMixin.",code_debt,slow_algorithm,"Fri, 2 Oct 2009 17:12:03 +0000","Sun, 14 Oct 2012 06:49:13 +0000","Thu, 2 Sep 2010 15:15:44 +0000",28937021,"This class was originally meant for functional testing only, so performance wasn't a concern. But now I'm using it for load testing. Two patches here. The first enables buffered I/O. The second allows the http server class to be specified, which allows users to use the ThreadingMixin.",0.1,0,positive
thrift,597,summary,Python THttpServer performance improvements,code_debt,slow_algorithm,"Fri, 2 Oct 2009 17:12:03 +0000","Sun, 14 Oct 2012 06:49:13 +0000","Thu, 2 Sep 2010 15:15:44 +0000",28937021,Python THttpServer performance improvements,0,0,positive
thrift,617,description,One of my thrift services does special handling upon receiving SIGHUP. This causes spurious exceptions about EINTR to be logged. Attached patch adds EINTR retry logic akin to the C++ implementation to handle this.,code_debt,low_quality_code,"Tue, 3 Nov 2009 00:18:50 +0000","Sat, 26 Jan 2019 14:39:42 +0000","Sat, 26 Jan 2019 14:39:42 +0000",291306052,One of my thrift services does special handling upon receiving SIGHUP. This causes spurious exceptions about EINTR to be logged. Attached patch adds EINTR retry logic akin to the C++ implementation to handle this.,-0.2223333333,-0.2223333333,negative
thrift,62,comment_3,"In talking with Kevin Clark, the benchmark this patch provides demonstrates a significant performance penalty between the old code and the new (new meaning my recent work). We've narrowed it down to two areas: MemoryBuffer's new implementation, and calling #dup on default values. The first issue is addressed by THRIFT-63. The second is still a problem, and I don't know if there's any way we can fix it, because calling #dup is fairly important to protect against destructive modification of default values.",code_debt,slow_algorithm,"Thu, 26 Jun 2008 00:32:20 +0000","Tue, 8 Jul 2008 00:48:12 +0000","Tue, 8 Jul 2008 00:48:12 +0000",1037752,"In talking with Kevin Clark, the benchmark this patch provides demonstrates a significant performance penalty between the old code and the new (new meaning my recent work). We've narrowed it down to two areas: MemoryBuffer's new implementation, and Thrift::Struct#initialize calling #dup on default values. The first issue is addressed by THRIFT-63. The second is still a problem, and I don't know if there's any way we can fix it, because calling #dup is fairly important to protect against destructive modification of default values.",0.05802083333,0.0576875,neutral
thrift,62,comment_4,"We definitely need the dup effect. If we really want to try and improve the performance of that code, we could try generating the default value stuff inline rather than storing it in a class-variable map which requires dups.",code_debt,slow_algorithm,"Thu, 26 Jun 2008 00:32:20 +0000","Tue, 8 Jul 2008 00:48:12 +0000","Tue, 8 Jul 2008 00:48:12 +0000",1037752,"We definitely need the dup effect. If we really want to try and improve the performance of that code, we could try generating the default value stuff inline rather than storing it in a class-variable map which requires dups.",0.01666666667,0.01666666667,neutral
thrift,62,description,"This is the updated patch which adds a C extension to speed up Ruby thrift serialization and deserialization. It's largely been reviewed already, but should still be looked over. The pre-Apache discussions of the bindings, including peer review and modifications happened here: Post that, there have been a few changes prompted by bugs found internally at Powerset, and I fixed the read buffering in This patch also adds automake compatability (make check and such) to the Ruby bindings.",code_debt,slow_algorithm,"Thu, 26 Jun 2008 00:32:20 +0000","Tue, 8 Jul 2008 00:48:12 +0000","Tue, 8 Jul 2008 00:48:12 +0000",1037752,"This is the updated patch which adds a C extension to speed up Ruby thrift serialization and deserialization. It's largely been reviewed already, but should still be looked over. The pre-Apache discussions of the bindings, including peer review and modifications happened here: http://publists.facebook.com/pipermail/thrift/2008-April/000890.html Post that, there have been a few changes prompted by bugs found internally at Powerset, and I fixed the read buffering in Thrift::BufferedTransport. This patch also adds automake compatability (make check and such) to the Ruby bindings.",0.09366666667,0.07025,neutral
thrift,635,description,"thrift --gen java:camel doesn't generate idiomatic names for getters and setters. For instance: struct SlicePredicate { 1: optional list<binary 2: optional SliceRange slice_range, } has generated getters and setters: getSlice_range setSlice_range",code_debt,low_quality_code,"Fri, 20 Nov 2009 06:19:12 +0000","Wed, 27 Oct 2010 18:53:01 +0000","Wed, 27 Oct 2010 18:53:01 +0000",29507629,"thrift --gen java:camel doesn't generate idiomatic names for getters and setters. For instance: struct SlicePredicate { 1: optional list<binary> column_names, 2: optional SliceRange slice_range, } has generated getters and setters: getSlice_range setSlice_range",0.25,0,neutral
thrift,639,description,"When a Timeout interrupts a client that is reading a Thrift response, the client may leave unread bytes in the read queue. If this transport instance/queue is reused in a later request, the extra bytes will corrupt that later response. We're currently working around this by having the rescue blocks of our TimeoutExceptions close the transport so that subsequent requests will have to create a new, clean one.",code_debt,low_quality_code,"Wed, 2 Dec 2009 02:43:45 +0000","Fri, 10 Jun 2011 00:31:01 +0000","Fri, 10 Jun 2011 00:31:01 +0000",47944036,"When a Timeout interrupts a client that is reading a Thrift response, the client may leave unread bytes in the read queue. If this transport instance/queue is reused in a later request, the extra bytes will corrupt that later response. We're currently working around this by having the rescue blocks of our TimeoutExceptions close the transport so that subsequent requests will have to create a new, clean one.",-0.15,-0.15,negative
thrift,653,description,"Now that enums are actually enums, their toStrings are acceptable in the overall struct toString. We should remove the special case we built in for this in the past.",code_debt,dead_code,"Wed, 16 Dec 2009 22:50:04 +0000","Fri, 18 Dec 2009 19:34:59 +0000","Fri, 18 Dec 2009 19:34:59 +0000",161095,"Now that enums are actually enums, their toStrings are acceptable in the overall struct toString. We should remove the special case we built in for this in the past.",0.578,0.578,neutral
thrift,673,comment_0,Updated patch which catches one more case of trailing whitespace.,code_debt,low_quality_code,"Wed, 13 Jan 2010 23:59:26 +0000","Thu, 2 Sep 2010 14:21:53 +0000","Thu, 2 Sep 2010 14:21:53 +0000",20010147,Updated patch which catches one more case of trailing whitespace.,0,0,neutral
thrift,673,description,The Python code generator produces code with a number of whitespace issues: - Trailing whitespace at the end of lines. - Multiple blank newlines at the end of files. Both of these issues cause problems if the code is used in a Git repository with `git diff --check' in a hook. The attached patch corrects the code generation to address these issues.,code_debt,low_quality_code,"Wed, 13 Jan 2010 23:59:26 +0000","Thu, 2 Sep 2010 14:21:53 +0000","Thu, 2 Sep 2010 14:21:53 +0000",20010147,The Python code generator produces code with a number of whitespace issues: Trailing whitespace at the end of lines. Multiple blank newlines at the end of files. Both of these issues cause problems if the code is used in a Git repository with `git diff --check' in a hook. The attached patch corrects the code generation to address these issues.,-0.1333333333,-0.1,negative
thrift,673,summary,Generated Python code has whitespace issues,code_debt,low_quality_code,"Wed, 13 Jan 2010 23:59:26 +0000","Thu, 2 Sep 2010 14:21:53 +0000","Thu, 2 Sep 2010 14:21:53 +0000",20010147,Generated Python code has whitespace issues,0,0,negative
thrift,678,summary,HTML generator should include per-field docstrings,code_debt,low_quality_code,"Thu, 14 Jan 2010 21:53:33 +0000","Fri, 15 Jan 2010 17:28:11 +0000","Fri, 15 Jan 2010 17:28:11 +0000",70478,HTML generator should include per-field docstrings,0,0,neutral
thrift,695,comment_3,"Here's a simpler to use, more pythonic version.",code_debt,complex_code,"Tue, 2 Feb 2010 00:04:30 +0000","Tue, 1 Nov 2011 02:53:48 +0000","Fri, 26 Feb 2010 00:56:40 +0000",2076730,"Here's a simpler to use, more pythonic version.",0,0,neutral
thrift,710,comment_0,"This patch makes TBinaryProtocol use direct buffer access in the relevant methods. My performance testing was somewhat rudimentary, but I think it may have as much as doubled performance. Obviously your performance boost will be really dependent on the contents of your struct, but this seems pretty great. As a side effect of this issue, I refactored the TCompactProtocol test so that we could exact TBinaryProtocol to the same bevy of test cases.",code_debt,slow_algorithm,"Thu, 18 Feb 2010 18:29:55 +0000","Tue, 2 Mar 2010 18:49:10 +0000","Tue, 2 Mar 2010 18:49:10 +0000",1037955,"This patch makes TBinaryProtocol use direct buffer access in the relevant methods. My performance testing was somewhat rudimentary, but I think it may have as much as doubled performance. Obviously your performance boost will be really dependent on the contents of your struct, but this seems pretty great. As a side effect of this issue, I refactored the TCompactProtocol test so that we could exact TBinaryProtocol to the same bevy of test cases.",0.09120833333,0.09120833333,positive
thrift,714,comment_1,"After reading up on it some more, I think this change is what we actually want. Up until the min number of threads, new ""core"" pool threads will be created. Once we pass this limit, new threads will be created up to the max number of threads, at which point the server will start rejecting executions. After (default) 60 seconds of idleness, the pool will start to kill threads, but go no lower than the min number of threads. This patch is probably incomplete, as if we get a rejected execution exception when trying to queue an invocation, we should respond to the client with an error immediately.",code_debt,low_quality_code,"Wed, 24 Feb 2010 16:20:58 +0000","Wed, 28 Jul 2010 21:32:18 +0000","Wed, 28 Jul 2010 21:32:18 +0000",13324280,"After reading up on it some more, I think this change is what we actually want. Up until the min number of threads, new ""core"" pool threads will be created. Once we pass this limit, new threads will be created up to the max number of threads, at which point the server will start rejecting executions. After (default) 60 seconds of idleness, the pool will start to kill threads, but go no lower than the min number of threads. This patch is probably incomplete, as if we get a rejected execution exception when trying to queue an invocation, we should respond to the client with an error immediately.",-0.04,-0.04,neutral
thrift,714,comment_3,I committed a patch to remove maxWorkerThreads and rename minWorkerThreads to workerThreads. This will keep the behavior the same and remove the ineffectual parameter.,code_debt,dead_code,"Wed, 24 Feb 2010 16:20:58 +0000","Wed, 28 Jul 2010 21:32:18 +0000","Wed, 28 Jul 2010 21:32:18 +0000",13324280,I committed a patch to remove maxWorkerThreads and rename minWorkerThreads to workerThreads. This will keep the behavior the same and remove the ineffectual parameter.,0.05,0.05,neutral
thrift,716,description,"Try creating a union with the field name ""value"", and the code won't compile. In writeFields for the generated class, you'll have something like the following: <code case VALUE: String value = return; </code ""String value"" conflicts with the parameter ""Object value"".",code_debt,low_quality_code,"Thu, 25 Feb 2010 01:23:17 +0000","Sun, 28 Feb 2010 05:19:49 +0000","Sun, 28 Feb 2010 05:19:49 +0000",273392,"Try creating a union with the field name ""value"", and the code won't compile. In writeFields for the generated class, you'll have something like the following: <code> case VALUE: String value = (String)getFieldValue(); oprot.writeString(value); return; </code> ""String value"" conflicts with the parameter ""Object value"".",-0.2405,-0.1603333333,neutral
thrift,716,summary,Field names can conflict with local variables in code for unions,code_debt,low_quality_code,"Thu, 25 Feb 2010 01:23:17 +0000","Sun, 28 Feb 2010 05:19:49 +0000","Sun, 28 Feb 2010 05:19:49 +0000",273392,Field names can conflict with local variables in code for unions,-0.45,-0.45,neutral
thrift,717,description,"The Thrift PHP library makes gratuitous use of the $GLOBALS array to store basic configuration. Globals in PHP are generally bad practice, so I suggest something else: Use constants. Being immutable, constants are more secure than globals (that could be overwritten in scripts susceptible to injection attacks); they also perform much better, since the $GLOBALS variable is a hash-table, lookups are comparatively expensive. I will attach a patch soon unless anyone has any better ideas.",code_debt,low_quality_code,"Thu, 25 Feb 2010 16:40:07 +0000","Tue, 1 Nov 2011 02:54:07 +0000","Mon, 11 Apr 2011 16:13:52 +0000",35422425,"The Thrift PHP library makes gratuitous use of the $GLOBALS array to store basic configuration. Globals in PHP are generally bad practice, so I suggest something else: Use constants. Being immutable, constants are more secure than globals (that could be overwritten in scripts susceptible to injection attacks); they also perform much better, since the $GLOBALS variable is a hash-table, lookups are comparatively expensive. I will attach a patch soon unless anyone has any better ideas.",0.0539375,0.0539375,negative
thrift,717,summary,Global variables should not be used for configuration of PHP library,code_debt,low_quality_code,"Thu, 25 Feb 2010 16:40:07 +0000","Tue, 1 Nov 2011 02:54:07 +0000","Mon, 11 Apr 2011 16:13:52 +0000",35422425,Global variables should not be used for configuration of PHP library,0,0,negative
thrift,728,description,"The patch * Generates Arbitrary instances for Thrift structs, enums and exceptions * Provides Arbitrary instances for GHC's Int64, Data.Map.Map and Data.Set.Set * Makes Thrift enums instances of Bounded and improves the Enum instance declaration Making a type an instance of specifies how to generate random instances of the struct. This is useful for testing. For example, consider the following simple Thrift declaration: With the patch, the following program (import statements elided) is a fuzzer for the log service. In implementing the Arbitrary instances, it was useful to make Thrift enums instances of Bounded and to improve the Enum instance. Specifically, whereas before, would throw an exception, now it behaves as expected without an exception. I consider the patch incomplete. It's more of a starting point for a discussion at this point than a serious candidate for inclusion. If it is of interest, I'd appreciate some direction on testing it as well as style, and I'd welcome any other comments or thoughts.",code_debt,low_quality_code,"Mon, 8 Mar 2010 00:00:50 +0000","Thu, 25 Sep 2014 03:18:28 +0000","Tue, 26 Aug 2014 22:10:17 +0000",141084567,"The patch Generates Arbitrary instances for Thrift structs, enums and exceptions Provides Arbitrary instances for GHC's Int64, Data.Map.Map and Data.Set.Set Makes Thrift enums instances of Bounded and improves the Enum instance declaration Making a type an instance of Test.QuickCheck.Arbitrary specifies how to generate random instances of the struct. This is useful for testing. For example, consider the following simple Thrift declaration: With the patch, the following program (import statements elided) is a fuzzer for the log service. In implementing the Arbitrary instances, it was useful to make Thrift enums instances of Bounded and to improve the Enum instance. Specifically, whereas before, would throw an exception, now it behaves as expected without an exception. I consider the patch incomplete. It's more of a starting point for a discussion at this point than a serious candidate for inclusion. If it is of interest, I'd appreciate some direction on testing it as well as style, and I'd welcome any other comments or thoughts.",0.2218263889,0.2258511905,neutral
thrift,750,comment_3,"Okay. This seems acceptable. There is a little bit of a performance hit, but it is not major. Can you attach a patch?",code_debt,slow_algorithm,"Fri, 2 Apr 2010 04:31:57 +0000","Thu, 14 Dec 2017 13:54:39 +0000","Thu, 26 Jan 2017 01:55:31 +0000",215213014,"Okay. This seems acceptable. There is a little bit of a performance hit, but it is not major. Can you attach a patch?",0.367,0.367,neutral
thrift,813,comment_2,"I have no idea - TJSONProtocol.java is a big file, and I didn't write it. personally, I'd just grep around for any occurrence of the @ sign or something that indicates URL-encoding is going on. Sadly, not really. It's a bit of a jungle in there. The very first thing to do in every case is to open a JIRA ticket so that we have a record of the bug. From there, either get out your machete and safari hat and dig in, or try to find someone who can fix it for you.",code_debt,complex_code,"Sun, 4 Jul 2010 09:35:36 +0000","Tue, 1 Nov 2011 02:52:22 +0000","Fri, 22 Oct 2010 13:28:27 +0000",9517971,"Where would I look in order to debug what's going on? What part of the TJSONProtocol would this be in. I have no idea - TJSONProtocol.java is a big file, and I didn't write it. personally, I'd just grep around for any occurrence of the @ sign or something that indicates URL-encoding is going on. Also, I've seen several bugs (I think) and I'd like to learn some about the code base. Are there any ramp up docs for contributors? Sadly, not really. It's a bit of a jungle in there. The very first thing to do in every case is to open a JIRA ticket so that we have a record of the bug. From there, either get out your machete and safari hat and dig in, or try to find someone who can fix it for you.",-0.1678571429,-0.06136363636,negative
thrift,840,comment_0,Throw exception instead of silent return on unrecognised type.,code_debt,low_quality_code,"Fri, 6 Aug 2010 20:46:27 +0000","Thu, 2 Sep 2010 00:26:34 +0000","Thu, 2 Sep 2010 00:26:34 +0000",2259607,Throw exception instead of silent return on unrecognised type.,0,0,neutral
thrift,840,comment_3,I committed this with the redundant parts removed. Thanks for the patch!,code_debt,duplicated_code,"Fri, 6 Aug 2010 20:46:27 +0000","Thu, 2 Sep 2010 00:26:34 +0000","Thu, 2 Sep 2010 00:26:34 +0000",2259607,I committed this with the redundant parts removed. Thanks for the patch!,0.45,0.45,positive
thrift,840,summary,Perl protocol handler could be more robust against unrecognised types,code_debt,low_quality_code,"Fri, 6 Aug 2010 20:46:27 +0000","Thu, 2 Sep 2010 00:26:34 +0000","Thu, 2 Sep 2010 00:26:34 +0000",2259607,Perl protocol handler could be more robust against unrecognised types,0.4,0.4,neutral
thrift,873,description,"All of the tests run in the same JVM, and it seems like is leaking sockets. As a quick fix, dropping that to use only 200 clients instead of 500, and changing each unit test to run in its own JVM instead of sharing them. Also allowing the port used for binding the test servers to be configured from the command line",code_debt,low_quality_code,"Fri, 27 Aug 2010 05:50:41 +0000","Tue, 1 Nov 2011 02:52:01 +0000","Fri, 27 Aug 2010 06:18:02 +0000",1641,"All of the tests run in the same JVM, and it seems like TestAsyncClientManager is leaking sockets. As a quick fix, dropping that to use only 200 clients instead of 500, and changing each unit test to run in its own JVM instead of sharing them. Also allowing the port used for binding the test servers to be configured from the command line",-0.5,-0.5,negative
thrift,897,comment_6,"LGTM. In the main.cc part, you could possibly check the part before the dot matches the name of the enum (possibly with the scope qualifier). Not critical.",code_debt,low_quality_code,"Fri, 10 Sep 2010 18:59:08 +0000","Sun, 12 Sep 2010 14:39:15 +0000","Sun, 12 Sep 2010 14:39:15 +0000",157207,"LGTM. In the main.cc part, you could possibly check the part before the dot matches the name of the enum (possibly with the scope qualifier). Not critical.",0,0,neutral
thrift,959,comment_2,This caused a big performance regression see THRIFT-1121,code_debt,slow_algorithm,"Fri, 15 Oct 2010 21:13:09 +0000","Thu, 3 Jan 2019 04:32:08 +0000","Thu, 3 Jan 2019 04:29:35 +0000",259312586,This caused a big performance regression see THRIFT-1121,0.2,0.2,negative
thrift,959,summary,TSocket seems to do its own buffering inefficiently,code_debt,low_quality_code,"Fri, 15 Oct 2010 21:13:09 +0000","Thu, 3 Jan 2019 04:32:08 +0000","Thu, 3 Jan 2019 04:29:35 +0000",259312586,TSocket seems to do its own buffering inefficiently,-0.4,-0.4,negative
thrift,962,comment_5,Brilliant! I was thinking on it but I thought it would be really hard to get done. It'd be awesome to get rid of this redundancy! We would basically be able to compile the web site ;),code_debt,complex_code,"Mon, 18 Oct 2010 23:53:50 +0000","Thu, 10 Jul 2014 13:42:34 +0000","Thu, 10 Jul 2014 13:42:34 +0000",117553724,Brilliant! I was thinking on it but I thought it would be really hard to get done. It'd be awesome to get rid of this redundancy! We would basically be able to compile the web site,0.46,0.496,positive
thrift,992,description,"While updating my FluentCassandra project using the latest Thrift-0.5.0.exe generator against the 0.7 Beta3 release of cassandra.thrift I experienced a generation problem that caused compiler errors when compiling the C# code. It appears the constructor fields are using the old naming convention for the fields, while every other part of the class seems to be using the new naming convention for fields with underscores. You can see a diff of the files I had to manually edit here: The paths starting with should be the ones that you need to worry about for this bug.",code_debt,low_quality_code,"Fri, 5 Nov 2010 12:35:39 +0000","Tue, 1 Nov 2011 02:51:43 +0000","Wed, 10 Nov 2010 21:24:02 +0000",463703,"While updating my FluentCassandra project using the latest Thrift-0.5.0.exe generator against the 0.7 Beta3 release of cassandra.thrift I experienced a generation problem that caused compiler errors when compiling the C# code. It appears the constructor fields are using the old naming convention for the fields, while every other part of the class seems to be using the new naming convention for fields with underscores. You can see a diff of the files I had to manually edit here: https://github.com/managedfusion/fluentcassandra/commit/d2d26f0bfd158cae3c39fd9cd47ec9097bc394f6 The paths starting with ""FluentCassandra/Apache/Cassandra"" should be the ones that you need to worry about for this bug.",-0.1018571429,-0.1018571429,neutral
thrift,992,summary,Naming convention in C# constructor is not consistent with other fields causes compile errors,code_debt,low_quality_code,"Fri, 5 Nov 2010 12:35:39 +0000","Tue, 1 Nov 2011 02:51:43 +0000","Wed, 10 Nov 2010 21:24:02 +0000",463703,Naming convention in C# constructor is not consistent with other fields causes compile errors,0.4,0.4,negative
thrift,1819,comment_3,"Closing, as patch broke things and was never addressed.",defect_debt,uncorrected_known_defects,"Sat, 5 Jan 2013 21:44:49 +0000","Thu, 17 Jan 2019 20:06:08 +0000","Thu, 17 Jan 2019 20:06:08 +0000",190333279,"Closing, as patch broke things and was never addressed.",-0.5625,-0.5625,negative
thrift,21,comment_2,"I'm going to go ahead and say this is a non-issue. I find it perfectly believable that all modern OS's do in fact implement accept() in a thread-safe manner, and even if that is not the case, ThreadPoolServer is going to be obsoleted by NonblockingServer very shortly.",defect_debt,uncorrected_known_defects,"Mon, 26 May 2008 23:45:00 +0000","Thu, 26 Jun 2008 17:33:40 +0000","Thu, 26 Jun 2008 17:33:40 +0000",2656120,"I'm going to go ahead and say this is a non-issue. I find it perfectly believable that all modern OS's do in fact implement accept() in a thread-safe manner, and even if that is not the case, ThreadPoolServer is going to be obsoleted by NonblockingServer very shortly.",0.24375,0.24375,neutral
thrift,2416,comment_1,"You could fix if for the platforms that you care about, and continue to let it error elsewhere. i386 and x64 are the main platforms to worry about, with ARM following behind that. A distant fourth would be IA64.",defect_debt,uncorrected_known_defects,"Thu, 20 Mar 2014 09:20:06 +0000","Thu, 10 Jul 2014 13:36:09 +0000","Mon, 7 Jul 2014 20:07:28 +0000",9456442,"You could fix if for the platforms that you care about, and continue to let it error elsewhere. i386 and x64 are the main platforms to worry about, with ARM following behind that. A distant fourth would be IA64.",-0.1355,-0.1355,neutral
thrift,1055,description,"resulted in TFramedTransport being improved so it wasn't necessary to disable Nagle. For the case of TServerSocket and TSocket, a simple server case, performance is still very poor, ~30 msg/sec vs. ~11k msg/sec with NoDelay = true. Java and cpp disable nagle in their TSocket and TServerSocket implementations and performance is MUCH improved with nagle disabled for csharp so I'm proposing that csharp should also disable nagle.",design_debt,non-optimal_design,"Tue, 8 Feb 2011 15:35:19 +0000","Tue, 8 Feb 2011 16:39:18 +0000","Tue, 8 Feb 2011 16:39:18 +0000",3839,"https://issues.apache.org/jira/browse/THRIFT-904 resulted in TFramedTransport being improved so it wasn't necessary to disable Nagle. For the case of TServerSocket and TSocket, a simple server case, performance is still very poor, ~30 msg/sec vs. ~11k msg/sec with NoDelay = true. Java and cpp disable nagle in their TSocket and TServerSocket implementations and performance is MUCH improved with nagle disabled for csharp so I'm proposing that csharp should also disable nagle.",0.3337777778,0.3337777778,neutral
thrift,1065,comment_3,"apparently the java server is not encoding undeclared exceptions ""properly"" (it could be my impl., please see patch), but it works fine on C++ Incoming content: [cpp] Outgoing content: is a [java] Outgoing content: Error:This is a",design_debt,non-optimal_design,"Thu, 17 Feb 2011 18:30:31 +0000","Wed, 9 Mar 2011 18:17:15 +0000","Tue, 22 Feb 2011 21:04:05 +0000",441214,"apparently the java server is not encoding undeclared exceptions ""properly"" (it could be my impl., please see patch), but it works fine on C++ Incoming content: [1,""testException"",1,0,{""1"":{""str"":""TApplicationException""}}] [cpp] Outgoing content: [1,""testException"",3,0,{""1"": {""str"":""This is a TApplicationException""} ,""2"":{""i32"":0}}] [java] Outgoing content: Error:This is a TApplicationException",0.3285714286,0.1666666667,neutral
thrift,1072,description,"- (id) (id<TProcessor is missing in from the Cocoa headers. Should be added, to let user know that they need to init with this method to delegate stuff to their own processor. Can't see other possibility to make server delegate incomming connections to protocol implementing classes. (no Cocoa server examples available!)",design_debt,non-optimal_design,"Wed, 23 Feb 2011 21:47:42 +0000","Fri, 27 Jan 2012 02:53:24 +0000","Fri, 27 Jan 2012 02:53:23 +0000",29135141,"(id) initWithSharedProcessor: (id<TProcessor>) sharedProcessor; is missing in TSharedProcessorFactory.h from the Cocoa headers. Should be added, to let user know that they need to init with this method to delegate stuff to their own processor. Can't see other possibility to make server delegate incomming connections to protocol implementing classes. (no Cocoa server examples available!)",-0.1,-0.08,negative
thrift,1121,comment_4,"Although this issue is closed and considered fixed, I would like to add my two cents on the combination of TFramedTransport and TSocket. I have worked with Hector, a Cassandra client, which is using Thrift v0.6.1, which is suffering from a large deal of overhead, at least in my analysis. I came to the conclusion that the performance regression is caused - at least in my setup - by overhead on the TCP layer. In my setup I use the binary protocol over the framed transport over the Thrift socket (without buffering in v0.6.1). I discovered, that two TCP segments are being sent for every frame sent. One for the length of the frame and one for the frame itself (or more if the frame is larger than the maximum a TCP segment can hold). With small messages, the overhead is substantial: 56 extra bytes of headers in case of Ethernet + IP + TCP. Also considering that in my setup the PSH flag was raised, causing the 4 bytes length of the frame to be pushed from the TCP stack to the application ... while the data itself is on its way. Now this issue is alleviated by using a buffered output stream in TSocket. However, I do think that this solution causes unnecessary memory overhead in cases where framed transport is used. Then the data is in memory twice. And the data is first written into the framed transport buffer, then written in the buffered output stream of the socket and then written to the TCP stack. The above goes for the writing side of things. As for the reading: it's a system call extra (first read the length and then in a separte call read the frame from the socket). I do not estimate this to account for the big difference in throughput. Of course in cases where the framed transport isn't used, buffered in and output streams are very useful.",design_debt,non-optimal_design,"Mon, 28 Mar 2011 18:06:04 +0000","Thu, 3 Jan 2019 04:32:08 +0000","Tue, 27 Sep 2011 17:08:01 +0000",15807717,"Although this issue is closed and considered fixed, I would like to add my two cents on the combination of TFramedTransport and TSocket. I have worked with Hector, a Cassandra client, which is using Thrift v0.6.1, which is suffering from a large deal of overhead, at least in my analysis. I came to the conclusion that the performance regression is caused - at least in my setup - by overhead on the TCP layer. In my setup I use the binary protocol over the framed transport over the Thrift socket (without buffering in v0.6.1). I discovered, that two TCP segments are being sent for every frame sent. One for the length of the frame and one for the frame itself (or more if the frame is larger than the maximum a TCP segment can hold). With small messages, the overhead is substantial: 56 extra bytes of headers in case of Ethernet + IP + TCP. Also considering that in my setup the PSH flag was raised, causing the 4 bytes length of the frame to be pushed from the TCP stack to the application ... while the data itself is on its way. Now this issue is alleviated by using a buffered output stream in TSocket. However, I do think that this solution causes unnecessary memory overhead in cases where framed transport is used. Then the data is in memory twice. And the data is first written into the framed transport buffer, then written in the buffered output stream of the socket and then written to the TCP stack. The above goes for the writing side of things. As for the reading: it's a system call extra (first read the length and then in a separte call read the frame from the socket). I do not estimate this to account for the big difference in throughput. Of course in cases where the framed transport isn't used, buffered in and output streams are very useful.",0.0561,0.0561,neutral
thrift,1130,comment_0,"Not sure if this is the best fix, but seems to be rather minimalistic By the way - this is my first attempt at contribution here, please let me know if there is some other procedure should be followed.",design_debt,non-optimal_design,"Tue, 5 Apr 2011 23:47:24 +0000","Thu, 13 Oct 2011 21:49:25 +0000","Thu, 13 Oct 2011 21:34:48 +0000",16494444,"Not sure if this is the best fix, but seems to be rather minimalistic By the way - this is my first attempt at contribution here, please let me know if there is some other procedure should be followed.",-0.3375,-0.3375,negative
thrift,1217,description,"As part of an effort to use the Non-Blocking server (using libevent) on Windows, it was necessary to remove the use of ""pipe"" for the notification mechanism to signal end-of-task. We propose to use evutil_socketpair instead (tested with libevent 2.0.12). Patch included. Please see for more details.",design_debt,non-optimal_design,"Thu, 23 Jun 2011 20:39:21 +0000","Thu, 1 Sep 2011 17:41:04 +0000","Fri, 8 Jul 2011 12:45:13 +0000",1267552,"As part of an effort to use the Non-Blocking server (using libevent) on Windows, it was necessary to remove the use of ""pipe"" for the notification mechanism to signal end-of-task. We propose to use evutil_socketpair instead (tested with libevent 2.0.12). Patch included. Please see https://github.com/aubonbeurre/thrift/blob/alex-0.6.1/README.non.blocking.Windows for more details.",0.19075,0.19075,neutral
thrift,1241,comment_0,"Thanks Darius, initial work looks great. Will want to change from namespace53 to just namespace since php5.3+ will have this option available and php5.4 is in rc right now so no sense in defining a specific version in the option. Can you attach this as a patch with asf inclusion please.",design_debt,non-optimal_design,"Wed, 20 Jul 2011 15:24:17 +0000","Wed, 10 Aug 2011 18:27:52 +0000","Thu, 4 Aug 2011 23:00:01 +0000",1323344,"Thanks Darius, initial work looks great. Will want to change from namespace53 to just namespace since php5.3+ will have this option available and php5.4 is in rc right now so no sense in defining a specific version in the option. Can you attach this as a patch with asf inclusion please.",0.2920666667,0.2920666667,positive
thrift,1243,comment_2,"Saw this a bit late but I'm concerned about this change. With this change, there is no way for implementors of to signal to the callers that something unexpected happened or any other error conditions. Just because the current implementation ignores the return values doesn't mean they're not useful. Is the recommendation now to throw a TException instead?",design_debt,non-optimal_design,"Sun, 24 Jul 2011 10:03:04 +0000","Tue, 1 Nov 2011 00:57:57 +0000","Sun, 11 Sep 2011 07:29:12 +0000",4224368,"Saw this a bit late but I'm concerned about this change. With this change, there is no way for implementors of sendMessage/recvMessage to signal to the callers that something unexpected happened or any other error conditions. Just because the current implementation ignores the return values doesn't mean they're not useful. Is the recommendation now to throw a TException instead?",-0.2,-0.2,negative
thrift,1243,comment_3,"@diwaker: Thanks for your comments, I share your concerns and am working currently on a patch for 0.8 that will use exception handling, and more generally handle/propagate libevent errors, log errors... For example I'll propose to remove 'abort' calls in TEvhttpServer.cpp, and replace them by exceptions. Also one significant issue I plan to cover, is to avoid propagating exceptions to libevent (which is C code): see THRIFT-1222 for example.",design_debt,non-optimal_design,"Sun, 24 Jul 2011 10:03:04 +0000","Tue, 1 Nov 2011 00:57:57 +0000","Sun, 11 Sep 2011 07:29:12 +0000",4224368,"@diwaker: Thanks for your comments, I share your concerns and am working currently on a patch for 0.8 that will use exception handling, and more generally handle/propagate libevent errors, log errors... For example I'll propose to remove 'abort' calls in TEvhttpClientChannel.cpp TEvhttpServer.cpp, and replace them by exceptions. Also one significant issue I plan to cover, is to avoid propagating exceptions to libevent (which is C code): see THRIFT-1222 for example.",0.02666666667,0.02,neutral
thrift,1243,description,"[Context: This patch is part of a larger patch to use thriftnb on Windows/VisualC++. See for more details.] When compiling using Visual C++ 2010, the compiler chokes on casting some callbacks bool(*)() to void(*)(). Although probably valid and supported by gcc, this is further complicated by the fact those casting seem unnecessary: for each of the callbacks returning bool in TAsyncChannel.h: 1. the returned value is never checked 2. they always return true Attached is a trivial patch based on 0.7.0, tested on Ubuntu 11.04 and Visual C++ 2010.",design_debt,non-optimal_design,"Sun, 24 Jul 2011 10:03:04 +0000","Tue, 1 Nov 2011 00:57:57 +0000","Sun, 11 Sep 2011 07:29:12 +0000",4224368,"[Context: This patch is part of a larger patch to use thriftnb on Windows/VisualC++. See https://github.com/aubonbeurre/thrift/blob/alex-0.7.0/README.non.blocking.Windows for more details.] When compiling using Visual C++ 2010, the compiler chokes on casting some callbacks bool() to void(). Although probably valid and supported by gcc, this is further complicated by the fact those casting seem unnecessary: for each of the callbacks returning bool in TAsyncChannel.h: 1. the returned value is never checked 2. they always return true Attached is a trivial patch based on 0.7.0, tested on Ubuntu 11.04 and Visual C++ 2010.",0.1150714286,0.1150714286,neutral
thrift,1248,description,"The ensureCanWrite function in TMemoryBuffer currently ""rebases"" the buffer pointers by subtracting the original buffer pointer from the pointer newly returned by realloc. While this seems to work fine on my linux setup(I couldn't force a reproducer), pointer subtraction between pointers that we allocated separately is an undefined operation. I have run into problems with this on platforms other than linux. I am attaching a patch that removes the undefined operation.",design_debt,non-optimal_design,"Fri, 29 Jul 2011 13:39:40 +0000","Fri, 6 May 2016 22:30:17 +0000","Sun, 10 May 2015 12:45:47 +0000",119315167,"The ensureCanWrite function in TMemoryBuffer currently ""rebases"" the buffer pointers by subtracting the original buffer pointer from the pointer newly returned by realloc. While this seems to work fine on my linux setup(I couldn't force a reproducer), pointer subtraction between pointers that we allocated separately is an undefined operation. I have run into problems with this on platforms other than linux. I am attaching a patch that removes the undefined operation.",0.025,0.025,neutral
thrift,1248,summary,pointer subtraction in TMemoryBuffer relies on undefined behavior,design_debt,non-optimal_design,"Fri, 29 Jul 2011 13:39:40 +0000","Fri, 6 May 2016 22:30:17 +0000","Sun, 10 May 2015 12:45:47 +0000",119315167,pointer subtraction in TMemoryBuffer relies on undefined behavior,0,0,neutral
thrift,1269,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Wed, 6 Jan 2010 02:10:19 +0000 Subject: [PATCH 01/33] thrift: handle undeclared exceptions in the async error cob Summary: This updates the generated TAsyncProcessor code to correctly handle when an undeclared exception type is passed into an error callback. It now sends back a T_EXCEPTION message to the client, just like the non-async code does. Previously the exception wouldn't be handled, causing the TEventWorker thread to exit. Test Plan: Tested changing the async sort tutorial to pass in a generic TException rather than a SortError, and verified that the client correctly received an exception. Conflicts:  | 38 1 files changed, 32 insertions(+), 6 deletions(-)",design_debt,non-optimal_design,"Wed, 17 Aug 2011 19:44:00 +0000","Tue, 1 Nov 2011 02:54:26 +0000","Mon, 22 Aug 2011 21:41:11 +0000",439031,"From 25366c3648ef4c2d436641a2051c057abde05975 Mon Sep 17 00:00:00 2001 From: Adam Simpkins <simpkins@fb.com> Date: Wed, 6 Jan 2010 02:10:19 +0000 Subject: [PATCH 01/33] thrift: handle undeclared exceptions in the async error cob Summary: This updates the generated TAsyncProcessor code to correctly handle when an undeclared exception type is passed into an error callback. It now sends back a T_EXCEPTION message to the client, just like the non-async code does. Previously the exception wouldn't be handled, causing the TEventWorker thread to exit. Test Plan: Tested changing the async sort tutorial to pass in a generic TException rather than a SortError, and verified that the client correctly received an exception. Conflicts: compiler/cpp/src/generate/t_cpp_generator.cc  compiler/cpp/src/generate/t_cpp_generator.cc | 38 +++++++++++++++++++++---- 1 files changed, 32 insertions, 6 deletions",0.0042,0.003,neutral
thrift,1269,summary,thrift: handle undeclared exceptions in the async,design_debt,non-optimal_design,"Wed, 17 Aug 2011 19:44:00 +0000","Tue, 1 Nov 2011 02:54:26 +0000","Mon, 22 Aug 2011 21:41:11 +0000",439031,thrift: handle undeclared exceptions in the async,0,0,neutral
thrift,1349,description,There are a couple of spurious calls which can lead to lots of output when talking between slightly different versions of generated code. For instance launching a server which returns a new field to a client will result in the client printing out information about an unknown fid. This leads to lots of logging which you probably don't want. I'd like to remove these unless anyone in dev objects.,design_debt,non-optimal_design,"Mon, 19 Sep 2011 05:07:55 +0000","Thu, 10 Jul 2014 13:42:24 +0000","Thu, 10 Jul 2014 13:42:24 +0000",88590869,There are a couple of spurious error_logger:info_msg/2 calls which can lead to lots of output when talking between slightly different versions of generated code. For instance launching a server which returns a new field to a client will result in the client printing out information about an unknown fid. This leads to lots of logging which you probably don't want. I'd like to remove these unless anyone in dev objects.,-0.21675,-0.183375,negative
thrift,137,comment_6,"Fourth iteration of the GWT patch. Now included is a TGWTProtocol JSON implementation that uses the native GWT JSON parser (compatible with the standard JSONProtocol implementation), and a TGWTProxyGenerator that provides deferred binding support for Thrift cilents. This means that it's now as easy to use Thrift from GWT as using the standard GWT RPC framework. Also, I made some build changes since the last patch which means that a single JAR can be used for both server and client code, but also that I did not have to modify or move any of the existing classes (almost). I suppose the patch is more or less committable, as it is now feature complete, but I'm sure there's a bunch of bugs in the GWT part of it and could definitely use a pair of extra eyes.....",design_debt,non-optimal_design,"Tue, 16 Sep 2008 09:01:30 +0000","Thu, 12 Apr 2012 04:04:58 +0000","Mon, 9 Apr 2012 18:25:15 +0000",112440225,"Fourth iteration of the GWT patch. Now included is a TGWTProtocol JSON implementation that uses the native GWT JSON parser (compatible with the standard JSONProtocol implementation), and a TGWTProxyGenerator that provides deferred binding support for Thrift cilents. This means that it's now as easy to use Thrift from GWT as using the standard GWT RPC framework. Also, I made some build changes since the last patch which means that a single JAR can be used for both server and client code, but also that I did not have to modify or move any of the existing classes (almost). I suppose the patch is more or less committable, as it is now feature complete, but I'm sure there's a bunch of bugs in the GWT part of it and could definitely use a pair of extra eyes.....",0.1609,0.1609,neutral
thrift,137,description,"As previously discussed on the mailinglist, using Thrift from Google Web Toolkit (GWT) applications (AJAX) would be nice, as it does not only allow you to consume existing Thrift services from GWT applications, but also means that you now can write GWT-consumable RPC services in any language (and say host them on Google Appengine) that are practically source-code compatible with the official GWT RPC framework. Doing this presents two challanges: 1) The GWT compiler only supports a subset of the JRE libraries (luckily, this is rather easy to work around). 2) As the A in AJAX hints, the only way of doing RPC is asynchronously, something not supported by Thrift, by using the XMLHttpRequest object in the browser. Here's what I've done (an excerpt from the mailing-list): This solution works really well for my problem, but it's half-assed in two ways. 1) It only allows for asynchronous client transports (as in the case of the XMLHttpRequest object) and not on the server side (with messages coming back in a non-sequential order). 2) I'm not sure how to solve the client library issues. Right now, I've moved the core classes (those required on the client (GWT) side of things) into while keeping everything else where they are. This allows the GWT compiler to translate while using the same jar both on the client and server. This is not very elegant for people not using GWT (which I suppose is 99.99% of the audience) but short of maintaining two separate Java client libraries, I'm not sure how to solve this issue. The attached patch is only for the compiler, and does not produce compilable client code without the modified client library. Just wanted to get some input before producing a somewhat committable patch. Comments? Ideas?",design_debt,non-optimal_design,"Tue, 16 Sep 2008 09:01:30 +0000","Thu, 12 Apr 2012 04:04:58 +0000","Mon, 9 Apr 2012 18:25:15 +0000",112440225,"As previously discussed on the mailinglist, using Thrift from Google Web Toolkit (GWT) applications (AJAX) would be nice, as it does not only allow you to consume existing Thrift services from GWT applications, but also means that you now can write GWT-consumable RPC services in any language (and say host them on Google Appengine) that are practically source-code compatible with the official GWT RPC framework. Doing this presents two challanges: 1) The GWT compiler only supports a subset of the JRE libraries (luckily, this is rather easy to work around). 2) As the A in AJAX hints, the only way of doing RPC is asynchronously, something not supported by Thrift, by using the XMLHttpRequest object in the browser. Here's what I've done (an excerpt from the mailing-list): -snip- 1) Created a stripped down jar of Thrift, axed most protocol, transport and server implementations, in order to get a JavaScript-translatable version of Thrift. I did not need to change any of the base Thrift classes, nor modify the compiler for GWT to translate the structs, but I might have missed something here (Mathias?). 2) Added an option for the Thrift Java compiler to generate asynchronous service interfaces and client proxies. This is manifested as: public class Repository { public interface Iface { public Document get_document(String uri) throws TException; public int get_count() throws TException; } public interface AsyncIface { public void get_document(String uri, TAsyncCallback<Document> callback) throws TException; public void get_count(TAsyncCallback<Integer> callback) throws TException; } ... This is done in line with GWT's RPC framework and gives the developer the standard synchronous interface to implement on the server side (I use it with embedded Jetty in a daemon) and an asynchonous interface to use in the GWT client. AsyncCallback<T> just has a plain onSuccess(T result) method. 3) Implemented a client transport using GWT's RequestBuilder (the XmlHttpRequest abstraction) that executes the TAsyncCallback asynchronously when the response has been received. 4) Modified the JSONProtocol slightly to be fully JavaScript-translatable. This could probably be more efficiently done by using GWT's JSNI framework, but I really haven't had the time to optimize anything yet. -snip- This solution works really well for my problem, but it's half-assed in two ways. 1) It only allows for asynchronous client transports (as in the case of the XMLHttpRequest object) and not on the server side (with messages coming back in a non-sequential order). 2) I'm not sure how to solve the client library issues. Right now, I've moved the core classes (those required on the client (GWT) side of things) into com.facebook.thrift.gwt, while keeping everything else where they are. This allows the GWT compiler to translate com.facebook.thrift.gwt.* while using the same jar both on the client and server. This is not very elegant for people not using GWT (which I suppose is 99.99% of the audience) but short of maintaining two separate Java client libraries, I'm not sure how to solve this issue. The attached patch is only for the compiler, and does not produce compilable client code without the modified client library. Just wanted to get some input before producing a somewhat committable patch. Comments? Ideas?",0.01553461538,-0.03387037037,positive
thrift,1440,comment_0,"Thanks for Reporting that, we like to apply your patches to improve packaging and build procedure in general. The Build Job for Debian Packages is here: currently we only hav a amd64 at Apache Build Infrastructure... Do some simply tricks or cross compile approaches exist to build more architectures on that build server?",design_debt,non-optimal_design,"Mon, 28 Nov 2011 22:14:06 +0000","Thu, 20 Jun 2013 16:12:38 +0000","Thu, 20 Jun 2013 16:12:38 +0000",49226312,"Thanks for Reporting that, we like to apply your patches to improve packaging and build procedure in general. The Build Job for Debian Packages is here: https://builds.apache.org/view/S-Z/view/Thrift/job/Thrift-Debian-Packages/ currently we only hav a amd64 at Apache Build Infrastructure... Do some simply tricks or cross compile approaches exist to build more architectures on that build server?",0.2,0.2,positive
thrift,1452,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Mon, 21 Jun 2010 20:24:50 +0000 Subject: [PATCH 3/5] generate a swap() method for all generated structs Summary: Andrei mentioned it would be convenient if thrift generated swap() methods for all C++ thrift types. Apparently the ads team manually writes swap() functions for many of their thrift data types, but have to keep updating them by hand when new fields are added to the thrift interface. This updates the thrift compiler to emit swap() methods for user-defined types. For now, I decided not to emit swap() methods for the internal XXX_args, XXX_pargs, XXX_result, and XXX_presult types. Test Plan: Tested compiling serveral internal projects. I didn't actually test the generated swap() functions, but they look okay. DiffCamp Revision: 124773 Reviewed By: aalexandre Commenters: dreiss, edhall CC: davidrecordon, achao, dreiss, kholst, aalexandre, simpkins, edhall, thrift-team@lists Revert Plan: OK git-svn-id: fb thing]@30392  | 70 1 files changed, 67 insertions(+), 3 deletions(-)",design_debt,non-optimal_design,"Wed, 7 Dec 2011 22:17:56 +0000","Fri, 9 Dec 2011 21:27:30 +0000","Thu, 8 Dec 2011 21:16:16 +0000",82700,"From baa275da65e023af50930a75f9a7ef2a991cdaef Mon Sep 17 00:00:00 2001 From: Adam Simpkins <simpkins@fb.com> Date: Mon, 21 Jun 2010 20:24:50 +0000 Subject: [PATCH 3/5] generate a swap() method for all generated structs Summary: Andrei mentioned it would be convenient if thrift generated swap() methods for all C++ thrift types. Apparently the ads team manually writes swap() functions for many of their thrift data types, but have to keep updating them by hand when new fields are added to the thrift interface. This updates the thrift compiler to emit swap() methods for user-defined types. For now, I decided not to emit swap() methods for the internal XXX_args, XXX_pargs, XXX_result, and XXX_presult types. Test Plan: Tested compiling serveral internal projects. I didn't actually test the generated swap() functions, but they look okay. DiffCamp Revision: 124773 Reviewed By: aalexandre Commenters: dreiss, edhall CC: davidrecordon, achao, dreiss, kholst, aalexandre, simpkins, edhall, thrift-team@lists Revert Plan: OK git-svn-id: svn+ssh://tubbs/svnapps/fbomb/trunk/[internal fb thing]@30392 2248de34-8caa-4a3c-bc55-5e52d9d7b73a  compiler/cpp/src/generate/t_cpp_generator.cc | 70 ++++++++++++++++++++++++- 1 files changed, 67 insertions, 3 deletions",0.4821428571,0.35,neutral
thrift,1533,description,should implement the {{Closable}} interface. Doing so will allow users to perform,design_debt,non-optimal_design,"Sat, 3 Mar 2012 16:37:56 +0000","Sat, 20 Feb 2021 15:27:43 +0000","Fri, 8 Apr 2016 05:45:42 +0000",129301666,org.apache.thrift.transport.TTransport should implement the Closable interface. Doing so will allow users to perform IOUtils.closeQuietly(transport);,0.1,0.02857142857,neutral
thrift,1533,summary,Make TTransport should be Closeable,design_debt,non-optimal_design,"Sat, 3 Mar 2012 16:37:56 +0000","Sat, 20 Feb 2021 15:27:43 +0000","Fri, 8 Apr 2016 05:45:42 +0000",129301666,Make TTransport should be Closeable,0,0,neutral
thrift,1595,description,The implementation of the java test server should be the same as the languages so our cross platform tests can work.,design_debt,non-optimal_design,"Fri, 4 May 2012 19:34:16 +0000","Sat, 8 Jun 2013 03:15:47 +0000","Sat, 8 Jun 2013 03:15:47 +0000",34501291,The implementation of the java test server should be the same as the languages so our cross platform tests can work.,0.6,0.6,neutral
thrift,163,comment_2,"This is actually way more primitive than I remembered. If I recall properly, it successfully builds the generators as shared objects. Since this was written, we've modified most of the generators to use a dynamic registry, which was one of the big hurdles to getting this to work. I think the big chunks that remain to be done are first modifying the compiler to load the appropriate shared objects with libltdl and investigating whether it is possible to compile some of the objects directly into the compiler so that we don't have to worry about library search paths.",design_debt,non-optimal_design,"Wed, 8 Oct 2008 18:46:10 +0000","Thu, 12 Apr 2012 04:04:50 +0000","Mon, 9 Apr 2012 18:47:28 +0000",110505678,"This is actually way more primitive than I remembered. If I recall properly, it successfully builds the generators as shared objects. Since this was written, we've modified most of the generators to use a dynamic registry, which was one of the big hurdles to getting this to work. I think the big chunks that remain to be done are first modifying the compiler to load the appropriate shared objects with libltdl and investigating whether it is possible to compile some of the objects directly into the compiler so that we don't have to worry about library search paths.",0.2945625,0.2945625,neutral
thrift,1672,description,"I'm trying to use Thrift C# library under MonoTouch. Seems like everything is working from the box. The only issue is Class THttpHandler using System.Web namespace to reach IHttpHandler interface and HttpContext class. Unfortunately, that namespace is not implemented for MonoTouch framework. Is it possible to implement THttpHandler in other way? Note that issue in Xamarin's Bugzilla is here:",design_debt,non-optimal_design,"Wed, 8 Aug 2012 06:37:47 +0000","Sat, 18 Aug 2012 18:53:18 +0000","Sat, 18 Aug 2012 18:53:18 +0000",908131,"I'm trying to use Thrift C# library under MonoTouch. Seems like everything is working from the box. The only issue is /src/Transport/THttpHandler.cs. Class THttpHandler using System.Web namespace to reach IHttpHandler interface and HttpContext class. Unfortunately, that namespace is not implemented for MonoTouch framework. Is it possible to implement THttpHandler in other way? Note that issue in Xamarin's Bugzilla is here: https://bugzilla.xamarin.com/show_bug.cgi?id=6420",-0.06428571429,-0.05,neutral
thrift,1799,description,"Improvements to HTML Generator * Removed HTML page name at <a href=""..."" * Option added to embed CSS inline instead of style.css. Both improvements are quite helpful in some situations, e.g. when the HTML is accessed in a way other than via the original file name, or when the separate CSS file cannot be provided along with the HTML for some reason.",design_debt,non-optimal_design,"Fri, 21 Dec 2012 20:16:32 +0000","Sat, 8 Jun 2013 03:15:51 +0000","Sat, 8 Jun 2013 03:15:50 +0000",14540358,"Improvements to HTML Generator Removed HTML page name at <a href=""...""> links for targets located in the same file Option added to embed CSS inline instead of style.css. Both improvements are quite helpful in some situations, e.g. when the HTML is accessed in a way other than via the original file name, or when the separate CSS file cannot be provided along with the HTML for some reason.",0.2083333333,0.2083333333,positive
thrift,1829,description,"If you attempt to build the cpp unit tests using 'make -j x' the build will fail as there's a race between the unit test code targets depending on the thrift-generated code targets. I think the real fix would be to use one of the approaches described in the 'Multiple Outputs' section of the automake manual (see I experimented with one of them and it seemed to help but never got it quite working. However an easier workaround is to simply disable parallel builds by using the "".NOTPARALLEL"" special target which forces make to run serially. from .NOTPARALLEL If .NOTPARALLEL is mentioned as a target, then this invocation of make will be run serially, even if the -j option is given. Any recursively invoked make command will still run recipes in parallel (unless its makefile also contains this target). Any prerequisites on this target are ignored. That's the approach I ended up taking as I'm short on time.",design_debt,non-optimal_design,"Tue, 15 Jan 2013 14:51:47 +0000","Sun, 17 Feb 2013 19:49:22 +0000","Tue, 15 Jan 2013 22:22:24 +0000",27037,"If you attempt to build the cpp unit tests using 'make -j x' the build will fail as there's a race between the unit test code targets depending on the thrift-generated code targets. I think the real fix would be to use one of the approaches described in the 'Multiple Outputs' section of the automake manual (see http://www.gnu.org/software/automake/manual/html_node/Multiple-Outputs.html). I experimented with one of them and it seemed to help but never got it quite working. However an easier workaround is to simply disable parallel builds by using the "".NOTPARALLEL"" special target which forces make to run serially. from http://www.gnu.org/software/make/manual/html_node/Special-Targets.html .NOTPARALLEL If .NOTPARALLEL is mentioned as a target, then this invocation of make will be run serially, even if the -j option is given. Any recursively invoked make command will still run recipes in parallel (unless its makefile also contains this target). Any prerequisites on this target are ignored. That's the approach I ended up taking as I'm short on time.",0.0381,0.03463636364,negative
thrift,1924,summary,Delphi: Inconsistency in serialization of optional fields,design_debt,non-optimal_design,"Tue, 9 Apr 2013 20:19:53 +0000","Sat, 8 Jun 2013 03:15:46 +0000","Sat, 8 Jun 2013 03:15:46 +0000",5122553,Delphi: Inconsistency in serialization of optional fields,0,0,neutral
thrift,1932,comment_1,"Hi Roger, here is the patch you requested. It does, however, only suppress the type-punned pointer warning. Since I'm new to Thrift, I'm quite unsure if the sequence of bytes read from file handle 'fd_' is already in host byte order. If this is not the case the patch is still wrong",design_debt,non-optimal_design,"Thu, 18 Apr 2013 16:10:01 +0000","Mon, 20 May 2013 03:13:13 +0000","Sun, 5 May 2013 21:38:30 +0000",1488509,"Hi Roger, here is the patch you requested. It does, however, only suppress the type-punned pointer warning. Since I'm new to Thrift, I'm quite unsure if the sequence of bytes read from file handle 'fd_' is already in host byte order. If this is not the case the patch is still wrong",-0.255375,-0.255375,negative
thrift,2032,comment_1,"Meanwhile I'm in doubt if IDisposable for the Iface is really such a good idea. To the class yes, but the Iface breaks Server code due to the missing method. The only reason for this was the using() use case, but maybe that's not enough of a reason here.",design_debt,non-optimal_design,"Sat, 15 Jun 2013 01:16:56 +0000","Thu, 25 Jul 2013 03:40:04 +0000","Tue, 25 Jun 2013 20:23:29 +0000",932793,"Meanwhile I'm in doubt if IDisposable for the Iface is really such a good idea. To the class yes, but the Iface breaks Server code due to the missing method. The only reason for this was the using() use case, but maybe that's not enough of a reason here.",-0.104,-0.104,negative
thrift,2032,comment_2,"Just to clarify, you are talking about the Client and the Processor; the IDisposable for the Client's Iface makes sense, but the IDisposable for the Processor's Iface doesn't. Unfortunately, they are the same Iface, so we need to find the correct middle ground. I'm much more in favor of having the Iface not be IDisposable, as it lowers the maintanence, with the client being explicitly IDisposable. I'm unsure of how we currently clean up state from the processor, but I don't think that IDispoable makes the most sense.",design_debt,non-optimal_design,"Sat, 15 Jun 2013 01:16:56 +0000","Thu, 25 Jul 2013 03:40:04 +0000","Tue, 25 Jun 2013 20:23:29 +0000",932793,"Just to clarify, you are talking about the Client and the Processor; the IDisposable for the Client's Iface makes sense, but the IDisposable for the Processor's Iface doesn't. Unfortunately, they are the same Iface, so we need to find the correct middle ground. I'm much more in favor of having the Iface not be IDisposable, as it lowers the maintanence, with the client being explicitly IDisposable. I'm unsure of how we currently clean up state from the processor, but I don't think that IDispoable makes the most sense.",0.09225,0.09225,neutral
thrift,2032,description,"The C# client code does not correctly clean up the transport used, so the programmer has to take care on his own about this. This may even lead to a program hang in certain scenarios. Furthermore, the generated client should support IDisposable. Note that in contrast, the server side handles this automatically without any explicit manual coding. TODO: * modify generated code to add IDisposable support * modify TProtocol to add IDisposable support * update the tutorial code accordingly",design_debt,non-optimal_design,"Sat, 15 Jun 2013 01:16:56 +0000","Thu, 25 Jul 2013 03:40:04 +0000","Tue, 25 Jun 2013 20:23:29 +0000",932793,"The C# client code does not correctly clean up the transport used, so the programmer has to take care on his own about this. This may even lead to a program hang in certain scenarios. Furthermore, the generated client should support IDisposable. Note that in contrast, the server side handles this automatically without any explicit manual coding. TODO: modify generated code to add IDisposable support modify TProtocol to add IDisposable support update the tutorial code accordingly",0.06666666667,0.06666666667,negative
thrift,2032,summary,C# client leaks sockets/handles,design_debt,non-optimal_design,"Sat, 15 Jun 2013 01:16:56 +0000","Thu, 25 Jul 2013 03:40:04 +0000","Tue, 25 Jun 2013 20:23:29 +0000",932793,C# client leaks sockets/handles,0,0,neutral
thrift,2116,description,Cocoa generator improvements relating to * *Server-side exception handling* in the actual TProcessor implementation for the service * Better handling of *service inheritance* ** Service clients can now use the client subclass directly to invoke calls to the base service ** Servers must only adopt the child protocol and will automatically adopt the base protocol over inheritance ** The improvement should not break any existing service implementations See the revisions for the implemented cocoa tutorial for instance * For a better overview of the resulting gen-files after the improvements see the attached [diff An *implementation* for the *Thrift tutorial* * Server and client implementation based on the corresponding Python version * Update relating to the improved handling of service inheritance,design_debt,non-optimal_design,"Sun, 11 Aug 2013 14:49:54 +0000","Thu, 10 Oct 2019 23:00:05 +0000","Mon, 14 Jan 2019 15:03:33 +0000",171245619,Cocoa generator improvements relating to Server-side exception handling in the actual TProcessor implementation for the service Better handling of service inheritance Service clients can now use the client subclass directly to invoke calls to the base service Servers must only adopt the child protocol and will automatically adopt the base protocol over inheritance The improvement should not break any existing service implementations See the revisions for the implemented cocoa tutorial for instance For a better overview of the resulting gen-files after the improvements see the attached diff file An implementation for the Thrift tutorial Server and client implementation based on the corresponding Python version Update relating to the improved handling of service inheritance,0.1488571429,0.1488571429,neutral
thrift,211,comment_2,"1/ Unify the setup of {Starter, Opts} so that you don't have to do the monitor keysearch twice. 2/ Consider checking the source of the DOWN message ... having an unverified DOWN message could lead to some very hard-to-trace bugs. Lg otherwise. Also, what do you think about converting all the Erl unit tests to use eunit at some point (now that it's standard as of R12B) ?",design_debt,non-optimal_design,"Thu, 20 Nov 2008 22:30:30 +0000","Tue, 1 Nov 2011 02:52:21 +0000","Thu, 4 Jun 2009 02:01:37 +0000",16860667,"1/ Unify the setup of {Starter, Opts} so that you don't have to do the monitor keysearch twice. 2/ Consider checking the source of the DOWN message ... having an unverified DOWN message could lead to some very hard-to-trace bugs. Lg otherwise. Also, what do you think about converting all the Erl unit tests to use eunit at some point (now that it's standard as of R12B) ?",-0.0880625,-0.0880625,neutral
thrift,2210,comment_8,Unfortunately the change caused the Hive object not able to be serialized into JSON with Here is the struct: struct SkewedInfo { 1: list<string 2: list<list<string 3: map<list<string} The field is defined as a map with list as the key. The structure can be serialized and deserialized correctly with libthrift 0.9.1. This change regressed the logic. People do use TSimpleJSONProtocol to serialize Hive objects for audit logs or such. This is a blocker for thrift upgrading. Could we remove the check for the case when the key is a 'list'? Thanks!,design_debt,non-optimal_design,"Thu, 26 Sep 2013 06:37:02 +0000","Sun, 6 Mar 2016 17:25:17 +0000","Fri, 27 Sep 2013 14:13:50 +0000",113808,"Unfortunately the change caused the Hive object not able to be serialized into JSON with TSimpleJSONProtocol: https://github.com/apache/hive/blob/master/metastore/if/hive_metastore.thrift#L237 Here is the struct: struct SkewedInfo { 1: list<string> skewedColNames, // skewed column names 2: list<list<string>> skewedColValues, //skewed values 3: map<list<string>, string> skewedColValueLocationMaps, //skewed value to location mappings } The skewedColValueLocationMaps field is defined as a map with list as the key. The structure can be serialized and deserialized correctly with libthrift 0.9.1. This change regressed the logic. People do use TSimpleJSONProtocol to serialize Hive objects for audit logs or such. This is a blocker for thrift upgrading. Could we remove the check for the case when the key is a 'list'? Thanks!",0.1428571429,0.1642857143,negative
thrift,221,description,"The way that the current Java library and test builds are set up, they rely on an external jar that's assumed to be at some specific path. This isn't where the jar resides on my machine, so every time I want to do some building, I have to hand edit the file, and then when I want to generate a diff for posting on jira, I have to revert the changes. We should make it so the class path is configurable in a way that doesn't require checkins. There are a couple different ways to do this, but a properties file with the classpath is probably one of the easiest.",design_debt,non-optimal_design,"Wed, 3 Dec 2008 18:31:15 +0000","Tue, 1 Nov 2011 02:54:09 +0000","Thu, 29 Jan 2009 01:46:28 +0000",4864513,"The way that the current Java library and test builds are set up, they rely on an external jar that's assumed to be at some specific path. This isn't where the jar resides on my machine, so every time I want to do some building, I have to hand edit the file, and then when I want to generate a diff for posting on jira, I have to revert the changes. We should make it so the class path is configurable in a way that doesn't require checkins. There are a couple different ways to do this, but a properties file with the classpath is probably one of the easiest.",-0.1375,-0.1375,neutral
thrift,221,summary,Make java build classpath more dynamic and configurable,design_debt,non-optimal_design,"Wed, 3 Dec 2008 18:31:15 +0000","Tue, 1 Nov 2011 02:54:09 +0000","Thu, 29 Jan 2009 01:46:28 +0000",4864513,Make java build classpath more dynamic and configurable,0,0,neutral
thrift,2225,description,destroy the SSLContext before cleanup the openSSL to avoid recreating some openssl internal resource.,design_debt,non-optimal_design,"Wed, 9 Oct 2013 19:59:14 +0000","Mon, 2 Apr 2018 22:53:08 +0000","Sun, 2 Feb 2014 22:57:23 +0000",10033089,destroy the SSLContext before cleanup the openSSL to avoid recreating some openssl internal resource.,-0.4,-0.4,neutral
thrift,2255,description,"Cpp generated Class of struct dosn't have a parent Class, this will cause the Program hold the Object must be using The instance class direct. The parent class may be helper full in for program to process manay type Struct batch. I hope the New version of Thrift implement it.",design_debt,non-optimal_design,"Thu, 7 Nov 2013 02:08:12 +0000","Thu, 10 Jul 2014 13:42:32 +0000","Thu, 10 Jul 2014 13:42:32 +0000",21209660,"Cpp generated Class of struct dosn't have a parent Class, this will cause the Program hold the Object must be using The instance class direct. The parent class may be helper full in for program to process manay type Struct batch. I hope the New version of Thrift implement it.",0.073,0.073,neutral
thrift,2263,description,Previously Thrift only generated high-quality hashCode implementations when configured due to the dependency on commons-lang3. After THRIFT-2260 we no long have this dependency and can unconditionally give the better implementation.,design_debt,non-optimal_design,"Sat, 16 Nov 2013 20:52:15 +0000","Sun, 14 Feb 2016 01:56:13 +0000","Sun, 8 Dec 2013 21:13:44 +0000",1902089,Previously Thrift only generated high-quality hashCode implementations when configured due to the dependency on commons-lang3. After THRIFT-2260 we no long have this dependency and can unconditionally give the better implementation.,0,0,neutral
thrift,2279,description,"TSerializer copies from the transport into a 1024 length byte array and returns the array, which will effectively clips off everything after the first 1k. The attached patch instead returns a copy of the underlying memory buffer used by the transport.",design_debt,non-optimal_design,"Mon, 2 Dec 2013 18:59:41 +0000","Fri, 13 Dec 2013 16:41:36 +0000","Tue, 3 Dec 2013 22:00:11 +0000",97230,"TSerializer copies from the transport into a 1024 length byte array and returns the array, which will effectively clips off everything after the first 1k. The attached patch instead returns a copy of the underlying memory buffer used by the transport.",0.4,0.4,neutral
thrift,240,description,"Now that we have a deep copy constructor, TBase should implement cloneable.",design_debt,non-optimal_design,"Sat, 20 Dec 2008 00:00:30 +0000","Tue, 1 Nov 2011 02:53:55 +0000","Thu, 22 Jan 2009 16:30:27 +0000",2910597,"Now that we have a deep copy constructor, TBase should implement cloneable.",0,0,neutral
thrift,2415,description,"The performance of the named pipes Delphi server is sub-optimal. Furthermore, BYTE message modes should be used (instead of MESSAGE)",design_debt,non-optimal_design,"Wed, 19 Mar 2014 22:35:18 +0000","Tue, 1 Apr 2014 20:02:22 +0000","Thu, 20 Mar 2014 20:52:07 +0000",80209,"The performance of the named pipes Delphi server is sub-optimal. Furthermore, BYTE message modes should be used (instead of MESSAGE)",0,0,negative
thrift,2431,comment_1,"Agree, this is a real issue; I'm not convinced adding timing based checks here is good for the Travis CI build system either. They should probably be removed. I'm sure the intention was to find poorly performing code paths, however that should be done with callgrind, not with a unit test.",design_debt,non-optimal_design,"Sun, 30 Mar 2014 05:34:11 +0000","Wed, 6 Jan 2016 02:06:07 +0000","Sun, 29 Nov 2015 16:28:18 +0000",52656847,"Agree, this is a real issue; I'm not convinced adding timing based checks here is good for the Travis CI build system either. They should probably be removed. I'm sure the intention was to find poorly performing code paths, however that should be done with callgrind, not with a unit test.",-0.2794444444,-0.2794444444,negative
thrift,2540,comment_0,"There is a simple workaround for this: It should be clear that this workaround is not ideal, and the configure script should be fixed.",design_debt,non-optimal_design,"Fri, 23 May 2014 10:39:14 +0000","Tue, 21 Jul 2015 02:21:13 +0000","Mon, 23 Mar 2015 17:42:16 +0000",26290982,"There is a simple workaround for this: It should be clear that this workaround is not ideal, and the configure script should be fixed.",0,0,negative
thrift,2555,description,produces which starts to become annoying the larger the gaps between the numbers are.,design_debt,non-optimal_design,"Wed, 28 May 2014 22:46:01 +0000","Fri, 30 May 2014 16:27:52 +0000","Fri, 30 May 2014 15:55:32 +0000",148171,produces which starts to become annoying the larger the gaps between the numbers are.,-1,-1,negative
thrift,255,description,is a thin wrapper around TFDTransport that simplifies the process of opening a file.,design_debt,non-optimal_design,"Sat, 10 Jan 2009 01:07:06 +0000","Tue, 1 Nov 2011 02:52:03 +0000","Thu, 26 Mar 2009 06:23:50 +0000",6499004,TSimpleFileTransport is a thin wrapper around TFDTransport that simplifies the process of opening a file.,0,0,neutral
thrift,2568,description,"Enhance the C# TLS-Transport with a paramater to give it a specific certificate handler function. This enables to easely accept untrusted certificates or accept a specific certificate, which can be useful while debuging.",design_debt,non-optimal_design,"Thu, 5 Jun 2014 08:51:11 +0000","Thu, 5 Jun 2014 20:34:50 +0000","Thu, 5 Jun 2014 20:04:49 +0000",40418,"Enhance the C# TLS-Transport with a paramater to give it a specific certificate handler function. This enables to easely accept untrusted certificates or accept a specific certificate, which can be useful while debuging.",0.15,0.15,neutral
thrift,2636,description,"The class has ""type"" and ""message"" fields, but these are not exposed as GObject properties. Instead clients are expected to modify the object's member variables directly&mdash;a bad practice. The attached patch exposes the two fields as GObject properties (and adds relevant test cases), allowing clients to set and access these fields in a conventional manner.",design_debt,non-optimal_design,"Thu, 24 Jul 2014 20:58:30 +0000","Wed, 5 Nov 2014 04:48:25 +0000","Thu, 24 Jul 2014 21:59:41 +0000",3671,"The ThriftApplicationException class has ""type"" and ""message"" fields, but these are not exposed as GObject properties. Instead clients are expected to modify the object's member variables directlya bad practice. The attached patch exposes the two fields as GObject properties (and adds relevant test cases), allowing clients to set and access these fields in a conventional manner.",-0.1936666667,-0.1936666667,negative
thrift,2666,comment_0,Replaced {{PYTHONHASHSEED}} by a literal. Any better solution is welcome.,design_debt,non-optimal_design,"Thu, 14 Aug 2014 20:14:46 +0000","Wed, 5 Nov 2014 04:48:58 +0000","Mon, 1 Sep 2014 21:08:54 +0000",1558448,Replaced PYTHONHASHSEED by a literal. Any better solution is welcome.,0.2416666667,0.2416666667,neutral
thrift,2666,description,"The hash function introduced with THRIFT-2621 breaks with Python older than 3.2, because PYTHONHASHSEED has been introduced with 3.2. The proposed solution is to define a constant PYTHONHASHSEED as a workaround, but only if PYTHONHASHSEED is not available.",design_debt,non-optimal_design,"Thu, 14 Aug 2014 20:14:46 +0000","Wed, 5 Nov 2014 04:48:58 +0000","Mon, 1 Sep 2014 21:08:54 +0000",1558448,"The hash function introduced with THRIFT-2621 breaks with Python older than 3.2, because PYTHONHASHSEED has been introduced with 3.2. The proposed solution is to define a constant PYTHONHASHSEED as a workaround, but only if PYTHONHASHSEED is not available.",0.1,0.1,neutral
thrift,2781,comment_1,"The idlgen feature is not strictly required for Thrift. So as a workaround, the patch simply removes it from the build run.",design_debt,non-optimal_design,"Fri, 10 Oct 2014 17:22:03 +0000","Wed, 5 Nov 2014 04:48:31 +0000","Fri, 10 Oct 2014 17:33:47 +0000",704,"The idlgen feature is not strictly required for Thrift. So as a workaround, the patch simply removes it from the build run.",0,0,neutral
thrift,2791,comment_5,"This is not the appropriate way to add buffering to the server. When you create your server, you can do something like transport, protocolFactory) and it will buffer the sockets it creates. After this patch, my code as above is creating a double layer of buffers, and that is not a good change. Can we please revert this patch and encourage creating buffered server sockets the correct way?",design_debt,non-optimal_design,"Fri, 24 Oct 2014 16:50:39 +0000","Mon, 10 Nov 2014 23:14:30 +0000","Wed, 29 Oct 2014 17:58:42 +0000",436083,"This is not the appropriate way to add buffering to the server. When you create your server, you can do something like thrift.NewTSimpleServer4(processor, transport, thrift.NewTBufferedTransportFactory(1024), protocolFactory) and it will buffer the sockets it creates. After this patch, my code as above is creating a double layer of buffers, and that is not a good change. Can we please revert this patch and encourage creating buffered server sockets the correct way?",-0.12875,-0.08583333333,negative
thrift,2833,comment_4,"Hey , our current java package/deploy is not the nicest and is using a mix of ant and maven. I have a ticket open and would like to switch everything over to using maven for the next 0.9.3 release and take care of all these types of issues in the java* packages of Thrift",design_debt,non-optimal_design,"Mon, 17 Nov 2014 13:56:36 +0000","Mon, 26 Jan 2015 01:56:37 +0000","Tue, 18 Nov 2014 18:35:48 +0000",103152,"Hey Zlika, our current java package/deploy is not the nicest and is using a mix of ant and maven. I have a ticket open and would like to switch everything over to using maven for the next 0.9.3 release and take care of all these types of issues in the java* packages of Thrift",0.2,0.2,negative
thrift,2851,comment_0,"It may not be the best way how it is implemented, but {{Peek()}} is a standard functionality for all Thrift transports. Throughout the library the core concepts and the implementations should be as consistent as possible. Even though {{Peek()}} is not listed (note that the list is marked as not exhaustive), IMHO making it better = @all: Other opinions welcome, though.",design_debt,non-optimal_design,"Mon, 24 Nov 2014 18:49:14 +0000","Mon, 8 Dec 2014 20:35:00 +0000","Tue, 25 Nov 2014 20:48:59 +0000",93585,"If there are useless, we can remove them right? It may not be the best way how it is implemented, but Peek() is a standard functionality for all Thrift transports. Throughout the library the core concepts and the implementations should be as consistent as possible. Even though Peek() is not listed here (note that the list is marked as not exhaustive), IMHO making it better => yes, removing => no. @all: Other opinions welcome, though.",0.375,0.294,neutral
thrift,2932,comment_1,"Both of these libs predate my involvement in Thrift but when first exposed to them a couple years ago they were primordial and supporting only very limited functionality. The browser lib only did JSON/XHR/HTTP and Node only did Binary/Socket so they could not talk to each other (browser was/is tested against Java). No npm or bower support at that time either. The libs are now out on npm and bower (which is a mess due to the whole repo download thing). Compact and JSON protocols were added to Node. Many things that didn't work at all were fixed and lots of tests were added. Other than the tests (which were needed just to get things working more broadly) all of this work has been feature oriented and incremental. People tend to provide patches to fix or add things they need, not clean up stuff that is a mess (partly because it might break existing code, for example changing the call back interface in the browser to add error objects). So we have some baggage... That said, you are not alone. Many would like to see the kind of improvements mentioned here and in other tickets. There's still time to make a big push before Thrift v1.0. Patches welcome!",design_debt,non-optimal_design,"Wed, 7 Jan 2015 05:51:21 +0000","Tue, 21 Jul 2015 02:21:10 +0000","Sat, 14 Feb 2015 23:02:36 +0000",3345075,"Both of these libs predate my involvement in Thrift but when first exposed to them a couple years ago they were primordial and supporting only very limited functionality. The browser lib only did JSON/XHR/HTTP and Node only did Binary/Socket so they could not talk to each other (browser was/is tested against Java). No npm or bower support at that time either. The libs are now out on npm and bower (which is a mess due to the whole repo download thing). Compact and JSON protocols were added to Node. Many things that didn't work at all were fixed and lots of tests were added. Other than the tests (which were needed just to get things working more broadly) all of this work has been feature oriented and incremental. People tend to provide patches to fix or add things they need, not clean up stuff that is a mess (partly because it might break existing code, for example changing the call back interface in the browser to add error objects). So we have some baggage... That said, you are not alone. Many would like to see the kind of improvements mentioned here and in other tickets. There's still time to make a big push before Thrift v1.0. Patches welcome!",-0.01326923077,-0.01326923077,neutral
thrift,2932,comment_3,"Hey Andrew, Thanks for the consolidated patch. Several nice improvements here. I am concerned about the new sequence id modification though. It propagates the sequencing data into the transport. This may seem reasonable in isolation but the transport should not be involved at this level of abstraction. In Apache Thrift, Protocols are responsible for the physical layout of bits (on the wire, on disk or in memory). This includes where, how and if sequence numbers are packaged. Transports are responsible for transmission of the data only. This may involve packetizing, framing, encrypting or what have you, but the key is that the Protocol and the Transport are isolated. The Transport receives/returns an opaque block of bits from/to the Protocol. The current node implementation is not clean in this regards and ultimately needs some structural refactoring. For example the multiplexing read implementation is a hack (paying no attention to the service name in the payload and rather wiring everything to seqid lookups). I think we should try to migrate things in the direction of the generalized Apache Thrift model. This is non-trivial due to the pure async approach of JavaScript (a late model option in the reference CPP and Java implementations). The heart of the node problem here is that Apache Thrift is a layered system. Lower layers provide services to upper layers and services do what you tell them, they dont call you with demands. The node implementation wires callbacks from the bottom up (with several call chains back down from proto to trans in the process). These conflicting approaches are merged with unsatisfactory results in my opinion. To respect the Apache Thrift layered approach the data event needs to be handled by the client (or by a much more focused connection helper) and this piece of code needs to then call down the layered stack like all other languages (client- In short, getting the node client side call return in order is a fair size chunk of work and should fall into a separate issue.",design_debt,non-optimal_design,"Wed, 7 Jan 2015 05:51:21 +0000","Tue, 21 Jul 2015 02:21:10 +0000","Sat, 14 Feb 2015 23:02:36 +0000",3345075,"Hey Andrew, Thanks for the consolidated patch. Several nice improvements here. I am concerned about the new sequence id modification though. It propagates the sequencing data into the transport. This may seem reasonable in isolation but the transport should not be involved at this level of abstraction. In Apache Thrift, Protocols are responsible for the physical layout of bits (on the wire, on disk or in memory). This includes where, how and if sequence numbers are packaged. Transports are responsible for transmission of the data only. This may involve packetizing, framing, encrypting or what have you, but the key is that the Protocol and the Transport are isolated. The Transport receives/returns an opaque block of bits from/to the Protocol. The current node implementation is not clean in this regards and ultimately needs some structural refactoring. For example the multiplexing read implementation is a hack (paying no attention to the service name in the payload and rather wiring everything to seqid lookups). I think we should try to migrate things in the direction of the generalized Apache Thrift model. This is non-trivial due to the pure async approach of JavaScript (a late model option in the reference CPP and Java implementations). The heart of the node problem here is that Apache Thrift is a layered system. Lower layers provide services to upper layers and services do what you tell them, they dont call you with demands. The node implementation wires callbacks from the bottom up (with several call chains back down from proto to trans in the process). These conflicting approaches are merged with unsatisfactory results in my opinion. To respect the Apache Thrift layered approach the data event needs to be handled by the client (or by a much more focused connection helper) and this piece of code needs to then call down the layered stack like all other languages (client->proto->trans) for reads or writes. In short, getting the node client side call return in order is a fair size chunk of work and should fall into a separate issue. Specific comments (line nos post patch): ---------------------------------------------------------- lib/nodejs/lib/thrift/http_connection.js LINE 157: This is an exotic case where the client does not implement a receive handler for the server message. An extreme edge case that by definition can have no callback, so emit error is probably the only reasonable out. Also keep in mind that prior to deserializing at the protocol level we have no idea what the transmitted seqid is. Without a seqid we cannot know the callback to invoke. lib/nodejs/lib/thrift/http_connection.js LINE 184: Here we are communicating normal application behavior using exceptions, which always smells a bit (this is not your code but it is making things murky here). If a real exception arises here it is appropriately scoped to the client rather than the call. It will be something like the connection failed. Call context exceptions should be transmitted by the protocol, received successfully around line 145 and presented to the callback in the err position. Anything else should be handled at a higher level of abstraction. If I am writing a client I dont want to have to handle call failure in every callback, that logic should be provided in a single error event handler across all calls. lib/nodejs/lib/thrift/thrift.js LINE 148: This appears to be an in house utility, also ""self.called"" never appears to be set anywhere. If you are fixing a specific problem in this code or if I am missing the point let me know and maybe give me an example case to look over. In the meantime Im going to patch commit the hunks that are clear improvements here (nextTick additions and the like). You can close this ticket and open new specific tickets for individual issues not handled here or we can carry on here, up to you. I will attach the partial patch committed. Best, Randy",0.06996491228,0.079075,neutral
thrift,2932,description,"I've been investigating using the Node.js client in a project however it seems like there are instances which don't follow Node.js best practices. In particular http_connection.js and connection.js throw errors during callbacks. This is considered an anti-pattern in Node because it both removes the Exception from the context of the callback making it hard to associate with a request as well as throwing it in the context of the EventEmitter code which can cause inconsistencies in the Node process. This means under some error conditions an uncaught exception would be thrown or at least an 'error' event on the singleton client (again removing it from the request context). Both transport receivers share the same copy-pasta code which contains: I'm working on a patch, but I'm curious about some of the history of the code. In particular the exception based loop flow control and the using the seqid to track the callback which makes it hard to properly associate it with exception handling.",design_debt,non-optimal_design,"Wed, 7 Jan 2015 05:51:21 +0000","Tue, 21 Jul 2015 02:21:10 +0000","Sat, 14 Feb 2015 23:02:36 +0000",3345075,"I've been investigating using the Node.js client in a project however it seems like there are instances which don't follow Node.js best practices. In particular http_connection.js and connection.js throw errors during callbacks. This is considered an anti-pattern in Node because it both removes the Exception from the context of the callback making it hard to associate with a request as well as throwing it in the context of the EventEmitter code which can cause inconsistencies in the Node process. This means under some error conditions an uncaught exception would be thrown or at least an 'error' event on the singleton client (again removing it from the request context). Both transport receivers share the same copy-pasta code which contains: I'm working on a patch, but I'm curious about some of the history of the code. In particular the exception based loop flow control and the using the seqid to track the callback which makes it hard to properly associate it with exception handling.",0.05186666667,0.05186666667,negative
thrift,2937,comment_1,I'm ok with this. However we should set it also to the same level(256MB) as we did for TNonblockingServer with THRIFT-1337,design_debt,non-optimal_design,"Fri, 9 Jan 2015 09:00:05 +0000","Tue, 21 Jul 2015 02:21:16 +0000","Sun, 15 Feb 2015 18:28:27 +0000",3230902,I'm ok with this. However we should set it also to the same level(256MB) as we did for TNonblockingServer with THRIFT-1337 https://github.com/apache/thrift/blob/master/lib/cpp/src/thrift/server/TNonblockingServer.h#L126,0.375,0.375,neutral
thrift,3027,comment_1,"Hi , the patch introduces a new dependency to boost into the Thrift compiler. Can we avoid that? Have a look into there are some similar functions that may serve as model. Next, the {{to_upper_copy()}} function expects a locale, the default is {{std::locale()}} which is ... Not sure if that is what we want. The generated code should not depend on a particular system locale.",design_debt,non-optimal_design,"Fri, 6 Mar 2015 14:23:55 +0000","Fri, 22 May 2015 17:24:08 +0000","Sat, 14 Mar 2015 14:39:43 +0000",692148,"Hi magrath, the patch introduces a new dependency to boost into the Thrift compiler. Can we avoid that? Have a look into t_c_glib_generator.cc, there are some similar functions that may serve as model. Next, the to_upper_copy() function expects a locale, the default is std::locale() which is ... Default constructor. Constructs a copy of the global C++ locale, which is the locale most recently used as the argument to std::locale::global or a copy of std::locale::classic if no call to std::locale::global has been made. Not sure if that is what we want. The generated code should not depend on a particular system locale.",-0.07,-0.0875,neutral
thrift,3027,description,"In Go, as per words in names that are initialisms or acronyms should have a consistent case. For example, if you have a struct like: One would expect it to compile to: Rather than:- It would be pretty difficult to handle all cases of initialisms in the Go compiler of course, but there is a set of common initialisms that have been identified by the authors of Golint and could be handled relatively easily:-",design_debt,non-optimal_design,"Fri, 6 Mar 2015 14:23:55 +0000","Fri, 22 May 2015 17:24:08 +0000","Sat, 14 Mar 2015 14:39:43 +0000",692148,"In Go, as per https://github.com/golang/go/wiki/CodeReviewComments#initialisms, words in names that are initialisms or acronyms should have a consistent case. For example, if you have a struct like: One would expect it to compile to: Rather than:- It would be pretty difficult to handle all cases of initialisms in the Go compiler of course, but there is a set of common initialisms that have been identified by the authors of Golint and could be handled relatively easily:- https://github.com/golang/lint/blob/master/lint.go#L692",0.08333333333,0.08333333333,neutral
thrift,3027,summary,Go compiler does not ensure common initialisms have consistent case,design_debt,non-optimal_design,"Fri, 6 Mar 2015 14:23:55 +0000","Fri, 22 May 2015 17:24:08 +0000","Sat, 14 Mar 2015 14:39:43 +0000",692148,Go compiler does not ensure common initialisms have consistent case,0.15,0.15,negative
thrift,3047,comment_1,I didn't include this in THRIFT-3041 because it made the generated sources exceedingly difficult to compare.,design_debt,non-optimal_design,"Thu, 19 Mar 2015 05:03:56 +0000","Thu, 30 Apr 2015 18:57:11 +0000","Thu, 16 Apr 2015 20:15:46 +0000",2473910,I didn't include this in THRIFT-3041 because it made the generated sources exceedingly difficult to compare.,-0.2,-0.2,negative
thrift,3157,description,Is there any reason this isn't the case? The wildcarding doesn't appear to give us anything extra. I've been told that it may help with uses of the Comparable. Can anyone think of a use?,design_debt,non-optimal_design,"Sat, 16 May 2015 04:49:32 +0000","Fri, 18 Mar 2016 18:02:05 +0000","Fri, 18 Mar 2016 18:02:05 +0000",26572353,Is there any reason this isn't the case? The wildcarding doesn't appear to give us anything extra. I've been told that it may help with uses of the Comparable. Can anyone think of a use?,0.1,0.1,neutral
thrift,3241,comment_1,"It is by design, and is considered a production issue. 1. Run with X memory. 2. Max out the load, and observe memory use. 3. Use less memory the next run if there is significant unused memory, more if they are OOM'ing. 4. If you cannot raise memory, lower the concurrency per server. Steps 1-3 are completely automated in Google production systems and elsewhere. So in Thrift what's important is exposing the concurrency limits in all pool/async implementations. In Go LimitListener is useful.",design_debt,non-optimal_design,"Mon, 13 Jul 2015 09:44:58 +0000","Fri, 31 Jul 2015 21:41:16 +0000","Sun, 26 Jul 2015 00:31:05 +0000",1089967,"It is by design, and is considered a production issue. 1. Run with X memory. 2. Max out the load, and observe memory use. 3. Use less memory the next run if there is significant unused memory, more if they are OOM'ing. 4. If you cannot raise memory, lower the concurrency per server. Steps 1-3 are completely automated in Google production systems and elsewhere. So in Thrift what's important is exposing the concurrency limits in all pool/async implementations. In Go LimitListener is useful.",0.05833333333,0.05833333333,neutral
thrift,3280,description,"Currently retry variables are only initialized after a connection has been successfully established. When the initial connection fails the retry logic is broken since the state has not been properly initialized. To solve this, we need to initialize the retry state before the initial connect() request.",design_debt,non-optimal_design,"Thu, 30 Jul 2015 07:05:39 +0000","Tue, 25 Aug 2015 04:44:50 +0000","Thu, 30 Jul 2015 21:54:16 +0000",53317,"Currently retry variables are only initialized after a connection has been successfully established. When the initial connection fails the retry logic is broken since the state has not been properly initialized. To solve this, we need to initialize the retry state before the initial connect() request.",0.175,0.175,negative
thrift,3280,summary,Initialize retry variables on construction,design_debt,non-optimal_design,"Thu, 30 Jul 2015 07:05:39 +0000","Tue, 25 Aug 2015 04:44:50 +0000","Thu, 30 Jul 2015 21:54:16 +0000",53317,Initialize retry variables on construction,0,0,neutral
thrift,3320,comment_4,"Hi , Thanks for your comments. At present, thrift doesn't not offer users a pure and complete asynchronous interface suite. Developers cannot write PDUs to a transport concurrently. Assuming that thrift has thread-safe transports and a set of asynchronous Interfaces in future releases, how to deserialize *_results from transports if are leveraged? In my proposal, we can get service names after ""TMessage""s are parsed. With service names and procedure names, specific instances of *_results can be selected to read data from transport. If we just store the IDs of requests, a mapping between id and *_result shall be maintained. And this is not elegant.",design_debt,non-optimal_design,"Wed, 9 Sep 2015 02:46:17 +0000","Sun, 10 Feb 2019 12:57:02 +0000","Sun, 10 Feb 2019 12:57:02 +0000",108036645,"Hi jking3, Thanks for your comments. At present, thrift doesn't not offer users a pure and complete asynchronous interface suite. Developers cannot write PDUs to a transport concurrently. Assuming that thrift has thread-safe transports and a set of asynchronous Interfaces in future releases, how to deserialize *_results from transports if ""TMultiplexedProtocol""s are leveraged? In my proposal, we can get service names after ""TMessage""s are parsed. With service names and procedure names, specific instances of *_results can be selected to read data from transport. If we just store the IDs of requests, a mapping between id and *_result shall be maintained. And this is not elegant.",-0.08483333333,-0.08483333333,negative
thrift,3447,comment_2,"I've looked a bit more into this and written a very stripped down test client and server using your excerpt as a starting point on the server. My client opens the transport but doesn't actually make any RPC calls to the Server. Looking at Parallel Stacks after stopping the server thread I can see that the client threads will not notice the stop request until either the underlying System.Net.Socket (In this particular case) receives at least one byte, or times out (which I think is probably a really long time in TCP). Just for reference - The server's Serve() thread does complete, so it should definitely not accept any new client connections. However, the client threads will stay blocked until the above scenario. Unfortunately as Thrift is targeted at .NET 3.5 the use of CancellationToken isn't available until .NET 4 which is a pity as that's quite a nice way to handle such things... It'll have to be resolved another way. I'll have a look at this further tomorrow to see if I can suggest an elegant solution. I'm most concerned about is whether there's a neat way to do this so that it doesn't cause issues with Transports which are not based on Sockets. Given that there are at least some similarities with the C++ case (I've not read it in full detail), it may be prudent to implement a similar solution.",design_debt,non-optimal_design,"Wed, 25 Nov 2015 12:27:12 +0000","Thu, 3 Jan 2019 03:31:15 +0000","Tue, 27 Feb 2018 17:55:11 +0000",71299679,"I've looked a bit more into this and written a very stripped down test client and server using your excerpt as a starting point on the server. My client opens the transport but doesn't actually make any RPC calls to the Server. Looking at Parallel Stacks after stopping the server thread I can see that the client threads will not notice the stop request until either the underlying System.Net.Socket (In this particular case) receives at least one byte, or times out (which I think is probably a really long time in TCP). Just for reference - The server's Serve() thread does complete, so it should definitely not accept any new client connections. However, the client threads will stay blocked until the above scenario. Unfortunately as Thrift is targeted at .NET 3.5 the use of CancellationToken isn't available until .NET 4 which is a pity as that's quite a nice way to handle such things... It'll have to be resolved another way. I'll have a look at this further tomorrow to see if I can suggest an elegant solution. I'm most concerned about is whether there's a neat way to do this so that it doesn't cause issues with Transports which are not based on Sockets. Given that there are at least some similarities with the C++ case (I've not read it in full detail), it may be prudent to implement a similar solution.",-0.04874358974,-0.04874358974,neutral
thrift,3447,comment_3,"This boils down to the use of Blocking Sockets in Thrift. It's the same problem described by many other people outside the thrift world, that you can't interrupt a socket whilst it's blocked. The unanimous answer for C# in general is to use non-blocking sockets. A work-around to the behaviour might be to get clients to regularly call a void function (such as defining a poll() function in the .thrift service definition). However this is wasteful of bandwidth and I only mention is as a possible way to achieve the desired behaviour prior to the existence of a code patch. I think this will require some significant coding effort. If I can find some time I'll look into adapting Thrift's C# lib to use async sockets - I'd welcome a 2nd opinion on this.",design_debt,non-optimal_design,"Wed, 25 Nov 2015 12:27:12 +0000","Thu, 3 Jan 2019 03:31:15 +0000","Tue, 27 Feb 2018 17:55:11 +0000",71299679,"This boils down to the use of Blocking Sockets in Thrift. It's the same problem described by many other people outside the thrift world, that you can't interrupt a socket whilst it's blocked. The unanimous answer for C# in general is to use non-blocking sockets. A work-around to the behaviour might be to get clients to regularly call a void function (such as defining a poll() function in the .thrift service definition). However this is wasteful of bandwidth and I only mention is as a possible way to achieve the desired behaviour prior to the existence of a code patch. I think this will require some significant coding effort. If I can find some time I'll look into adapting Thrift's C# lib to use async sockets - I'd welcome a 2nd opinion on this.",0.2213333333,0.2213333333,neutral
thrift,3495,summary,Minor enhancements and fixes for cross test,design_debt,non-optimal_design,"Sun, 20 Dec 2015 12:52:23 +0000","Wed, 6 Jan 2016 02:06:10 +0000","Wed, 23 Dec 2015 17:48:48 +0000",276985,Minor enhancements and fixes for cross test,0,0,neutral
thrift,3535,description,The default Dart compiler behavior is to generate a new library for each thrift file. Add an argument to generate files and imports that are usable in an existing library. This produces a file structure like:,design_debt,non-optimal_design,"Sat, 9 Jan 2016 20:29:29 +0000","Fri, 18 Mar 2016 17:54:34 +0000","Fri, 18 Mar 2016 17:54:34 +0000",5952305,The default Dart compiler behavior is to generate a new library for each thrift file. Add an argument to generate files and imports that are usable in an existing library. This produces a file structure like:,-0.1666666667,-0.1666666667,neutral
thrift,3619,comment_4," yes, see what I had to do for Parquet: It would be nice if Thrift would not pass on this burden to thirdparty users, but in the meantime we are stuck with the released 0.9.2 and 0.9.3 versions where this workaround is necessary.",design_debt,non-optimal_design,"Fri, 12 Feb 2016 18:54:01 +0000","Mon, 22 Feb 2016 13:17:21 +0000","Thu, 18 Feb 2016 22:04:34 +0000",529833,"richardtsai  yes, see what I had to do for Parquet: https://github.com/apache/parquet-cpp/blob/master/CMakeLists.txt#L115 It would be nice if Thrift would not pass on this burden to thirdparty users, but in the meantime we are stuck with the released 0.9.2 and 0.9.3 versions where this workaround is necessary.",0.085,0.085,neutral
thrift,3733,description,The socket timeout handling has room for improvements,design_debt,non-optimal_design,"Thu, 10 Mar 2016 19:03:44 +0000","Thu, 17 Mar 2016 19:08:54 +0000","Thu, 10 Mar 2016 19:14:10 +0000",626,The socket timeout handling has room for improvements,0,0,neutral
thrift,3760,description,"* Perl: /usr/usr/local -* Python: /usr/local - perl fix for ""local"" part is frankly bad but at least it works. The patch also removes .pyo files from python package that should be generated by install process instead. This resolves most of lintian warnings.",design_debt,non-optimal_design,"Fri, 25 Mar 2016 00:00:32 +0000","Thu, 14 Apr 2016 10:27:47 +0000","Tue, 29 Mar 2016 17:51:44 +0000",409872,"Perl: /usr/usr/local -> /usr Python: /usr/local -> /usr perl fix for ""local"" part is frankly bad but at least it works. The patch also removes .pyo files from python package that should be generated by install process instead. This resolves most of lintian warnings.",-0.025,-0.025,neutral
thrift,3864,comment_3,"There is no clean way to implement this without a fairly ugly BC break. Go's garbage collector has also improved dramatically since this issue was opened, so I'm going to close this. If anyone still has concerns or ideas, please reopen.",design_debt,non-optimal_design,"Mon, 27 Jun 2016 14:23:22 +0000","Thu, 3 Jan 2019 14:33:44 +0000","Thu, 3 Jan 2019 14:33:44 +0000",79488622,"There is no clean way to implement this without a fairly ugly BC break. Go's garbage collector has also improved dramatically since this issue was opened, so I'm going to close this. If anyone still has concerns or ideas, please reopen.",0.2579444444,0.2579444444,negative
thrift,3864,description,"I have a service (Golang) with a handler that returns a large slice. A new one is allocated every time, resulting in large heap allocations per client call, while it could be reused through a sync.Pool and improve the overall performance through fewer GC calls. (.thrift example) struct Slice { 1: required list<Element} service MyService { Slice GetSlice() throws (...) } I have experimented with modifying the auto-generated code and got this functionality through adding a (.go) pool.Put(*Slice) // adding the newly generated return value to a pool) call in but doing so creates a nasty dependency between the handler and the processor. Modifying the signature of the handler should also work (.go) GetSlice(*Slice) (*Slice, error) but does breaks all compatibility with previous compilers... Has some solution to this problem been explored? If nothing else some optional Release(retval) after oprot.Flush() in would be very helpful",design_debt,non-optimal_design,"Mon, 27 Jun 2016 14:23:22 +0000","Thu, 3 Jan 2019 14:33:44 +0000","Thu, 3 Jan 2019 14:33:44 +0000",79488622,"I have a service (Golang) with a handler that returns a large slice. A new one is allocated every time, resulting in large heap allocations per client call, while it could be reused through a sync.Pool and improve the overall performance through fewer GC calls. (.thrift example) struct Slice { 1: required list<Element> element } service MyService { Slice GetSlice() throws (...) } I have experimented with modifying the auto-generated code and got this functionality through adding a (.go) pool.Put(*Slice) // adding the newly generated return value to a pool) call in https://github.com/apache/thrift/blob/master/compiler/cpp/src/generate/t_go_generator.cc#L2798 but doing so creates a nasty dependency between the handler and the processor. Modifying the signature of the handler should also work (.go) GetSlice(*Slice) (*Slice, error) but does breaks all compatibility with previous compilers... Has some solution to this problem been explored? If nothing else some optional Release(retval) after oprot.Flush() in https://github.com/apache/thrift/blob/master/compiler/cpp/src/generate/t_go_generator.cc#L2798 would be very helpful",0.1818181818,0.1818181818,neutral
thrift,3868,description,"The identity check is cheap and should be done before comparing fields of a struct. Idiomatic equals methods always include this check especially if the field by field comparison can be expensive. Check to add: if(that == this) return true; 1864 out << indent() << ""public boolean equals("" << tstruct-1865 indent_up(); 1866 out << indent() << ""if (that == null)"" << endl << indent() << "" return false;"" << endl; INSERT IDENTITY CHECK HERE 1867 1868 const vector<t_field*1869 vector<t_field*1870 for (m_iter = members.begin(); m_iter != members.end(); ++m_iter) { 1871 out << endl; 1872",design_debt,non-optimal_design,"Wed, 29 Jun 2016 14:11:27 +0000","Tue, 11 Oct 2016 01:12:35 +0000","Fri, 7 Oct 2016 17:08:23 +0000",8650616,"The identity check is cheap and should be done before comparing fields of a struct. Idiomatic equals methods always include this check especially if the field by field comparison can be expensive. Check to add: if(that == this) return true; 1864 out << indent() << ""public boolean equals("" << tstruct->get_name() << "" that) {"" << endl; 1865 indent_up(); 1866 out << indent() << ""if (that == null)"" << endl << indent() << "" return false;"" << endl; INSERT IDENTITY CHECK HERE 1867 1868 const vector<t_field*>& members = tstruct->get_members(); 1869 vector<t_field*>::const_iterator m_iter; 1870 for (m_iter = members.begin(); m_iter != members.end(); ++m_iter) { 1871 out << endl; 1872",0.03108333333,0.03108333333,neutral
thrift,3868,summary,Java struct equals should do identity check before field comparison,design_debt,non-optimal_design,"Wed, 29 Jun 2016 14:11:27 +0000","Tue, 11 Oct 2016 01:12:35 +0000","Fri, 7 Oct 2016 17:08:23 +0000",8650616,Java struct equals should do identity check before field comparison,0,0,neutral
thrift,3905,description,"In Dart it is desirable to initialize bool (false), int (0), and double (0.0) required properties (those that are not marked optional), instead of leaving them with the value of null.",design_debt,non-optimal_design,"Sun, 28 Aug 2016 00:33:13 +0000","Wed, 28 Sep 2016 13:28:39 +0000","Wed, 31 Aug 2016 20:38:41 +0000",331528,"In Dart it is desirable to initialize bool (false), int (0), and double (0.0) required properties (those that are not marked optional), instead of leaving them with the value of null.",0.3375,0.3375,neutral
thrift,3941,description,"thrift_poll() for WINVER <= 0x0502 in shadows the 'time_out' variable, and it ends up passing the destructed copy to select(): timeval time_out; timeval* time_out_ptr = NULL; if (timeout timeval time_out = {timeout / 1000, (timeout % 1000) * 1000}; time_out_ptr = &time_out; } else { // to avoid compiler warnings (void)time_out; (void)timeout; } int sktready = select(1, read_fds_ptr, write_fds_ptr, NULL, time_out_ptr); Stepping through this code in the debugger, it looks like MSVC reserves a large enough stack frame to avoid overwriting the variable when calling select(), which may be why this hasn't been caught yet.",design_debt,non-optimal_design,"Tue, 4 Oct 2016 15:56:29 +0000","Tue, 11 Oct 2016 01:12:30 +0000","Wed, 5 Oct 2016 10:33:06 +0000",66997,"thrift_poll() for WINVER <= 0x0502 in thrift/windows/WinFnctl.cpp shadows the 'time_out' variable, and it ends up passing the destructed copy to select(): timeval time_out; timeval* time_out_ptr = NULL; if (timeout >= 0) { timeval time_out = {timeout / 1000, (timeout % 1000) * 1000} ; time_out_ptr = &time_out; } else { // to avoid compiler warnings (void)time_out; (void)timeout; } int sktready = select(1, read_fds_ptr, write_fds_ptr, NULL, time_out_ptr); Stepping through this code in the debugger, it looks like MSVC reserves a large enough stack frame to avoid overwriting the variable when calling select(), which may be why this hasn't been caught yet.",0.1455,0.07275,neutral
thrift,3941,summary,WinXP version of thrift_poll() relies on undefined behavior by passing a destructed variable to select(),design_debt,non-optimal_design,"Tue, 4 Oct 2016 15:56:29 +0000","Tue, 11 Oct 2016 01:12:30 +0000","Wed, 5 Oct 2016 10:33:06 +0000",66997,WinXP version of thrift_poll() relies on undefined behavior by passing a destructed variable to select(),0.688,0.688,neutral
thrift,4030,description,"The main motivation for me to introduce this script was to avoid docker build which was failing too often due to occasional apt download failures. But for some unknown reason apt is not failing any longer. Although reusing reduces build time by ~10min for each job, we may well remove the script and always build images from scratch.",design_debt,non-optimal_design,"Sat, 14 Jan 2017 07:06:27 +0000","Fri, 1 Feb 2019 05:29:49 +0000","Fri, 1 Feb 2019 05:29:49 +0000",64621402,"The main motivation for me to introduce this script was to avoid docker build which was failing too often due to occasional apt download failures. But for some unknown reason apt is not failing any longer. Although reusing reduces build time by ~10min for each job, we may well remove the script and always build images from scratch.",0.277,0.277,neutral
thrift,4130,description,"There is a connection leak in the THttpClient when using the Apache HttpClient with the Without calling releaseConnection on the HttpPost object, the connections are never returned to the pool. Under heavy load, this can lead to both failures for subsequent calls to be able to get a connection from the pool and connections being held by the underlying OS, eventually resulting in the inability to grab another client port for outgoing connections. Per the Apache HttpClient ""In order to ensure correct deallocation of system resources the user MUST either fully consume the response content or abort request execution by calling This might have not been an issue when using the 3.x version of the HttpClient, but it's definitely an issue in the 4.x line. See for more details.",design_debt,non-optimal_design,"Wed, 22 Mar 2017 22:53:52 +0000","Thu, 14 Dec 2017 13:55:26 +0000","Thu, 23 Mar 2017 00:34:25 +0000",6033,"There is a connection leak in the THttpClient when using the Apache HttpClient with the PoolingClientConnectionManager. Without calling releaseConnection on the HttpPost object, the connections are never returned to the pool. Under heavy load, this can lead to both failures for subsequent calls to be able to get a connection from the pool and connections being held by the underlying OS, eventually resulting in the inability to grab another client port for outgoing connections. Per the Apache HttpClient examples/documentation: ""In order to ensure correct deallocation of system resources the user MUST either fully consume the response content or abort request execution by calling HttpGet#releaseConnection()."" This might have not been an issue when using the 3.x version of the HttpClient, but it's definitely an issue in the 4.x line. See https://hc.apache.org/httpcomponents-client-4.2.x/quickstart.html for more details.",0.1042222222,0.1007916667,negative
thrift,4164,description,"In a project where thrift is used, i was investigating a core in an assertion in (pthread variety). The mutex in question was one of the locking mutexes that thrift gives to openssl. The core occurred in where the mutexes are destroyed (on the last line). I suspect that we might be changing the locking callbacks too early in the cleanup process; perhaps one of the other cleanup calls that follows it would have released a mutex in some situations? In any case, this needs to be investigated and I am assigning it to myself.",design_debt,non-optimal_design,"Mon, 3 Apr 2017 14:24:40 +0000","Thu, 14 Dec 2017 13:55:39 +0000","Tue, 4 Apr 2017 13:37:13 +0000",83553,"In a project where thrift is used, i was investigating a core in an assertion in apache::thrift::concurrency::~Mutex (pthread variety). The mutex in question was one of the locking mutexes that thrift gives to openssl. The core occurred in TSSLSocket::cleanupOpenSSL() where the mutexes are destroyed (on the last line). I suspect that we might be changing the locking callbacks too early in the cleanup process; perhaps one of the other cleanup calls that follows it would have released a mutex in some situations? In any case, this needs to be investigated and I am assigning it to myself.",-0.16,-0.16,neutral
thrift,418,comment_0,Attaching patch. There are 2 changes: 1) Modified t_rb_generator.cc to generate a FIELD_IDS constant for each struct + a struct_field_ids() accessor method. 2) Use instead of to iterate through struct fields id-sorted order. Updated the native code as well. All tests pass. Should be targeted for next major release as the change is not code-compatible with 0.6-generated files. Can be re-implement without a compiler change if we want to keep full compatibility between 0.6 gen-rb files and 0.7 library.,design_debt,non-optimal_design,"Wed, 1 Apr 2009 20:39:58 +0000","Wed, 10 Aug 2011 18:27:52 +0000","Thu, 16 Jun 2011 03:15:03 +0000",69575705,Attaching patch. There are 2 changes: 1) Modified t_rb_generator.cc to generate a FIELD_IDS constant for each struct + a struct_field_ids() accessor method. 2) Use struct_field_ids.each instead of struct_fields.keys.sort.each to iterate through struct fields id-sorted order. Updated the native code as well. All tests pass. Should be targeted for next major release as the change is not code-compatible with 0.6-generated files. Can be re-implement without a compiler change if we want to keep full compatibility between 0.6 gen-rb files and 0.7 library.,-0.02425,-0.01616666667,neutral
thrift,418,comment_3,Reopening as I've thought about it and a patch which doesn't change the compiler or generated code ends up being much cleaner.,design_debt,non-optimal_design,"Wed, 1 Apr 2009 20:39:58 +0000","Wed, 10 Aug 2011 18:27:52 +0000","Thu, 16 Jun 2011 03:15:03 +0000",69575705,Reopening as I've thought about it and a patch which doesn't change the compiler or generated code ends up being much cleaner.,-0.4,-0.4,positive
thrift,418,description,"Currently the ruby struct code sorts the field ids of each struct every time it is serialized, which is an unnecessary drag on performance.",design_debt,non-optimal_design,"Wed, 1 Apr 2009 20:39:58 +0000","Wed, 10 Aug 2011 18:27:52 +0000","Thu, 16 Jun 2011 03:15:03 +0000",69575705,"Currently the ruby struct code sorts the field ids of each struct every time it is serialized, which is an unnecessary drag on performance.",-0.2,-0.2,negative
thrift,4230,description,After a lot of tests with HBASE Thrift server we found a problem. If the connection is dropped on the client side (using route or iptables) it may be still opened on the Thrift server side. Such situation will occur in case of unstable connection. After several iterations the Thrift server application will have a lot of opened connections and *will not accept *any new one. The only WA found is to restart the Thrift server. I believe Thrift server should have something like socket timeouts and heartbeats.,design_debt,non-optimal_design,"Fri, 16 Jun 2017 07:05:05 +0000","Fri, 1 Feb 2019 03:58:09 +0000","Fri, 1 Feb 2019 03:58:09 +0000",51396784,After a lot of tests with HBASE Thrift server we found a problem. If the connection is dropped on the client side (using route or iptables) it may be still opened on the Thrift server side. Such situation will occur in case of unstable connection. After several iterations the Thrift server application will have a lot of opened connections and *will not accept *any new one. The only WA found is to restart the Thrift server. I believe Thrift server should have something like socket timeouts and heartbeats.,-0.1599166667,-0.1599166667,negative
thrift,4308,comment_2,"Switching to ld.gold exposed other issues (some good, some bad) and I'm not sure I can use it on everything yet.",design_debt,non-optimal_design,"Mon, 4 Sep 2017 17:57:27 +0000","Thu, 27 Dec 2018 15:24:55 +0000","Tue, 30 Jan 2018 13:00:19 +0000",12769372,"Switching to ld.gold exposed other issues (some good, some bad) and I'm not sure I can use it on everything yet.",0.047,0.047,negative
thrift,4362,comment_1,"Do you think I can submit this patch as-is or do you have any suggestions for improvement? Edit: As mentioned, with my patch the size may be checked multiple times. So I think it's a good idea to remove the other calls to the size-check method.",design_debt,non-optimal_design,"Tue, 10 Oct 2017 18:05:00 +0000","Thu, 14 Dec 2017 13:55:02 +0000","Wed, 25 Oct 2017 12:42:05 +0000",1276625,"Do you think I can submit this patch as-is or do you have any suggestions for improvement? Edit: As mentioned, with my patch the size may be checked multiple times. So I think it's a good idea to remove the other calls to the size-check method.",0.4393333333,0.4393333333,neutral
thrift,4362,description,"In some cases the method size)}} gets called with a ""size"" parameter that has not been validated by the existing method size)}}. This is true if the method is called by of the same class. The method {{readString()}} checks the size correctly before calling size)}}. Since the methods size)}} and are public, there may be other callers who don't check the size correctly. We encountered this issue in production several times. Because of this we are currently using our own patched version of libthrift-0.9.3. The patch is attached, but it is surely not the best solution, because with this patch the size may be checked twice, depending on the caller.",design_debt,non-optimal_design,"Tue, 10 Oct 2017 18:05:00 +0000","Thu, 14 Dec 2017 13:55:02 +0000","Wed, 25 Oct 2017 12:42:05 +0000",1276625,"In some cases the method org.apache.thrift.protocol.TBinaryProtocol.readStringBody(int size) gets called with a ""size"" parameter that has not been validated by the existing method checkStringReadLength(int size). This is true if the method is called by readMessageBegin() of the same class. The method readString() checks the size correctly before calling readStringBody(int size). Since the methods readStringBody(int size) and readMessageBegin() are public, there may be other callers who don't check the size correctly. We encountered this issue in production several times. Because of this we are currently using our own patched version of libthrift-0.9.3. The patch is attached, but it is surely not the best solution, because with this patch the size may be checked twice, depending on the caller.",-0.05161111111,-0.03317857143,negative
thrift,4362,summary,Missing size-check can lead to huge memory allocation,design_debt,non-optimal_design,"Tue, 10 Oct 2017 18:05:00 +0000","Thu, 14 Dec 2017 13:55:02 +0000","Wed, 25 Oct 2017 12:42:05 +0000",1276625,Missing size-check can lead to huge memory allocation,0.05,0.05,negative
thrift,4416,description,"The perl library has a few files and a shell script designed to automate the production of a perl package for CPAN. It mostly works, however: 1. CPAN cannot parse the package version references we use when we point to lib/Thrift.pm from other packages. This is fixed by adding a provides{} stanza to META.json. 2. Users of CPAN don't care about anything but obtaining the ""Thrift"" package that contains all the others, so identifying the others (like is not necessary. This is done by adding a provides{} stanza to META.json. 3. The CPAN package no longer needs the META.yml file, as META.json is sufficient. 4. These changes were made by hand to the 0.11.0 package before uploading to CPAN. This needs to be automated.",design_debt,non-optimal_design,"Sat, 9 Dec 2017 13:29:01 +0000","Thu, 27 Dec 2018 15:25:06 +0000","Sat, 9 Dec 2017 22:02:41 +0000",30820,"The perl library has a few files and a shell script designed to automate the production of a perl package for CPAN. It mostly works, however: 1. CPAN cannot parse the package version references we use when we point to lib/Thrift.pm from other packages. This is fixed by adding a provides{} stanza to META.json. 2. Users of CPAN don't care about anything but obtaining the ""Thrift"" package that contains all the others, so identifying the others (like Thrift::BinaryProtocol) is not necessary. This is done by adding a provides{} stanza to META.json. 3. The CPAN package no longer needs the META.yml file, as META.json is sufficient. 4. These changes were made by hand to the 0.11.0 package before uploading to CPAN. This needs to be automated.",0.075,0.075,neutral
thrift,4437,description,"When using a WebSocket Transport and doing two service calls immediately, without waiting for the first to return, e.g. like this: The callback to the first invocation is called twice, and the second never, i.e. console shows: instead of the expected I suspect this bug was introduced with the patch for where for some reason the callback registered twice when set:",design_debt,non-optimal_design,"Wed, 27 Dec 2017 11:08:19 +0000","Thu, 27 Dec 2018 15:25:16 +0000","Thu, 28 Dec 2017 13:03:35 +0000",93316,"When using a WebSocket Transport and doing two service calls immediately, without waiting for the first to return, e.g. like this: The callback to the first invocation is called twice, and the second never, i.e. console shows: instead of the expected I suspect this bug was introduced with the patch for https://issues.apache.org/jira/browse/THRIFT-4131 where for some reason the callback registered twice when set: https://github.com/apache/thrift/pull/1372/files",-0.1083333333,-0.1083333333,neutral
thrift,4437,summary,JS WebSocket client callbacks invoked twice on parallel requests,design_debt,non-optimal_design,"Wed, 27 Dec 2017 11:08:19 +0000","Thu, 27 Dec 2018 15:25:16 +0000","Thu, 28 Dec 2017 13:03:35 +0000",93316,JS WebSocket client callbacks invoked twice on parallel requests,0,0,neutral
thrift,4446,description,"In the C# and .NET Core libraries, the JSONProtocol's Binary Encoding to Base64 trims padding from the user provided byte arrays before encoding into Base64. This behavior is incorrect, as the user provided data should be encoded exactly as provided. Otherwise, data may be lost. Fixed by no longer trimming padding on encode. Padding must still be trimmed on decode, in accordance with the Base64 specification. For example: * Before this patch, encoding the byte array [0x01, 0x3d, 0x3d] yields [0x01] upon decode. This is incorrect, as I should decode the exact data that I encoded. * After this patch, it yields [0x01, 0x3d, 0x3d], as expected. I have submitted a pull request",design_debt,non-optimal_design,"Tue, 9 Jan 2018 17:44:26 +0000","Sat, 2 Feb 2019 13:47:56 +0000","Thu, 11 Jan 2018 02:14:22 +0000",116996,"In the C# and .NET Core libraries, the JSONProtocol's Binary Encoding to Base64 trims padding from the user provided byte arrays before encoding into Base64. This behavior is incorrect, as the user provided data should be encoded exactly as provided. Otherwise, data may be lost. Fixed by no longer trimming padding on encode. Padding must still be trimmed on decode, in accordance with the Base64 specification. For example: Before this patch, encoding the byte array [0x01, 0x3d, 0x3d] yields [0x01] upon decode. This is incorrect, as I should decode the exact data that I encoded. After this patch, it yields [0x01, 0x3d, 0x3d], as expected. I have submitted a pull request here",0.03535,0.03535,negative
thrift,4446,summary,JSONProtocol Base64 Encoding Trims Padding,design_debt,non-optimal_design,"Tue, 9 Jan 2018 17:44:26 +0000","Sat, 2 Feb 2019 13:47:56 +0000","Thu, 11 Jan 2018 02:14:22 +0000",116996,JSONProtocol Base64 Encoding Trims Padding,0,0,neutral
thrift,447,summary,Make an abstract base Client class so we can generate less code,design_debt,non-optimal_design,"Thu, 9 Apr 2009 17:38:54 +0000","Tue, 8 Feb 2011 17:26:55 +0000","Tue, 8 Feb 2011 17:26:55 +0000",57887281,Make an abstract base Client class so we can generate less code,0,0,neutral
thrift,452,comment_2,"That's because you didn't make your foo field optional. If you make it optional, the isset will be checked. Personally, I think the ""default"" requiredness is confusing - this is the problem you end up with.",design_debt,non-optimal_design,"Sun, 12 Apr 2009 04:16:47 +0000","Tue, 1 Nov 2011 02:52:17 +0000","Fri, 2 Oct 2009 00:58:39 +0000",14935312,"That's because you didn't make your foo field optional. If you make it optional, the isset will be checked. Personally, I think the ""default"" requiredness is confusing - this is the problem you end up with.",-0.1485555556,-0.1485555556,negative
thrift,452,comment_4,"I agree that it is weird that we initialize the enum to an invalid value. Maybe the default value for enums should default to the first declared value, rather than 0?",design_debt,non-optimal_design,"Sun, 12 Apr 2009 04:16:47 +0000","Tue, 1 Nov 2011 02:52:17 +0000","Fri, 2 Oct 2009 00:58:39 +0000",14935312,"I agree that it is weird that we initialize the enum to an invalid value. Maybe the default value for enums should default to the first declared value, rather than 0?",-0.3,-0.3,negative
thrift,4715,description,"`union` option for netcore generator was fixed in 0.12, but what it generates doesn't seem very user-friendly. Following thrift: Generates: Usage: Is there a reason for the `public abstract object Data`? If we get rid of that and instead generate a strongly-type getter we don't need to cast `Data`: I could have sworn it worked like that with the ""csharp"" generator in 0.11.0 but it generates the same now. Is the intended usage different from what I'm doing?",design_debt,non-optimal_design,"Fri, 4 Jan 2019 05:54:29 +0000","Wed, 16 Oct 2019 22:27:24 +0000","Thu, 24 Jan 2019 17:29:12 +0000",1769683,"`union` option for netcore generator was fixed in 0.12, but what it generates doesn't seem very user-friendly. Following thrift: ```thrift struct PlayMsg { 1: string url, } union RequestMsg { 1: PlayMsg Play, } ``` Generates: Usage: Is there a reason for the `public abstract object Data`? If we get rid of that and instead generate a strongly-type getter we don't need to cast `Data`:   I could have sworn it worked like that with the ""csharp"" generator in 0.11.0 but it generates the same now. Is the intended usage different from what I'm doing?",-0.3125,-0.3125,negative
thrift,4822,description,"Certain CTORs accept two boolean flags {{public SomeTransport( arg1, arg2, ..., bool useBufferedSockets = false, bool useFramedTransport = false)}} The only valid combinations here are in fact (false,false), (true,false), (false,true) - the forth combination does not make sense because framed by design already acts as a buffer. Not to mention, that multiple boolean arguments are usually less coder-friendly. Therefore, the parameterlist should be shortened to the more readable, maintainable and concise style like so (proposal):",design_debt,non-optimal_design,"Thu, 14 Mar 2019 08:08:30 +0000","Wed, 16 Oct 2019 22:27:30 +0000","Fri, 15 Mar 2019 00:35:52 +0000",59242,"Certain CTORs accept two boolean flags public SomeTransport( arg1, arg2, ..., bool useBufferedSockets = false, bool useFramedTransport = false) The only valid combinations here are in fact (false,false), (true,false), (false,true) - the forth combination does not make sense because framed by design already acts as a buffer. Not to mention, that multiple boolean arguments are usually less coder-friendly. Therefore, the parameterlist should be shortened to the more readable, maintainable and concise style like so (proposal):",0.1262,0.1262,neutral
thrift,4857,comment_3,"All right, I just created a [pull Let me know if I missed anything. The contribution instructions weren't clear whether the language indication (e.g. {{java}}) in the pull request should be uppercase or lowercase. I saw both in historical commits. As the official contribution instructions showed e.g. {{perl}}, I guess that you want to follow the token form used in the {{thrift --gen}} CLI, so I went with {{java}} rather than {{Java}} in the commit message. I have my own opinions about approaches to generating hash codes (doing it manually is tedious and error-prone), and I could quibble about the approach to checking for class equivalence. For example, the equality check compares exact classes, which the author apparently thought would be more efficient, but this will break if anyone ever subclasses {{TField}}. Either someone should make {{TField}} a {{final}} class, or use a normal {{instanceof}} in comparison. But these issues are ancillary to the core bug here, so I tried to make my change as surgical as possible, especially as this is my first contribution. Let me know what you think!",design_debt,non-optimal_design,"Fri, 3 May 2019 16:47:14 +0000","Wed, 16 Oct 2019 22:26:37 +0000","Mon, 13 May 2019 20:54:17 +0000",878823,"All right, I just created a pull request. Let me know if I missed anything. The contribution instructions weren't clear whether the language indication (e.g. java) in the pull request should be uppercase or lowercase. I saw both in historical commits. As the official contribution instructions showed e.g. perl, I guess that you want to follow the token form used in the thrift --gen CLI, so I went with java rather than Java in the commit message. I have my own opinions about approaches to generating hash codes (doing it manually is tedious and error-prone), and I could quibble about the approach to checking for class equivalence. For example, the equality check compares exact classes, which the author apparently thought would be more efficient, but this will break if anyone ever subclasses TField. Either someone should make TField a final class, or use a normal instanceof in comparison. But these issues are ancillary to the core bug here, so I tried to make my change as surgical as possible, especially as this is my first contribution. Let me know what you think!",0.1394074074,0.1429666667,neutral
thrift,4862,comment_0,"NB. Slight compilcation comes from the rather unfortunate (but documented) fact, that [Delphi generates RTTI info only for some specific types of Hence, in some cases the values can be printed as names, but in others we still have to print the numeric value.",design_debt,non-optimal_design,"Thu, 9 May 2019 21:58:50 +0000","Wed, 20 Nov 2019 02:04:34 +0000","Fri, 10 May 2019 22:01:44 +0000",86574,"NB. Slight compilcation comes from the rather unfortunate (but documented) fact, thatDelphi generates RTTI info only for some specific types of enums. Hence, in some cases the values can be printed as names, but in others we still have to print the numeric value.",-0.05475,-0.0365,negative
thrift,487,description,"The method and its associated class have errors and race conditions that cause frequent failures in the test. Some failure modes return to the caller, but others just hang the test forever. The main problem is that the test function relies on indirect indications that its tasks have reached certain known conditions. The indications that it sees are actually caused by the ThreadManager's Workers, and this results in a number of race conditions between the test function and the BlockTasks. There are 2 patch files attached. One patches the file to fix the test. The other patches Tests.cpp so that it will run the blockTest in a loop. In my experience, an unpatched version of generally fails within 100 iterations. After being patched, the test will still fail unless a separate patch is applied to the ThreadManager.cpp file. That patch is part of another issue that I haven't entered yet (because it needs to refer to this one). When I know its number I will add a comment. Anyway, if the patch is applied to but not to ThreadManager.cpp, then the test will still fail, generally with an assert, but sometimes with a Bus Error. Test Procedure: 1) Apply the Tests.cpp patch. This makes the thread-manager portion of the test run ONLY the blockTest in an effectively infinite loop. 2) Run a make in lib/cpp in order to rebuild concurrency-test 3) Run concurrency test with the command line argument ""thread-manager"". This will start the blockTest loop. It should fail in a fairly short time. Repeated runs may fail different ways, including infinite hangs. 4) Apply the patch to 5) Run make in lib/cpp to rebuild concurrency-test 6) Run concurrency_test as before. It should probably run for a longer period of time. I have seen it run for an hour or more after beng patched. Eventually it should fail either with an assert in Monitor.cpp while trying to destroy a pthread_mutex_t, or it will get a Bus Error because it tried to execute invalid memory. I have also seen it hang forever. In order to resolve the remaining issues, a patch needs to be applied to ThreadManager.cpp. I will add a comment about that as soon as the issue is filed and the patch is available.",design_debt,non-optimal_design,"Tue, 5 May 2009 23:10:30 +0000","Thu, 11 Jan 2018 12:43:09 +0000","Thu, 11 Jan 2018 12:43:01 +0000",274109551,"The ThreadManagerTests::blockTest() method and its associated ThreadManagerTests::BlockTask class have errors and race conditions that cause frequent failures in the test. Some failure modes return to the caller, but others just hang the test forever. The main problem is that the test function relies on indirect indications that its tasks have reached certain known conditions. The indications that it sees are actually caused by the ThreadManager's Workers, and this results in a number of race conditions between the test function and the BlockTasks. There are 2 patch files attached. One patches the ThreadManagerTests.h file to fix the test. The other patches Tests.cpp so that it will run the blockTest in a loop. In my experience, an unpatched version of ThreadManagerTests.h generally fails within 100 iterations. After being patched, the test will still fail unless a separate patch is applied to the ThreadManager.cpp file. That patch is part of another issue that I haven't entered yet (because it needs to refer to this one). When I know its number I will add a comment. Anyway, if the patch is applied to ThreadManagerTests.h, but not to ThreadManager.cpp, then the test will still fail, generally with an assert, but sometimes with a Bus Error. Test Procedure: 1) Apply the Tests.cpp patch. This makes the thread-manager portion of the test run ONLY the blockTest in an effectively infinite loop. 2) Run a make in lib/cpp in order to rebuild concurrency-test 3) Run concurrency test with the command line argument ""thread-manager"". This will start the blockTest loop. It should fail in a fairly short time. Repeated runs may fail different ways, including infinite hangs. 4) Apply the patch to ThreadManagerTests.h. 5) Run make in lib/cpp to rebuild concurrency-test 6) Run concurrency_test as before. It should probably run for a longer period of time. I have seen it run for an hour or more after beng patched. Eventually it should fail either with an assert in Monitor.cpp while trying to destroy a pthread_mutex_t, or it will get a Bus Error because it tried to execute invalid memory. I have also seen it hang forever. In order to resolve the remaining issues, a patch needs to be applied to ThreadManager.cpp. I will add a comment about that as soon as the issue is filed and the patch is available.",-0.0575,-0.05092857143,negative
thrift,4949,description,"I am currently improving the HTTP/1 test case. I found that the servlet 2.5 dependency package is relatively old, and users need to create the http environment themselves, so I added the embedded package of tomcat, Second, i improved the test case of the http application layer, so the client and server can communicate with each other and print the string.",design_debt,non-optimal_design,"Thu, 5 Sep 2019 14:00:41 +0000","Thu, 11 Feb 2021 22:27:45 +0000","Mon, 21 Oct 2019 14:22:33 +0000",3975712,"I am currently improving the HTTP/1 test case. I found that the servlet 2.5 dependency package is relatively old, and users need to create the http environment themselves, so I added the embedded package of tomcat, Second, i improved the test case of the http application layer, so the client and server can communicate with each other and print the string.",0.45,0.45,neutral
thrift,544,description,"The current generator produces multiple -define statements with the same name, which isn't valid erlang code (and also isn't valid semantically if we want two different values). produces: In the patched version, it produces this:",design_debt,non-optimal_design,"Tue, 21 Jul 2009 15:00:39 +0000","Fri, 10 Sep 2010 17:04:55 +0000","Thu, 5 Aug 2010 23:24:18 +0000",32862219,"The current generator produces multiple -define statements with the same name, which isn't valid erlang code (and also isn't valid semantically if we want two different values). produces: In the patched version, it produces this:",-0.1,-0.1,negative
thrift,593,summary,Thrift Perl Client Library Performance Improvement patch,design_debt,non-optimal_design,"Mon, 28 Sep 2009 16:34:33 +0000","Tue, 29 Sep 2009 00:19:08 +0000","Tue, 29 Sep 2009 00:19:08 +0000",27875,Thrift Perl Client Library Performance Improvement patch,0.542,0.542,neutral
thrift,595,comment_0,I made some slight cosmetic changes to your v6 patch to get this.,design_debt,non-optimal_design,"Thu, 1 Oct 2009 20:51:14 +0000","Tue, 1 Nov 2011 02:54:08 +0000","Thu, 1 Oct 2009 20:54:29 +0000",195,I made some slight cosmetic changes to your v6 patch to get this.,0,0,neutral
thrift,628,description,"It turns out that Enums in Java don't have good hashcode behavior. It uses object ID, which can be inconsistent between different invocations of the same application, which breaks things like Hadoop partitioning. We should use the hash of the actual thrift field id instead of the hash of the enum version of the field.",design_debt,non-optimal_design,"Tue, 17 Nov 2009 21:27:06 +0000","Tue, 17 Nov 2009 21:56:51 +0000","Tue, 17 Nov 2009 21:56:51 +0000",1785,"It turns out that Enums in Java don't have good hashcode behavior. It uses object ID, which can be inconsistent between different invocations of the same application, which breaks things like Hadoop partitioning. We should use the hash of the actual thrift field id instead of the hash of the enum version of the field.",-0.446,-0.446,negative
thrift,636,comment_2,"I think this would be somewhat clunky and inconsistent to use. If you'd like to submit a patch, feel free to reopen.",design_debt,non-optimal_design,"Fri, 20 Nov 2009 21:50:05 +0000","Thu, 2 May 2013 02:29:23 +0000","Fri, 26 Mar 2010 23:16:22 +0000",10891577,"I think this would be somewhat clunky and inconsistent to use. If you'd like to submit a patch, feel free to reopen.",0.1,0.1,negative
thrift,636,description,"The intention of this idea is to minimize the amount of typing necessary to navigate thrift structures. The idea is to be able to ""include"" a field within a struct, like so (I don't know the syntax of Thrift annotations but this is the idea): struct B { 1: required i32 f1; 2: required i32 f2; } struct A { 1: @include required B b; 2: required i32 field2; } If we have an instance of A named ""a"", we can access the inner B's fields by saying ""a.get_f1()"". There's the obvious problem of name conflicts, but I think it's fine to leave it to the programmer to make sure the code is safe.",design_debt,non-optimal_design,"Fri, 20 Nov 2009 21:50:05 +0000","Thu, 2 May 2013 02:29:23 +0000","Fri, 26 Mar 2010 23:16:22 +0000",10891577,"The intention of this idea is to minimize the amount of typing necessary to navigate thrift structures. The idea is to be able to ""include"" a field within a struct, like so (I don't know the syntax of Thrift annotations but this is the idea): struct B { 1: required i32 f1; 2: required i32 f2; } struct A { 1: @include required B b; 2: required i32 field2; } If we have an instance of A named ""a"", we can access the inner B's fields by saying ""a.get_f1()"". There's the obvious problem of name conflicts, but I think it's fine to leave it to the programmer to make sure the code is safe.",0.03933333333,0.03933333333,neutral
thrift,685,comment_0,"+1. The template code that Chad and I worked on for C\+\+ (almost ready for trunk) gets the complier to do something similar for us, and we saw a really significant boost.",design_debt,non-optimal_design,"Thu, 21 Jan 2010 23:07:10 +0000","Thu, 18 Feb 2010 18:28:23 +0000","Thu, 18 Feb 2010 18:28:23 +0000",2402473,"+1. The template code that Chad and I worked on for C++ (almost ready for trunk) gets the complier to do something similar for us, and we saw a really significant boost.",0.1541666667,0.1541666667,positive
thrift,685,comment_1,"Here's my initial effort. This produces a pretty noticeable speedup in deserialization time when using the compact protocol and TDeserializer. This patch notably does not yet address the binary protocol implementation. I'd love some people to review it and give me feedback. Ideally, we'd commit what I have here as a starting point and then continue to improve other parts.",design_debt,non-optimal_design,"Thu, 21 Jan 2010 23:07:10 +0000","Thu, 18 Feb 2010 18:28:23 +0000","Thu, 18 Feb 2010 18:28:23 +0000",2402473,"Here's my initial effort. This produces a pretty noticeable speedup in deserialization time when using the compact protocol and TDeserializer. This patch notably does not yet address the binary protocol implementation. I'd love some people to review it and give me feedback. Ideally, we'd commit what I have here as a starting point and then continue to improve other parts.",0.204,0.204,positive
thrift,685,description,"After poking around a bit and comparing how Thrift performs versus Protocol Buffers, I think we should change our transports and protocols to support optional direct buffer access behavior. Basically, the way this works is that if the transport is backed by a buffer, then it can give access to that buffer to the protocol. The protocol can then do things like read a byte without instantiating a new one-byte array or decode a string without an intermediate byte[] copy. In my initial testing, we can reduce the amount of time it takes to deserialize a struct by at least 25%. There are probably further gains to be had as well.",design_debt,non-optimal_design,"Thu, 21 Jan 2010 23:07:10 +0000","Thu, 18 Feb 2010 18:28:23 +0000","Thu, 18 Feb 2010 18:28:23 +0000",2402473,"After poking around a bit and comparing how Thrift performs versus Protocol Buffers, I think we should change our transports and protocols to support optional direct buffer access behavior. Basically, the way this works is that if the transport is backed by a buffer, then it can give access to that buffer to the protocol. The protocol can then do things like read a byte without instantiating a new one-byte array or decode a string without an intermediate byte[] copy. In my initial testing, we can reduce the amount of time it takes to deserialize a struct by at least 25%. There are probably further gains to be had as well.",0.2870666667,0.2870666667,neutral
thrift,685,summary,Direct buffer access to improve deserialization performance,design_debt,non-optimal_design,"Thu, 21 Jan 2010 23:07:10 +0000","Thu, 18 Feb 2010 18:28:23 +0000","Thu, 18 Feb 2010 18:28:23 +0000",2402473,Direct buffer access to improve deserialization performance,0.4,0.4,neutral
thrift,695,comment_1,"This seems like a very faithful translation of the Java version, which has benefits and drawbacks. I'm okay committing it, but the patch seems corrupt (""%ld"" where there should be a line number).",design_debt,non-optimal_design,"Tue, 2 Feb 2010 00:04:30 +0000","Tue, 1 Nov 2011 02:53:48 +0000","Fri, 26 Feb 2010 00:56:40 +0000",2076730,"This seems like a very faithful translation of the Java version, which has benefits and drawbacks. I'm okay committing it, but the patch seems corrupt (""%ld"" where there should be a line number).",0.3354166667,0.3354166667,neutral
thrift,701,description,"In glancing at one of our jars of generated Thrift class files, I noticed that it was pretty huge. Further digging showed that while certainly most of the bulk was attributable to the number of class files we have, part of it was due to lots of unnecessary internal classes. This is because we use a specific syntax for defining some maps inline in structs and enums. Since this is only syntactic sugar, I think it would make sense to write a tiny bit of helper code and avoid the need for a dynamic internal class.",design_debt,non-optimal_design,"Wed, 10 Feb 2010 23:42:07 +0000","Tue, 23 Mar 2010 05:39:48 +0000","Tue, 23 Mar 2010 05:39:48 +0000",3477461,"In glancing at one of our jars of generated Thrift class files, I noticed that it was pretty huge. Further digging showed that while certainly most of the bulk was attributable to the number of class files we have, part of it was due to lots of unnecessary internal classes. This is because we use a specific syntax for defining some maps inline in structs and enums. Since this is only syntactic sugar, I think it would make sense to write a tiny bit of helper code and avoid the need for a dynamic internal class.",0.25625,0.25625,neutral
thrift,710,description,TBinaryProtocol can probably take even more advantage of direct buffer access than TCompactProtocol. Identify the relevant spots and clean it up.,design_debt,non-optimal_design,"Thu, 18 Feb 2010 18:29:55 +0000","Tue, 2 Mar 2010 18:49:10 +0000","Tue, 2 Mar 2010 18:49:10 +0000",1037955,TBinaryProtocol can probably take even more advantage of direct buffer access than TCompactProtocol. Identify the relevant spots and clean it up.,0.523,0.523,neutral
thrift,714,description,"THsHaServer instantiates its ThreadPoolExecutor with a That behavior is documented in java as: There are three general strategies for queuing: ... 2. Unbounded queues. Using an unbounded queue (for example a LinkedBlockingQueue without a predefined capacity) will cause new tasks to wait in the queue when all corePoolSize threads are busy. Thus, no more than corePoolSize threads will ever be created. (And the value of the maximumPoolSize therefore doesn't have any effect.) This may be appropriate when each task is completely independent of others, so tasks cannot affect each others execution; for example, in a web page server. While this style of queuing can be useful in smoothing out transient bursts of requests, it admits the possibility of unbounded work queue growth when commands continue to arrive on average faster than they can be processed. therefore changing maxWorkerThreads (passed as maximumPoolSize) has no effect. The parameter should probably just be removed and minWorkerThreads renamed to numThreads, since setting minWorkerThreads does have an effect and is a workaround.",design_debt,non-optimal_design,"Wed, 24 Feb 2010 16:20:58 +0000","Wed, 28 Jul 2010 21:32:18 +0000","Wed, 28 Jul 2010 21:32:18 +0000",13324280,"THsHaServer instantiates its ThreadPoolExecutor with a LinkedBlockingQueue. That behavior is documented in java as: There are three general strategies for queuing: ... 2. Unbounded queues. Using an unbounded queue (for example a LinkedBlockingQueue without a predefined capacity) will cause new tasks to wait in the queue when all corePoolSize threads are busy. Thus, no more than corePoolSize threads will ever be created. (And the value of the maximumPoolSize therefore doesn't have any effect.) This may be appropriate when each task is completely independent of others, so tasks cannot affect each others execution; for example, in a web page server. While this style of queuing can be useful in smoothing out transient bursts of requests, it admits the possibility of unbounded work queue growth when commands continue to arrive on average faster than they can be processed. therefore changing maxWorkerThreads (passed as maximumPoolSize) has no effect. The parameter should probably just be removed and minWorkerThreads renamed to numThreads, since setting minWorkerThreads does have an effect and is a workaround.",0.1194444444,0.1075,neutral
thrift,717,comment_3,"Well, I can't accept this patch because it doesn't update every use of THRIFT_ROOT. However, I'm not convinced that this would be a good approach even if the patch were complete. I think the security argument is completely pointless, since any attacker capable of injecting PHP code into your execution would not need to mess with Thrift to take over your server. The performance argument is also not compelling, since the cost of a single global lookup is tiny compared to the cost of including a library file (even with APC (or HipHop)). Finally, I think that leaving the value in a global gives users more freedom when setting up their environment. It is easy to determine if it is already set and easy to fix after-the-fact if you need to hack around something in your sandbox.",design_debt,non-optimal_design,"Thu, 25 Feb 2010 16:40:07 +0000","Tue, 1 Nov 2011 02:54:07 +0000","Mon, 11 Apr 2011 16:13:52 +0000",35422425,"Well, I can't accept this patch because it doesn't update every use of THRIFT_ROOT. However, I'm not convinced that this would be a good approach even if the patch were complete. I think the security argument is completely pointless, since any attacker capable of injecting PHP code into your execution would not need to mess with Thrift to take over your server. The performance argument is also not compelling, since the cost of a single global lookup is tiny compared to the cost of including a library file (even with APC (or HipHop)). Finally, I think that leaving the value in a global gives users more freedom when setting up their environment. It is easy to determine if it is already set and easy to fix after-the-fact if you need to hack around something in your sandbox.",0.1018611111,0.1018611111,negative
thrift,717,comment_4,I agree with David on all points above. the globals lookup is meaningless for performance next to the multi-millisecond RPC you're about to send out :),design_debt,non-optimal_design,"Thu, 25 Feb 2010 16:40:07 +0000","Tue, 1 Nov 2011 02:54:07 +0000","Mon, 11 Apr 2011 16:13:52 +0000",35422425,I agree with David on all points above. the globals lookup is meaningless for performance next to the multi-millisecond RPC you're about to send out,0.1,-0.1,neutral
thrift,717,comment_6,"Ok, sure, the performance and security issues are fairly pointless. The biggest problem here is pollution of the global namespace, which is a bit of a show-stopper when integrating in to larger frameworks or libraries. Constants work because they don't pollute the $GLOBALS super-global and are immutable, so other system components can't accidentally modify them (i.e. through naming clashes) without the developer being alerted to it. I thought I'd got all references to I take it some reside outside of lib/php/src/ ?",design_debt,non-optimal_design,"Thu, 25 Feb 2010 16:40:07 +0000","Tue, 1 Nov 2011 02:54:07 +0000","Mon, 11 Apr 2011 16:13:52 +0000",35422425,"Ok, sure, the performance and security issues are fairly pointless. The biggest problem here is pollution of the global namespace, which is a bit of a show-stopper when integrating in to larger frameworks or libraries. Constants work because they don't pollute the $GLOBALS super-global and are immutable, so other system components can't accidentally modify them (i.e. through naming clashes) without the developer being alerted to it. I thought I'd got all references to $GLOBALS['THRIFT_ROOT'], I take it some reside outside of lib/php/src/ ?",0.1354166667,0.1354166667,negative
thrift,717,comment_7,"Constants pollute a different global namespace, so I don't see the win. Plus, the likelihood of a collision with ""THRIFT_ROOT"" is small. Yes, immutability protects you from another library messing you up by running later, but exposes you to another library messing you up by running sooner, so I don't see the win. Plus, it reduces your flexibility with how you choose to set THRIFT_ROOT. Yes, there are references in the generated code, created by the compiler.",design_debt,non-optimal_design,"Thu, 25 Feb 2010 16:40:07 +0000","Tue, 1 Nov 2011 02:54:07 +0000","Mon, 11 Apr 2011 16:13:52 +0000",35422425,"Constants pollute a different global namespace, so I don't see the win. Plus, the likelihood of a collision with ""THRIFT_ROOT"" is small. Yes, immutability protects you from another library messing you up by running later, but exposes you to another library messing you up by running sooner, so I don't see the win. Plus, it reduces your flexibility with how you choose to set THRIFT_ROOT. Yes, there are references in the generated code, created by the compiler.",0.0374,0.0374,negative
thrift,730,description,"The configure script that's generated fails the ""checking for boost"" part if g++ is not installed. This confuses people into thinking something's wrong with the boost installation, whereas in fact missing g++ is the issue. A g++ check should be moved in front of the boost check.",design_debt,non-optimal_design,"Thu, 11 Mar 2010 21:08:42 +0000","Mon, 20 Sep 2010 22:57:01 +0000","Mon, 20 Sep 2010 22:57:01 +0000",16681699,"The configure script that's generated fails the ""checking for boost"" part if g++ is not installed. This confuses people into thinking something's wrong with the boost installation, whereas in fact missing g++ is the issue. A g++ check should be moved in front of the boost check.",0.0375,0.0375,negative
thrift,750,description,"The C++ Compiler currelty emits most functions in the *Client class as non-virtual. This makes it difficult to subclass the generated *Client class and override its functions. In particular, if a subclass overrides the *_send and *_recv functions, it must also override the function itself. Otherwise, the *Client version of the function calls the *Client versions of *_send and *_recv. A workaround is to inherit from the interface class *If, which has virtual functions, and use them to call *Client class member functions. But this can be cumbersome in some situations, and still requires additional functions to be overridden. I propose to add a virtual option to the C++ compiler that emits function declarations as virtual. I have attached a patched version of t_cpp_generator.cc from Thrift 0.2.0 - I can work out how to turn it into a patch file if needed. Is this worth merging into the trunk?",design_debt,non-optimal_design,"Fri, 2 Apr 2010 04:31:57 +0000","Thu, 14 Dec 2017 13:54:39 +0000","Thu, 26 Jan 2017 01:55:31 +0000",215213014,"The C++ Compiler currelty emits most functions in the *Client class as non-virtual. This makes it difficult to subclass the generated *Client class and override its functions. In particular, if a subclass overrides the *_send and *_recv functions, it must also override the function itself. Otherwise, the *Client version of the function calls the *Client versions of *_send and *_recv. A workaround is to inherit from the interface class *If, which has virtual functions, and use them to call *Client class member functions. But this can be cumbersome in some situations, and still requires additional functions to be overridden. I propose to add a virtual option to the C++ compiler that emits function declarations as virtual. I have attached a patched version of t_cpp_generator.cc from Thrift 0.2.0 - I can work out how to turn it into a patch file if needed. Is this worth merging into the trunk?",0.12405,0.12405,neutral
thrift,813,comment_4,"Hi Jordan, This too is most a browser side encoding issue. The js implementation of writeString uses I suppose this isn't consistent, with the non javascript protocol readers. but the js readString uses We could implement our own encoder to encode only JSON special chars. -Jake",design_debt,non-optimal_design,"Sun, 4 Jul 2010 09:35:36 +0000","Tue, 1 Nov 2011 02:52:22 +0000","Fri, 22 Oct 2010 13:28:27 +0000",9517971,"Hi Jordan, This too is most a browser side encoding issue. The js implementation of writeString uses encodeURIComponent() I suppose this isn't consistent, with the non javascript protocol readers. but the js readString uses decodeURIComponent() We could implement our own encoder to encode only JSON special chars. -Jake",0,0,negative
thrift,840,comment_1,"I'm no Perl guy, but it seems like there's no reason to have two different kinds of ""die"" cases. The one with the if seems to be subsumed by the one without - am I wrong?",design_debt,non-optimal_design,"Fri, 6 Aug 2010 20:46:27 +0000","Thu, 2 Sep 2010 00:26:34 +0000","Thu, 2 Sep 2010 00:26:34 +0000",2259607,"I'm no Perl guy, but it seems like there's no reason to have two different kinds of ""die"" cases. The one with the if seems to be subsumed by the one without - am I wrong?",0.425,0.425,negative
thrift,840,description,"The Perl protocol implementation can loop arbitrarily on corrupt data because Protocol::skip() skips nothing if it doesn't recognise a type. It might be nice if it threw an exception instead. For example, the loop to skip a map will keep looking at the same invalid bytes for every iteration if it hits an unrecognised type. If corrupt data has resulted in a bogus huge map size, then the loop goes on for a long while, rather than dying quickly after running out of available data. I recognise that input validation probably isn't a priority for Thrift (usually communications are well-formed!), but wonder if you'd consider the attached patch which dies if skip or skipBinary encounter types that they don't know what to do with. It probably needs more thought than I've given it, as I'm not sure what the correct behaviour should be for valid types not handled by the existing code: in the patch I'm throwing an exception for those saying that they cannot be skipped. An additional TType constant of MAX_TYPE might be helpful in writing a better solution. I know it's a bit weird to be suggesting this; I'm using Thrift for serialisation among other things, and with an data store. This ""infinite"" loop was the only instance in which your existing error handling didn't adequately flag up bad data. Clearly I should also be doing checksums on the data beforehand, but I just thought I'd suggest this to you. Thanks, Conrad",design_debt,non-optimal_design,"Fri, 6 Aug 2010 20:46:27 +0000","Thu, 2 Sep 2010 00:26:34 +0000","Thu, 2 Sep 2010 00:26:34 +0000",2259607,"The Perl protocol implementation can loop arbitrarily on corrupt data because Protocol::skip() skips nothing if it doesn't recognise a type. It might be nice if it threw an exception instead. For example, the loop to skip a map (lib/perl/lib/Thrift/Protocol.pm:374) will keep looking at the same invalid bytes for every iteration if it hits an unrecognised type. If corrupt data has resulted in a bogus huge map size, then the loop goes on for a long while, rather than dying quickly after running out of available data. I recognise that input validation probably isn't a priority for Thrift (usually communications are well-formed!), but wonder if you'd consider the attached patch which dies if skip or skipBinary encounter types that they don't know what to do with. It probably needs more thought than I've given it, as I'm not sure what the correct behaviour should be for valid types not handled by the existing code: in the patch I'm throwing an exception for those saying that they cannot be skipped. An additional TType constant of MAX_TYPE might be helpful in writing a better solution. I know it's a bit weird to be suggesting this; I'm using Thrift for serialisation among other things, and with an occasionally-imperfect data store. This ""infinite"" loop was the only instance in which your existing error handling didn't adequately flag up bad data. Clearly I should also be doing checksums on the data beforehand, but I just thought I'd suggest this to you. Thanks, Conrad",-0.04775,-0.04407692308,negative
thrift,897,comment_4,"I think the smart resolution thing is only worthwhile if we really want to keep backwards compatibility. I'm not crazy about breaking people's IDL, but I think that's preferable in the long term to supporting a more convoluted syntax. If we were starting from scratch today, I think that we'd want to use the fully-qualified approach, and the cost of fixing the breakage *should* be reasonably low, especially because you're getting a consistency improvement at the same time. This is certainly my preference.",design_debt,non-optimal_design,"Fri, 10 Sep 2010 18:59:08 +0000","Sun, 12 Sep 2010 14:39:15 +0000","Sun, 12 Sep 2010 14:39:15 +0000",157207,"I think the smart resolution thing is only worthwhile if we really want to keep backwards compatibility. I'm not crazy about breaking people's IDL, but I think that's preferable in the long term to supporting a more convoluted syntax. If we were starting from scratch today, I think that we'd want to use the fully-qualified approach, and the cost of fixing the breakage should be reasonably low, especially because you're getting a consistency improvement at the same time. This is certainly my preference.",0.4149375,0.4149375,neutral
thrift,897,description,"Through looking at THRIFT-544 and THRIFT-895, it's come to my attention that we currently register each of every enum's values as a global (and scoped) constant. This allows you to do things like: This is handy, insofar as you might want to use the values of an enum in constant or default circumstances. However, this behavior is unstable - if you have two enums with values that have the same name, all constant references will point at the last occurrence of the name. Further, in order to allow this to go on, we must not check if any constant has been declared twice, which means you can get stupid, detectable errors in your IDL very easily. I propose that we stop allowing this method of access, and instead require the enum values referenced in constant context to be prefixed with the enum type's name. For instance:",design_debt,non-optimal_design,"Fri, 10 Sep 2010 18:59:08 +0000","Sun, 12 Sep 2010 14:39:15 +0000","Sun, 12 Sep 2010 14:39:15 +0000",157207,"Through looking at THRIFT-544 and THRIFT-895, it's come to my attention that we currently register each of every enum's values as a global (and scoped) constant. This allows you to do things like: This is handy, insofar as you might want to use the values of an enum in constant or default circumstances. However, this behavior is unstable - if you have two enums with values that have the same name, all constant references will point at the last occurrence of the name. Further, in order to allow this to go on, we must not check if any constant has been declared twice, which means you can get stupid, detectable errors in your IDL very easily. I propose that we stop allowing this method of access, and instead require the enum values referenced in constant context to be prefixed with the enum type's name. For instance:",-0.03411904762,-0.03411904762,neutral
thrift,906,description,"The current haskell type mappings are awkward and error prone to work with: binary -string -byte -i16 -i32 -i64 - This patch updates the mappings to the canonical types of the correct length in Haskell: binary -string -byte -i16 -i32 -i64 - THIS BREAKS EXISTING CODE. It is, however, very arguably broken already. For convenience of patching, this patch is a superset of THRIFT-743. Thoughts?",design_debt,non-optimal_design,"Sun, 19 Sep 2010 02:20:29 +0000","Wed, 22 Sep 2010 00:49:12 +0000","Wed, 22 Sep 2010 00:49:12 +0000",253723,"The current haskell type mappings are awkward and error prone to work with: binary -> String string -> String byte -> Int i16 -> Int i32 -> Int i64 -> Int This patch updates the mappings to the canonical types of the correct length in Haskell: binary -> Data.ByteString.Lazy.ByteString string -> String byte -> Data.Word.Word8 i16 -> Data.Int.Int16 i32 -> Data.Int.Int32 i64 -> Data.Int.Int64 THIS BREAKS EXISTING CODE. It is, however, very arguably broken already. For convenience of patching, this patch is a superset of THRIFT-743. Thoughts?",0.00025,-0.01326666667,negative
thrift,931,description,"Right now, slf4j-simple is used when the tests are run. Unfortunately, the ""simple"" logger doesn't support debug level messages at all, which is somewhat of a pain. It would be good to switch to slf4j-log4j for the tests so we have debug logs available.",design_debt,non-optimal_design,"Mon, 27 Sep 2010 21:35:01 +0000","Tue, 1 Nov 2011 02:54:12 +0000","Mon, 27 Sep 2010 23:51:32 +0000",8191,"Right now, slf4j-simple is used when the tests are run. Unfortunately, the ""simple"" logger doesn't support debug level messages at all, which is somewhat of a pain. It would be good to switch to slf4j-log4j for the tests so we have debug logs available.",0.2232222222,0.2232222222,negative
thrift,959,comment_1,I just committed a tiny fix for this. I found that removing the buffer improved the distribution of latencies by a small but notable percentage.,design_debt,non-optimal_design,"Fri, 15 Oct 2010 21:13:09 +0000","Thu, 3 Jan 2019 04:32:08 +0000","Thu, 3 Jan 2019 04:29:35 +0000",259312586,I just committed a tiny fix for this. I found that removing the buffer improved the distribution of latencies by a small but notable percentage.,0.6,0.6,positive
thrift,959,description,"I was looking through TSocket today while reviewing THRIFT-106 and I noticed that in TSocket, when we open the socket/stream, we wrap the input/output streams with objects and use those for reading and writing. Two things stand out about this. Firstly, for some reason we're setting the buffer size specifically to 1KB, which is 1/8 the default. I think that number should be *at least* 8KB and more likely something like 32KB would be better. Anyone have any idea why we chose this size? Secondly, though, is the fact that we probably shouldn't be doing buffering here at all. The general pattern is to open a TSocket and wrap it in a TFramedTransport, which means that today, even though we're fully buffering in the framed transport, we're wastefully buffering again in the TSocket. This means we're wasting time and memory, and I wouldn't be surprised if this is artificially slowing down throughput, specifically for multi-KB requests and responses. If we remove the buffering from TSocket, I think we will probably need to add a TBufferedTransport to support users who are talking to non-Framed servers but still need buffering for performance.",design_debt,non-optimal_design,"Fri, 15 Oct 2010 21:13:09 +0000","Thu, 3 Jan 2019 04:32:08 +0000","Thu, 3 Jan 2019 04:29:35 +0000",259312586,"I was looking through TSocket today while reviewing THRIFT-106 and I noticed that in TSocket, when we open the socket/stream, we wrap the input/output streams with Buffered(Input|Output)Stream objects and use those for reading and writing. Two things stand out about this. Firstly, for some reason we're setting the buffer size specifically to 1KB, which is 1/8 the default. I think that number should be at least 8KB and more likely something like 32KB would be better. Anyone have any idea why we chose this size? Secondly, though, is the fact that we probably shouldn't be doing buffering here at all. The general pattern is to open a TSocket and wrap it in a TFramedTransport, which means that today, even though we're fully buffering in the framed transport, we're wastefully buffering again in the TSocket. This means we're wasting time and memory, and I wouldn't be surprised if this is artificially slowing down throughput, specifically for multi-KB requests and responses. If we remove the buffering from TSocket, I think we will probably need to add a TBufferedTransport to support users who are talking to non-Framed servers but still need buffering for performance.",0.04124074074,0.04124074074,neutral
thrift,978,summary,better error message when pkg-config is missing,design_debt,non-optimal_design,"Fri, 29 Oct 2010 05:49:36 +0000","Mon, 7 Mar 2011 21:22:56 +0000","Mon, 7 Mar 2011 21:22:56 +0000",11201600,better error message when pkg-config is missing,-0.1,-0.1,negative
thrift,1063,description,"A user was having issues with the erlang tutorial as it was out of date. I fixed and commited the change, but figured people would like a record of it being done.",documentation_debt,outdated_documentation,"Wed, 16 Feb 2011 18:08:52 +0000","Wed, 9 Mar 2011 18:17:14 +0000","Wed, 16 Feb 2011 18:10:25 +0000",93,"A user was having issues with the erlang tutorial as it was out of date. I fixed and commited the change, but figured people would like a record of it being done.",0,0,negative
thrift,1135,description,It would be great to have a tutorial for the Node.js implementation located at tutorial/nodejs/,documentation_debt,low_quality_documentation,"Fri, 8 Apr 2011 21:20:38 +0000","Wed, 12 Mar 2014 02:36:59 +0000","Wed, 12 Mar 2014 02:36:59 +0000",92294181,It would be great to have a tutorial for the Node.js implementation located at tutorial/nodejs/,0.3,0.3,positive
thrift,1294,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Tue, 6 Apr 2010 20:59:32 +0000 Subject: [PATCH 20/33] thrift: fix log message typos in TSimpleServer Summary: TSimpleSimple -- Also cleaned up some references that could be const. Test Plan: It compiles. Revert Plan: OK  | 15 +++++++++ 1 files changed, 9 insertions(+), 6 deletions(-)",documentation_debt,low_quality_documentation,"Thu, 25 Aug 2011 16:50:47 +0000","Thu, 25 Aug 2011 18:12:56 +0000","Thu, 25 Aug 2011 17:44:41 +0000",3234,"From 1cedaf9061760446e5c70de0797b93c3837e9841 Mon Sep 17 00:00:00 2001 From: Adam Simpkins <simpkins@fb.com> Date: Tue, 6 Apr 2010 20:59:32 +0000 Subject: [PATCH 20/33] thrift: fix log message typos in TSimpleServer Summary: TSimpleSimple --> TSimpleServer Also cleaned up some references that could be const. Test Plan: It compiles. Revert Plan: OK  lib/cpp/src/server/TSimpleServer.cpp | 15 +++++++++------ 1 files changed, 9 insertions, 6 deletions",0.25,0.1875,neutral
thrift,1294,summary,thrift: fix log message typos in TSimpleServer,documentation_debt,low_quality_documentation,"Thu, 25 Aug 2011 16:50:47 +0000","Thu, 25 Aug 2011 18:12:56 +0000","Thu, 25 Aug 2011 17:44:41 +0000",3234," thrift: fix log message typos in TSimpleServer
",0,0,negative
thrift,1426,description,"dist package missing javame, cpp concurrency headers, nodejs, erl tutorial, all windows additions, and other random files",documentation_debt,low_quality_documentation,"Tue, 15 Nov 2011 13:18:27 +0000","Wed, 16 Nov 2011 13:25:27 +0000","Wed, 16 Nov 2011 12:59:10 +0000",85243,"dist package missing javame, cpp concurrency headers, nodejs, erl tutorial, all windows additions, and other random files",-0.4,-0.4,negative
thrift,1466,comment_0,"Currently there are no tutorials available for this lib, also not all generators create a skeleton as you have seen. Your best starting point is the test cases included with the lib lib/c_glib/test/. We are working on trying to create better documentation as we redo the website, but right now your best bet is to email the dev list with any questions you have",documentation_debt,low_quality_documentation,"Thu, 15 Dec 2011 15:35:32 +0000","Fri, 27 Jan 2012 02:34:29 +0000","Fri, 27 Jan 2012 02:34:28 +0000",3668336,"Currently there are no tutorials available for this lib, also not all generators create a skeleton as you have seen. Your best starting point is the test cases included with the lib lib/c_glib/test/. We are working on trying to create better documentation as we redo the website, but right now your best bet is to email the dev list with any questions you have",0.4251333333,0.4251333333,neutral
thrift,1466,description,"When I try to generate the cglib code for the Cassandra Interface, I do not get a skeleton file. Also, I do not see any example files onto how to use the generated code, and I can't seem to find any examples around. So can anybody please offer some proper examples on how to use the Thrift generated glibc code for Cassandra ? Thanks",documentation_debt,low_quality_documentation,"Thu, 15 Dec 2011 15:35:32 +0000","Fri, 27 Jan 2012 02:34:29 +0000","Fri, 27 Jan 2012 02:34:28 +0000",3668336,"When I try to generate the cglib code for the Cassandra Interface, I do not get a skeleton file. Also, I do not see any example files onto how to use the generated code, and I can't seem to find any examples around. So can anybody please offer some proper examples on how to use the Thrift generated glibc code for Cassandra ? Thanks",0.191375,0.191375,negative
thrift,1466,summary,Proper Documentation for Thrift C Glib,documentation_debt,low_quality_documentation,"Thu, 15 Dec 2011 15:35:32 +0000","Fri, 27 Jan 2012 02:34:29 +0000","Fri, 27 Jan 2012 02:34:28 +0000",3668336,Proper Documentation for Thrift C Glib,0.531,0.531,neutral
thrift,147,description,"Generated code should include the provided docstrings (from the IDL) as RDoc on the classes. If the fields weren't generated at runtime, they should have RDoc too.",documentation_debt,low_quality_documentation,"Mon, 22 Sep 2008 20:52:23 +0000","Thu, 2 May 2013 02:29:17 +0000","Fri, 31 Oct 2008 00:33:45 +0000",3296482,"Generated code should include the provided docstrings (from the IDL) as RDoc on the classes. If the fields weren't generated at runtime, they should have RDoc too.",0,0,neutral
thrift,147,summary,Ruby generated classes should include class doc strings,documentation_debt,low_quality_documentation,"Mon, 22 Sep 2008 20:52:23 +0000","Thu, 2 May 2013 02:29:17 +0000","Fri, 31 Oct 2008 00:33:45 +0000",3296482,Ruby generated classes should include class doc strings,0,0,neutral
thrift,1702,description,"Attached is a manual for the Thrift compiler in mdoc format, and a patch to eliminate the brutal 80-line error message that was usage(). The man page may be previewed at The tail end is incomplete because the information didn't happen to be included in the usage() text. I hope you agree the manual is easier to read and more convenient to use.",documentation_debt,low_quality_documentation,"Sat, 22 Sep 2012 03:20:42 +0000","Sat, 29 Sep 2012 00:31:31 +0000","Sat, 29 Sep 2012 00:31:31 +0000",594649,"Attached is a manual for the Thrift compiler in mdoc format, and a patch to eliminate the brutal 80-line error message that was usage(). The man page may be previewed at http://www.schemamania.org/thrift/thrift.pdf. The tail end is incomplete because the information didn't happen to be included in the usage() text. I hope you agree the manual is easier to read and more convenient to use.",0.175,0.13125,neutral
thrift,1734,description,"The front webpage of is still displaying the current release version as v0.8, instead of the newly released v0.9 as displayed on the downloads page. I presume this is because the instructions at do not specify that this page needs updating as well.",documentation_debt,outdated_documentation,"Thu, 18 Oct 2012 08:31:21 +0000","Sat, 8 Jun 2013 03:15:40 +0000","Sat, 8 Jun 2013 03:15:39 +0000",20112258,"The front webpage of http://thrift.apache.org is still displaying the current release version as v0.8, instead of the newly released v0.9 as displayed on the downloads page. I presume this is because the instructions at http://thrift.apache.org/docs/committers/HowToPublish do not specify that this page needs updating as well.",0.228,0.228,negative
thrift,1734,summary,Front webpage is still advertising v0.8 as current release,documentation_debt,outdated_documentation,"Thu, 18 Oct 2012 08:31:21 +0000","Sat, 8 Jun 2013 03:15:40 +0000","Sat, 8 Jun 2013 03:15:39 +0000",20112258,Front webpage is still advertising v0.8 as current release,0.1405,0.1405,neutral
thrift,1745,comment_2,"Yes there is a typo in TJSONProtocol.py line 49, it should be ""rec"" instead of ""rect"" sorry... Also i run the test again and it seems there are other problems, coming from somewhere else and affect all protocols",documentation_debt,low_quality_documentation,"Tue, 30 Oct 2012 15:31:10 +0000","Sat, 20 Feb 2021 15:27:14 +0000","Tue, 20 Nov 2012 22:12:50 +0000",1838500,"Yes there is a typo in TJSONProtocol.py line 49, it should be ""rec"" instead of ""rect"" sorry... Also i run the test again and it seems there are other problems, coming from somewhere else and affect all protocols: Apache Thrift - integration test suite Fri Nov 2 09:31:33 CET 2012 ====================================================== client-server: protocol: transport: result: cpp-cpp binary buffered-ip success cpp-cpp binary buffered-domain success cpp-cpp binary framed-ip success cpp-cpp binary framed-domain success cpp-cpp binary http-ip success cpp-cpp binary http-domain success cpp-cpp json buffered-ip success cpp-cpp json buffered-domain success cpp-cpp json framed-ip success cpp-cpp json framed-domain success cpp-cpp json http-ip success cpp-cpp json http-domain success py-py binary buffered-ip success py-py json buffered-ip success py-cpp binary buffered-ip success py-cpp json buffered-ip success cpp-py binary buffered-ip failure =================== server message =================== Traceback (most recent call last): File ""py/../../lib/py/build/lib.linux-x86_64-2.7/thrift/server/TServer.py"", line 84, in serve self.processor.process(iprot, oprot) File ""py/gen-py/ThriftTest/ThriftTest.py"", line 1006, in process self._processMap[name](self, seqid, iprot, oprot) File ""py/gen-py/ThriftTest/ThriftTest.py"", line 1170, in process_testMapMap result.write(oprot) File ""py/gen-py/ThriftTest/ThriftTest.py"", line 3054, in write oprot.writeMapBegin(TType.I32, TType.MAP, len(self.success)) TypeError: object of type 'int' has no len() =================== client message =================== testMap( {0 => -10, 1 => -9, 2 => -8, 3 => -7, 4 => -6}) = {0 => -10, 1 => -9, 2 => -8, 3 => -7, 4 => -6} testSet({-2, -1, 0, 1, 2}) = {-2, -1, 0, 1, 2} testList({-2, -1, 0, 1, 2}) = {-2, -1, 0, 1, 2} testEnum(ONE) = 1 testEnum(TWO) = 2 testEnum(THREE) = 3 testEnum(FIVE) = 5 testEnum(EIGHT) = 8 testTypedef(309858235082523) = 309858235082523 testMapMap(1)Aborted ====================================================== client-server: protocol: transport: result: cpp-py json buffered-ip failure =================== server message =================== Traceback (most recent call last): File ""py/../../lib/py/build/lib.linux-x86_64-2.7/thrift/server/TServer.py"", line 84, in serve self.processor.process(iprot, oprot) File ""py/gen-py/ThriftTest/ThriftTest.py"", line 1006, in process self._processMap[name](self, seqid, iprot, oprot) File ""py/gen-py/ThriftTest/ThriftTest.py"", line 1170, in process_testMapMap result.write(oprot) File ""py/gen-py/ThriftTest/ThriftTest.py"", line 3054, in write oprot.writeMapBegin(TType.I32, TType.MAP, len(self.success)) TypeError: object of type 'int' has no len() =================== client message =================== testMap( {0 => -10, 1 => -9, 2 => -8, 3 => -7, 4 => -6}) = {0 => -10, 1 => -9, 2 => -8, 3 => -7, 4 => -6} testSet({-2, -1, 0, 1, 2}) = {-2, -1, 0, 1, 2} testList({-2, -1, 0, 1, 2}) = {-2, -1, 0, 1, 2} testEnum(ONE) = 1 testEnum(TWO) = 2 testEnum(THREE) = 3 testEnum(FIVE) = 5 testEnum(EIGHT) = 8 testTypedef(309858235082523) = 309858235082523 testMapMap(1)Aborted ======================================================",-0.125,0.00561622807,negative
thrift,1745,comment_3,"Thanks! committed the typo fix. Yes, there are some other issues we need to fix with the cross language test suite: THRIFT-847 Test Framework harmonization across all languages any help is welcome!",documentation_debt,low_quality_documentation,"Tue, 30 Oct 2012 15:31:10 +0000","Sat, 20 Feb 2021 15:27:14 +0000","Tue, 20 Nov 2012 22:12:50 +0000",1838500,"Thanks! committed the typo fix. Yes, there are some other issues we need to fix with the cross language test suite: THRIFT-847 Test Framework harmonization across all languages any help is welcome!",0.45,0.45,neutral
thrift,1800,description,"Comments containing non-ascii characters (like for example the swedish ""Vi mter temperaturer i C"") show up incorrectly in the generated HTML file. Only a handful of characters is encoded at all, and only at a few rare occasions.",documentation_debt,low_quality_documentation,"Fri, 21 Dec 2012 21:45:26 +0000","Sat, 8 Jun 2013 03:15:43 +0000","Sat, 8 Jun 2013 03:15:43 +0000",14535017,"Comments containing non-ascii characters (like for example the swedish ""Vi mter temperaturer i C"") show up incorrectly in the generated HTML file. Only a handful of characters is encoded at all, and only at a few rare occasions.",0,0,negative
thrift,1800,summary,Documentation text not always escaped correctly when rendered to HTML,documentation_debt,low_quality_documentation,"Fri, 21 Dec 2012 21:45:26 +0000","Sat, 8 Jun 2013 03:15:43 +0000","Sat, 8 Jun 2013 03:15:43 +0000",14535017,Documentation text not always escaped correctly when rendered to HTML,-0.5,-0.5,neutral
thrift,1879,description,"It would be nice if thrift_c_glib bindings supported GObject Introspection (see This would make Vala bindings (THRIFT-1741) easy. Actually adding G-I is pretty easy (I already have a patch to do it), but would require migrating the documentation to the gtk-doc format, so before I invest the time I'd like to make sure this is something you're interested in.",documentation_debt,outdated_documentation,"Mon, 11 Mar 2013 07:14:27 +0000","Sat, 8 Jun 2013 18:34:31 +0000","Sat, 8 Jun 2013 18:34:31 +0000",7730404,"It would be nice if thrift_c_glib bindings supported GObject Introspection (see https://live.gnome.org/GObjectIntrospection). This would make Vala bindings (THRIFT-1741) easy. Actually adding G-I is pretty easy (I already have a patch to do it), but would require migrating the documentation to the gtk-doc format, so before I invest the time I'd like to make sure this is something you're interested in.",0.4015,0.4006111111,positive
thrift,1883,description,"Right now thrift_c_glib contains a few sporadic doxygen-style comments which are, AFAICT, not actually used to generate any documentation. In order to generate a GObject Introspection repository g-ir-scanner scans the source code and looks for GTK-Doc comments which contain annotations (see necessary for the GIR to work properly. I've fully documented thrift_c_glib (well, the API is fully covered, although I'm the first to admit the documentation could be better) using GTK-Doc comments and added support to the build system to go ahead and actually build a manual.",documentation_debt,low_quality_documentation,"Thu, 14 Mar 2013 07:41:50 +0000","Sat, 8 Jun 2013 18:35:53 +0000","Sat, 8 Jun 2013 18:35:53 +0000",7469643,"Right now thrift_c_glib contains a few sporadic doxygen-style comments which are, AFAICT, not actually used to generate any documentation. In order to generate a GObject Introspection repository g-ir-scanner scans the source code and looks for GTK-Doc comments which contain annotations (see https://live.gnome.org/GObjectIntrospection/Annotations) necessary for the GIR to work properly. I've fully documented thrift_c_glib (well, the API is fully covered, although I'm the first to admit the documentation could be better) using GTK-Doc comments and added support to the build system to go ahead and actually build a manual.",0.1932333333,0.1932333333,neutral
thrift,195,comment_0,"The docs about an apache/php endpoint are sorta facebook specific. Other than that, LGTM.",documentation_debt,low_quality_documentation,"Sat, 8 Nov 2008 00:59:00 +0000","Tue, 1 Nov 2011 02:51:54 +0000","Sat, 31 Jan 2009 22:13:12 +0000",7334052,"The docs about an apache/php endpoint are sorta facebook specific. Other than that, LGTM.",0,0,neutral
thrift,2088,description,"There are a few typos in the delphi Compiler options text. Additionally, the text is not really well written.",documentation_debt,low_quality_documentation,"Sun, 14 Jul 2013 11:49:07 +0000","Thu, 25 Jul 2013 03:40:06 +0000","Sun, 14 Jul 2013 11:57:14 +0000",487,"There are a few typos in the delphi Compiler options text. Additionally, the text is not really well written.",-0.3155,-0.3155,negative
thrift,2088,summary,Typos in Thrift compiler help text,documentation_debt,low_quality_documentation,"Sun, 14 Jul 2013 11:49:07 +0000","Thu, 25 Jul 2013 03:40:06 +0000","Sun, 14 Jul 2013 11:57:14 +0000",487,Typos in Thrift compiler help text,0.4,0.4,negative
thrift,2116,comment_0,"If there is anyone out there spending the time to review this and THRIFT-2144, I'd be happy to commit it. Any comments on whether or not the stuff from the Wiki is still useful for the new tutorial pages or is outdated would be highly appreciated too.",documentation_debt,outdated_documentation,"Sun, 11 Aug 2013 14:49:54 +0000","Thu, 10 Oct 2019 23:00:05 +0000","Mon, 14 Jan 2019 15:03:33 +0000",171245619,"If there is anyone out there spending the time to review this and THRIFT-2144, I'd be happy to commit it. Any comments on whether or not the stuff from the Wiki is still useful for the new tutorial pages or is outdated would be highly appreciated too.",0.4343333333,0.4343333333,positive
thrift,2150,comment_1,"Fair enough, might be worth documenting somewhere that the thrift cpp library use Winsock2 and therefore users should be aware of the fact that their own code (and other third-party libraries like Qt) might cause issues if they include Windows.h before the thrift headers.",documentation_debt,low_quality_documentation,"Mon, 26 Aug 2013 12:29:13 +0000","Wed, 6 May 2015 13:19:11 +0000","Wed, 6 May 2015 13:19:10 +0000",53398197,"Fair enough, might be worth documenting somewhere that the thrift cpp library use Winsock2 and therefore users should be aware of the fact that their own code (and other third-party libraries like Qt) might cause issues if they include Windows.h before the thrift headers.",0.232875,0.232875,neutral
thrift,2150,comment_2,"Yes, it is worth to document this. Could some of you create a patch to update this: Thanks! roger",documentation_debt,low_quality_documentation,"Mon, 26 Aug 2013 12:29:13 +0000","Wed, 6 May 2015 13:19:11 +0000","Wed, 6 May 2015 13:19:10 +0000",53398197,"Yes, it is worth to document this. Could some of you create a patch to update this: \lib\cpp\README_WINDOWS Thanks! roger",0.2333333333,0.2333333333,positive
thrift,2290,description,"-When THRIFT-2232 has been accepted, the Go tutorial (both code and web site) will need an appropriate update.- No changes necessary, only another minor bug has been uncovered while building the Go tutorial:",documentation_debt,outdated_documentation,"Sat, 14 Dec 2013 12:23:57 +0000","Thu, 2 Jan 2014 22:30:12 +0000","Tue, 24 Dec 2013 16:20:52 +0000",878215,"When THRIFT-2232 has been accepted, the Go tutorial (both code and web site) will need an appropriate update. No changes necessary, only another minor bug has been uncovered while building the Go tutorial:",0.49,0.4916666667,neutral
thrift,2351,comment_1,"Patch for above issues. I tried to get integration tests to verify this however it seems there are no tests that exercise PHP on the service end and given the trivial nature of this patch I wanted to to submit it without committing several more hours to understand and implement more complete coverage of PHP services in the integration test harness. I was interested to see that only Java is currently tested with the compact protocol which seems an odd decision - certainly the documentation and many ""comparisons"" of thirft vs XXX on the web make a big thing about the compact protocol and give it the appearance of a first class citizen and yet it seems no one ever tried to test it for cross-language RPC? Anyway, hopefully you can see from this patch that these are obvious mistakes and typos and should be included even without the extra effort of making integration test runner able to demonstrate the bug. Thanks. Let me know if I need to submit this differently.",documentation_debt,low_quality_documentation,"Thu, 6 Feb 2014 18:00:17 +0000","Mon, 29 Jun 2015 21:32:53 +0000","Thu, 10 Jul 2014 13:42:25 +0000",13290128,"Patch for above issues. I tried to get integration tests to verify this however it seems there are no tests that exercise PHP on the service end and given the trivial nature of this patch I wanted to to submit it without committing several more hours to understand and implement more complete coverage of PHP services in the integration test harness. I was interested to see that only Java is currently tested with the compact protocol which seems an odd decision - certainly the documentation and many ""comparisons"" of thirft vs XXX on the web make a big thing about the compact protocol and give it the appearance of a first class citizen and yet it seems no one ever tried to test it for cross-language RPC? Anyway, hopefully you can see from this patch that these are obvious mistakes and typos and should be included even without the extra effort of making integration test runner able to demonstrate the bug. Thanks. Let me know if I need to submit this differently.",0.04554444444,0.04554444444,neutral
thrift,2375,description,"This was working ""right"" (at least how I thought it should work) back in 0.9.0 going back at least to 0.7.0 and introduced in 0.9.1 The issue is that there is a '<br I will attach complete sample thrift files and html output, but here is a summary: This is what the output USED to look like And this is what it looks like today",documentation_debt,low_quality_documentation,"Sun, 23 Feb 2014 02:53:28 +0000","Tue, 1 Apr 2014 20:01:32 +0000","Tue, 11 Mar 2014 20:33:57 +0000",1446029,"This was working ""right"" (at least how I thought it should work) back in 0.9.0 going back at least to 0.7.0 and introduced in 0.9.1 https://git-wip-us.apache.org/repos/asf?p=thrift.git;a=commit;h=63e3c63 The issue is that there is a '<br>' in HTML after every newline in the source comment (thrift). I assume that this was not the intention of the THRIFT-1800 change. I will attach complete sample thrift files and html output, but here is a summary: This is what the output USED to look like <tr><td>1</td><td>theThing</td><td><code>i32</code></td><td>Some comments can go for quite a while and may span multiple lines. What looks like a good spot to break a line in the thrift file may turn out to be not so great in HTML. In fact in HTML we should let the browser decide when to start a new lines. Users can still insert a break<br> when they really need/want it </td><td>required</td><td></td></tr> </table><br/></div></div></body></html> And this is what it looks like today <tr><td>1</td><td>theThing</td><td><code>i32</code></td><td>Some comments can go for quite a while and may span multiple<br/>lines. What looks like a good spot to break a li ne in the<br/>thrift file may turn out to be not so great in HTML. In<br/>fact in HTML we should let the browser decide when to start<br/>a new lines. Users can still<br/> insert a break<br> when they really need/want it<br/></td><td>required</td><td></td></tr> </table><br/></div></div></body></html>",0.5756666667,0.335462963,neutral
thrift,2375,summary,Excessive <br>'s in generated HTML,documentation_debt,low_quality_documentation,"Sun, 23 Feb 2014 02:53:28 +0000","Tue, 1 Apr 2014 20:01:32 +0000","Tue, 11 Mar 2014 20:33:57 +0000",1446029,Excessive <br>'s in generated HTML,0,0,neutral
thrift,2487,description,"Several people had problems following the tutorial, especially under Windows. The Thrift web site offers an EXE download and the {{tutorial.thrift}} file on the [tutorial However, the file {{shared.thrift}}, which is also needed as it is included in {{tutorial.thrift}} is not available there. The workaround is to get the file from teh source tree, e.g. from",documentation_debt,low_quality_documentation,"Sat, 19 Apr 2014 21:13:04 +0000","Thu, 10 Jul 2014 13:42:30 +0000","Thu, 10 Jul 2014 13:42:30 +0000",7057766,"Several people had problems following the tutorial, especially under Windows. The Thrift web site offers an EXE download and the tutorial.thrift file on the tutorial pages. However, the file shared.thrift, which is also needed as it is included in tutorial.thrift is not available there. The workaround is to get the file from teh source tree, e.g. from https://git-wip-us.apache.org/repos/asf?p=thrift.git;a=blob_plain;f=tutorial/shared.thrift",-0.03333333333,-0.02857142857,negative
thrift,2545,summary,Test CPP fails to build (possibly typo),documentation_debt,low_quality_documentation,"Mon, 26 May 2014 03:51:14 +0000","Thu, 10 Jul 2014 14:14:02 +0000","Thu, 10 Jul 2014 14:14:02 +0000",3925368,Test CPP fails to build (possibly typo),-0.4,-0.4,negative
thrift,2555,summary,"excessive ""unused field"" comments",documentation_debt,low_quality_documentation,"Wed, 28 May 2014 22:46:01 +0000","Fri, 30 May 2014 16:27:52 +0000","Fri, 30 May 2014 15:55:32 +0000",148171,"excessive ""unused field"" comments",0,0,negative
thrift,2677,description,"Some recent changes in the {{network}} library for Haskell require conditional compilation, both the haskell tutorial and library need to be updated to take this into account. Description of the solution is on {{network}}'s hackage page: Travis fails with: Travis run with this patch applied: Tutorial successfully compiles: cc:",documentation_debt,outdated_documentation,"Tue, 26 Aug 2014 05:21:45 +0000","Wed, 5 Nov 2014 04:49:05 +0000","Mon, 1 Sep 2014 19:56:18 +0000",570873,"Some recent changes in the network library for Haskell require conditional compilation, both the haskell tutorial and library need to be updated to take this into account. Description of the solution is on network's hackage page: http://hackage.haskell.org/package/network-2.6.0.1 Travis fails with: Travis run with this patch applied: https://travis-ci.org/cheecheeo/thrift/builds/33556425 Tutorial successfully compiles: cc: roger.meier",0.07083333333,0.04722222222,neutral
thrift,275,comment_1,"This patch removes all the deprecation stuff and the t*.rb classes that were only placeholders. In addition, I've changed the implementations of some ""abstract"" methods to throw NotImplementedError instead of returning nil, and fixed the test accordingly. Finally, I removed the no longer required borrow and consume methods from all the transport implementations that had them. (Borrow and consume have been supplanted by the thrift_native package.) All the specs and unit test pass, so I think the job is done. It seems like the only documentation that needs to be made is to indicate that when you see you just change it to and your problems more or less go away. (You will have to replace the T with Thrift:: on some declarations, but that's pretty easy too.) I'd love to have this reviewed and get it committed.",documentation_debt,outdated_documentation,"Sat, 17 Jan 2009 00:44:02 +0000","Tue, 1 Nov 2011 02:54:12 +0000","Tue, 24 Mar 2009 05:31:09 +0000",5719627,"This patch removes all the deprecation stuff and the t*.rb classes that were only placeholders. In addition, I've changed the implementations of some ""abstract"" methods to throw NotImplementedError instead of returning nil, and fixed the test accordingly. Finally, I removed the no longer required borrow and consume methods from all the transport implementations that had them. (Borrow and consume have been supplanted by the thrift_native package.) All the specs and unit test pass, so I think the job is done. It seems like the only documentation that needs to be made is to indicate that when you see you just change it to and your problems more or less go away. (You will have to replace the T with Thrift:: on some declarations, but that's pretty easy too.) I'd love to have this reviewed and get it committed.",0.02566666667,0.02566666667,neutral
thrift,2833,description,"It is written in the download page that Thrift 0.9.2 is available on Maven Central. Sadly, this is not the case.",documentation_debt,low_quality_documentation,"Mon, 17 Nov 2014 13:56:36 +0000","Mon, 26 Jan 2015 01:56:37 +0000","Tue, 18 Nov 2014 18:35:48 +0000",103152,"It is written in the download page (https://thrift.apache.org/download) that Thrift 0.9.2 is available on Maven Central. Sadly, this is not the case.",-0.1875,-0.1875,negative
thrift,2853,comment_1,Thanks for applying my patch so quickly! I forgot to remove the comments that doesn't apply anymore. This PR removes them. I try to be more careful on future patches.,documentation_debt,outdated_documentation,"Mon, 24 Nov 2014 22:11:49 +0000","Tue, 25 Nov 2014 00:18:22 +0000","Mon, 24 Nov 2014 23:40:16 +0000",5307,Thanks for applying my patch so quickly! I forgot to remove the comments that doesn't apply anymore. This PR removes them. I try to be more careful on future patches.,0.15,0.15,neutral
thrift,2853,description,THRIFT-2852 changed behavior of which made some comments invalid. This PR removes the remaining comments. PR:,documentation_debt,outdated_documentation,"Mon, 24 Nov 2014 22:11:49 +0000","Tue, 25 Nov 2014 00:18:22 +0000","Mon, 24 Nov 2014 23:40:16 +0000",5307,THRIFT-2852 changed behavior of iostream_transport.go which made some comments invalid. This PR removes the remaining comments. PR: https://github.com/apache/thrift/pull/286,0,0.125,negative
thrift,298,comment_2,"I guessed just enough to fix the problem but I don't understand the code enough to write a correct spec for this. I think the original author of the feature (Kevin Ballard (committed by Kevin Clark), I believe, according to the log) and whoever is refactoring ruby protocols (in this case, you, Bryan) should write specs for all the features of thrift.",documentation_debt,outdated_documentation,"Sat, 31 Jan 2009 09:36:34 +0000","Tue, 3 Feb 2009 01:17:25 +0000","Tue, 3 Feb 2009 00:32:30 +0000",226556,"I guessed just enough to fix the problem but I don't understand the code enough to write a correct spec for this. I think the original author of the feature (Kevin Ballard (committed by Kevin Clark), I believe, according to the log) and whoever is refactoring ruby protocols (in this case, you, Bryan) should write specs for all the features of thrift.",0.090625,0.090625,negative
thrift,3274,comment_1,"Better, but not perfect. The tutorial seems to suffer from the same problems:",documentation_debt,low_quality_documentation,"Tue, 28 Jul 2015 20:52:27 +0000","Sun, 29 Nov 2015 15:52:49 +0000","Tue, 3 Nov 2015 22:41:47 +0000",8473760,"Better, but not perfect. The tutorial seems to suffer from the same problems:",-0.2,-0.2,negative
thrift,3297,description,"The tutorial says: However, when I run I get: There is no definition in other files either: What am I missing?",documentation_debt,low_quality_documentation,"Mon, 17 Aug 2015 17:51:41 +0000","Tue, 25 Aug 2015 04:44:51 +0000","Wed, 19 Aug 2015 17:39:42 +0000",172081,"The tutorial says: For each service the Thrift compiler generates an abstract base class from which handler implementations should inherit. In our case TutorialCalculatorHandler inherits from CalculatorHandler, defined in gen-c_glib/calculator.h. However, when I run I get: There is no definition in other files either: What am I missing?",0.4,0.3,negative
thrift,3311,description,The readme formatting is incorrect - see the attached screenshot.,documentation_debt,low_quality_documentation,"Sun, 30 Aug 2015 17:14:51 +0000","Fri, 25 Sep 2015 02:43:16 +0000","Sun, 30 Aug 2015 18:01:51 +0000",2820,The readme formatting is incorrect - see the attached screenshot.,0,0,negative
thrift,3311,summary,Top level README.md has incorrect formmating,documentation_debt,low_quality_documentation,"Sun, 30 Aug 2015 17:14:51 +0000","Fri, 25 Sep 2015 02:43:16 +0000","Sun, 30 Aug 2015 18:01:51 +0000",2820,Top level README.md has incorrect formmating,0.2,0.2,negative
thrift,3419,comment_5,"The release cycle for thrift seems to be quite long. Given that this bug renders the plugin unusable, it may make sense to release this soon rather that waiting until the next thrift release. Its easy enough to build one's own released version of the plugin, but having to do so adds an extra point of friction to getting started with a tool that's already short on documentation.",documentation_debt,low_quality_documentation,"Thu, 12 Nov 2015 05:07:21 +0000","Sat, 20 Feb 2021 15:28:06 +0000","Thu, 17 Jan 2019 19:54:34 +0000",100450033,"The release cycle for thrift seems to be quite long. Given that this bug renders the plugin unusable, it may make sense to release this soon rather that waiting until the next thrift release. Its easy enough to build one's own released version of the plugin, but having to do so adds an extra point of friction to getting started with a tool that's already short on documentation.",0.1486666667,0.1486666667,negative
thrift,353,comment_3,here is the correct version. mispelled function name.,documentation_debt,low_quality_documentation,"Wed, 4 Mar 2009 19:14:17 +0000","Thu, 5 Mar 2009 00:42:52 +0000","Wed, 4 Mar 2009 21:34:23 +0000",8406,here is the correct version. mispelled function name.,0.4375,0.4375,neutral
thrift,3619,comment_1,"Ah, I misread Mark's comment, apologies. Let me verify in the Parquet build environment and I will report back to confirm (and add this to the developer documentation).",documentation_debt,low_quality_documentation,"Fri, 12 Feb 2016 18:54:01 +0000","Mon, 22 Feb 2016 13:17:21 +0000","Thu, 18 Feb 2016 22:04:34 +0000",529833,"Ah, I misread Mark's comment, apologies. Let me verify in the Parquet build environment and I will report back to confirm (and add this to the developer documentation).",-0.1,-0.1,neutral
thrift,3957,comment_4,"Okay, I fixed up the pull request that's outstanding. Sadly, the documentation on TSocket.h isn't very descriptive about setting the timeout and the implications of such. I've seen looping code in TSocket that revolves around TIMED_OUT however my guess is that it existed in the past to attempt to give the socket thread(s) a chance to unblock from a poll, select, or recv call and check to see if the server has been stopped. That's no longer needed since 0.9.3 because of the interruptable socket implementation, so this change works for me. The code that was in TConnectedClient was pulled from the three different server implementations and I believe all of them used to ignore TIMED_OUT so this could cause a behavioral difference in future versions that folks may not expect, however it is ""more correct"".",documentation_debt,low_quality_documentation,"Thu, 3 Nov 2016 14:26:38 +0000","Mon, 12 Dec 2016 18:02:35 +0000","Thu, 10 Nov 2016 21:03:25 +0000",628607,"Okay, I fixed up the pull request that's outstanding. Sadly, the documentation on TSocket.h isn't very descriptive about setting the timeout and the implications of such. I've seen looping code in TSocket that revolves around TIMED_OUT however my guess is that it existed in the past to attempt to give the socket thread(s) a chance to unblock from a poll, select, or recv call and check to see if the server has been stopped. That's no longer needed since 0.9.3 because of the interruptable socket implementation, so this change works for me. The code that was in TConnectedClient was pulled from the three different server implementations and I believe all of them used to ignore TIMED_OUT so this could cause a behavioral difference in future versions that folks may not expect, however it is ""more correct"".",0.2940833333,0.2940833333,neutral
thrift,4015,description,"Some people prefer to spell Thrift as ""Thirft"", which is not really correct.",documentation_debt,low_quality_documentation,"Mon, 26 Dec 2016 10:34:49 +0000","Sat, 14 Jan 2017 09:04:50 +0000","Mon, 26 Dec 2016 10:43:55 +0000",546,"Some people prefer to spell Thrift as ""Thirft"", which is not really correct.",-0.875,-0.875,negative
thrift,4015,summary,"Fix wrongly spelled ""Thirft""s",documentation_debt,low_quality_documentation,"Mon, 26 Dec 2016 10:34:49 +0000","Sat, 14 Jan 2017 09:04:50 +0000","Mon, 26 Dec 2016 10:43:55 +0000",546,"Fix wrongly spelled ""Thirft""s",-0.25,-0.25,negative
thrift,502,summary,typo on incubator page in python client example,documentation_debt,low_quality_documentation,"Thu, 14 May 2009 00:04:21 +0000","Tue, 1 Nov 2011 02:54:29 +0000","Thu, 14 May 2009 00:17:10 +0000",769,typo on incubator page in python client example,0,0,neutral
thrift,788,comment_0,"Refining my notes bellow according to the comment of original creator of this issue: Mine is happening to SuperColumnFamily with 9 or more column per row. Moving discoveries from CASSANDRA-1199 to here: I am comparing the following * Reading of 100 SuperColumns in 1 SCF row using multiget_slice() with 1 key and 1 column name in 100 loop iterations * Reading of 100 SuperColumns in 1 SCF row using multiget_slice() with 1 key and 100 column names in a single call I always get a consistent result and that is the single call takes more time then 100 calls. After some investigation, it seamed that the time it took to execute multiget_slice with 100 columns is always close to the TSocket- This only happens if is used. I have attached my code to reproduce this issue. You can set the timeouts to see how it affects the read call in multiget_slice. Please investigate. This a timing example for the above scenario with TSocket's default timeouts: 100 Sequential Writes took: 0.4047749042511 seconds; 100 Sequential Reads took: 0.16357207298279 seconds; 100 Batch Read took: 0.77017998695374 seconds;",documentation_debt,low_quality_documentation,"Mon, 24 May 2010 16:02:00 +0000","Wed, 10 Aug 2011 18:27:51 +0000","Mon, 11 Apr 2011 15:46:52 +0000",27819892,"Refining my notes bellow according to the comment of original creator of this issue: Mine is happening to SuperColumnFamily with 9 or more column per row. Moving discoveries from CASSANDRA-1199 to here: I am comparing the following Reading of 100 SuperColumns in 1 SCF row using multiget_slice() with 1 key and 1 column name in 100 loop iterations Reading of 100 SuperColumns in 1 SCF row using multiget_slice() with 1 key and 100 column names in a single call I always get a consistent result and that is the single call takes more time then 100 calls. After some investigation, it seamed that the time it took to execute multiget_slice with 100 columns is always close to the TSocket->recvTimeout, Increasing the recvTimeout results that call to take that much time before retuning. After digged into TSocket->read (TSocket.php line 261) and looking at some of the meta data of fread, it seams that none of the buffer chunks get the eof flag=1. And the stream waits till timeout has reached. This only happens if TBinaryProtocolAccelerated (thrift_protocol.so) is used. I have attached my code to reproduce this issue. You can set the timeouts to see how it affects the read call in multiget_slice. Please investigate. This a timing example for the above scenario with TSocket's default timeouts: 100 Sequential Writes took: 0.4047749042511 seconds; 100 Sequential Reads took: 0.16357207298279 seconds; 100 Batch Read took: 0.77017998695374 seconds;",0.05238095238,0.04722222222,neutral
thrift,813,comment_1,"Bryan, Where would I look in order to debug what's going on? What part of the TJSONProtocol would this be in. Also, I've seen several bugs (I think) and I'd like to learn some about the code base. Are there any ramp up docs for contributors?",documentation_debt,low_quality_documentation,"Sun, 4 Jul 2010 09:35:36 +0000","Tue, 1 Nov 2011 02:52:22 +0000","Fri, 22 Oct 2010 13:28:27 +0000",9517971,"Bryan, Where would I look in order to debug what's going on? What part of the TJSONProtocol would this be in. Also, I've seen several bugs (I think) and I'd like to learn some about the code base. Are there any ramp up docs for contributors?",0.125,0.125,neutral
thrift,962,comment_0,"fully agree Bryan! It is really important to have a consistent tutorial across all Languages using the same tutorial.thrift file and possible interaction between these tutorials might be great! Another place is the wiki, some ThriftUsage pages do not use the tutorial example, they have their own",documentation_debt,low_quality_documentation,"Mon, 18 Oct 2010 23:53:50 +0000","Thu, 10 Jul 2014 13:42:34 +0000","Thu, 10 Jul 2014 13:42:34 +0000",117553724,"fully agree Bryan! It is really important to have a consistent tutorial across all Languages using the same tutorial.thrift file and possible interaction between these tutorials might be great! Another place is the wiki, some ThriftUsage pages do not use the tutorial example, they have their own http://wiki.apache.org/thrift/ThriftUsage",0.3,0.3,positive
thrift,962,comment_1,just did the nodejs tutorial an had a look on other missing ones... managing tutorial for the website as copy of existing tutorial source is a maintenance nightmare. I think we should move the website to the git repo within a www folder and reference markdown files within source repo directly. source =let's go for markdown: rename all files such as README to README.md we can still publish the web site via svn. any thoughts? ;-r,documentation_debt,low_quality_documentation,"Mon, 18 Oct 2010 23:53:50 +0000","Thu, 10 Jul 2014 13:42:34 +0000","Thu, 10 Jul 2014 13:42:34 +0000",117553724,just did the nodejs tutorial an had a look on other missing ones... managing tutorial for the website as copy of existing tutorial source is a maintenance nightmare. I think we should move the website to the git repo within a www folder and reference markdown files within source repo directly. source => web site let's go for markdown: rename all files such as README to README.md we can still publish the web site via svn. any thoughts? ;-r,0.0635,0.0635,negative
thrift,962,comment_2,"Anthing that helps towards getting better documentation is fine with me. There are some old tutorials in the old wiki and I really would like to see them on the web site. Not sure if git will help with this ;-), but I don't have any problems with it either, as long as it produces less overhead, instead of more.",documentation_debt,low_quality_documentation,"Mon, 18 Oct 2010 23:53:50 +0000","Thu, 10 Jul 2014 13:42:34 +0000","Thu, 10 Jul 2014 13:42:34 +0000",117553724,"Anthing that helps towards getting better documentation is fine with me. There are some old tutorials in the old wiki and I really would like to see them on the web site. Not sure if git will help with this , but I don't have any problems with it either, as long as it produces less overhead, instead of more.",0.1888888889,0.1444444444,positive
thrift,962,comment_4,"I would love to have the capability to include source code directly on the web site especially for tutorial, test, IDL's. see here: = People do probably not know how to contribute documentation to our project, having the web site within the source tree can simplify this. What about patches including code and web site fixes?",documentation_debt,low_quality_documentation,"Mon, 18 Oct 2010 23:53:50 +0000","Thu, 10 Jul 2014 13:42:34 +0000","Thu, 10 Jul 2014 13:42:34 +0000",117553724,"I would love to have the capability to include source code directly on the web site especially for tutorial, test, IDL's. see here: http://octopress.org/docs/plugins/include-code/ => simple maintenance, no duplicates People do probably not know how to contribute documentation to our project, having the web site within the source tree can simplify this. What about patches including code and web site fixes?",0.095,0.095,positive
thrift,962,comment_7,"tutorials are now incorporating code from repo /tutorials, this will always be a work in progress to make better and improve what we can provide to the community. closing as we now have something available",documentation_debt,low_quality_documentation,"Mon, 18 Oct 2010 23:53:50 +0000","Thu, 10 Jul 2014 13:42:34 +0000","Thu, 10 Jul 2014 13:42:34 +0000",117553724,"tutorials are now incorporating code from repo /tutorials, this will always be a work in progress to make better and improve what we can provide to the community. closing as we now have something available",0.2375,0.2375,positive
thrift,962,description,"It's just a weak skeleton. I mean, really, really weak. At the very least, we should point to TRUNK/tutorial/ for where to look at code examples.",documentation_debt,low_quality_documentation,"Mon, 18 Oct 2010 23:53:50 +0000","Thu, 10 Jul 2014 13:42:34 +0000","Thu, 10 Jul 2014 13:42:34 +0000",117553724,"It's just a weak skeleton. I mean, really, really weak. At the very least, we should point to TRUNK/tutorial/ for where to look at code examples.",-0.2666666667,-0.2666666667,negative
thrift,962,summary,Tutorial page on our website is really unhelpful,documentation_debt,low_quality_documentation,"Mon, 18 Oct 2010 23:53:50 +0000","Thu, 10 Jul 2014 13:42:34 +0000","Thu, 10 Jul 2014 13:42:34 +0000",117553724,Tutorial page on our website is really unhelpful,-0.5,-0.5,negative
thrift,137,comment_5,"New version of the modifications; now including the client library side of it that wasn't submitted the last time. Compile the Thrift Java library with 'ant compile-gwt dist' to enable the GWT changes. This should now provide fully working Thrift/GWT integration. What remains to be done is rewriting the TGWTJSONProtocol, which is currently a butchered version of the standard TJSONProtocol.",requirement_debt,requirement_partially_implemented,"Tue, 16 Sep 2008 09:01:30 +0000","Thu, 12 Apr 2012 04:04:58 +0000","Mon, 9 Apr 2012 18:25:15 +0000",112440225,"New version of the modifications; now including the client library side of it that wasn't submitted the last time. Compile the Thrift Java library with 'ant compile-gwt dist' to enable the GWT changes. This should now provide fully working Thrift/GWT integration. What remains to be done is rewriting the TGWTJSONProtocol, which is currently a butchered version of the standard TJSONProtocol.",0.275,0.275,neutral
thrift,1503,comment_1,"Is there any progress on this? It would be great to have FramedTransport support in cocoa, as it's being used more and more.",requirement_debt,requirement_partially_implemented,"Tue, 24 Jan 2012 15:10:45 +0000","Wed, 1 Oct 2014 19:41:18 +0000","Wed, 1 Oct 2014 19:41:18 +0000",84774633,"Is there any progress on this? It would be great to have FramedTransport support in cocoa, as it's being used more and more.",0.45,0.45,positive
thrift,2031,description,"It's a socket option that we haven't made configurable yet, and sometimes you need it.",requirement_debt,requirement_partially_implemented,"Fri, 14 Jun 2013 20:57:16 +0000","Fri, 12 Aug 2016 01:29:46 +0000","Fri, 8 Apr 2016 05:36:10 +0000",88850334,"It's a socket option that we haven't made configurable yet, and sometimes you need it.",0,0,neutral
thrift,21,comment_0,"I Googled around for this a bit and found this which states that accept is defined to be thread-safe (which I verified) but that not all OSes comply with this (which I did not verify). If anyone can find an example of an OS that we want to support that does not provide a thread-safe accept, I think we should consider implementing a workaround.",requirement_debt,non-functional_requirements_not_fully_satisfied,"Mon, 26 May 2008 23:45:00 +0000","Thu, 26 Jun 2008 17:33:40 +0000","Thu, 26 Jun 2008 17:33:40 +0000",2656120,"I Googled around for this a bit and found this <http://books.google.com/books?id=tdsZHyH9bQEC&pg=PA785&dq=thread-safe+accept&ei=DlA7SNbbH4PitgPN2tmdAw&client=firefox-a&sig=AnRQunQLb4lcMvlxiCVI-HvjijA>, which states that accept is defined to be thread-safe (which I verified) but that not all OSes comply with this (which I did not verify). If anyone can find an example of an OS that we want to support that does not provide a thread-safe accept, I think we should consider implementing a workaround.",0.2,0.2,negative
thrift,21,description,"TThreadPoolServer currently accepts incoming connections in threads. This means that at any time, as long as the thread pool is not completely full of running connections, there are multiple threads currently blocking on the #accept call. This is dangerous because the accept syscall is not documented as being thread-safe. The only reason this actually works in ruby is because of the cooperative threading, but if this library is used in any ruby interpreter that supports native threads (e.g. MacRuby, jruby, etc) I would expect it to start manifesting strange bugs.",requirement_debt,non-functional_requirements_not_fully_satisfied,"Mon, 26 May 2008 23:45:00 +0000","Thu, 26 Jun 2008 17:33:40 +0000","Thu, 26 Jun 2008 17:33:40 +0000",2656120,"TThreadPoolServer currently accepts incoming connections in threads. This means that at any time, as long as the thread pool is not completely full of running connections, there are multiple threads currently blocking on the #accept call. This is dangerous because the accept syscall is not documented as being thread-safe. The only reason this actually works in ruby is because of the cooperative threading, but if this library is used in any ruby interpreter that supports native threads (e.g. MacRuby, jruby, etc) I would expect it to start manifesting strange bugs.",0.04166666667,0.04166666667,negative
thrift,21,summary,TThreadPoolServer has dangerous thread-safety issue in accepting connections,requirement_debt,non-functional_requirements_not_fully_satisfied,"Mon, 26 May 2008 23:45:00 +0000","Thu, 26 Jun 2008 17:33:40 +0000","Thu, 26 Jun 2008 17:33:40 +0000",2656120,TThreadPoolServer has dangerous thread-safety issue in accepting connections,-0.1166666667,-0.1166666667,negative
thrift,2292,comment_1,"Right now i working on jumio plugin for cordova And jumio sdk require adding their android library project to android build. According to cordova plugin specs its not implemented. More info about referencing android library projects to your project you can get here To add a reference to a library project, navigate to the <sdk android update project \ --target <target_ID--path --library",requirement_debt,requirement_partially_implemented,"Fri, 20 Dec 2013 09:53:21 +0000","Mon, 23 Dec 2013 16:12:40 +0000","Mon, 23 Dec 2013 16:12:40 +0000",281959,"Right now i working on jumio plugin for cordova http://www.jumio.com/netverify/netverify-mobile/ And jumio sdk require adding their android library project to android build. According to cordova plugin specs its not implemented. More info about referencing android library projects to your project you can get here http://developer.android.com/tools/projects/projects-cmdline.html To add a reference to a library project, navigate to the <sdk>/tools/ directory and use this command: android update project \ --target <target_ID> \ --path path/to/your/project --library path/to/library_projectA",0.3545,0.3545,neutral
thrift,2292,description,Importing of Android Library Project into Android build from plugin dont implemented. So i cant add some sdks into native plugin that use dependencies as android library project.,requirement_debt,requirement_partially_implemented,"Fri, 20 Dec 2013 09:53:21 +0000","Mon, 23 Dec 2013 16:12:40 +0000","Mon, 23 Dec 2013 16:12:40 +0000",281959,Importing of Android Library Project into Android build from plugin dont implemented. So i cant add some sdks into native plugin that use dependencies as android library project.,0,0,negative
thrift,2329,comment_0,"Had this on my todo list, will get it taken care of, thanks for the ticket",requirement_debt,requirement_partially_implemented,"Tue, 21 Jan 2014 22:28:01 +0000","Thu, 10 Jul 2014 13:42:32 +0000","Thu, 10 Jul 2014 13:42:32 +0000",14656471,"Had this on my todo list, will get it taken care of, thanks for the ticket",0.4,0.4,positive
thrift,2404,description,"As long as there is no special treatment of {{list<byte Thus, the compiler should emit an adequate warning when {{list<byte",requirement_debt,requirement_partially_implemented,"Fri, 14 Mar 2014 21:42:30 +0000","Tue, 1 Apr 2014 20:01:29 +0000","Sun, 16 Mar 2014 14:49:06 +0000",147996,"As long as there is no special treatment of list<byte> implemented, it is a common beginner's trap to choose list<byte> where really binary would be the better choice. Thus, the compiler should emit an adequate warning when list<byte> is found in the IDL.",0.054,0.102,neutral
thrift,240,summary,TBase should implement Cloneable,requirement_debt,requirement_partially_implemented,"Sat, 20 Dec 2008 00:00:30 +0000","Tue, 1 Nov 2011 02:53:55 +0000","Thu, 22 Jan 2009 16:30:27 +0000",2910597,TBase should implement Cloneable,0,0,neutral
thrift,2622,description,"When I deploy my application in production Environment it always generate error ""Expecting Source code is // framed transport while (data.length) { if (frameLeft === 0) { // TODO assumes we have all 4 bytes if (data.length < 4) { residual = data; break; //throw Error(""Expecting } frameLeft = 0); frame = new Buffer(frameLeft); framePos = 0; data = data.slice(4, data.length); } I don't know why Can anybody help me",requirement_debt,requirement_partially_implemented,"Fri, 11 Jul 2014 06:17:23 +0000","Sun, 27 Jul 2014 02:52:19 +0000","Tue, 22 Jul 2014 09:24:42 +0000",961639,"When I deploy my application in production Environment it always generate error ""Expecting > 4 bytes, found only 2"" every one or two minutes Source code is // framed transport while (data.length) { if (frameLeft === 0) { // TODO assumes we have all 4 bytes if (data.length < 4) { console.log(""Expecting > 4 bytes, found only "" + data.length); residual = data; break; //throw Error(""Expecting > 4 bytes, found only "" + data.length); } frameLeft = binary.readI32(data, 0); frame = new Buffer(frameLeft); framePos = 0; data = data.slice(4, data.length); } I don't know why Can anybody help me",-0.24,-0.09166666667,negative
thrift,3011,summary,C# test server testException() not implemented according to specs,requirement_debt,requirement_partially_implemented,"Thu, 26 Feb 2015 18:34:35 +0000","Tue, 3 Mar 2015 20:51:01 +0000","Thu, 26 Feb 2015 23:27:18 +0000",17563,C# test server testException() not implemented according to specs,-0.5,-0.5,negative
thrift,3592,description,"Unlike test server, client can be partially implemented. The patch just converts the existing manual test client to cross test client by adding argument support.",requirement_debt,requirement_partially_implemented,"Mon, 1 Feb 2016 12:57:29 +0000","Sat, 27 Feb 2016 09:26:30 +0000","Sat, 27 Feb 2016 08:01:02 +0000",2228613,"Unlike test server, client can be partially implemented. The patch just converts the existing manual test client to cross test client by adding argument support.",0.2,0.2,neutral
thrift,4164,comment_2,"PR #1235 exposed the fact that before that PR, we were quietly stopping openssl from using the locking callbacks we were providing. This would lead to unsafe multi-thread behavior during shutdown. I looked at guaranteeing this in core: I noodled through it and the solutions were all a bit ugly, involving the factory to keep a smart pointer of each socket it made, and a callback from TSSLSocket::close() to the factory to take it out of a set of sockets it had made. Then in ~TSSLSocketFactory we would need to guarantee the set of sockets left in the factory were the last instance (nobody else was holding onto them as well) so we could guarantee that when the set of sockets is reset, all the sockets the factory ever made were released.Then the factory could clean up openssl safely. OR, we clearly document the requirement that a factory must outlive any sockets it makes. This is what I'm going to do, so will revise the PR to fix our own TestClient.cpp which was in violation of the requirement that a TSSLSocketFactory must outlive any TSSLSocket it creates when the TSSLSocketFactory is managing openssl initialization and destruction, which was causing the build failures.",requirement_debt,non-functional_requirements_not_fully_satisfied,"Mon, 3 Apr 2017 14:24:40 +0000","Thu, 14 Dec 2017 13:55:39 +0000","Tue, 4 Apr 2017 13:37:13 +0000",83553,"PR #1235 exposed the fact that before that PR, we were quietly stopping openssl from using the locking callbacks we were providing. This would lead to unsafe multi-thread behavior during shutdown. I looked at guaranteeing this in core: I noodled through it and the solutions were all a bit ugly, involving the factory to keep a smart pointer of each socket it made, and a callback from TSSLSocket::close() to the factory to take it out of a set of sockets it had made. Then in ~TSSLSocketFactory we would need to guarantee the set of sockets left in the factory were the last instance (nobody else was holding onto them as well) so we could guarantee that when the set of sockets is reset, all the sockets the factory ever made were released.Then the factory could clean up openssl safely. OR, we clearly document the requirement that a factory must outlive any sockets it makes. This is what I'm going to do, so will revise the PR to fix our own TestClient.cpp which was in violation of the requirement that a TSSLSocketFactory must outlive any TSSLSocket it creates when the TSSLSocketFactory is managing openssl initialization and destruction, which was causing the build failures.",-0.01329166667,-0.01329166667,negative
thrift,4443,description,"The skip function is unsupported in the node.js implementation of the json_protocol. Interestingly, the compact_protocol implements the skip function, as does the JavaScript version.",requirement_debt,requirement_partially_implemented,"Fri, 5 Jan 2018 22:39:16 +0000","Thu, 27 Dec 2018 15:25:21 +0000","Thu, 11 Jan 2018 02:21:47 +0000",445351,"The skip function is unsupported in the node.js implementation of the json_protocol. Interestingly, the compact_protocol implements the skip function, as does the JavaScript version.",0.03333333333,0.03333333333,neutral
thrift,4468,comment_1,"OK. With this approach, this class is really better to remove. Its name is misleading about thread safety",requirement_debt,non-functional_requirements_not_fully_satisfied,"Mon, 22 Jan 2018 14:29:43 +0000","Thu, 27 Dec 2018 15:24:53 +0000","Thu, 25 Jan 2018 23:16:49 +0000",290826,"OK.With this approach, this class is really better to remove. Its name is misleading about thread safety",0.325,0.325,negative
thrift,4468,summary,Make the class TGUIConsole thread-safe,requirement_debt,non-functional_requirements_not_fully_satisfied,"Mon, 22 Jan 2018 14:29:43 +0000","Thu, 27 Dec 2018 15:24:53 +0000","Thu, 25 Jan 2018 23:16:49 +0000",290826,Make the class TGUIConsole thread-safe,0.25,0.25,neutral
thrift,4829,description,"The current implementation of the has no possibility to use layered protocols like framed, because the class does not support it. Patch follows.",requirement_debt,requirement_partially_implemented,"Wed, 20 Mar 2019 13:48:34 +0000","Wed, 16 Oct 2019 22:27:40 +0000","Thu, 21 Mar 2019 08:07:32 +0000",65938,"The current implementation of the THttpServerTransport has no possibility to use layered protocols like framed, because the class does not support it. Patch follows.",-0.2,-0.2,negative
thrift,4882,description,"WinHTTP comes with limited AutoProxy support. In order to actually use it, there is some extra work required at the application level. See",requirement_debt,requirement_partially_implemented,"Thu, 6 Jun 2019 22:34:43 +0000","Wed, 16 Oct 2019 22:27:40 +0000","Fri, 7 Jun 2019 20:27:25 +0000",78762,"WinHTTP comes with limited AutoProxy support. In order to actually use it, there is some extra work required at the application level. See https://docs.microsoft.com/en-us/windows/desktop/winhttp/winhttp-autoproxy-api",0.2166666667,0.2166666667,negative
thrift,5152,description,"Currently go TSocket only contains *single timeout* for connection timeout and read/write timeout. Meanwhile in java TSocket contains *connectTimeout_* and *socketTimeout_* for connection timeout and read/write timeout. In the real production environment, we do need different timeout for connection timeout and read/write timeout, so I created this Jira to improve this.",requirement_debt,requirement_partially_implemented,"Thu, 26 Mar 2020 09:39:04 +0000","Thu, 11 Feb 2021 22:26:33 +0000","Thu, 4 Feb 2021 10:19:14 +0000",27218410,"Currently go TSocket only contains single timeout for connection timeout and read/write timeout. Meanwhile in java TSocket containsconnectTimeout_ and socketTimeout_forconnection timeout and read/write timeout. In the real production environment, we do need different timeout forconnection timeout and read/write timeout, so I created this Jira to improve this.",0.3,0.3,neutral
thrift,1062,comment_4,THRIFT-1735 integrates Python Tutorials into regular build please create a test case for the test suite: - test/test.sh (interoperability) - test/py/ - test/py.twisted,test_debt,lack_of_tests,"Wed, 16 Feb 2011 11:38:17 +0000","Sat, 8 Jun 2013 03:15:38 +0000","Sat, 8 Jun 2013 03:15:38 +0000",72805041,THRIFT-1735 integrates Python Tutorials into regular build please create a test case for the test suite: test/test.sh (interoperability) test/py/ test/py.twisted,0.06666666667,0.06666666667,neutral
thrift,1103,comment_2,"Thanks for comitting THRIFT-1094, I'll update the patch this evening (and attach as _v2) to include testing TZlibTransport wrapping in the code (as a cmdline --zlib argument to both scripts). FYI, currently the hudson(jenkins) build seems to be failing on the javascript jslint tasks, stopped the build tests from progressing past the 'test/js' directory...",test_debt,lack_of_tests,"Mon, 21 Mar 2011 01:48:29 +0000","Tue, 22 Mar 2011 18:06:16 +0000","Tue, 22 Mar 2011 18:06:16 +0000",145067,"Thanks for comitting THRIFT-1094, I'll update the patch this evening (and attach as _v2) to include testing TZlibTransport wrapping in the TestServer.py/TestClient.py code (as a cmdline --zlib argument to both scripts). FYI, currently the hudson(jenkins) build seems to be failing on the javascript jslint tasks, stopped the build tests from progressing past the 'test/js' directory...",0.05,0.025,positive
thrift,1103,description,"New implementation of zlib compressed transport for python. The attached patch provides a zlib compressed transport wrapper for python. It is similar to the TFramedTransport, in that it wraps another transport, implementing the data compression as a transformation layer on top of the underlying transport that it wraps. The compression level is configurable in the constructor, from 0 (none) to 9 (best) and defaults to 9 for best compression. The way this works is that every write() to the transport appends more data to the internal cStringIO write buffer. When the transport's flush() method is called, the buffered bytes are then passed to a zlib Compressor object and flush()ed with zlib.Z_SYNC_FLUSH. Because the thrift API calls the transport's flush() after writeMessageEnd(), this means very small thrift RPC calls don't get compressed well. This transport works best on thrift protocols where the payload contains strings longer than 10 characters. As with all data compression, the more redundancy in the uncompressed input, the greater the resulting compression. The TZlibTransport class also implements some basic statistics that track the number of raw bytes written and read, versus the decompressed equivalent. The getCompRatio() method returns a tuple of where ratio is computed using: (So 10 compression is 0.10, meaning smaller numbers are better.) The getCompSavings() method returns the actual number of which might be negative when the compression of non-compressible data ends up expanding the data. So hopefully, anyone who uses this transport will be able to tell whether the compression is saving bandwidth or not. I will add the patch in a few minutes. I haven't tested this against the C++ TZlibTransport, only against itself.",test_debt,lack_of_tests,"Mon, 21 Mar 2011 01:48:29 +0000","Tue, 22 Mar 2011 18:06:16 +0000","Tue, 22 Mar 2011 18:06:16 +0000",145067,"New implementation of zlib compressed transport for python. The attached patch provides a zlib compressed transport wrapper for python. It is similar to the TFramedTransport, in that it wraps another transport, implementing the data compression as a transformation layer on top of the underlying transport that it wraps. The compression level is configurable in the constructor, from 0 (none) to 9 (best) and defaults to 9 for best compression. The way this works is that every write() to the transport appends more data to the internal cStringIO write buffer. When the transport's flush() method is called, the buffered bytes are then passed to a zlib Compressor object and flush()ed with zlib.Z_SYNC_FLUSH. Because the thrift API calls the transport's flush() after writeMessageEnd(), this means very small thrift RPC calls don't get compressed well. This transport works best on thrift protocols where the payload contains strings longer than 10 characters. As with all data compression, the more redundancy in the uncompressed input, the greater the resulting compression. The TZlibTransport class also implements some basic statistics that track the number of raw bytes written and read, versus the decompressed equivalent. The getCompRatio() method returns a tuple of (readCompressionRatio,writeCompressionRatio) where ratio is computed using: compressed_bytes/uncompressed_bytes. (So 10 compression is 0.10, meaning smaller numbers are better.) The getCompSavings() method returns the actual number of (saved_read_bytes,saved_write_bytes) which might be negative when the compression of non-compressible data ends up expanding the data. So hopefully, anyone who uses this transport will be able to tell whether the compression is saving bandwidth or not. I will add the patch in a few minutes. I haven't tested this against the C++ TZlibTransport, only against itself.",0.1592708333,0.1812745098,neutral
thrift,1130,comment_2,"I haven't tested this patch yet, but my concern is that by adding these tokens, you may possibly have broken string literals or identifiers named ""true"" and ""false"". Can you add some examples to ThriftTest.thrift that exercises these corner cases?",test_debt,lack_of_tests,"Tue, 5 Apr 2011 23:47:24 +0000","Thu, 13 Oct 2011 21:49:25 +0000","Thu, 13 Oct 2011 21:34:48 +0000",16494444,"I haven't tested this patch yet, but my concern is that by adding these tokens, you may possibly have broken string literals or identifiers named ""true"" and ""false"". Can you add some examples to ThriftTest.thrift that exercises these corner cases?",-0.0325,-0.0325,negative
thrift,1130,comment_3,"This was my concern too, and my threshold for passage was to ensure that it will provide an error, obviously its not the same error as it originally did. At the end of the day, this seems to be pretty much like a c-pre-processor macros: #define true 1 #define false 0 The the items NOT in my negative tests that WOULD fail are attempts to use true/false as field numbers (for example) What you are asking for is a negative test (which I performed locally), I only saw one but it is commented out (so its not in any automated test) Is this how you would like to see it? If not can you point me at an example of a negative test in ThriftTest.thrift? This is what I tried and what I had in mind ...",test_debt,lack_of_tests,"Tue, 5 Apr 2011 23:47:24 +0000","Thu, 13 Oct 2011 21:49:25 +0000","Thu, 13 Oct 2011 21:34:48 +0000",16494444,"This was my concern too, and my threshold for passage was to ensure that it will provide an error, obviously its not the same error as it originally did. At the end of the day, this seems to be pretty much like a c-pre-processor macros: #define true 1 #define false 0 The the items NOT in my negative tests that WOULD fail are attempts to use true/false as field numbers (for example) What you are asking for is a negative test (which I performed locally), I only saw one but it is commented out (so its not in any automated test) Is this how you would like to see it? If not can you point me at an example of a negative test in ThriftTest.thrift? This is what I tried and what I had in mind ... struct boolDefValTst { // positive tests: 1: optional bool myBool_1 = true, 2: optional bool myBool_2 = false, 3: optional bool myBool_3 = 1, 4: optional bool myBool_4 = 0, 5: optional bool myBool_5, 6: optional i32 myI32_1 = true, // negative tests: // (uncomment each, one at a time to test) // - Test that we get an error using boolean constants as field names //7: optional i32 true, // [ERROR:...] (last token was 'true') syntax error //8: optional i32 false, // [ERROR:...] (last token was 'false') syntax error // - Test that we get an error using boolean constants type names //9: optional true myI32_2, // [ERROR:...] (last token was 'true') syntax error //10: optional false myI32_3, // [ERROR:...] (last token was 'false') syntax error // - Test that we get an error using boolean constants as values for incompatible types //11: optional string myStr1 = true, // [FAILURE:...] type error: const ""myStr"" was declared as string } Thanks",0.1083333333,0.07961568627,negative
thrift,1199,summary,Union structs should have generated methods to test whether a specific field is currently set,test_debt,low_coverage,"Tue, 7 Jun 2011 20:29:48 +0000","Thu, 9 Jun 2011 22:33:47 +0000","Wed, 8 Jun 2011 17:47:12 +0000",76644,Union structs should have generated methods to test whether a specific field is currently set,0,0,neutral
thrift,1217,comment_2,"@Roger: to avoid some confusion: - the attached patch does not depend on pthread. The suggestion here is merely to use evutil_socketpair instead of pipe. - evutil_socketpair is part of libevent 1.4 on ubuntu 10.10, although I did not test it. All the other changes related to win32 in the attached patch attached are not to take literally (Winsock2.h, static cast...). About THRIFT-1031: - my understanding is that THRIFT-1031 does not support async/libevent server on Windows (??) - libevent+thrift server seems VERY fast on win, and is needed for my project (blocking server won't do) - APR was not strictly necessary for the Non-Blocking server win32 port (i.e. although I tried the THRIFT-1031 patch and observed it made the port a bit cleaner, so I would not refrain from adding APR to thrift-C++ I decided after reviewing the win32 patches, it was better to open a separate issue, since the change here is atomic and should bare little consequences on linux. I was hoping that (naively) the win32 delta would get smaller by using the compatible call. All that said, I'd be really happy to contribute to THRIFT-1031 for it to go through. For this to happen, I would hope to have access to a shared implementation on top of 0.6.1 (the way I provided it on github), so I can test it regularly. Let me know if I can contribute somehow! As a conclusion, I think this patch is somewhat unrelated to THRIFT-1031, but it will help a future port of the NB server on win32 (assuming also someone will replace the pthread dependency by boost, which does not seem trivial at all).",test_debt,lack_of_tests,"Thu, 23 Jun 2011 20:39:21 +0000","Thu, 1 Sep 2011 17:41:04 +0000","Fri, 8 Jul 2011 12:45:13 +0000",1267552,"@Roger: to avoid some confusion: the attached patch does not depend on pthread. The suggestion here is merely to use evutil_socketpair instead of pipe. evutil_socketpair is part of libevent 1.4 on ubuntu 10.10, although I did not test it. All the other changes related to win32 in the attached patch attached are not to take literally (Winsock2.h, static cast...). About THRIFT-1031: my understanding is that THRIFT-1031 does not support async/libevent server on Windows (??) libevent+thrift server seems VERY fast on win, and is needed for my project (blocking server won't do) APR was not strictly necessary for the Non-Blocking server win32 port (i.e. libevent+pthread-win32), although I tried the THRIFT-1031 patch and observed it made the port a bit cleaner, so I would not refrain from adding APR to thrift-C++ I decided after reviewing the win32 patches, it was better to open a separate issue, since the change here is atomic and should bare little consequences on linux. I was hoping that (naively) the win32 delta would get smaller by using the compatible call. All that said, I'd be really happy to contribute to THRIFT-1031 for it to go through. For this to happen, I would hope to have access to a shared implementation on top of 0.6.1 (the way I provided it on github), so I can test it regularly. Let me know if I can contribute somehow! As a conclusion, I think this patch is somewhat unrelated to THRIFT-1031, but it will help a future port of the NB server on win32 (assuming also someone will replace the pthread dependency by boost, which does not seem trivial at all).",0.001736111111,0.001602564103,neutral
thrift,1311,description,"I see that some people have been working very hard on the Java library and that's really appreciated, however, some of the changes seem to have interfered with the java server JSON/HTTP implementation which is used to test the JS client. Please see below: thrift/lib/js/test$ ant testserver [...] [java] New connection thread [java] Incoming content: [java] Outgoing content: Error:Unexpected character:[ [java] New connection thread [java] Incoming content: [java] Outgoing content: Error:Unexpected character:[ ps.: I hope one day we can automatize these tests with node.js, or something like that, so we don't need to run this manually",test_debt,low_coverage,"Tue, 30 Aug 2011 13:36:47 +0000","Fri, 2 Sep 2011 09:09:34 +0000","Fri, 2 Sep 2011 09:09:34 +0000",243167,"I see that some people have been working very hard on the Java library and that's really appreciated, however, some of the changes seem to have interfered with the java server JSON/HTTP implementation which is used to test the JS client. Please see below: thrift/lib/js/test$ ant testserver [...] [java] New connection thread [java] Incoming content: [1,""testSet"",1,0,{""1"":{""set"":[""i32"",3,1,2,3]}}] [java] Outgoing content: Error:Unexpected character:[ [java] New connection thread [java] Incoming content: [1,""testList"",1,0,{""1"":{""lst"":[""i32"",3,1,2,3]}}] [java] Outgoing content: Error:Unexpected character:[ ps.: I hope one day we can automatize these tests with node.js, or something like that, so we don't need to run this manually",0.1458333333,0.1099358974,positive
thrift,1431,comment_1,committed. Could you please create the patch from thrift source root directory? This makes it much easier to handle. If you have some spare time... we need a test suite for node.js THRIFT-1134 ;-),test_debt,low_coverage,"Sat, 19 Nov 2011 11:39:17 +0000","Sat, 26 Nov 2011 18:39:16 +0000","Wed, 23 Nov 2011 20:46:30 +0000",378433,committed. Could you please create the patch from thrift source root directory? This makes it much easier to handle. If you have some spare time... we need a test suite for node.js THRIFT-1134,0.22,0.14,positive
thrift,1745,comment_0,"Frederic, Thank you for your work on this. Would you please attach the patch for this to the ticket. We can not accept gist or pull requests from github due to ASF licensing requirements. Once the test cases are added i'll gladly review and commit this patch.",test_debt,lack_of_tests,"Tue, 30 Oct 2012 15:31:10 +0000","Sat, 20 Feb 2021 15:27:14 +0000","Tue, 20 Nov 2012 22:12:50 +0000",1838500,"Frederic, Thank you for your work on this. Would you please attach the patch for this to the ticket. We can not accept gist or pull requests from github due to ASF licensing requirements. Once the test cases are added i'll gladly review and commit this patch.",0.175,0.175,neutral
thrift,1810,comment_2,initial patch without cross language test,test_debt,lack_of_tests,"Thu, 27 Dec 2012 00:19:05 +0000","Sun, 23 Feb 2014 20:56:22 +0000","Sun, 23 Feb 2014 20:56:14 +0000",36621429,initial patch without cross language test,0,0,neutral
thrift,1810,description,"we need ruby as part of the cross language test suite, currently just a shell script (test/test.sh) running on Linux. references THRIFT-1766",test_debt,lack_of_tests,"Thu, 27 Dec 2012 00:19:05 +0000","Sun, 23 Feb 2014 20:56:22 +0000","Sun, 23 Feb 2014 20:56:14 +0000",36621429,"we need ruby as part of the cross language test suite, currently just a shell script (test/test.sh) running on Linux. references THRIFT-1766",0,0,neutral
thrift,1853,comment_0,"Hi Nick, Do you have any updates on this? If you have any unit-test for this please also attach it here. Thank you, Henrique",test_debt,lack_of_tests,"Mon, 11 Feb 2013 05:09:57 +0000","Sun, 12 May 2013 23:49:35 +0000","Sun, 12 May 2013 23:49:35 +0000",7843178,"Hi Nick, Do you have any updates on this? If you have any unit-test for this please also attach it here. Thank you, Henrique",0.2,0.2,neutral
thrift,2097,comment_1,please help fix and test this THRIFT-2229 first thanks! -roger,test_debt,lack_of_tests,"Sun, 21 Jul 2013 01:58:26 +0000","Thu, 10 Oct 2019 23:00:14 +0000","Mon, 14 Jan 2019 15:03:22 +0000",173106296,please help fix and test this THRIFT-2229 first thanks! -roger,0.1666666667,0.1666666667,neutral
thrift,2171,comment_0,"Hi Red, I totally agree and more tests are very welcome! We don't have any tests for the new JSON protocol for example :( I also think we should probably move thrift/test/nodejs to like in java. A lot of people get thrift for node through npm, which only gives you the lib folder... Any thoughts?",test_debt,lack_of_tests,"Fri, 6 Sep 2013 05:04:24 +0000","Sun, 6 Jul 2014 04:01:01 +0000","Thu, 3 Apr 2014 23:36:43 +0000",18124339,"Hi Red, I totally agree and more tests are very welcome! We don't have any tests for the new JSON protocol for example I also think we should probably move thrift/test/nodejs to thrift/lib/nodejs/test, like in java. A lot of people get thrift for node through npm, which only gives you the lib folder... Any thoughts?",0.025,0.1583333333,neutral
thrift,2171,description,"There is hardly any test coverage for the NodeJS implementation. Please comment if you have ideas for how to architect tests. Patches welcome, too.",test_debt,low_coverage,"Fri, 6 Sep 2013 05:04:24 +0000","Sun, 6 Jul 2014 04:01:01 +0000","Thu, 3 Apr 2014 23:36:43 +0000",18124339,"There is hardly any test coverage for the NodeJS implementation. Please comment if you have ideas for how to architect tests. Patches welcome, too.",0.3166666667,0.3166666667,neutral
thrift,2171,summary,NodeJS implementation has extremely low test coverage,test_debt,low_coverage,"Fri, 6 Sep 2013 05:04:24 +0000","Sun, 6 Jul 2014 04:01:01 +0000","Thu, 3 Apr 2014 23:36:43 +0000",18124339,NodeJS implementation has extremely low test coverage,0,0,negative
thrift,2210,description,"TSimpleJSONProtocol can emit JSON with maps whose keys are not string (which is not allowed is the JSON spec). This happens if the key in a map is anything other than a String (int, enum, etc) For example, it can emit JSON like this: which should be: I have a path that fixes this, I'll upload it shortly (still trying to get my dev environment to run the tests). Also AFAICT there is no unit test for TSimpleJSONProtocol -- I'll try and add one to the patch. Thanks! Alex",test_debt,lack_of_tests,"Thu, 26 Sep 2013 06:37:02 +0000","Sun, 6 Mar 2016 17:25:17 +0000","Fri, 27 Sep 2013 14:13:50 +0000",113808,"TSimpleJSONProtocol can emit JSON with maps whose keys are not string (which is not allowed is the JSON spec). This happens if the key in a map is anything other than a String (int, enum, etc) For example, it can emit JSON like this: which should be: I have a path that fixes this, I'll upload it shortly (still trying to get my dev environment to run the tests). Also AFAICT there is no unit test for TSimpleJSONProtocol  I'll try and add one to the patch. Thanks! Alex",0.09873333333,0.09873333333,neutral
thrift,2351,comment_4,"no test case and *make cross* integration, we can close this if you like.",test_debt,lack_of_tests,"Thu, 6 Feb 2014 18:00:17 +0000","Mon, 29 Jun 2015 21:32:53 +0000","Thu, 10 Jul 2014 13:42:25 +0000",13290128,"no test case and make cross integration, we can close this if you like.",0.4,0.4,neutral
thrift,2351,description,"There are two typos in that prevent correct decoding of any message. They are on consecutive lines: First $seqId does not match case of argument $seqid so this is not set PHP is not case sensitive for class names but IS for variable names. Second and more importantly, on the following line $name has the decoded length assigned to it instead of $result. This means the method name ""hello"" ends up being decoded as ""6"" (the length of the string plus length prefix) and hence any processor trying to dispatch the event will fail. I can submit a patch/pull request but its only 6 chars to change and clearly there are no automated tests for this protocol implementation as that should have been caught before. I guess not many other people are using PHP as a thrift service processor with this protocol.",test_debt,low_coverage,"Thu, 6 Feb 2014 18:00:17 +0000","Mon, 29 Jun 2015 21:32:53 +0000","Thu, 10 Jul 2014 13:42:25 +0000",13290128,"There are two typos in TCompactProtocol::readMessageBegin that prevent correct decoding of any message. They are on consecutive lines: https://github.com/apache/thrift/blob/master/lib/php/lib/Thrift/Protocol/TCompactProtocol.php#L390-L391 First $seqId does not match case of argument $seqid so this is not set PHP is not case sensitive for class names but IS for variable names. Second and more importantly, on the following line $name has the decoded length assigned to it instead of $result. This means the method name ""hello"" ends up being decoded as ""6"" (the length of the string plus length prefix) and hence any processor trying to dispatch the event will fail. I can submit a patch/pull request but its only 6 chars to change and clearly there are no automated tests for this protocol implementation as that should have been caught before. I guess not many other people are using PHP as a thrift service processor with this protocol.",-0.1013611111,-0.1013611111,negative
thrift,2416,comment_3,"The attached patch looks fine to me (though I haven't tested). +1. When the MSVC + ARM developers complain, they can put the appropriate code behind an _M_ARM guard.",test_debt,lack_of_tests,"Thu, 20 Mar 2014 09:20:06 +0000","Thu, 10 Jul 2014 13:36:09 +0000","Mon, 7 Jul 2014 20:07:28 +0000",9456442,"The attached patch looks fine to me (though I haven't tested). +1. When the MSVC + ARM developers complain, they can put the appropriate code behind an _M_ARM guard.",-0.04383333333,-0.04383333333,positive
thrift,2431,description,"This test is flaky, I can easily make it fail: Seems like this has hit a in Jenkins already.",test_debt,flaky_test,"Sun, 30 Mar 2014 05:34:11 +0000","Wed, 6 Jan 2016 02:06:07 +0000","Sun, 29 Nov 2015 16:28:18 +0000",52656847,"This test is flaky, I can easily make it fail: Seems like this has hit a couple times in Jenkins already.",-0.4,-0.4,negative
thrift,2449,comment_1,"Hey Jens, I also think this is a good distinction to have manifest at the compiler level. Haven't tested the code but the patch looks good to me. +1 -Randy",test_debt,lack_of_tests,"Sat, 5 Apr 2014 09:48:18 +0000","Sun, 13 Apr 2014 22:37:07 +0000","Sun, 13 Apr 2014 20:04:00 +0000",728142,"Hey Jens, I also think this is a good distinction to have manifest at the compiler level. Haven't tested the code but the patch looks good to me. +1 -Randy",0.5025,0.5025,positive
thrift,2589,description,Compiling a thrift file containing const definitions for basetype variables results in static properties {{public static whatsits}} being generated. Should generate const properties {{public const whatsits}}. Current version generates this from The code should instead look like this. A patch for this change is supplied. As i don't really know yet how the testing for these kind of changes is done i haven't supplied one. But generating all the .thrift files in test with a before and after version of the compiler and comparing the output of both looked good to me.,test_debt,lack_of_tests,"Wed, 25 Jun 2014 09:06:27 +0000","Wed, 5 Nov 2014 04:48:22 +0000","Thu, 7 Aug 2014 20:44:10 +0000",3757063,Compiling a thrift file containing const definitions for basetype variables results in static properties public static whatsits being generated. Should generate const properties public const whatsits. Current version generates this from test/ConstantsDemo.thrift The code should instead look like this. A patch for this change is supplied. As i don't really know yet how the testing for these kind of changes is done i haven't supplied one. But generating all the .thrift files in test with a before and after version of the compiler and comparing the output of both looked good to me.,-0.01414285714,-0.012375,neutral
thrift,2590,comment_2,Patch that adds the three specified files to the project file. Please verify if it's working as i haven't installed the necessary dependencies to compile the c++ library so i couldn't really test it. But it should work.,test_debt,lack_of_tests,"Thu, 26 Jun 2014 15:17:36 +0000","Sun, 6 Jul 2014 04:00:57 +0000","Fri, 4 Jul 2014 20:18:56 +0000",709280,Patch that adds the three specified files to the project file. Please verify if it's working as i haven't installed the necessary dependencies to compile the c++ library so i couldn't really test it. But it should work.,0.3444444444,0.3444444444,neutral
thrift,2659,comment_0,I guess we miss th following test defined here:,test_debt,lack_of_tests,"Tue, 12 Aug 2014 19:16:20 +0000","Fri, 18 Mar 2016 17:54:30 +0000","Fri, 18 Mar 2016 17:54:30 +0000",50452690,I guess we miss th following test defined here: test/ThriftTest.thrift,0.05,0.025,neutral
thrift,2659,comment_1,"Yes, this test is currently not available in TestClient.py. Adding this will fail py server - py client cross tests also since issue in TestServer.py I mentioned above.",test_debt,lack_of_tests,"Tue, 12 Aug 2014 19:16:20 +0000","Fri, 18 Mar 2016 17:54:30 +0000","Fri, 18 Mar 2016 17:54:30 +0000",50452690,"Yes, this test is currently not available in TestClient.py. Adding this will fail py server - py client cross tests also since issue in TestServer.py I mentioned above.",-0.05,-0.05,negative
thrift,278,comment_2,"Does this work? Maybe I'm reading it wrong, but: {{+ indent(out) << ""throw new field '"" << field- Looks like it just prints the field name as the value. Got a test?",test_debt,lack_of_tests,"Tue, 20 Jan 2009 00:52:35 +0000","Tue, 1 Nov 2011 02:54:22 +0000","Wed, 18 Mar 2009 01:51:07 +0000",4928312,"Does this work? Maybe I'm reading it wrong, but: + indent(out) << ""throw new TProtocolException(\""The field '"" << field->get_name() << ""' has been assigned the invalid value \"" + "" << field->get_name() << "");"" << endl; Looks like it just prints the field name as the value. Got a test?",0.1166666667,0.1166666667,neutral
thrift,3191,comment_0,"Linking a few things together. Given the lack of test infrastructure for perl, I will be fixing these at the same time. I am updating ""make cross"" for THRIFT-3053 to support perl client and server, ssl or no ssl. Getting that working required me to solve THRIFT-3189 and THRIFT-3191 along the way.",test_debt,lack_of_tests,"Tue, 16 Jun 2015 20:20:02 +0000","Tue, 25 Aug 2015 04:44:52 +0000","Thu, 30 Jul 2015 13:10:22 +0000",3775820,"Linking a few things together. Given the lack of test infrastructure for perl, I will be fixing these at the same time. I am updating ""make cross"" for THRIFT-3053 to support perl client and server, ssl or no ssl. Getting that working required me to solve THRIFT-3189 and THRIFT-3191 along the way.",0.1,0.1,neutral
thrift,3276,comment_1,"I haven't, but neither TBase64Utils.java nor TJSONProtocol.java have changed in any relevant way since 0.9.1. I suspect this was just never tested against a client that sends padded base64 strings.",test_debt,lack_of_tests,"Wed, 29 Jul 2015 18:11:53 +0000","Wed, 6 Jan 2016 02:06:15 +0000","Sat, 10 Oct 2015 23:08:36 +0000",6325003,"I haven't, but neither TBase64Utils.java nor TJSONProtocol.java have changed in any relevant way since 0.9.1. I suspect this was just never tested against a client that sends padded base64 strings.",-0.095375,-0.095375,negative
thrift,3413,comment_0,"The attached files and are enriched test cases, as there are some more issues.",test_debt,low_coverage,"Mon, 9 Nov 2015 23:36:08 +0000","Fri, 18 Mar 2016 17:54:29 +0000","Fri, 18 Mar 2016 17:54:29 +0000",11211501,"The attached files base.thrift and extended.thrift are enriched test cases, as there are some more issues.",0,0,neutral
thrift,3440,description,"It runs every combination of I want to reduce this keeping the same code coverage, so that we can run it more often.",test_debt,expensive_tests,"Mon, 23 Nov 2015 10:00:28 +0000","Wed, 6 Jan 2016 02:06:12 +0000","Mon, 23 Nov 2015 14:54:38 +0000",17650,"It runs every combination of compiler-option/server/transport/protocol/zlib/ssl. I want to reduce this keeping the same code coverage, so that we can run it more often.",0.2,0.1,neutral
thrift,3642,summary,Speed up cross test runner,test_debt,expensive_tests,"Wed, 17 Feb 2016 16:41:24 +0000","Sat, 20 Feb 2016 04:00:06 +0000","Fri, 19 Feb 2016 16:01:59 +0000",170435,Speed up cross test runner,0,0,neutral
thrift,3731,summary,Perl multiplex test is flaky,test_debt,flaky_test,"Thu, 10 Mar 2016 00:17:01 +0000","Thu, 10 Mar 2016 05:59:32 +0000","Thu, 10 Mar 2016 02:41:15 +0000",8654,Perl multiplex test is flaky,0,0,negative
thrift,427,comment_1,"The patch looks good, and everything compiles and the tests pass still. It'd be nice if this code was exercised through at least a test .thrift file though.",test_debt,lack_of_tests,"Fri, 3 Apr 2009 22:37:56 +0000","Tue, 1 Nov 2011 02:51:46 +0000","Tue, 7 Apr 2009 20:51:53 +0000",339237,"The patch looks good, and everything compiles and the tests pass still. It'd be nice if this code was exercised through at least a test .thrift file though.",0.4511666667,0.4511666667,positive
thrift,4419,description,"Related to THRIFT-4390 Description copied form there: While working on improving test coverage and fixing busted cross tests I reworked the cpp test client to send binary in at size 0, 1, 2, 4, 6, 16, ..., 131072 and after 4096 the rust server gave up.",test_debt,low_coverage,"Mon, 11 Dec 2017 16:12:36 +0000","Thu, 27 Dec 2018 15:25:18 +0000","Sat, 17 Mar 2018 08:40:11 +0000",8267255,"Related to THRIFT-4390 Description copied form there: While working on improving test coverage and fixing busted cross tests I reworked the cpp test client to send binary in at size 0, 1, 2, 4, 6, 16, ..., 131072 and after 4096 the rust server gave up.",0.55,0.55,neutral
thrift,4437,comment_1,"This issue would have been caught by the `ThriftWS` test suite, which is not enabled for some reason. I will add a commit to my PR that enables these tests.",test_debt,lack_of_tests,"Wed, 27 Dec 2017 11:08:19 +0000","Thu, 27 Dec 2018 15:25:16 +0000","Thu, 28 Dec 2017 13:03:35 +0000",93316,"This issue would have been caught by the `ThriftWS` test suite, which is not enabled for some reason. I will add a commit to my PR that enables these tests.",0.1,0.1,neutral
thrift,5010,comment_0,"If it is wrong, we need a test case and should mark it as ""bug"". If it works and ""improvement"" is correct, then claim is wrong (""should do X"") because it obvuiously does that already.",test_debt,lack_of_tests,"Fri, 15 Nov 2019 07:47:45 +0000","Thu, 11 Feb 2021 22:27:03 +0000","Sat, 23 Nov 2019 21:27:08 +0000",740363,"If it is wrong, we need a test case and should mark it as ""bug"". If it works and ""improvement"" is correct, then claim is wrong (""should do X"") because it obvuiously does that already.",0.095875,0.095875,negative
thrift,5010,comment_2,"Test case reque RISC-based processor computer :) It small improverment to make avaible run code on other processor architectures, then x86 & ARM. Class avaible as extension for netstandard 2.0 in nuget package System.Memory.",test_debt,lack_of_tests,"Fri, 15 Nov 2019 07:47:45 +0000","Thu, 11 Feb 2021 22:27:03 +0000","Sat, 23 Nov 2019 21:27:08 +0000",740363,"Test case reque RISC-based processor computer It small improverment to make avaible run code on other processor architectures, then x86 & ARM. blob/master/lib/netstd/Thrift/Transport/TFramedTransport.cs Class System.Buffers.Binary.BinaryPrinitives avaible as extension for netstandard 2.0 in nuget package System.Memory.",0.1333333333,0.07142857143,positive
thrift,612,comment_0,+1 Looks good to me. Do you have test so that we make sure there are no regressions in the future?,test_debt,lack_of_tests,"Tue, 27 Oct 2009 18:39:47 +0000","Thu, 19 Aug 2010 05:36:37 +0000","Wed, 28 Oct 2009 17:07:23 +0000",80856,+1 Looks good to me. Do you have test so that we make sure there are no regressions in the future?,0.388,0.388,positive
thrift,612,comment_2,That's why I was asking for a test before committing the patch.,test_debt,lack_of_tests,"Tue, 27 Oct 2009 18:39:47 +0000","Thu, 19 Aug 2010 05:36:37 +0000","Wed, 28 Oct 2009 17:07:23 +0000",80856,"> How did this happen? It's an obvious bug/fix, but this path shouldn't actually be triggered > under normal operation. That's why I was asking for a test before committing the patch.",0.2,0.06666666667,neutral
thrift,62,comment_1,"We need setup.rb back for the automake stuff to work, and more generally until the RubyGems work is completed. I'm going to apply this to the tree shortly, but until then you'll need to apply this yourself for testing.",test_debt,lack_of_tests,"Thu, 26 Jun 2008 00:32:20 +0000","Tue, 8 Jul 2008 00:48:12 +0000","Tue, 8 Jul 2008 00:48:12 +0000",1037752,"We need setup.rb back for the automake stuff to work, and more generally until the RubyGems work is completed. I'm going to apply this to the tree shortly, but until then you'll need to apply this yourself for testing.",0.2,0.2,neutral
thrift,758,comment_3,"Oh sorry, could you include a test case for this under lib/perl/test please? Thanks!",test_debt,lack_of_tests,"Wed, 14 Apr 2010 21:20:35 +0000","Thu, 2 Sep 2010 00:54:30 +0000","Thu, 2 Sep 2010 00:54:30 +0000",12108835,"Oh sorry, could you include a test case for this under lib/perl/test please? Thanks!",0.125,0.125,neutral
thrift,758,comment_4,"Jake, I unfortunately have no clue how to generate a with the provided ThriftTest. I didn't find a way to run the tests automatically either. I manually executed: perl -Iblib/lib -Ilib -Itest/gen-perl test/processor.t Thanks, Yann",test_debt,lack_of_tests,"Wed, 14 Apr 2010 21:20:35 +0000","Thu, 2 Sep 2010 00:54:30 +0000","Thu, 2 Sep 2010 00:54:30 +0000",12108835,"Jake, I unfortunately have no clue how to generate a TApplicationException with the provided ThriftTest. I didn't find a way to run the tests automatically either. I manually executed: perl -Iblib/lib -Ilib -Itest/gen-perl test/processor.t Thanks, Yann",-0.24375,-0.24375,negative
thrift,813,comment_0,"It could be in Java's TJSONProtocol implementation. I'm not super familiar with that implementation, so I don't know where to look off hand, but maybe you could add a test case that exercises this?",test_debt,lack_of_tests,"Sun, 4 Jul 2010 09:35:36 +0000","Tue, 1 Nov 2011 02:52:22 +0000","Fri, 22 Oct 2010 13:28:27 +0000",9517971,"It could be in Java's TJSONProtocol implementation. I'm not super familiar with that implementation, so I don't know where to look off hand, but maybe you could add a test case that exercises this?",-0.271,-0.271,neutral
thrift,873,comment_1,"I like all of this patch except for the separate JVMs. I know that's its probably a more responsible way to do things, but it makes the test run slower. Are you certain that this is what is necessary in order to make the test pass? Wouldn't it be possible for us just to clean up after ourselves in the test for",test_debt,expensive_tests,"Fri, 27 Aug 2010 05:50:41 +0000","Tue, 1 Nov 2011 02:52:01 +0000","Fri, 27 Aug 2010 06:18:02 +0000",1641,"I like all of this patch except for the separate JVMs. I know that's its probably a more responsible way to do things, but it makes the test run slower. Are you certain that this is what is necessary in order to make the test pass? Wouldn't it be possible for us just to clean up after ourselves in the test for TAsyncClientManager?",0.1166666667,0.1166666667,neutral
thrift,873,comment_2,"On my box it goes a bit slower, but still pretty fast. With forkmode=""once"" I did indeed see failures that went away with the separate JVMs. It also made it harder to figure out which test was actually at fault, since the problem with AsyncClientManager caused a bunch of later tests to fail too.",test_debt,expensive_tests,"Fri, 27 Aug 2010 05:50:41 +0000","Tue, 1 Nov 2011 02:52:01 +0000","Fri, 27 Aug 2010 06:18:02 +0000",1641,"On my box it goes a bit slower, but still pretty fast. With forkmode=""once"" I did indeed see failures that went away with the separate JVMs. It also made it harder to figure out which test was actually at fault, since the problem with AsyncClientManager caused a bunch of later tests to fail too.",-0.1865,-0.1865,neutral