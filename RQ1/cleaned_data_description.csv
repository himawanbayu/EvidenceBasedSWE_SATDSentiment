project,issue_number,issue_type,text,classification,indicator,zz_created,zz_updated,zz_resolved,zz_duration,zz_text,zz_wink_a,zz_wink_b,roberta
camel,10153,description,"Camel 2.17.x upgraded spring version to 4.x in most of the components. but for camel-cxf component, it still has to use spring-dm and spring version 3.x, the spring version range in the Import-Package should keep [3.2,4), not [4.1,5). Now the ERROR will happen when install camel-cxf feature into karaf container (in case of both Spring 4.x and Spring 3.x are installed in the container) To fix it, make change to the pom.xml",architecture_debt,using_obsolete_technology,"Mon, 18 Jul 2016 06:14:14 +0000","Sat, 20 Aug 2016 13:44:18 +0000","Sat, 20 Aug 2016 13:44:17 +0000",2878203,"Camel 2.17.x upgraded spring version to 4.x in most of the components. but for camel-cxf component, it still has to use spring-dm and spring version 3.x, the spring version range in the Import-Package should keep [3.2,4), not [4.1,5). Now the ERROR will happen when install camel-cxf feature into karaf container (in case of both Spring 4.x and Spring 3.x are installed in the container) To fix it, make change to the pom.xml",-0.01322222222,-0.01322222222,neutral
camel,11734,description,It'd be nice if we could upgrade camel-grpc to use the latest grpc-java library as there are some improvements to how it does class loading:,architecture_debt,using_obsolete_technology,"Fri, 1 Sep 2017 06:17:00 +0000","Fri, 8 Sep 2017 08:49:52 +0000","Fri, 8 Sep 2017 08:49:52 +0000",613972,It'd be nice if we could upgrade camel-grpc to use the latest grpc-java library as there are some improvements to how it does class loading: https://github.com/grpc/grpc-java/releases/tag/v1.6.1,0.675,0.675,positive
camel,11868,description,The current java transport client is due EOL in near future and it will be a good idea we switch the the new high level rest client instead. The new high level rest client is only released from version 5.6.0 and towards so an general upgrade of the dependencies is required for the ELK component. This also add the support for basic authentication without the need of x-pack. This is usefull when running ELK behind a reverse proxy.,architecture_debt,using_obsolete_technology,"Sat, 30 Sep 2017 05:29:19 +0000","Wed, 18 Oct 2017 06:54:13 +0000","Wed, 18 Oct 2017 06:54:13 +0000",1560294,The current java transport client is due EOL in near future and it will be a good idea we switch the the new high level rest client instead. The new high level rest client is only released from version 5.6.0 and towards so an general upgrade of the dependencies is required for the ELK component. https://www.elastic.co/guide/en/elasticsearch/client/java-rest/current/java-rest-high.html This also add the support for basic authentication without the need of x-pack. This is usefull when running ELK behind a reverse proxy. https://www.elastic.co/guide/en/elasticsearch/client/java-rest/current/_basic_authentication.html,0.30025,0.2402,neutral
camel,2535,description,"As we don't use the CxfSoap component any more, it's time to clean it up.",architecture_debt,using_obsolete_technology,"Wed, 10 Mar 2010 10:06:54 +0000","Sun, 24 Apr 2011 10:01:27 +0000","Thu, 11 Mar 2010 09:13:31 +0000",83197,"As we don't use the CxfSoap component any more, it's time to clean it up.",0.4,0.4,neutral
camel,2901,description,HawtDB 1.1 has been released. Change log at: We should upgrade to pick up the listed bug fixes:,architecture_debt,using_obsolete_technology,"Fri, 2 Jul 2010 13:12:52 +0000","Sun, 24 Apr 2011 10:01:39 +0000","Fri, 2 Jul 2010 13:13:18 +0000",26,"HawtDB 1.1 has been released. Change log at: http://github.com/chirino/hawtdb/blob/master/changelog.md We should upgrade to pick up the listed bug fixes: Fixing BTree node next linking.. It was possible that a next link would not properly get set in some conditions during a node removal. You can add get callbacks when a commit gets flushed to disk. Changed the way the journal was handling callback based write completed notifications. They are now delivered in batch form to a single JournalListener. This reduces thread contention and increases throughput. Moved the built in predicate implementations into a Predicates class. Added close method to the Transaction interface. Implementation now asserts it is no longer used after a close. Making the appender's max write batch size configurable. Revamped how Update and DefferedUpdates track shadow pages. A little easier to follow now. - changed the interface to PagedAccessor so that instead of removing the linked pages, it just needs to report what the linked pages are. Got rid of the WriteKey wrapper class, updated logging. Better looking printStrucuture BTree method Added a few Logging classes to reduce the number of places we need to update if in case we decided to switch logging APIs. Fixing free page allocation bug when using deferred updates. Javadoc improvements Expose a config property to control the read cache size. Reworked how snapshot tracking was being done. Fixes errors that occurred during heavy concurrent access. Added a non-blocking flush method to the TxPageFile Read cache was not getting updated when a update batch was performed. Cached entries that were updated and flushed to disk continued returning stale data. Fixed an recovery edge cases Don't start the thread from the thread factory. that causes illegal state exceptions Fixed journal bug where getting next location could return a the current location Renamed EncoderDecoder to PagedAccessor The util.buffer package has moved into it's own project at http://github.com/chirino/hawtbuf Fixes #4 : Errors occur when you re-open an empty data file. Extracted a SortedIndex interface from the Index interface to non sorted indexes having to deal with that leaky abstraction. added a free() method to the Paged for symmetry with the alloc() method. Improved website documentation",0.2625,0.05144230769,neutral
camel,4132,description,The camel-atom component is using an ancient incubator version of abdera which will make it hard to work with camel-cxf. Updating to 1.1.2 which is what CXF uses would help.,architecture_debt,using_obsolete_technology,"Tue, 21 Jun 2011 19:03:36 +0000","Wed, 22 Jun 2011 14:05:54 +0000","Wed, 22 Jun 2011 14:05:54 +0000",68538,The camel-atom component is using an ancient incubator version of abdera which will make it hard to work with camel-cxf. Updating to 1.1.2 which is what CXF uses would help.,0.3,0.3,negative
camel,4357,description,I am currently looking into the dependencies betwen packages in camel-core. The packages org.apache.camel and form the camel api. So I am trying to make them not depend on other packages from camel-core. One problem there is the starter class Main. It needs access to impl packages as it needs to start camel. So it should not live in org.apache.camel. I propose to move it to To not break anything right now I will create a deprecated class Main in org.apache.camel that extends the moved Main. We can remove the deprecated version in camel 3.0,architecture_debt,violation_of_modularity,"Fri, 19 Aug 2011 13:38:41 +0000","Mon, 22 Aug 2011 17:08:15 +0000","Mon, 22 Aug 2011 13:34:55 +0000",258974,I am currently looking into the dependencies betwen packages in camel-core. The packages org.apache.camel and org.apache.camel.spi form the camel api. So I am trying to make them not depend on other packages from camel-core. One problem there is the starter class Main. It needs access to impl packages as it needs to start camel. So it should not live in org.apache.camel. I propose to move it to org.apache.camel.main. To not break anything right now I will create a deprecated class Main in org.apache.camel that extends the moved Main. We can remove the deprecated version in camel 3.0,-0.08764285714,-0.05842857143,neutral
camel,4543,description,"Camel Blueprint support is limited/hardcoded to Aries. This makes it impossible to use in JBoss 7 or with another blueprint implementation like Gemini. The following classes use various things from Aries. * * * * * Now obviously the last three are related to the custom namespace handler for Aries. It would be good if these were moved into their own module, something like or the like. That people can choose to include if they would like to use the custom blueprint namespace handler in Aries. Otherwise the camel-blueprint module should be implementation agnostic and work on all blueprint containers. Not just Aries.",architecture_debt,violation_of_modularity,"Thu, 13 Oct 2011 16:19:13 +0000","Sun, 1 May 2016 10:43:46 +0000","Sun, 1 May 2016 10:43:46 +0000",143576673,"Camel Blueprint support is limited/hardcoded to Aries. This makes it impossible to use in JBoss 7 or with another blueprint implementation like Gemini. The following classes use various things from Aries. BlueprintContainerRegistry BlueprintPropertiesParser CamelContextFactoryBean CamelProxyFactoryBean CamelNamespaceHandler Now obviously the last three are related to the custom namespace handler for Aries. It would be good if these were moved into their own module, something like camel-aries-namespace or the like. That people can choose to include if they would like to use the custom blueprint namespace handler in Aries. Otherwise the camel-blueprint module should be implementation agnostic and work on all blueprint containers. Not just Aries.",0.1845,0.1845,neutral
camel,4593,description,"There are bunch fixes of ABDERA-281, ABDERA-290 can help camel-atom work smoothly with OSGi platform. We need to upgrade the the abdera to 1.1.3 once it is out.",architecture_debt,using_obsolete_technology,"Fri, 28 Oct 2011 08:59:47 +0000","Sun, 26 May 2013 09:54:16 +0000","Sun, 26 May 2013 09:54:16 +0000",49769669,"There are bunch fixes of ABDERA-281, ABDERA-290 can help camel-atom work smoothly with OSGi platform. We need to upgrade the the abdera to 1.1.3 once it is out.",0.5125,0.5125,positive
camel,7127,description,CXF 2.x uses deprecated class removed in Spring 4. Because of that camel-cxf doesn't work with Spring 4. This issue has been discussed on this (2) forum thread already. (1) (2),architecture_debt,using_obsolete_technology,"Sat, 11 Jan 2014 17:44:01 +0000","Tue, 27 May 2014 07:22:44 +0000","Tue, 27 May 2014 07:22:44 +0000",11713123,CXF 2.x uses deprecated org.springframework.jms.connection.SingleConnectionFactory102 class removed in Spring 4. Because of that camel-cxf doesn't work with Spring 4. This issue has been discussed on this (2) forum thread already. (1) https://builds.apache.org/view/A-D/view/Camel/job/Camel.trunk.fulltest.spring4/lastBuild/org.apache.camel$camel-cxf/testReport/org.apache.camel.component.cxf.jms/CxfEndpointJMSConsumerTest/testInvocation (2) http://camel.465427.n5.nabble.com/Spring-4-update-td5745746.html,-0.12,-0.06666666667,negative
camel,789,description,When downloading artifacts we have a many repos. Several of them are specific for a single/few components. We should move these repo settings to these targeted components and let the uber pom.xml have only the major public maven repos. Maybe some of the repos is out-of-date and not used anymore.,architecture_debt,violation_of_modularity,"Thu, 7 Aug 2008 04:15:15 +0000","Thu, 23 Oct 2008 04:37:12 +0000","Thu, 7 Aug 2008 18:24:53 +0000",50978,When downloading artifacts we have a many repos. Several of them are specific for a single/few components. We should move these repo settings to these targeted components and let the uber pom.xml have only the major public maven repos. Maybe some of the repos is out-of-date and not used anymore.,0.0,0.0,neutral
camel,9338,description,"Should be updated to latest release as we are far behind on this one. It may no longer work in OSGi and that is fine, just remove the feature",architecture_debt,using_obsolete_technology,"Thu, 19 Nov 2015 07:20:38 +0000","Thu, 10 Dec 2015 09:32:15 +0000","Thu, 10 Dec 2015 08:12:59 +0000",1817541,"Should be updated to latest release as we are far behind on this one. It may no longer work in OSGi and that is fine, just remove the feature",-0.5,-0.5,negative
camel,3315,description,Jasypt 1.7 has trimmed down on the 3rd party dependencies which means we can get rid of some libraries,build_debt,over-declared_dependencies,"Fri, 5 Nov 2010 14:18:01 +0000","Tue, 25 Oct 2011 11:36:22 +0000","Fri, 25 Feb 2011 06:58:03 +0000",9650402,Jasypt 1.7 has trimmed down on the 3rd party dependencies which means we can get rid of some libraries http://www.jasypt.org/whatsnew17.html,-0.292,-0.292,positive
camel,5342,description,"Package is included/shaded inside the camel-core jar. It is not very nice if is already on the path. It is a deal breaker, if their versions are different. For example cassandra-1.1.1 requires which is missing from the version included in camel. It would be nice if was included as a normal dependency. Comment in the pom.xml says ""Shade the googlecode stuff for OSGi"". Well, if that is strictly required, maybe it could be better included in camel-core-osgi package. In any case, if it must be shaded at all, it would be safer to use relocation property of the maven-shade-plugin. In this case, camel could stay with the version it wants, without conflicting with explicit dependencies.",build_debt,build_others,"Thu, 7 Jun 2012 01:33:39 +0000","Thu, 14 Jun 2012 13:25:01 +0000","Thu, 14 Jun 2012 13:25:01 +0000",647482,"Package com.googlecode.concurrentlinkedhashmap:concurrentlinkedhashmap-lru is included/shaded inside the camel-core jar. It is not very nice if concurrentlinkedhashmap-lru.jar is already on the path. It is a deal breaker, if their versions are different. For example cassandra-1.1.1 requires ConcurrentLinkedHashMap$Builder.maximumWeightedCapacity(), which is missing from the version included in camel. It would be nice if concurrentlinkedhashmap-lru was included as a normal dependency. Comment in the pom.xml says ""Shade the googlecode stuff for OSGi"". Well, if that is strictly required, maybe it could be better included in camel-core-osgi package. In any case, if it must be shaded at all, it would be safer to use relocation property of the maven-shade-plugin. In this case, camel could stay with the version it wants, without conflicting with explicit dependencies.",0.047125,0.03534375,neutral
camel,9709,description,"Without proper dependency definition, clients may see a variety of versions",build_debt,build_others,"Tue, 15 Mar 2016 12:03:24 +0000","Tue, 15 Mar 2016 13:47:05 +0000","Tue, 15 Mar 2016 13:47:05 +0000",6221,"Without proper dependency definition, clients may see a variety of versions",-0.531,-0.531,negative
camel,9721,description,"Hi all the camel-spring-batch component does only depend on spring-batch (by pom.xml) but in the karaf features.xml it is said that it depends on camel-spring (which is not correct but in test). So this has a very downside, that drags the deprecated sprin-dm and impossible to run anythign on spring higher than 3.2. And the spring-batch version used by camel (2.16.2 needs spring-batch 3.0.4, that depends on spring 4, which is blocked by adding spring-camel), result, jar hell, and unable to read the XML Namespace errors. Good this is: removing the dependency of camel-spring and adding spring directly, solves the issue and can run spring-batch in any upper version correctly. thanks!",build_debt,build_others,"Thu, 17 Mar 2016 12:19:12 +0000","Thu, 17 Mar 2016 13:21:25 +0000","Thu, 17 Mar 2016 12:46:03 +0000",1611,"Hi all the camel-spring-batch component does only depend on spring-batch (by pom.xml) but in the karaf features.xml it is said that it depends on camel-spring (which is not correct but in test). So this has a very downside, that drags the deprecated sprin-dm and impossible to run anythign on spring higher than 3.2. And the spring-batch version used by camel (2.16.2 needs spring-batch 3.0.4, that depends on spring 4, which is blocked by adding spring-camel), result, jar hell, and unable to read the XML Namespace errors. Good this is: removing the dependency of camel-spring and adding spring directly, solves the issue and can run spring-batch in any upper version correctly. thanks!",-0.1096071429,-0.1096071429,negative
camel,10507,description,"Addressing the issue raised in the review, Jackson TypeReferences should be declared constant.",code_debt,low_quality_code,"Tue, 22 Nov 2016 11:07:21 +0000","Tue, 22 Nov 2016 12:28:26 +0000","Tue, 22 Nov 2016 11:24:34 +0000",1033,"Addressing the issue raised in the PR1265 review, Jackson TypeReferences should be declared constant.",0.0,0.0,neutral
camel,1107,description,"When using the options to configure httpClient using URI option, they should be removed from the uri that is left over to the HTTPProducer. should remove the httpClient.xxx so it's",code_debt,dead_code,"Fri, 21 Nov 2008 10:23:50 +0000","Fri, 31 Jul 2009 06:33:43 +0000","Fri, 21 Nov 2008 18:43:37 +0000",29987,"When using the options to configure httpClient using URI option, they should be removed from the uri that is left over to the HTTPProducer. should remove the httpClient.xxx so it's",0.0,0.0,neutral
camel,11104,description,"We have implemented a camel route where we are having camel sftp producer to transfer a files to remote SFTP location but on performance testing on client environment and on our local environment we have observed degradation in the time for transferring files to remote SFTP location. Please find the detailed analysis below. The we tried the various test in our local environment. In each test we put around 22 files on camel file consumer and each file took below time to write the file. PFB details When target directory having 20,000 files. Camel sftp producer took around 1 minute 43 second to a transfer file DEBUG 07:00:38 (Camel thread #6 - INFO 07:00:38 (Camel thread #6 - INFO 07:00:38 (Camel thread #6 - DEBUG 07:00:38 (Camel thread #6 - DEBUG 07:02:19 (Camel thread #6 - DEBUG 07:02:19 (Camel thread #6 - DEBUG 07:02:19 (Camel thread #6 - DEBUG 07:02:20 (Camel thread #6 - DEBUG 07:02:20 (Camel thread #6 - INFO 07:02:20 (Camel thread #6 - When target directory having 40,000 files. Camel sftp producer took around 3 minute 17 second to transfer file DEBUG 07:47:23 (Camel thread #6 - using exchange: INFO 07:47:23 (Camel thread #6 - from IOP FTP directory INFO 07:47:23 (Camel thread #6 - : to ICOMS FTP directory DEBUG 07:47:23 (Camel thread #6 - DEBUG 07:50:40 (Camel thread #6 - DEBUG 07:50:40 (Camel thread #6 - DEBUG 07:50:40 (Camel thread #6 - DEBUG 07:50:41 (Camel thread #6 - DEBUG 07:50:41 (Camel thread #6 - INFO 07:50:41 (Camel thread #6 - Similarly when we achieved the files from target directory. It took around 6 sec.It seems like there is a performance issue with camel sftp component. Does it list the files in target directory which is taking time. PFB the producer route which we set up",code_debt,slow_algorithm,"Tue, 4 Apr 2017 07:32:26 +0000","Tue, 4 Apr 2017 07:49:13 +0000","Tue, 4 Apr 2017 07:35:55 +0000",209,"We have implemented a camel route where we are having camel sftp producer to transfer a files to remote SFTP location but on performance testing on client environment and on our local environment we have observed degradation in the time for transferring files to remote SFTP location. Please find the detailed analysis below. The we tried the various test in our local environment. In each test we put around 22 files on camel file consumer and each file took below time to write the file. PFB details ======================================================================================== When target directory having 20,000 files. Camel sftp producer took around 1 minute 43 second to a transfer file ======================================================================================== DEBUG 07:00:38 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.component.file.FileConsumer> About to process file: GenericFile[/data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1/ST30PERFORPMAJAR17020726.txt] using exchange: Exchange[ST30PERFORPMAJAR17020726.txt] INFO 07:00:38 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) com.sigma.samp.imp.virginmedia.bss.voice.camelroutes.iopRoute.IOPResponseFtpRouteBuilder> Picked IOP response file : ST30PERFORPMAJAR17020726.txt from IOP FTP directory INFO 07:00:38 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) com.sigma.samp.imp.virginmedia.bss.voice.camelroutes.iopRoute.IOPResponseFtpRouteBuilder> Sending IOP response file : ST30PERFORPMAJAR17020726.txt to ICOMS FTP directory DEBUG 07:00:38 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.processor.SendProcessor> >>>> Endpoint[sftp://10.100.150.190/icoms/1?download=false&maxMessagesPerPoll=10&password=xxxxxx&tempPrefix=Q&username=sigmauser] Exchange[ST30PERFORPMAJAR17020726.txt] DEBUG 07:02:19 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.component.file.GenericFileConverter> Read file /data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1/ST30PERFORPMAJAR17020726.txt (no charset) DEBUG 07:02:19 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.component.file.remote.SftpOperations> About to store file: QST30PERFORPMAJAR17020726.txt using stream: java.io.BufferedInputStream@54a89ff5 DEBUG 07:02:19 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.component.file.remote.SftpOperations> Took 0.658 seconds (658 millis) to store file: QST30PERFORPMAJAR17020726.txt and FTP client returned: true DEBUG 07:02:20 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.component.file.remote.RemoteFileProducer> Wrote [icoms/1/QST30PERFORPMAJAR17020726.txt] to [Endpoint[sftp://10.100.150.190/icoms/1?download=false&maxMessagesPerPoll=10&password=xxxxxx&tempPrefix=Q&username=sigmauser]] DEBUG 07:02:20 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.component.file.remote.SftpOperations> Renaming file: icoms/1/QST30PERFORPMAJAR17020726.txt to: icoms/1/ST30PERFORPMAJAR17020726.txt INFO 07:02:20 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) com.sigma.samp.imp.virginmedia.bss.voice.camelroutes.iopRoute.IOPResponseFtpRouteBuilder> IOP response file : ST30PERFORPMAJAR17020726.txt successfully sent to ICOMS FTP directory ======================================================================================= When target directory having 40,000 files. Camel sftp producer took around 3 minute 17 second to transfer file ======================================================================================= DEBUG 07:47:23 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.component.file.FileConsumer> About to process file: GenericFile[/data/users/slvm02/smp53/domains/v mb/Icoms/iop_responses/1/SX30RPMAJAR1702483756.txt] using exchange: Exchange[SX30RPMAJAR1702483756.txt] INFO 07:47:23 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) com.sigma.samp.imp.virginmedia.bss.voice.camelroutes.iopRoute.IOPResponseFtpRouteBuilder> Picked IOP response file : SX30RPMAJAR1702483756.txt from IOP FTP directory INFO 07:47:23 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) com.sigma.samp.imp.virginmedia.bss.voice.camelroutes.iopRoute.IOPResponseFtpRouteBuilder> Sending IOP response file : SX30RPMAJAR1702483756.txt to ICOMS FTP directory DEBUG 07:47:23 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.processor.SendProcessor> >>>> Endpoint[sftp://10.100.150.190/icoms/1?download=false&maxMessagesPer Poll=10&password=xxxxxx&tempPrefix=Q&username=sigmauser] Exchange[SX30RPMAJAR1702483756.txt] DEBUG 07:50:40 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.component.file.GenericFileConverter> Read file /data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1/SX30RPMAJAR1702483756.txt (no charset) DEBUG 07:50:40 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.component.file.remote.SftpOperations> About to store file: QSX30RPMAJAR1702483756.txt using stream: java.io.BufferedInputStream@7954cfa8 DEBUG 07:50:40 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.component.file.remote.SftpOperations> Took 0.659 seconds (659 millis) to store file: QSX30RPMAJAR1702483756.txt and FTP client returned: true DEBUG 07:50:41 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.component.file.remote.RemoteFileProducer> Wrote [icoms/1/QSX30RPMAJAR1702483756.txt] to [Endpoint[sftp://10.100.150.190/icoms/1?download=false&maxMessagesPerPoll=10&password=xxxxxx&tempPrefix=Q&username=sigmauser]] DEBUG 07:50:41 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) org.apache.camel.component.file.remote.SftpOperations> Renaming file: icoms/1/QSX30RPMAJAR1702483756.txt to: icoms/1/SX30RPMAJAR1702483756.txt INFO 07:50:41 (Camel (cfgMgrCamelContext) thread #6 - file:///data/users/slvm02/smp53/domains/vmb/Icoms/iop_responses/1) com.sigma.samp.imp.virginmedia.bss.voice.camelroutes.iopRoute.IOPResponseFtpRouteBuilder> IOP response file : SX30RPMAJAR1702483756.txt successfully sent to ICOMS FTP directory ==================================================================================================== Similarly when we achieved the files from target directory. It took around 6 sec.It seems like there is a performance issue with camel sftp component. Does it list the files in target directory which is taking time. =================================================================================================== PFB the producer route which we set up sftp://10.100.150.190/icoms/1?download=false&maxMessagesPerPoll=10&password=xxxxxx&tempPrefix=Q&username=sigmauser",0.1284090909,-0.03657758621,neutral
camel,1138,description,"Using a memory profiler, we've identified what appears to be a substantial memory leak in FileConsumer in Camel 1.5.0. It appears that the noopMap is constantly having items added to it, but nothing performs a remove on it when the file is consumed. This causes a very large amount of string data to be accumulated in the heap. In our application, this was a leak of several hundred megabytes and is a showstopper. Considering the apparent severity of this issue, it would really be nice if a fix could be incorporated into a 1.5.1 version.",code_debt,low_quality_code,"Tue, 2 Dec 2008 16:36:14 +0000","Mon, 23 Mar 2009 08:40:34 +0000","Wed, 3 Dec 2008 17:15:46 +0000",88772,"Using a memory profiler, we've identified what appears to be a substantial memory leak in FileConsumer in Camel 1.5.0. It appears that the noopMap is constantly having items added to it, but nothing performs a remove on it when the file is consumed. This causes a very large amount of string data to be accumulated in the heap. In our application, this was a leak of several hundred megabytes and is a showstopper. Considering the apparent severity of this issue, it would really be nice if a fix could be incorporated into a 1.5.1 version. http://www.nabble.com/Memory-leak-in-FileConsumer-in-Camel-1.5.0-td20794405s22882.html#a20794405",0.125,0.1041666667,negative
camel,1256,description,"The camel-cxf component is dued for some code cleanup and refactoring. We can clean some of of the interfaces and redundant code, etc.",code_debt,dead_code,"Wed, 14 Jan 2009 15:13:12 +0000","Fri, 31 Jul 2009 06:33:58 +0000","Wed, 28 Jan 2009 03:48:19 +0000",1168507,"The camel-cxf component is dued for some code cleanup and refactoring. We can clean some of of the interfaces and redundant code, etc.",0.2,0.2,neutral
camel,1270,description,The countdown latch in MainSupport is not completed when Main is stopping. Then we have a hanging thread. Can bee seen using ctrl + \,code_debt,multi-thread_correctness,"Sun, 18 Jan 2009 09:56:18 +0000","Fri, 31 Jul 2009 06:33:59 +0000","Sun, 18 Jan 2009 10:06:27 +0000",609,The countdown latch in MainSupport is not completed when Main is stopping. Then we have a hanging thread. Can bee seen using ctrl + \,0.06666666667,0.06666666667,negative
camel,1447,description,"See article, listening 2 We should consider renaming *handle* to *catchBlock* so its using the same name as try .. catch. And we could add a finallyBlock so end user got them all.",code_debt,low_quality_code,"Wed, 11 Mar 2009 06:02:04 +0000","Sat, 21 Nov 2009 11:57:55 +0000","Fri, 17 Apr 2009 15:38:43 +0000",3231399,"See article, listening 2 http://architects.dzone.com/articles/fuse-esb-4-osgi-based?page=0,2 We should consider renaming handle to catchBlock so its using the same name as try .. catch. And we could add a finallyBlock so end user got them all.",0.0,0.0,neutral
camel,1507,description,"To send a email you have to follow a pretty specific course. This adds a property (which is poorly named in this patch) to the MailConfiguration that names the header that contains the plaintext version of the email, and adds a property where you can embed images inline. If an attachment has a filename starting with ""cid:"" then this will add the ""Content-ID"" header to that multipart body - which will allow the email client to put the image in the appropriate place when it is viewed. (i.e. the html email has something like <image src=""cid:0001"" /",code_debt,low_quality_code,"Wed, 1 Apr 2009 00:50:08 +0000","Sat, 27 Nov 2010 06:42:58 +0000","Thu, 2 Apr 2009 07:42:27 +0000",111139,"To send a multipart/alternative email you have to follow a pretty specific course. This adds a property (which is poorly named in this patch) to the MailConfiguration that names the header that contains the plaintext version of the email, and adds a property where you can embed images inline. If an attachment has a filename starting with ""cid:"" then this will add the ""Content-ID"" header to that multipart body - which will allow the email client to put the image in the appropriate place when it is viewed. (i.e. the html email has something like <image src=""cid:0001"" /> and the attachment is named 'cid:0001' - when it sees an inline attachment with ""Content-ID: 0001"" it will put it in the right spot)",0.03795833333,0.1663333333,neutral
camel,1944,description,Currently only the camel-irc producer installs an event listener on the IRC connection to log incoming messages from the IRC server. This patch revamps the logging so that the component installs an event adapter to do logging before the connection is even established so it's easier to debug various issues that could be occurring with the connection to the IRC server.,code_debt,low_quality_code,"Wed, 26 Aug 2009 15:47:30 +0000","Thu, 3 Jun 2010 07:23:59 +0000","Thu, 27 Aug 2009 02:51:53 +0000",39863,Currently only the camel-irc producer installs an event listener on the IRC connection to log incoming messages from the IRC server. This patch revamps the logging so that the component installs an event adapter to do logging before the connection is even established so it's easier to debug various issues that could be occurring with the connection to the IRC server.,0.0,0.0,neutral
camel,251,description,"displays the file instead of directory (a boolean) automagically attempt to reconnect if from their destinations. This should probably be shared with Consumers, and could maybe done more cleanly using camel's try/catch features, but there you go. Smarter directory/file building/handling. This lets you handle URI's like similarly, avoid putting files in '/' on servers that expose the full filesystem, etc. More verbose logging. Stuff like ""what file went where,"" reconnection attempts, etc.",code_debt,low_quality_code,"Thu, 29 Nov 2007 19:16:53 +0000","Mon, 12 May 2008 07:56:35 +0000","Fri, 7 Dec 2007 03:52:39 +0000",635746,"RemoteFileConfiguration.toString() displays the file instead of directory (a boolean) FtpProducer/SftpProducer automagically attempt to reconnect if disconnected/timed-out from their destinations. This should probably be shared with Consumers, and could maybe done more cleanly using camel's try/catch features, but there you go. Smarter directory/file building/handling. This lets you handle URI's like ftp://somehost/somedir, ftp://somehost/somedir/ similarly, avoid putting files in '/' on servers that expose the full filesystem, etc. More verbose logging. Stuff like ""what file went where,"" reconnection attempts, etc.",0.09166666667,0.06428571429,neutral
camel,4007,description,"If attachment file names contain unicode characters, they do not appear correctly encoded in the message.",code_debt,low_quality_code,"Mon, 23 May 2011 20:45:18 +0000","Tue, 25 Oct 2011 11:35:26 +0000","Sat, 11 Jun 2011 17:56:38 +0000",1631480,"If attachment file names contain unicode characters, they do not appear correctly encoded in the message.",-0.5,-0.5,negative
camel,4139,description,"Currently, spring parses into a CxfEndpointBean whereas blueprint parses into a CxfEndpoint directly. This mismatch then requires extra casting in the CxfComponent. Also, it means there are features supported by Spring that are not supported yet by blueprint (like configuring interceptors). I'm attaching a patch that removes the spring specific CxfEndpointBean stuff and allows both spring and blueprint to parse directly into the CxfEndpoint (well, subclasses of) so the two approaches can be maintained together. It also reduces the usage of ""Strings"" for properties in CxfEndpoint and uses QNames where appropriate (added a Converter for that) and Class<?",code_debt,complex_code,"Wed, 22 Jun 2011 15:17:58 +0000","Thu, 23 Jun 2011 00:57:02 +0000","Thu, 23 Jun 2011 00:30:12 +0000",33134,"Currently, spring parses into a CxfEndpointBean whereas blueprint parses into a CxfEndpoint directly. This mismatch then requires extra casting in the CxfComponent. Also, it means there are features supported by Spring that are not supported yet by blueprint (like configuring interceptors). I'm attaching a patch that removes the spring specific CxfEndpointBean stuff and allows both spring and blueprint to parse directly into the CxfEndpoint (well, subclasses of) so the two approaches can be maintained together. It also reduces the usage of ""Strings"" for properties in CxfEndpoint and uses QNames where appropriate (added a Converter for that) and Class<?> for the service class. That will allow OSGi to provide the Class directly instead of having to Class.forName the thing. (still need to do that, but this is a start)",0.2694,0.2031111111,neutral
camel,4202,description,"See nabble When using persistent replyTo queues for request/reply over JMS, then the ReplyManager should be faster to pickup replies. The default spring-jms timeout is 1 sec and it impacts the performance. Likewise the receiveTimeout should not be set on the reply managers as that does not apply here.",code_debt,slow_algorithm,"Sat, 9 Jul 2011 14:05:41 +0000","Mon, 19 Sep 2011 20:32:51 +0000","Sun, 14 Aug 2011 10:07:44 +0000",3096123,"See nabble http://camel.465427.n5.nabble.com/slow-reply-for-jms-component-when-url-contains-replyTo-tp4563075p4563075.html When using persistent replyTo queues for request/reply over JMS, then the ReplyManager should be faster to pickup replies. The default spring-jms timeout is 1 sec and it impacts the performance. Likewise the receiveTimeout should not be set on the reply managers as that does not apply here.",-0.1666666667,-0.1666666667,neutral
camel,4230,description,"See nabble If for some reason the method cannot be invoked, you may get a caused exception We should catch this and provide a wrapped exception with a more descriptive error message, about the bean and method attempted to invoke.",code_debt,low_quality_code,"Fri, 15 Jul 2011 05:17:52 +0000","Mon, 2 Apr 2012 17:57:42 +0000","Wed, 7 Mar 2012 22:30:40 +0000",20452368,"See nabble http://camel.465427.n5.nabble.com/How-to-impl-bean-side-of-proxy-w-Future-return-tp4581104p4581104.html If for some reason the method cannot be invoked, you may get a caused exception We should catch this and provide a wrapped exception with a more descriptive error message, about the bean and method attempted to invoke.",-0.4,-0.4,negative
camel,4317,description,"After upgrading to CXF 2.4.x, we don't need to import the resources file like any more, so it's time to clean up these imports.",code_debt,dead_code,"Tue, 9 Aug 2011 03:20:04 +0000","Tue, 9 Aug 2011 14:24:05 +0000","Tue, 9 Aug 2011 14:24:05 +0000",39841,"After upgrading to CXF 2.4.x, we don't need to import the resources file like /META-INF/cxf/cxf-extension-xxx.xml any more, so it's time to clean up these imports.",0.2,0.1333333333,neutral
camel,4958,description,"When using a JMS route, and an exception occurs, then you get a WARN logging from Spring JMS that there is no error handler configured. And you get the stacktrace etc. However Camel itself will also by default log the stacktrace, so we have it logged twice. We should make this less verbose. And allow end users to more easily customize the logging level and/or whether stracktraces should be included.",code_debt,low_quality_code,"Tue, 31 Jan 2012 06:28:34 +0000","Tue, 31 Jan 2012 08:22:28 +0000","Tue, 31 Jan 2012 08:22:28 +0000",6834,"When using a JMS route, and an exception occurs, then you get a WARN logging from Spring JMS that there is no error handler configured. And you get the stacktrace etc. However Camel itself will also by default log the stacktrace, so we have it logged twice. We should make this less verbose. And allow end users to more easily customize the logging level and/or whether stracktraces should be included.",-0.06,-0.06,neutral
camel,5012,description,"When starting and shutting down Camel, it reports a bit stuff at INFO level. We should make it less verbose. For example the type converter logs 3-4 lines, we should just log 1 line instead.",code_debt,low_quality_code,"Thu, 16 Feb 2012 17:07:49 +0000","Thu, 16 Feb 2012 17:18:42 +0000","Thu, 16 Feb 2012 17:18:42 +0000",653,"When starting and shutting down Camel, it reports a bit stuff at INFO level. We should make it less verbose. For example the type converter logs 3-4 lines, we should just log 1 line instead.",-0.09733333333,-0.09733333333,neutral
camel,721,description,"Currently log component only logs the payload. We have a nice ExchangeFormatter that can format an exchange with all kind of options. The options could be enabled on the log component so you can customize your logging. Also there should be a *multiline* option to the exchange formatter so it can log all the stuff on multi lines if for instance there are many options, they get very long.",code_debt,low_quality_code,"Tue, 15 Jul 2008 06:56:17 +0000","Thu, 17 Jul 2008 12:01:14 +0000","Tue, 15 Jul 2008 09:33:28 +0000",9431,"Currently log component only logs the payload. We have a nice ExchangeFormatter that can format an exchange with all kind of options. The options could be enabled on the log component so you can customize your logging. Also there should be a multiline option to the exchange formatter so it can log all the stuff on multi lines if for instance there are many options, they get very long.",0.20625,0.20625,neutral
camel,7319,description,"The test in is ""dead"" since checkin git-svn-id: if you enable xalan on testing classpath.",code_debt,dead_code,"Fri, 21 Mar 2014 18:51:05 +0000","Fri, 28 Mar 2014 03:09:29 +0000","Fri, 28 Mar 2014 03:09:29 +0000",548304,"The test testUsingJavaExtensions in camel-core/src/test/java/org/apache/camel/builder/xml/XPathTest.java is ""dead"" since checkin git-svn-id: https://svn.apache.org/repos/asf/camel/trunk@824639 13f79535-47bb-0310-9956-ffa450edef68 if you enable xalan on testing classpath.",-0.05,-0.025,neutral
camel,7644,description,"Since the camel DSL is invoked prior to being invoked there is no camel context set on the delegate java RouteBuilder which causes it to create a new context when the first dsl method is invoked. With the implementation of CAMEL-7327 introduced in 2.13.1 which stores created camel contexts in a set in this causes instances of DefaultCamelContext to be leaked, they are never removed from the static set. This is especially aparrent during unit testing. The following test shows that an additional context is registered for the scala route builder as opposed to java. Verification of the leak can be requires profiler and capturing of heap after termination of the test case (in ParentRunner.java).",code_debt,low_quality_code,"Mon, 28 Jul 2014 16:39:53 +0000","Tue, 3 Mar 2015 09:12:32 +0000","Tue, 3 Mar 2015 09:12:32 +0000",18808359,"Since the camel DSL is invoked prior to `.addRoutesToCamelContext(CamelContext)` being invoked there is no camel context set on the delegate java RouteBuilder which causes it to create a new context when the first dsl method is invoked. With the implementation of CAMEL-7327 introduced in 2.13.1 which stores created camel contexts in a set in `Container.Instance#CONTEXT`; this causes instances of DefaultCamelContext to be leaked, they are never removed from the static set. This is especially aparrent during unit testing. The following test shows that an additional context is registered for the scala route builder as opposed to java. Verification of the leak can be requires profiler and capturing of heap after termination of the test case (in ParentRunner.java).",0.02291666667,0.0171875,neutral
camel,7715,description,"SjmsConsumer and SjmsProducer always register a new ThreadPool on Camel context every time a new instance is created for an endpoint. If consumer or producer is stopped or removed or even component is removed, thread pool still exists.",code_debt,low_quality_code,"Tue, 19 Aug 2014 00:37:45 +0000","Fri, 5 Sep 2014 07:09:36 +0000","Wed, 20 Aug 2014 05:51:21 +0000",105216,"SjmsConsumer and SjmsProducer always register a new ThreadPool on Camel context ExecutorServiceManager every time a new instance is created for an endpoint. If consumer or producer is stopped or removed or even component is removed, thread pool still exists.",0.02025,0.02025,neutral
camel,9752,description,"When any file:// or ftp:// consumer has a quartz2 schedule it can start throwing exceptions because too many worker-threads are busy at the same time Both workers can find the same files during filling the maxMessagesPerPoll, or during processing of those files. This happens when the route can not process all files before the next trigger happens. example (every minute): Attached you can find a stacktrace that would happen very often if worker1 processes and moves files that worker2 would also like to start processing. This does not happen when using scheduler=spring or when using delay=1m. The only way I have found to make sure a file or ftp component does not use multiple threads while consuming very large batches is to annotate the class with I am not familiar enough with the Camelcode to say what side effects it has and if this would prevent any quartz job in camel to now be single threaded, even if the user does not want it to be. But to me it looks like an oversight when moving from quartz to quartz2. A file or ftp consumer should be single threaded while retrieving.",code_debt,multi-thread_correctness,"Thu, 24 Mar 2016 08:51:36 +0000","Thu, 24 Mar 2016 10:30:53 +0000","Thu, 24 Mar 2016 10:30:53 +0000",5957,"When any file:// or ftp:// consumer has a quartz2 schedule it can start throwing exceptions because too many worker-threads are busy at the same time (FileNotFoundException). Both workers can find the same files during filling the maxMessagesPerPoll, or during processing of those files. This happens when the route can not process all files before the next trigger happens. example (every minute): from(file:///mnt/sl-nl/bij/outbox/?sortBy=ignoreCase:file:name&filter=#fileFilter&recursive=false&move=processed&moveFailed=failed&scheduler.cron=0+0/1+0-23+?+*+1,2,3,4,5,6,7&scheduler=quartz2&scheduler.triggerId=nl_bij-export-to-archive-276) to(file:///data/work/sl/work-archive/work/276/) Attached you can find a stacktrace that would happen very often if worker1 processes and moves files that worker2 would also like to start processing. This does not happen when using scheduler=spring or when using delay=1m. The only way I have found to make sure a file or ftp component does not use multiple threads while consuming very large batches is to annotate the QuartzScheduledPollConsumerJob class with @DisallowConcurrentExecution. I am not familiar enough with the Camelcode to say what side effects it has and if this would prevent any quartz job in camel to now be single threaded, even if the user does not want it to be. But to me it looks like an oversight when moving from quartz to quartz2. A file or ftp consumer should be single threaded while retrieving.",-0.1298611111,-0.090625,neutral
camel,9812,description,"After shutting down a camel context, there are still threads running kafka consumers. In the logs after the shutdown I can see: So in theory the context is stopped, but I can see threads running with the polling of the sockets of kafka consumers (see attached immage). This deployed in an application server (wilfly in my case), causes a lot of issues, because apps get deployed and undeployed without stopping the JVM, but threads from previous deployments are left there. Please also bear in mind that kafka (9.0.1) throws warning messages due to the fact that un expected config items are thrown to the kafka consumer properties. Thanks!",code_debt,multi-thread_correctness,"Mon, 4 Apr 2016 13:00:56 +0000","Tue, 5 Apr 2016 11:19:48 +0000","Tue, 5 Apr 2016 09:24:07 +0000",73391,"After shutting down a camel context, there are still threads running kafka consumers. In the logs after the shutdown I can see: So in theory the context is stopped, but I can see threads running with the polling of the sockets of kafka consumers (see attached immage). This deployed in an application server (wilfly in my case), causes a lot of issues, because apps get deployed and undeployed without stopping the JVM, but threads from previous deployments are left there. Please also bear in mind that kafka (9.0.1) throws warning messages due to the fact that un expected config items are thrown to the kafka consumer properties. Thanks!",0.0389,0.0389,neutral
camel,10048,description,"RoutingSlip has a cache of error handlers implemented as a ConcurrentHashMap. This map stores error handlers as values, and uses some synthetic objects as keys. For some kind of destinations provided in routing slip, map lookup operation does not work. Hence, new error handlers are always added to the map and existing error handlers never reused. Finally, the program runs out of memory. The synthetic keys are actually instances of class Such key is based on two objects: RouteContext and destination Processor. Neither RouteContext nor Processor do not require their implementations to provide equals() and hashCode() methods. Strictly speaking, caching implementation in RoutingSlip is incorrect, because it uses hash map in the discouraged way. However, for some cases it works. The problem occurs when routing slip contains a 'sync' destination, in other words - destination is a Processor that does not implement AsyncProcessor interface. RoutingSlip determines destination producer via and the latter uses method. This method creates new instance of Processor for every processor that is not an instance of AsyncProcessor. This is where the problem hides: new object has different hash code (defined by Object.hashCode()) and new object isn't equal to the object used as a key in the hash map (well, Object.equals()). Finally, new key for the hash map is calculated, lookup operation cannot find this key in the hash map, new key-value pair is put into the hash map.",design_debt,non-optimal_design,"Sat, 11 Jun 2016 21:41:44 +0000","Thu, 4 Jan 2018 09:11:44 +0000","Sun, 12 Jun 2016 09:44:54 +0000",43390,"RoutingSlip has a cache of error handlers implemented as a ConcurrentHashMap. This map stores error handlers as values, and uses some synthetic objects as keys. For some kind of destinations provided in routing slip, map lookup operation does not work. Hence, new error handlers are always added to the map and existing error handlers never reused. Finally, the program runs out of memory. The synthetic keys are actually instances of class RoutingSlip.PreparedErrorHandler. Such key is based on two objects: RouteContext and destination Processor. Neither RouteContext nor Processor do not require their implementations to provide equals() and hashCode() methods. Strictly speaking, caching implementation in RoutingSlip is incorrect, because it uses hash map in the discouraged way. However, for some cases it works. The problem occurs when routing slip contains a 'sync' destination, in other words - destination is a Processor that does not implement AsyncProcessor interface. RoutingSlip determines destination producer via ProducerCache.doInAsyncProducer(), and the latter uses AsyncProcessorConverterHelper.convert() method. This method creates new instance of Processor for every processor that is not an instance of AsyncProcessor. This is where the problem hides: new object has different hash code (defined by Object.hashCode()) and new object isn't equal to the object used as a key in the hash map (well, Object.equals()). Finally, new key for the hash map is calculated, lookup operation cannot find this key in the hash map, new key-value pair is put into the hash map.",-0.02797916667,-0.02238333333,neutral
camel,10476,description,"Problem: When running with a Camel Blueprint project a configAdminFile is not used to populate propertyplacehoders in when exectued with So a user can't run camel locally in a similar way to running in Karaf with file based property placeholder values. Workaround: I think, but haven't tested yet, that you can work around this locally using the methods described here: and/or how this solution appears to use exec:java locally and loads the properties via To reproduce the problem: Create a new project using (You need to change the log4j config to make it run.) To reduce the time, I created a project that runs here: Instead of using a default in the blueprint XML for the I setup the POM to include the following: In Camel 2.15.2 or earlier, this file would be loaded when mvn camel:run was invoked and the properties would be available via the PID at run time. After the changes made in CAMEL-9313, it appears that the method is only called in when the createTestBundle pathway is taken in java.lang.String, boolean, java.lang.String, java.lang.String, java.lang.String, So it appears test using get this functionality (as shown by the tests) but things executed from camel:run do not. Here you can see in Camel 2.14 that call to is made after the bundelContext is created. In the master branch version, that call is no longer made from main after the context is returned. I made a change locally to add a similar call to in Camel 2.18: Here is the output of the log statement from the example before this change: Here is the output of the log statement from the example after this change: As you can see before the change, the ${greeting} property is not poplulated via propertyplacehoder. After the change it is replaced. Given all the discussion of timing related issues in CAMEL-9313, I'm hesitant to say this is a good enough solution or that it aligns with the intention of the changes made in that fix. Given that configAdminFileName and configAdminPid are passed into perhaps the call to should happen inside createBundleContext or one of it sub-methods. Overall, I ""think"" a user should be able to use the configAdminPid and configAdminFileName settings to load properties via camel:run rather than work aound it, but I could be persumptious there.",design_debt,non-optimal_design,"Mon, 14 Nov 2016 16:09:36 +0000","Fri, 16 Dec 2016 08:10:55 +0000","Fri, 18 Nov 2016 17:09:14 +0000",349178,"Problem: When running with a Camel Blueprint project a configAdminFile is not used to populate propertyplacehoders in camel-test-blueprint when exectued with camel-maven-plugin(camel:run). So a user can't run camel locally in a similar way to running in Karaf with file based property placeholder values. Workaround: I think, but haven't tested yet, that you can work around this locally using the methods described here: http://ggrzybek.blogspot.com/2015/12/camel-blueprint-test-support.html and/or how this solution https://github.com/cschneider/Karaf-Tutorial/tree/master/camel/order/src appears to use exec:java locally and loads the properties via PropertiesComponent. To reproduce the problem: Create a new project using camel-archetype-blueprint. (You need to change the log4j config to make it run.) To reduce the time, I created a project that runs here: https://github.com/ryanco/propertyconfig. Instead of using a default in the blueprint XML for the propertyplaceholder, I setup the POM to include the following: In Camel 2.15.2 or earlier, this file would be loaded when mvn camel:run was invoked and the properties would be available via the PID at run time. After the changes made in CAMEL-9313, it appears that the method org.apache.camel.test.blueprint.CamelBlueprintHelper#setPersistentFileForConfigAdmin is only called in when the createTestBundle pathway is taken in org.apache.camel.test.blueprint.CamelBlueprintHelper#createBundleContext(java.lang.String, java.lang.String, boolean, java.lang.String, java.lang.String, java.lang.String, java.lang.String[]...). So it appears test using CamelBlueprintTestSupport get this functionality (as shown by the tests) but things executed from camel:run do not. Here you can see in Camel 2.14 that call to org.apache.camel.test.blueprint.CamelBlueprintHelper#setPersistentFileForConfigAdmin is made after the bundelContext is created. https://github.com/apache/camel/blob/camel-2.14.x/components/camel-test-blueprint/src/main/java/org/apache/camel/test/blueprint/Main.java#L103 In the master branch version, that call is no longer made from main after the context is returned. https://github.com/apache/camel/blob/master/components/camel-test-blueprint/src/main/java/org/apache/camel/test/blueprint/Main.java#L106 I made a change locally to add a similar call to org.apache.camel.test.blueprint.CamelBlueprintHelper#setPersistentFileForConfigAdmin in Camel 2.18: Here is the output of the log statement from the example before this change: Here is the output of the log statement from the example after this change: As you can see before the change, the ${greeting} property is not poplulated via propertyplacehoder. After the change it is replaced. Given all the discussion of timing related issues in CAMEL-9313, I'm hesitant to say this is a good enough solution or that it aligns with the intention of the changes made in that fix. Given that configAdminFileName and configAdminPid are passed into createBundleContext, perhaps the call to org.apache.camel.test.blueprint.CamelBlueprintHelper#setPersistentFileForConfigAdmin should happen inside createBundleContext or one of it sub-methods. Overall, I ""think"" a user should be able to use the configAdminPid and configAdminFileName settings to load properties via camel:run rather than work aound it, but I could be persumptious there.",0.002364035088,-0.004440251572,neutral
camel,10678,description,We should use each individual fields instead of a string field eg use fields - from - to - name - model As individual fields. And use name as the index field if its intended to be unique. This allows users to better query/browse the registry from JMX as well to find which transformers there is. Also remember to adjust the code in the jolokia command.,design_debt,non-optimal_design,"Fri, 6 Jan 2017 08:23:41 +0000","Fri, 6 Jan 2017 13:45:21 +0000","Fri, 6 Jan 2017 13:45:21 +0000",19300,We should use each individual fields instead of a string field https://github.com/apache/camel/blob/master/camel-core/src/main/java/org/apache/camel/management/mbean/ManagedTransformerRegistry.java#L86 eg use fields from to name model As individual fields. And use name as the index field if its intended to be unique. This allows users to better query/browse the registry from JMX as well to find which transformers there is. Also remember to adjust the code in the jolokia command. https://github.com/apache/camel/blob/master/platforms/commands/commands-jolokia/src/main/java/org/apache/camel/commands/jolokia/DefaultJolokiaCamelController.java#L763,0.266375,0.2131,neutral
camel,10950,description,"One of the drawbacks with the docker-java library used in the camel-docker component is that it depends on Jersey for the JAX-RS client. This can be problematic if you already have another JAX-RS implementation on the classpath. Therefore, it'd be nice if you could specify the that you want to work with. By default it'd be but you could choose to use the if you wanted to avoid Jersey. Similarly, users could implement their own and have camel-docker load this when it comes to build the Docker client.",design_debt,non-optimal_design,"Mon, 6 Mar 2017 13:53:42 +0000","Tue, 7 Mar 2017 12:50:27 +0000","Tue, 7 Mar 2017 12:50:27 +0000",82605,"One of the drawbacks with the docker-java library used in the camel-docker component is that it depends on Jersey for the JAX-RS client. This can be problematic if you already have another JAX-RS implementation on the classpath. Therefore, it'd be nice if you could specify the DockerCmdExecFactory that you want to work with. By default it'd be JerseyDockerCmdExecFactory, but you could choose to use the NettyDockerCmdExecFactory if you wanted to avoid Jersey. Similarly, users could implement their own DockerCmdExecFactory and have camel-docker load this when it comes to build the Docker client.",0.03833333333,0.03833333333,neutral
camel,1112,description,"when reading files from file,ftp/sftp component, it is often neccesarry to get the files in a predifined order (e.g. created date or filename). to solve this, an extra uri parameter could be used, to set the order of files beeing processed. ordering can be done after reading the file-list. i know this can also be solved by applying a resequencer pattern in the route, but i think for large files it would be hard to apply this. also a processing in an created date order would be more natural.",design_debt,non-optimal_design,"Mon, 24 Nov 2008 15:48:05 +0000","Fri, 31 Jul 2009 06:33:43 +0000","Tue, 2 Dec 2008 07:41:32 +0000",662007,"when reading files from file,ftp/sftp component, it is often neccesarry to get the files in a predifined order (e.g. created date or filename). to solve this, an extra uri parameter could be used, to set the order of files beeing processed. ordering can be done after reading the file-list. i know this can also be solved by applying a resequencer pattern in the route, but i think for large files it would be hard to apply this. also a processing in an created date order would be more natural.",0.08,0.08,neutral
camel,11196,description,"A Camel connector can be configured on two levels - component - endpoint Just like a regular Camel component. But we should allow users to configure a connector in one place, and not worry about if its component or endpoint level. And then let camel-connector when it prepares the connector figure out all of this for you. This may require supporting loading configuration from external resource files such as .properties files etc. Or something else. But from end users we can make this easier.",design_debt,non-optimal_design,"Mon, 24 Apr 2017 14:22:57 +0000","Mon, 15 May 2017 15:18:48 +0000","Mon, 15 May 2017 15:18:48 +0000",1817751,"A Camel connector can be configured on two levels component endpoint Just like a regular Camel component. But we should allow users to configure a connector in one place, and not worry about if its component or endpoint level. And then let camel-connector when it prepares the connector figure out all of this for you. This may require supporting loading configuration from external resource files such as .properties files etc. Or something else. But from end users we can make this easier.",0.06521428571,0.06521428571,neutral
camel,11282,description,We should extend the plain DefaultComponent (the is deprecated) ensure there is a plain default no-arg constructor so it makes using and configuring components easier. See eg SO If it was just a plain no-arg constructor then the bean style would have worked.,design_debt,non-optimal_design,"Tue, 16 May 2017 08:07:42 +0000","Thu, 25 Jul 2019 13:39:20 +0000","Thu, 25 Jul 2019 13:39:20 +0000",69139898,We should extend the plain DefaultComponent (the UriEndpointComponent is deprecated) ensure there is a plain default no-arg constructor so it makes using and configuring components easier. See eg SO http://stackoverflow.com/questions/43918252/how-to-increase-or-configure-maxthreads-in-apache-came-restlet?noredirect=1#comment74982775_43918252 If it was just a plain no-arg constructor then the bean style would have worked.,-0.535,-0.535,neutral
camel,12104,description,"There is very strange behavior in Camel cxf and cxfrs timeouts which could lead to sensitive data being released. Below is a code sample which illustrates the unexpected behavior. I think any developer would expect the test API to return ""Valid Response"" or some kind exception, but in fact it returns ""SENSITIVE DATA"" due to the default continuationTimeout of 30 seconds. This issue seems to have been introduced by",design_debt,non-optimal_design,"Wed, 27 Dec 2017 18:31:14 +0000","Mon, 26 Mar 2018 15:56:24 +0000","Tue, 20 Mar 2018 01:27:00 +0000",7109746,"There is very strange behavior in Camel cxf and cxfrs timeouts which could lead to sensitive data being released. Below is a code sample which illustrates the unexpected behavior. I think any developer would expect the test API to return ""Valid Response"" or some kind exception, but in fact it returns ""SENSITIVE DATA"" due to the default continuationTimeout of 30 seconds. This issue seems to have been introduced by https://issues.apache.org/jira/browse/CAMEL-7401",-0.175,-0.175,negative
camel,12624,description,"Currently we are running Camel AMQP component against Active MQ 5 (Amazon MQ) we want to move to an Artemis solution but this hasn't worked seamlessly. In CAMEL-9204 I believe a hardcoded topic prefix of ""topic://"" was introduced I think for a workaround of an Active MQ 5 bug. In Artemis this means Camel connects to a topic named instead of and therefore receives no events. The only possible workaround is to manually create the connection factory which means then the topic prefix is not set. My believe is that the hardcoded topic prefix is a bug and should be removed or an option to override at the AMQP component level should be introduced. Bug location: AMQPComponent.java line 59 Workaround:",design_debt,non-optimal_design,"Wed, 4 Jul 2018 21:44:12 +0000","Wed, 18 Jul 2018 03:44:00 +0000","Fri, 13 Jul 2018 15:29:16 +0000",755104,"Currently we are running Camel AMQP component against Active MQ 5 (Amazon MQ) we want to move to an Artemis solution but this hasn't worked seamlessly. In CAMEL-9204I believe a hardcoded topic prefix of ""topic://"" was introduced I think for a workaround of an Active MQ 5 bug. In Artemis this means Camel connects to a topic named ""topic://example.topic.event"" instead of ""example.topic.event"" and therefore receives no events. The only possible workaround is to manually create the connection factory which means then the topic prefix is not set. My believe is that the hardcoded topic prefix is a bug and should be removed or an option to override at the AMQP component level should be introduced. Bug location: AMQPComponent.java line 59 Workaround:",-0.007142857143,-0.005555555556,negative
camel,12646,description,"If you have complex types like and wants to allow to configure this via spring boot autoconfiguration in - then the generated spring boot classes with all the options will use getter/setter of types That seems correct, but the spring-boot tooling itself (that generates additional json file) will skip those as it only support primitives and string types. So we may need to fool, and generate the getter/setter as String type as you use it for configuring it as a bean reference by id anyway, eg = #myDataSource We can add in the javadoc that the type is",design_debt,non-optimal_design,"Thu, 12 Jul 2018 12:44:45 +0000","Sun, 15 Jul 2018 07:00:07 +0000","Sun, 15 Jul 2018 07:00:07 +0000",238522,"If you have complex types like javax.sql.DataSource and wants to allow to configure this via spring boot autoconfiguration in application.properties - then the generated spring boot classes with all the options will use getter/setter of types javax.sql.DataSource. That seems correct, but the spring-boot tooling itself (that generates additional json file) will skip those as it only support primitives and string types. So we may need to fool, and generate the getter/setter as String type as you use it for configuring it as a bean reference by id anyway, eg camel.component.jdbc.data-source = #myDataSource We can add in the javadoc that the type is javax.sql.DataSource.",0.04583333333,0.03365384615,neutral
camel,13681,description,Any of the options you can configure via such as: camel.main.name And so on should be configurable via ENV variables which will override any existing configuration. This is good practice in containers and also how SB can do etc.,design_debt,non-optimal_design,"Tue, 25 Jun 2019 09:40:24 +0000","Wed, 26 Jun 2019 12:46:38 +0000","Wed, 26 Jun 2019 12:46:38 +0000",97574,Any of the options you can configure via application.properties such as: camel.main.name camel.component.xxx=yyy And so on should be configurable via ENV variables which will override any existing configuration. This is good practice in containers and also how SB can do etc.,0.1595,0.09114285714,neutral
camel,149,description,Brian McCallister had this great idea.. add a thread() method to the DSL instead of using the seda component to do async processing. We should be able to: or ThreadPoolExecutor pool = ...,design_debt,non-optimal_design,"Mon, 17 Sep 2007 23:45:09 +0000","Mon, 12 May 2008 07:56:31 +0000","Mon, 8 Oct 2007 15:14:25 +0000",1783756,"Brian McCallister had this great idea.. add a thread() method to the DSL instead of using the seda component to do async processing. We should be able to: from(""file:foo"").thread(10).maxSize(20).to(""wombat:nugget"") or ThreadPoolExecutor pool = ... from(""file:foo"").thread(pool).to(""wombat:nugget"")",0.644,0.184,positive
camel,1608,description,"When profiles are explicitly activated in maven using -P, the <activeByDefault However there is one more blocker I will have to take care before 1.6.1 can be released. During the release:prepare phase the poms are regenerated and the <?xml> decl, the apache license notice is lost and there are issues with white spaces. This seems related to the fact that the <project> element is not on one line as per the new maven release guide. Fixing this may take a day or two as we have so many poms but I'll do it as the highest prio. Once I'll get it to work and release 1.6.1, I'll tackle 2.0-M2. The good news is that once we have this fixed, the release process will be much simplified.",design_debt,non-optimal_design,"Wed, 13 May 2009 03:03:57 +0000","Sat, 21 Nov 2009 11:58:00 +0000","Tue, 16 Jun 2009 02:16:40 +0000",2934763,"When profiles are explicitly activated in maven using -P, the <activeByDefault> value specified in a profile activation configuration is not considered. This is the case with the maven-release-plugin configuration where the profiles to be activated are specified as <arguments>. As the enable-schemagen profile (and a couple others) where not specified, camel-schema.xsd was not generated and pretty much all tests using the xml dsl were failing. Many thanks to Dan Kulp from the Apache Maven team for helping with this. However there is one more blocker I will have to take care before 1.6.1 can be released. During the release:prepare phase the poms are regenerated and the <?xml> decl, the apache license notice is lost and there are issues with white spaces. This seems related to the fact that the <project> element is not on one line as per the new maven release guide. Fixing this may take a day or two as we have so many poms but I'll do it as the highest prio. Once I'll get it to work and release 1.6.1, I'll tackle 2.0-M2. The good news is that once we have this fixed, the release process will be much simplified.",0.2429285714,0.183375,neutral
camel,2094,description,We now have option {{autoStartup}} on routes. Lets also have this on the camel context itself so it replaces the option that is a bit confusing. For example the code above will not start Camel on startup. You can then later start camel manually by And then all runs. This also means that we will remove the option so if you use this option you must migrate to the autoStartup instead.,design_debt,non-optimal_design,"Thu, 22 Oct 2009 06:01:59 +0000","Thu, 3 Jun 2010 07:24:59 +0000","Thu, 22 Oct 2009 07:02:18 +0000",3619,We now have option autoStartup on routes. Lets also have this on the camel context itself so it replaces the shouldStartContext option that is a bit confusing. For example the code above will not start Camel on startup. You can then later start camel manually by And then all runs. This also means that we will remove the option shouldStartContext! so if you use this option you must migrate to the autoStartup instead.,0.0126,0.0105,neutral
camel,2617,description,Currently CamelContext have a {{Properties}} for misc options. We should introduce a so we have type safe options in Java DSL and Spring XML. Then we also have one place to *advert* and document the options we currently support.,design_debt,non-optimal_design,"Tue, 6 Apr 2010 12:02:47 +0000","Sun, 24 Apr 2011 10:01:07 +0000","Wed, 16 Jun 2010 08:19:01 +0000",6120974,Currently CamelContext have a Properties for misc options. We should introduce a CamelContextPropertiesDefinition so we have type safe options in Java DSL and Spring XML. Then we also have one place to advert and document the options we currently support.,0.2166666667,0.2166666667,neutral
camel,2650,description,"This only applies for prototype scoped POJOs which uses @Produce or @Consume to inject a Camel Producer/Consumer. As the prototype scoped POJO is short lived and there could potentially be created many of those POJOs. Then @Produce / @Consume will inject new instances of Producer / Consumer as well. And since there is no standard way of knowing when the POJO is no longer in need, which is where we can to stop the Producer/Consumer. For singleton scoped this is not a problem as CamelContext will keep track on the created Producer/Consumer in its internal _servicesToClose_. Which is then stopped when CamelContext stops. For prototype we need a different strategy such as - proxy it to use pooled producers/consumers which CamelContext manage the lifecycle - use a shared ProducerTemplate / ConsumerTemplate instead which CamelContext manages the lifecycle - other - maybe some thread local tricks",design_debt,non-optimal_design,"Thu, 15 Apr 2010 11:26:28 +0000","Wed, 25 Mar 2015 08:24:56 +0000","Wed, 25 Mar 2015 08:24:56 +0000",155941108,"This only applies for prototype scoped POJOs which uses @Produce or @Consume to inject a Camel Producer/Consumer. As the prototype scoped POJO is short lived and there could potentially be created many of those POJOs. Then @Produce / @Consume will inject new instances of Producer / Consumer as well. And since there is no standard way of knowing when the POJO is no longer in need, which is where we can to stop the Producer/Consumer. For singleton scoped this is not a problem as CamelContext will keep track on the created Producer/Consumer in its internal servicesToClose. Which is then stopped when CamelContext stops. For prototype we need a different strategy such as proxy it to use pooled producers/consumers which CamelContext manage the lifecycle use a shared ProducerTemplate / ConsumerTemplate instead which CamelContext manages the lifecycle other maybe some thread local tricks",0.1022142857,0.1022142857,neutral
camel,2682,description,This allows end users to continue routing the original message. Instead of now the just get the last splitted message which is confusing.,design_debt,non-optimal_design,"Thu, 29 Apr 2010 16:05:25 +0000","Sun, 24 Apr 2011 10:00:53 +0000","Fri, 30 Apr 2010 09:24:13 +0000",62328,This allows end users to continue routing the original message. Instead of now the just get the last splitted message which is confusing.,-0.2185,-0.2185,neutral
camel,2879,description,"The file strategies was not designed to be as flexible as we may want them today, when introducing more and more options to dictate what should happens with the files when success or failure. As we got - noop - delete - move - moveFailed etc. you may combine then in ways in which we current doesn't support. You should be able to say that if OK then just delete the file, but if ERROR then move it to this error folder instead.",design_debt,non-optimal_design,"Wed, 30 Jun 2010 04:17:15 +0000","Sun, 24 Apr 2011 10:00:44 +0000","Wed, 30 Jun 2010 07:01:45 +0000",9870,"The file strategies was not designed to be as flexible as we may want them today, when introducing more and more options to dictate what should happens with the files when success or failure. As we got noop delete move moveFailed etc. you may combine then in ways in which we current doesn't support. You should be able to say that if OK then just delete the file, but if ERROR then move it to this error folder instead.",-0.05794444444,-0.05794444444,negative
camel,3048,description,The current sample is based on Direct component which is a bad sample as its a very special component. We should do a HelloWorldComponent - the producer will just print the message body to system out - the consumer is scheduled and triggers every 5th second with a dummy exchange,design_debt,non-optimal_design,"Thu, 12 Aug 2010 08:26:16 +0000","Sun, 24 Apr 2011 09:57:16 +0000","Mon, 23 Aug 2010 01:22:49 +0000",924993,The current sample is based on Direct component which is a bad sample as its a very special component. We should do a HelloWorldComponent the producer will just print the message body to system out the consumer is scheduled and triggers every 5th second with a dummy exchange,-0.3,-0.3,negative
camel,3050,description,"Spring 3 changed internally how dependency resolution works. Its now worse as we have to play tricks in the Camel namespace handler to tell Camel the various pieces you can ref, should depend on Camel. Otherwise the dependency resolution in Spring is not working properly. This used to work like a charm in Spring 2.5.6.",design_debt,non-optimal_design,"Thu, 12 Aug 2010 13:30:15 +0000","Sun, 24 Apr 2011 09:57:37 +0000","Thu, 12 Aug 2010 13:51:59 +0000",1304,"Spring 3 changed internally how dependency resolution works. Its now worse as we have to play tricks in the Camel namespace handler to tell Camel the various pieces you can ref, should depend on Camel. Otherwise the dependency resolution in Spring is not working properly. This used to work like a charm in Spring 2.5.6.",0.096875,0.096875,negative
camel,3077,description,"EhCache often has a bit of lag time when invalidating expired cache elements, first setting the Element value to null and then removing the key. If you are hitting a cache rapidly one often will run across a key that is present in the cache that still has a null element entry. The logic for successfully key retrieval just needs to be slightly tweaked to check for null values.",design_debt,non-optimal_design,"Wed, 25 Aug 2010 03:22:51 +0000","Sun, 24 Apr 2011 09:57:39 +0000","Wed, 25 Aug 2010 11:52:31 +0000",30580,"EhCache often has a bit of lag time when invalidating expired cache elements, first setting the Element value to null and then removing the key. If you are hitting a cache rapidly one often will run across a key that is present in the cache that still has a null element entry. The logic for successfully key retrieval just needs to be slightly tweaked to check for null values.",0.2815555556,0.2815555556,neutral
camel,3112,description,To make it easier to read/write files using that given encoding. Currently you have to set that as a property on the Exchange using setProperty in the DSL. But when using @Produce or @Consume you only got the endpoint uri. So having this in the component makes it easier.,design_debt,non-optimal_design,"Thu, 9 Sep 2010 10:37:37 +0000","Sun, 24 Apr 2011 09:58:25 +0000","Fri, 10 Sep 2010 11:34:15 +0000",89798,To make it easier to read/write files using that given encoding. Currently you have to set that as a property on the Exchange using setProperty in the DSL. But when using @Produce or @Consume you only got the endpoint uri. So having this in the component makes it easier.,0.0,0.0,neutral
camel,3125,description,"The problem is with the default implementation of When an error occurs rollback() is called, and this method just logs the error and returns false, which means the polling will not be retried for that execution of the run() method. This means it will be retried after the delay. I created an implementation that suspends the Consumer, which seems like acceptable default behavior. It also allows for extension and adding some hooks for custom stuff. In order for that to be useful it should be made easier to set your own implementation.",design_debt,non-optimal_design,"Tue, 14 Sep 2010 14:09:51 +0000","Sun, 24 Apr 2011 09:57:58 +0000","Fri, 17 Sep 2010 10:52:20 +0000",247349,"The problem is with the default implementation of org.apache.camel.spi.PollingConsumerPollStrategy: org.apache.camel.impl.DefaultPollingConsumerPollStrategy When an error occurs rollback() is called, and this method just logs the error and returns false, which means the polling will not be retried for that execution of the run() method. This means it will be retried after the delay. I created an implementation that suspends the Consumer, which seems like acceptable default behavior. It also allows for extension and adding some hooks for custom stuff. In order for that to be useful it should be made easier to set your own PollingConsumerPollStrategy implementation.",-0.0004,-0.03117948718,negative
camel,3139,description,The Java UUID is too slow. Switching back to AMQ based will improve performance. We should just have to switch to use the JDK UUID for the camel-gae component as it cannot use the AMQ based.,design_debt,non-optimal_design,"Tue, 21 Sep 2010 06:32:37 +0000","Sun, 24 Apr 2011 09:58:20 +0000","Tue, 21 Sep 2010 09:13:33 +0000",9656,The Java UUID is too slow. Switching back to AMQ based will improve performance. We should just have to switch to use the JDK UUID for the camel-gae component as it cannot use the AMQ based.,0.1333333333,0.1333333333,negative
camel,3524,description,"By default Jetty uses 30 sec timeout for continuations. We should allow end users to configure a value of choice, in case their system may take longer time to process an exchange.",design_debt,non-optimal_design,"Mon, 10 Jan 2011 14:34:35 +0000","Tue, 25 Oct 2011 11:35:20 +0000","Mon, 10 Jan 2011 15:27:37 +0000",3182,"By default Jetty uses 30 sec timeout for continuations. We should allow end users to configure a value of choice, in case their system may take longer time to process an exchange.",-0.15,-0.15,neutral
camel,3576,description,When using camel-jms the consumers dont provide a default task executor. That means spring just creates a thread manually and dont reuse the thread. We should provided a task executor from camel using the This allows us to use human readable thread names which can provide details about the consumer. The thread pool is also managed and even a 3rd party provider can manage thread creation. This only works when using Spring 3 as the task executor API is now aligned with the executor service api from the JDK. For Spring 2.x users we should not do this. We will then have to detect the spring version in use.,design_debt,non-optimal_design,"Sat, 22 Jan 2011 08:23:42 +0000","Tue, 25 Oct 2011 11:36:12 +0000","Sun, 6 Feb 2011 13:25:12 +0000",1314090,When using camel-jms the consumers dont provide a default task executor. That means spring just creates a thread manually and dont reuse the thread. We should provided a task executor from camel using the ExecutorServiceStrategy. This allows us to use human readable thread names which can provide details about the consumer. The thread pool is also managed and even a 3rd party provider can manage thread creation. This only works when using Spring 3 as the task executor API is now aligned with the executor service api from the JDK. For Spring 2.x users we should not do this. We will then have to detect the spring version in use.,0.075,0.06666666667,neutral
camel,3677,description,"When splitting inside another split, the custom aggregationStrategy is not used. For example in the route: (where the does nothing more than to concat the bodies with a space inbetween.) The expected results would be: and But that is not what happens. The actual results are two times the same: The reason is, that the strategy is not used. In the class in the method {{protected AggregationStrategy exchange)}}, the first step is to find an aggregationStrategy in the Exchange. This is set to and because it is not null, this aggregation strategy will be used, not the one declared for the splitter.  A workaround would be to remove the AggregationStrategy of the Exchange, before it is aggregated, by using a processor with the following process method: After integrating this in my route, I got the desired results.",design_debt,non-optimal_design,"Thu, 17 Feb 2011 12:01:28 +0000","Thu, 17 Feb 2011 12:47:51 +0000","Thu, 17 Feb 2011 12:47:51 +0000",2783,"When splitting inside another split, the custom aggregationStrategy is not used. For example in the route: (where the concatWithSpaceStrategy does nothing more than to concat the bodies with a space inbetween.) The expected results would be: and But that is not what happens. The actual results are two times the same: The reason is, that the strategy is not used. In the class org.apache.camel.processor.MulticastProcessor, in the method protected AggregationStrategy getAggregationStrategy(Exchange exchange), the first step is to find an aggregationStrategy in the Exchange. This is set to UseOriginalAggregationStrategy, and because it is not null, this aggregation strategy will be used, not the one declared for the splitter.  A workaround would be to remove the AggregationStrategy of the Exchange, before it is aggregated, by using a processor with the following process method: After integrating this in my route, I got the desired results.",0.09285714286,0.065,neutral
camel,3764,description,to replace the use of the hacked and make it consistent across all components.,design_debt,non-optimal_design,"Tue, 8 Mar 2011 01:37:25 +0000","Tue, 25 Oct 2011 11:35:20 +0000","Tue, 8 Mar 2011 01:38:43 +0000",78,to replace the use of the hacked maven-jaxb-schemagen-plugin and make it consistent across all components.,-0.2,-0.2,neutral
camel,3777,description,Put and in it's own component (camel-osgi-test) to allow users to reuse it in its own OSGI integration tests It's the same as with CamelTestSupport and,design_debt,non-optimal_design,"Thu, 10 Mar 2011 17:42:37 +0000","Sat, 11 Jul 2015 22:19:13 +0000","Sat, 11 Jul 2015 22:19:13 +0000",136874196,Put OSGiIntegrationSpringTestSupport and OSGiIntegrationTestSupport in it's own component (camel-osgi-test) to allow users to reuse it in its own OSGI integration tests It's the same as with CamelTestSupport and CamelSpringTestSupport,0.2,0.2,neutral
camel,4417,description,"Several classes in impl are used or extended by components. We should avoid this. The base classes should be moved to support. Examples are DefaultComponent, DefaultEndpoint, DefaultProducer. Another case is the The typeconverter is well placed in impl but the class also has a public static convert method that is used from many components. So this functionality should be moved to processor so it is available to components.",design_debt,non-optimal_design,"Mon, 5 Sep 2011 13:46:59 +0000","Tue, 22 Oct 2013 07:34:27 +0000","Tue, 22 Oct 2013 07:34:27 +0000",67196848,"Several classes in impl are used or extended by components. We should avoid this. The base classes should be moved to support. Examples are DefaultComponent, DefaultEndpoint, DefaultProducer. Another case is the AsyncProcessorTypeConverter. The typeconverter is well placed in impl but the class also has a public static convert method that is used from many components. So this functionality should be moved to processor so it is available to components.",0.013,0.01114285714,neutral
camel,4430,description,"As the proxy lifecycle cleanup work are done in CXF 2.4.3, we are facing some test failed in camel-cxf. After digging the code, I found the proxy instance which is created by the CxfProxyFactoryBean will be GC and the CXF client which is used in CxfProducer will be affect. The endpoint which is set on the conduit will gone, and CxfProducer will complain it with a NPE exception. We can use the to create client instead of CxfProxyFactoryBean to avoid the GC and NPE exception. I checked the difference between using CxfProxyFactoryBean and they are same in most case. We just need to take care of handler setting part which is used in JAXWS frontend.",design_debt,non-optimal_design,"Fri, 9 Sep 2011 10:02:59 +0000","Sat, 10 Sep 2011 05:59:05 +0000","Sat, 10 Sep 2011 05:59:05 +0000",71766,"As the proxy lifecycle cleanup work are done in CXF 2.4.3, we are facing some test failed in camel-cxf. After digging the code, I found the proxy instance which is created by the CxfProxyFactoryBean will be GC and the CXF client which is used in CxfProducer will be affect. The endpoint which is set on the conduit will gone, and CxfProducer will complain it with a NPE exception. We can use the CxfClientFactoryBean to create client instead of CxfProxyFactoryBean to avoid the GC and NPE exception. I checked the difference between using CxfProxyFactoryBean and CxfClientFactoryBean, they are same in most case. We just need to take care of handler setting part which is used in JAXWS frontend.",-0.01666666667,-0.01666666667,neutral
camel,4657,description,"See nabble This bug is in ActiveMQ, but creating a ticket to get it resolved as the leak is apparent when using Spring DMLC with CACHE_SESSION, which Camel by default does when doing request/reply over JMS with fixed replyTo queues. Then the consumer is not cached, and therefore created on each poll, but the ActiveMQSessionPool keeps growing in its internal list of created consumers, as the session is cached. Most likely a patch is needed to fix this in the AMQ side",design_debt,non-optimal_design,"Thu, 10 Nov 2011 11:05:43 +0000","Fri, 11 Nov 2011 08:34:44 +0000","Fri, 11 Nov 2011 08:34:44 +0000",77341,"See nabble http://camel.465427.n5.nabble.com/Possible-memory-leak-in-org-apache-activemq-pool-PooledSession-tp4964951p4964951.html This bug is in ActiveMQ, but creating a ticket to get it resolved as the leak is apparent when using Spring DMLC with CACHE_SESSION, which Camel by default does when doing request/reply over JMS with fixed replyTo queues. Then the consumer is not cached, and therefore created on each poll, but the ActiveMQSessionPool keeps growing in its internal list of created consumers, as the session is cached. Most likely a patch is needed to fix this in the AMQ side",0.03333333333,0.03333333333,negative
camel,4676,description,"split into camel-script-groovy camel-script-jruby so that we can control granularity much better, and version upgrades will be more manageable. In most cases, if the user does scripting, he'll probably standardise on only one particular language for all his routes.",design_debt,non-optimal_design,"Mon, 14 Nov 2011 06:50:13 +0000","Mon, 14 Nov 2011 07:11:47 +0000","Mon, 14 Nov 2011 07:11:47 +0000",1294,"split camel-script-optional into camel-script-javascript camel-script-groovy camel-script-jruby so that we can control granularity much better, and version upgrades will be more manageable. In most cases, if the user does scripting, he'll probably standardise on only one particular language for all his routes.",0.2813333333,0.2813333333,neutral
camel,4741,description,"This is a trivial issue, yet it is sometimes a bit annoying. People that are familiar with jms/activemq components expect that the default behavior of the a queue producer is to add the message to the queue, without having to specify a special operation. Hazelcast Queue Producer instead will through an exception if no operation is defined. It would be good if it was consistent with the rest of the queue producers.",design_debt,non-optimal_design,"Mon, 5 Dec 2011 09:03:15 +0000","Mon, 5 Dec 2011 09:33:39 +0000","Mon, 5 Dec 2011 09:33:39 +0000",1824,"This is a trivial issue, yet it is sometimes a bit annoying. People that are familiar with jms/activemq components expect that the default behavior of the a queue producer is to add the message to the queue, without having to specify a special operation. Hazelcast Queue Producer instead will through an exception if no operation is defined. It would be good if it was consistent with the rest of the queue producers.",-0.306,-0.306,negative
camel,4793,description,I would like to improve the aws-sdb component in the following places: - Using the [Amazon operation instead of CamelAwsSdbXXX -- Add OSGI integration tests I would like to resolve this issue for Camel 2.9.0 because renaming of the operation names are not backwards compatible. I will resolve this issue today.,design_debt,non-optimal_design,"Sun, 18 Dec 2011 13:09:51 +0000","Tue, 27 Dec 2011 22:43:26 +0000","Mon, 19 Dec 2011 14:26:25 +0000",90994,I would like to improve the aws-sdb component in the following places: Using the Amazon operation names instead of CamelAwsSdbXXX -> With this we can easily support more operations (in the future) and the users are more familiar with this names Add OSGI integration tests I would like to resolve this issue for Camel 2.9.0 because renaming of the operation names are not backwards compatible. I will resolve this issue today.,0.159375,0.1339285714,neutral
camel,4928,description,It would be great to have timer component support asynchronous API. Such a feature can be useful when timer component generates events which must be processed by multiple threads. Current implementation of the timer component makes a blocking call so the usage of thread pools hardly possible to process multiple timer event simultaneously.,design_debt,non-optimal_design,"Sun, 22 Jan 2012 03:07:05 +0000","Wed, 7 Aug 2013 16:49:11 +0000","Wed, 7 Aug 2013 16:49:11 +0000",48692526,It would be great to have timer component support asynchronous API. Such a feature can be useful when timer component generates events which must be processed by multiple threads. Current implementation of the timer component makes a blocking call so the usage of thread pools hardly possible to process multiple timer event simultaneously.,0.2666666667,0.2666666667,positive
camel,5008,description,"When working with streams, stream caching must be activated in order to use log:set trace, otherwise the streams will be consumed, as stated here (""#Using Streaming Bodies""). When the stream caching now gets activated, the streams will be reseted after each step (as far as I Understand). This makes it impossible to work with InputStreams in a pipe manner (e.g. Read the first char, then in the next step work with the next chars), as the stream is after this every time in the beginning. I would except that the stream caching provides a mechanism for the ""user"" to be able to read it more than once. Also its the right procedure to reset the streams after they are traced with the tracing mechanism, BUT the should be reseted to the state they were before and not to the very first beginning. I didn't dig into the code that deep but it seems that exactly this happens from user perspective. So to summarize there are several problems: - Working in stream in camel is impossible when log:set debug trace get enabled. (Thus enable Stream caching) - When Stream caching is enabled it becomes impossible to work with ""stream pointers"" as camel reset the streams to the very beginning. I illustrated the problem in the attached jUnit test.",design_debt,non-optimal_design,"Wed, 15 Feb 2012 11:51:24 +0000","Thu, 16 Feb 2012 08:02:43 +0000","Thu, 16 Feb 2012 05:42:50 +0000",64286,"When working with streams, stream caching must be activated in order to use log:set trace, otherwise the streams will be consumed, as stated here http://camel.apache.org/jbi.html (""#Using Streaming Bodies""). When the stream caching now gets activated, the streams will be reseted after each step (as far as I Understand). This makes it impossible to work with InputStreams in a pipe manner (e.g. Read the first char, then in the next step work with the next chars), as the stream is after this every time in the beginning. I would except that the stream caching provides a mechanism for the ""user"" to be able to read it more than once. Also its the right procedure to reset the streams after they are traced with the tracing mechanism, BUT the should be reseted to the state they were before and not to the very first beginning. I didn't dig into the code that deep but it seems that exactly this happens from user perspective. So to summarize there are several problems: Working in stream in camel is impossible when log:set debug trace get enabled. (Thus enable Stream caching) When Stream caching is enabled it becomes impossible to work with ""stream pointers"" as camel reset the streams to the very beginning. I illustrated the problem in the attached jUnit test.",0.1553703704,0.1553703704,neutral
camel,5045,description,"If you add and remove a lot of routes to CamelContext and have JMX enabled, then the will accumulate a map of provisional JMX performance counters for the route mbeans. The map should be cleared after usage, as the map is no longer needed. Memory will accumulate as the map has reference to old objects which cannot be GC.",design_debt,non-optimal_design,"Sun, 26 Feb 2012 14:42:40 +0000","Mon, 27 Feb 2012 07:12:49 +0000","Mon, 27 Feb 2012 07:10:37 +0000",59277,"If you add and remove a lot of routes to CamelContext and have JMX enabled, then the DefaultManagementLifecycleStrategy will accumulate a map of provisional JMX performance counters for the route mbeans. The map should be cleared after usage, as the map is no longer needed. Memory will accumulate as the map has reference to old objects which cannot be GC.",0.0,0.0,neutral
camel,612,description,"When you define a route with a choice() and no matching when() clause is found, the Exchange just ends successfully without doing anything. In my mind, it should fail by default in this case (or we should at least have an easy way to get this behavior).",design_debt,non-optimal_design,"Mon, 16 Jun 2008 06:38:46 +0000","Fri, 11 Jul 2008 04:21:45 +0000","Mon, 23 Jun 2008 12:41:58 +0000",626592,"When you define a route with a choice() and no matching when() clause is found, the Exchange just ends successfully without doing anything. In my mind, it should fail by default in this case (or we should at least have an easy way to get this behavior).",0.2111666667,0.2111666667,negative
camel,6188,description,"The CXF consumer copies the content-type http header to the camel exchange. This header may indicate the character set used in the request (for instance and if so this should be made available in the normal place for Camel (i.e. a property in the exchange called This may (of course) be done in each route by a separate processor, but it simplifies life if this is done by default. seems the logical place) Sample processor that performs this job.",design_debt,non-optimal_design,"Wed, 20 Mar 2013 13:20:03 +0000","Thu, 28 Mar 2013 02:25:58 +0000","Thu, 28 Mar 2013 02:25:58 +0000",651955,"The CXF consumer copies the content-type http header to the camel exchange. This header may indicate the character set used in the request (for instance ""text/xml;charset=UTF-8""), and if so this should be made available in the normal place for Camel (i.e. a property in the exchange called 'CamelCharsetName'). This may (of course) be done in each route by a separate processor, but it simplifies life if this is done by default. (org.apache.camel.component.cxf.DefaultCxfBinding.populateExchangeFromCxfRequest() seems the logical place) Sample processor that performs this job.",0.2083333333,0.0625,neutral
camel,6403,description,"When using a {{camel-jetty}} endpoint, the {{UnitOfWork}} is not being managed by the servlet handling the request but by the Camel route that's being invoked. This means that some resources have already been removed/cleaned up when the servlet is writing the response, e.g. files for cached streams have already removed before the servlet gets a chance to read from them. It would be nice to have an option available to configure the servlet itself to handle the unit of work and mark it {{done}} after the HTTP response has been written. That way, the unit of work can be matched up with the actual HTTP request.",design_debt,non-optimal_design,"Tue, 28 May 2013 07:25:26 +0000","Mon, 12 Aug 2013 17:45:21 +0000","Fri, 9 Aug 2013 07:14:44 +0000",6306558,"When using a camel-jetty endpoint, the UnitOfWork is not being managed by the servlet handling the request but by the Camel route that's being invoked. This means that some resources have already been removed/cleaned up when the servlet is writing the response, e.g. files for cached streams have already removed before the servlet gets a chance to read from them. It would be nice to have an option available to configure the servlet itself to handle the unit of work and mark it done after the HTTP response has been written. That way, the unit of work can be matched up with the actual HTTP request.",0.415625,0.415625,neutral
camel,6635,description,"Due the recent SPI which allows to plugin a different scheduler we can improved this and use a non scheduled thread pool, which avoids the suspend/resume and run for at least one poll ""hack"" we have today in the codebase. Instead we can use a regular thread pool as the scheduler, and then submit the task on demand when receive() is called on the PollingConsumer API.",design_debt,non-optimal_design,"Wed, 14 Aug 2013 08:13:10 +0000","Wed, 14 Aug 2013 14:55:18 +0000","Wed, 14 Aug 2013 14:55:18 +0000",24128,"Due the recent SPI which allows to plugin a different scheduler we can improved this and use a non scheduled thread pool, which avoids the suspend/resume and run for at least one poll ""hack"" we have today in the codebase. Instead we can use a regular thread pool as the scheduler, and then submit the task on demand when receive() is called on the PollingConsumer API.",-0.1,-0.1,neutral
camel,7015,description,"It would be nice to have a possibility of a timeout starting from the first Exchange for messages having the same when using Aggregation. This scenario is currently not supported because ""Its a bit tougher to implement as it would require an improvement to TimeoutMap to support that as well."" The full scenario is described in this thread:",design_debt,non-optimal_design,"Tue, 26 Nov 2013 22:24:15 +0000","Fri, 5 Sep 2014 07:48:10 +0000","Fri, 5 Sep 2014 07:48:10 +0000",24398635,"It would be nice to have a possibility of a timeout starting from the first Exchange for messages having the same correlationExpression when using Aggregation. This scenario is currently not supported because ""Its a bit tougher to implement as it would require an improvement to TimeoutMap to support that as well."" The full scenario is described in this thread: http://camel.465427.n5.nabble.com/A-simple-Aggregator-use-case-td5743633.html#a5743634",0.37275,0.37275,neutral
camel,7201,description,"In the current PGPDataFormat implementation, you provide the public and secret keyring as file or byte array and the keyrings are parsed into object representation during each call. This is fine if you want dynamically exchange the keyrings. However, this has a performance impact. In the provided patch I added the possibility that the PGP keys can be cached so that the performance can be improved. I did this by adding the two interfaces PGPPublicKeyAccess and PGPSecretKeyAccess. So user can now implement his own key access or use the provided default implementations and",design_debt,non-optimal_design,"Thu, 13 Feb 2014 09:42:56 +0000","Wed, 19 Feb 2014 13:28:43 +0000","Wed, 19 Feb 2014 13:28:43 +0000",531947,"In the current PGPDataFormat implementation, you provide the public and secret keyring as file or byte array and the keyrings are parsed into object representation during each call. This is fine if you want dynamically exchange the keyrings. However, this has a performance impact. In the provided patch I added the possibility that the PGP keys can be cached so that the performance can be improved. I did this by adding the two interfaces PGPPublicKeyAccess and PGPSecretKeyAccess. So user can now implement his own key access or use the provided default implementations PGPPublicKeyAccessDefault and PGPSecretKeyAccessDefault.",0.023,0.023,neutral
camel,7461,description,"See for a background. Basically there is inconsistency between a idempotent consumer and the repository. The repository is capable of holding any type, while the consumer is non-parameterized and uses String as it's message type. It would be very handy to have the messageid as a domain type for any application, and thus the consumer should allow for a parameterized type T. This will also probably mean that should allow for any persistent type. If doing this camel would be generic on the type, and allow for supporting application domain types w/o customizations.",design_debt,non-optimal_design,"Fri, 23 May 2014 07:49:45 +0000","Thu, 29 Mar 2018 11:30:12 +0000","Thu, 29 Mar 2018 11:30:12 +0000",121491627,"See http://camel.465427.n5.nabble.com/Idempotent-inconsistencies-td5751484.html for a background. Basically there is inconsistency between a idempotent consumer and the repository. The repository is capable of holding any type, while the consumer is non-parameterized and uses String as it's message type. It would be very handy to have the messageid as a domain type for any application, and thus the consumer should allow for a parameterized type T. This will also probably mean that JpaMessageIdRepository should allow for any persistent type. If doing this camel would be generic on the type, and allow for supporting application domain types w/o customizations.",0.2466666667,0.2466666667,neutral
camel,7587,description,"The MessageHistory feature currently keeps passwords in plain text in case they are part of the URI. does some sanitizing, but only for the from node - other nodes/processors are currently not sanitized. In order to prevent handling sensitive information in the message history in general, I would suggest to sanitize the URI already when storing a MessageHistory item.",design_debt,non-optimal_design,"Wed, 9 Jul 2014 14:08:49 +0000","Thu, 10 Jul 2014 05:57:23 +0000","Thu, 10 Jul 2014 05:57:23 +0000",56914,"The MessageHistory feature currently keeps passwords in plain text in case they are part of the URI. MessageHelper.doDumpMessageHistoryStacktrace() does some sanitizing, but only for the from node - other nodes/processors are currently not sanitized. In order to prevent handling sensitive information in the message history in general, I would suggest to sanitize the URI already when storing a MessageHistory item.",-0.2,-0.15,neutral
camel,7813,description,"When a JMS queue is used as a camel consumer for a route it may well be one of possibly many intermediate stops in a chain of processing. If the previous processing step itself used Camel to route the message, then both the JMS replyTo and the camel-header JMSReplyTo will both be populated with the same value. This will cause an infinite loop. Of course, this is in some sense a developer error, but it is a pain to constantly add code to clear the camel JMSReplyTo header if it equals the destination. This should probably be internal to the camel-jms component itself.",design_debt,non-optimal_design,"Sun, 14 Sep 2014 02:28:11 +0000","Tue, 26 Jan 2016 16:42:37 +0000","Tue, 8 Sep 2015 09:11:40 +0000",31041809,"When a JMS queue is used as a camel consumer for a route it may well be one of possibly many intermediate stops in a chain of processing. If the previous processing step itself used Camel to route the message, then both the JMS replyTo and the camel-header JMSReplyTo will both be populated with the same value. This will cause an infinite loop. Of course, this is in some sense a developer error, but it is a pain to constantly add code to clear the camel JMSReplyTo header if it equals the destination. This should probably be internal to the camel-jms component itself.",-0.0719,-0.0719,neutral
camel,7954,description,Currently camel-olingo2 uses SSLContext directly to create connections; it should use,design_debt,non-optimal_design,"Fri, 24 Oct 2014 20:18:18 +0000","Tue, 6 Aug 2019 05:14:42 +0000","Tue, 6 Aug 2019 05:14:42 +0000",150886584,Currently camel-olingo2 uses SSLContext directly to create connections; it should use SSLContextParameters.,0.0,0.0,neutral
camel,7956,description,Currently camel-olingo2 uses SSLContext directly to create connections; it should use,design_debt,non-optimal_design,"Fri, 24 Oct 2014 20:19:09 +0000","Tue, 6 Aug 2019 05:14:50 +0000","Tue, 6 Aug 2019 05:14:50 +0000",150886541,Currently camel-olingo2 uses SSLContext directly to create connections; it should use SSLContextParameters.,0.0,0.0,neutral
camel,8091,description,"The does not consider the context property that is supposed to limit the size of the logged payload. It is possible to set a maxChars on the but that has a different semantics (limits the length of the formatted exchange, not of the message payload) and is complicated to set in some cases (e.g. in the case of the default error handler) The attached extension also honors the context property when formatting the exchange.",design_debt,non-optimal_design,"Fri, 28 Nov 2014 09:20:59 +0000","Tue, 9 Dec 2014 19:26:23 +0000","Tue, 9 Dec 2014 19:26:23 +0000",986724,"The DefaultExchangeFormatter does not consider the Exchange.LOG_DEBUG_BODY_MAX_CHARS context property that is supposed to limit the size of the logged payload. It is possible to set a maxChars on the DefaultExchangeFormatter, but that has a different semantics (limits the length of the formatted exchange, not of the message payload) and is complicated to set in some cases (e.g. in the case of the default error handler) The attached extension also honors the Exchange.LOG_DEBUG_BODY_MAX_CHARS context property when formatting the exchange.",-0.1833333333,-0.09166666667,neutral
camel,8844,description,"Currently there are issues when directory in (s)ftp(s) endpoint is absolute (or contains arbitrary number of leading slashes). Sometimes the path is normalized, sometimes don't and camel-ftp code can't find correct prefix in _absolute_ path when retrieving remote file. [Documentation I suggest normalizing the behavior and treating absolute FTP endpoint URI directories as relative. When URI contains absolute path, WARNing will be printed:",design_debt,non-optimal_design,"Mon, 8 Jun 2015 09:35:14 +0000","Tue, 9 Jun 2015 10:23:10 +0000","Mon, 8 Jun 2015 10:11:38 +0000",2184,"Currently there are issues when directory in (s)ftp(s) endpoint is absolute (or contains arbitrary number of leading slashes). Sometimes the path is normalized, sometimes don't and camel-ftp code can't find correct prefix in absolute path when retrieving remote file. Documentation says: Absolute path is not supported. I suggest normalizing the behavior and treating absolute FTP endpoint URI directories as relative. When URI contains absolute path, WARNing will be printed:",-0.3270833333,-0.3416666667,negative
camel,903,description,"The camel-ftp component uses stored as last poll time. The remote file timestamp is used for comparing against last poll time if its new and thus a candidate for download. As timestamps over FTP is not reliable we should not use this feature by default, but turn it off. It should only be there for test or experimental usage. Most FTP servers only sent file timestamp as HH:mm (no seconds). And as a bonus we avoid timezone issues as well. Instead end-users should use a different strategy for ""marking files done"" such as: - delete consumed files - rename consumed files See nabble:",design_debt,non-optimal_design,"Mon, 15 Sep 2008 09:26:57 +0000","Thu, 23 Oct 2008 04:37:13 +0000","Tue, 16 Sep 2008 06:18:43 +0000",75106,"The camel-ftp component uses System.currentTimeMillis stored as last poll time. The remote file timestamp is used for comparing against last poll time if its new and thus a candidate for download. As timestamps over FTP is not reliable we should not use this feature by default, but turn it off. It should only be there for test or experimental usage. Most FTP servers only sent file timestamp as HH:mm (no seconds). And as a bonus we avoid timezone issues as well. Instead end-users should use a different strategy for ""marking files done"" such as: delete consumed files rename consumed files See nabble: http://www.nabble.com/Ftp-consumer-td19489526s22882.html",-0.02673809524,-0.02339583333,neutral
camel,9181,description,"Previous ScrHelper could break when there are certain XML libraries in the classpath (e.g. XOM or Saxon). Also, it doesn't work with component description files generated by the latest version 1.21.0 (format has changed). This is a simpler, less picky implementation using StAX.",design_debt,non-optimal_design,"Mon, 28 Sep 2015 14:32:41 +0000","Mon, 28 Sep 2015 15:09:04 +0000","Mon, 28 Sep 2015 15:09:04 +0000",2183,"Previous ScrHelper could break when there are certain XML libraries in the classpath (e.g. XOM or Saxon). Also, it doesn't work with component description files generated by the latest org.apache.felix/maven-scr-plugin version 1.21.0 (format has changed). This is a simpler, less picky implementation using StAX.",-0.3,-0.18,negative
camel,946,description,We should improve the <package Also when doing unit testing it would be great to be sure only YOUR route is loaded and not all in the same package.,design_debt,non-optimal_design,"Mon, 29 Sep 2008 12:17:19 +0000","Sat, 21 Nov 2009 11:57:54 +0000","Tue, 14 Apr 2009 09:40:38 +0000",17011399,"We should improve the <package> or to allow setting 1..N classes instead or ref to spring ids. The <package> is sometimes a bit to ""magic"" for new users. Also when doing unit testing it would be great to be sure only YOUR route is loaded and not all in the same package.",0.5,0.3,neutral
camel,9499,description,We use colon today which makes the uri ambigious and confusing. As colon is a separator for other options. So we should use dash instead temp:queue -temp:topic -,design_debt,non-optimal_design,"Mon, 11 Jan 2016 12:04:01 +0000","Tue, 19 Jan 2016 18:15:04 +0000","Tue, 19 Jan 2016 18:15:04 +0000",713463,We use colon today which makes the uri ambigious and confusing. As colon is a separator for other options. So we should use dash instead temp:queue -> temp-queue temp:topic -> temp-topic,-0.1456666667,-0.1456666667,neutral
camel,9616,description,"For the moment, a custom {{MetricRegistry}} bean can be provided but must be named. It would be easier to relax that constraint in case only one {{MetricRegistry}} bean exist and do the lookup by type only.",design_debt,non-optimal_design,"Thu, 18 Feb 2016 14:52:17 +0000","Thu, 18 Feb 2016 16:44:48 +0000","Thu, 18 Feb 2016 16:22:30 +0000",5413,"For the moment, a custom MetricRegistry bean can be provided but must be named. It would be easier to relax that constraint in case only one MetricRegistry bean exist and do the lookup by type only.",0.0,0.0,neutral
camel,9690,description,"Hello Im using camel 2.16.2 and Im finding the bean parameter binding doesnt seem to work very well on overloaded methods. See below for an example Here are the routes And here are the tests I dont understand why test2Param_string and test2Param_classB throw ambiguous call exceptions. Heres a sample stack trace. From looking at the code in BeanInfo, I *think* it just tries to match the type on the body and if it sees multiple possible methods then it throws the exception. I believe it should go further and try to match the type on the other parameters as well? To get around this issue temporarily, Ive had to write an adapter class that wraps around ClassA but its not an ideal solution.",design_debt,non-optimal_design,"Wed, 9 Mar 2016 23:33:29 +0000","Tue, 22 Mar 2016 12:02:20 +0000","Tue, 22 Mar 2016 12:02:20 +0000",1081731,"Hello Im using camel 2.16.2 and Im finding the bean parameter binding doesnt seem to work very well on overloaded methods. See below for an example Here are the routes And here are the tests I dont understand why test2Param_string and test2Param_classB throw ambiguous call exceptions. Heres a sample stack trace. From looking at the code in BeanInfo, I think it just tries to match the type on the body and if it sees multiple possible methods then it throws the exception. I believe it should go further and try to match the type on the other parameters as well? To get around this issue temporarily, Ive had to write an adapter class that wraps around ClassA but its not an ideal solution.",0.1806944444,-0.02447222222,negative
camel,11408,description,"On page the section ""Sample when using OSGi"" is missing the code snippets. Can these examples be put back in and, if possible, be verified? Thanks much!",documentation_debt,low_quality_documentation,"Wed, 14 Jun 2017 07:16:37 +0000","Thu, 1 Feb 2018 15:34:14 +0000","Thu, 1 Feb 2018 15:34:14 +0000",20074657,"On page http://camel.apache.org/servlet.html the section ""Sample when using OSGi"" is missing the code snippets. Can these examples be put back in and, if possible, be verified? Thanks much!",0.1666666667,0.1666666667,neutral
camel,11504,description,"We should be accessible and there should be no broken pages in the website, to do that we should incorporate",documentation_debt,low_quality_documentation,"Mon, 3 Jul 2017 08:41:20 +0000","Mon, 22 Apr 2019 16:21:52 +0000","Mon, 22 Apr 2019 16:21:44 +0000",56878824,"We should be accessible and there should be no broken pages in the website, to do that we should incorporate check-pages.",0.3815,0.3815,neutral
camel,1173,description,See We should be able to do this in Camel already... just need to cook up an example/doco for it.,documentation_debt,low_quality_documentation,"Tue, 9 Dec 2008 19:01:43 +0000","Mon, 23 Mar 2009 08:40:31 +0000","Mon, 2 Feb 2009 19:39:35 +0000",4754272,See http://www.eaipatterns.com/BroadcastAggregate.html We should be able to do this in Camel already... just need to cook up an example/doco for it.,0.688,0.688,positive
camel,12042,description,"The rendering of code snippets on the camel homepage is broken. For example see I expect the following should be java syntax highlighting? {{ javaEndpoint endpoint = PollingConsumer consumer = Exchange exchange = Also further down the page I get: {{The example below illustrates I'm not sure if this is the right place to report this, but since the homepage is part of the github repository it seems to make sense using the projects bug tracker. I checked the Gitter history and had a look at the ML and nobody seems to mention this. I've also tried to search on JIRA but always end up with Issues for Zookeeper or other projects.",documentation_debt,low_quality_documentation,"Mon, 27 Nov 2017 11:45:17 +0000","Mon, 27 Nov 2017 12:21:12 +0000","Mon, 27 Nov 2017 12:09:36 +0000",1459,"The rendering of code snippets on the camel homepage is broken. For example see http://camel.apache.org/polling-consumer.html I expect the following should be java syntax highlighting? {{ javaEndpoint endpoint = context.getEndpoint(""activemq:my.queue""); PollingConsumer consumer = endpoint.createPollingConsumer(); Exchange exchange = consumer.receive();}} Also further down the page I get: {{The example below illustrates this: {snippet:id=e1|lang=xml|url=camel/components/camel-spring/src/test/resources/org/apache/camel/spring/SpringConsumerTemplateTest-context.xml} }} I'm not sure if this is the right place to report this, but since the homepage is part of the github repository it seems to make sense using the projects bug tracker. I checked the Gitter history and had a look at the ML and nobody seems to mention this. I've also tried to search on JIRA but always end up with Issues for Zookeeper or other projects.",-0.03626666667,-0.04355,negative
camel,13111,description,"I noticed that we dont have in some component docs, the spring boot START END markers for the SB auto configuration docs we see WARNs when building SB starter JARs",documentation_debt,low_quality_documentation,"Wed, 23 Jan 2019 10:18:52 +0000","Fri, 1 Mar 2019 13:25:05 +0000","Thu, 7 Feb 2019 17:45:36 +0000",1322804,"I noticed that we dont have in some component docs, the spring boot START END markers for the SB auto configuration docs we see WARNs when building SB starter JARs",-0.4,-0.4,neutral
camel,1317,description,We can definitely do this in Camel already... just need an example/docs for it.,documentation_debt,outdated_documentation,"Thu, 5 Feb 2009 14:40:46 +0000","Fri, 31 Jul 2009 06:34:03 +0000","Thu, 5 Feb 2009 21:07:47 +0000",23221,We can definitely do this in Camel already... just need an example/docs for it.,0.0,0.0,positive
camel,168,description,camel-ognl has a typo in its packaging. This causes the classes to be not included in the snapshot,documentation_debt,low_quality_documentation,"Tue, 9 Oct 2007 15:24:36 +0000","Mon, 12 May 2008 07:56:32 +0000","Tue, 9 Oct 2007 15:39:02 +0000",866,camel-ognl has a typo in its packaging. This causes the classes to be not included in the snapshot,0.0,0.0,negative
camel,1846,description,"This affects at least version 1.5.0 of the docs. If I look through the xsddocs on the website (starting from the Camel Xml Reference page) for the throwFault element, it's not there even though it _is_ present in the actual xsd. I guess something went wrong with the generation of those pages and it makes it rather difficult to figure out what's valid and what isn't.",documentation_debt,low_quality_documentation,"Wed, 22 Jul 2009 20:15:26 +0000","Sun, 7 Feb 2010 09:58:27 +0000","Sun, 13 Sep 2009 18:55:42 +0000",4574416,"This affects at least version 1.5.0 of the docs. If I look through the xsddocs on the website (starting from the Camel Xml Reference page) for the throwFault element, it's not there even though it is present in the actual xsd. I guess something went wrong with the generation of those pages and it makes it rather difficult to figure out what's valid and what isn't.",-0.075,-0.075,negative
camel,231,description,"I tried to figure out how i could edit the wiki page myself - i created an account but do not have edit rights, or I could not find the edit button. Type converts has a broken link: In the chapter ""Discovering Type Converts"" the link for @Converter is missing the h in http so the link is broken.",documentation_debt,low_quality_documentation,"Sun, 18 Nov 2007 14:58:28 +0000","Mon, 12 May 2008 07:48:06 +0000","Mon, 19 Nov 2007 11:42:46 +0000",74658,"I tried to figure out how i could edit the wiki page myself - i created an account but do not have edit rights, or I could not find the edit button. Type converts has a broken link: http://cwiki.apache.org/confluence/display/CAMEL/Type+Converter In the chapter ""Discovering Type Converts"" the link for @Converter is missing the h in http so the link is broken. ""ttp://activemq.apache.org/camel/maven/camel-core/apidocs/org/apache/camel/Converter""",-0.1333333333,-0.08888888889,negative
camel,3157,description,The Exchange java doc should be updated to help end users more about the API of the Exchange,documentation_debt,low_quality_documentation,"Sat, 25 Sep 2010 10:38:18 +0000","Sun, 24 Apr 2011 09:58:17 +0000","Mon, 27 Sep 2010 14:16:36 +0000",185898,The Exchange java doc should be updated to help end users more about the API of the Exchange,0.4,0.4,neutral
camel,3168,description,We need a REST based example to show how to do that. It should accept XML/JSON as input.,documentation_debt,low_quality_documentation,"Tue, 28 Sep 2010 04:25:29 +0000","Wed, 25 Mar 2015 08:27:23 +0000","Wed, 25 Mar 2015 08:27:23 +0000",141624114,We need a REST based example to show how to do that. It should accept XML/JSON as input.,0.1,0.1,neutral
camel,3440,description,"After the jira migration from activemq to the main ASF jira, the project id changed from 11020 to 12311211. As a result all links to Release Notes are broken and need to be updated.",documentation_debt,outdated_documentation,"Sat, 18 Dec 2010 03:51:01 +0000","Fri, 28 Jan 2011 00:46:44 +0000","Fri, 28 Jan 2011 00:46:36 +0000",3531335,"After the jira migration from activemq to the main ASF jira, the project id changed from 11020 to 12311211. As a result all links to Release Notes are broken and need to be updated.",-0.1,-0.1,negative
camel,3469,description,"Upgrading to newer versions of the dependencies is indeed normally a simple procedure. If we add a wiki page with the procedure (which pom should be updated and checked, do we need an OSGI bundle from the ServiceMix guys, run the full test suite, ...), it could be one of the ""low hanging fruits"" for new to work on Camel.",documentation_debt,low_quality_documentation,"Tue, 28 Dec 2010 18:19:17 +0000","Sun, 24 Apr 2011 09:58:05 +0000","Tue, 28 Dec 2010 23:11:56 +0000",17559,"Upgrading to newer versions of the dependencies is indeed normally a simple procedure. If we add a wiki page with the procedure (which pom should be updated and checked, do we need an OSGI bundle from the ServiceMix guys, run the full test suite, ...), it could be one of the ""low hanging fruits"" for new contributors/committers to work on Camel.",0.3,0.3,neutral
camel,3637,description,"The Camel Karaf feature camel-eventAdmin is not correct. The features is described as follow: <feature version=""2.6.0"" <feature version=""2.6.0"" <bundle</feature but the camel-eventAdmin artifact correct name is camel-eventadmin: This typo mistake provides: Downloading: [INFO] Unable to find resource in repository central [INFO] [ERROR] BUILD FAILURE [INFO] [INFO] Can't resolve bundle [INFO] I'm gonna submit a patch to fix that.",documentation_debt,low_quality_documentation,"Mon, 7 Feb 2011 07:47:05 +0000","Tue, 25 Oct 2011 11:35:31 +0000","Mon, 7 Feb 2011 09:41:20 +0000",6855,"The Camel Karaf feature camel-eventAdmin is not correct. The features is described as follow: <feature name=""camel-eventAdmin"" version=""2.6.0""> <feature version=""2.6.0"">camel-core</feature> <bundle>mvn:org.apache.camel/camel-eventAdmin/2.6.0</bundle> </feature> but the camel-eventAdmin artifact correct name is camel-eventadmin: http://repo2.maven.org/maven2/org/apache/camel/camel-eventadmin/2.6.0/camel-eventadmin-2.6.0.jar This typo mistake provides: Downloading: http://repo1.maven.org/maven2/org/apache/camel/camel-eventAdmin/2.6.0/camel-eventAdmin-2.6.0.jar [INFO] Unable to find resource 'org.apache.camel:camel-eventAdmin:jar:2.6.0' in repository central (http://repo1.maven.org/maven2) [INFO] ------------------------------------------------------------------------ [ERROR] BUILD FAILURE [INFO] ------------------------------------------------------------------------ [INFO] Can't resolve bundle org.apache.camel:camel-eventAdmin:jar:2.6.0 [INFO] ------------------------------------------------------------------------ I'm gonna submit a patch to fix that.",-0.53125,-0.15625,negative
camel,3864,description,The file ehcache.xml which is the default configuration file for ehcache initialized by camel-cache component contains deprecated description about jms replication.,documentation_debt,outdated_documentation,"Thu, 14 Apr 2011 17:11:50 +0000","Sat, 16 Apr 2011 12:19:08 +0000","Sat, 16 Apr 2011 12:19:08 +0000",155238,The file ehcache.xml which is the default configuration file for ehcache initialized by camel-cache component contains deprecated description about jms replication.,-0.25,-0.25,neutral
camel,3888,description,We should ensure all deprecated classes/methods in camel-core is documented what alternatives to use. Also if possible give a hint when it could be removed.,documentation_debt,low_quality_documentation,"Thu, 21 Apr 2011 12:12:23 +0000","Mon, 27 Jun 2011 08:05:27 +0000","Mon, 27 Jun 2011 08:05:27 +0000",5773984,We should ensure all deprecated classes/methods in camel-core is documented what alternatives to use. Also if possible give a hint when it could be removed.,0.1,0.1,neutral
camel,493,description,The camel mail component registers nntp as a supported protocol but there is no javacode or documentation how to use it. It is currently not support out-of-the-box in Camel. This feature has been requested by an end-user by private mail correspondence.,documentation_debt,outdated_documentation,"Sun, 4 May 2008 07:31:50 +0000","Sun, 7 Feb 2010 09:54:14 +0000","Thu, 12 Nov 2009 19:56:47 +0000",48169497,The camel mail component registers nntp as a supported protocol but there is no javacode or documentation how to use it. It is currently not support out-of-the-box in Camel. This feature has been requested by an end-user by private mail correspondence.,0.0,0.0,negative
camel,52,description,"sometimes we get some strange stuff inserted into the generated docbook. e.g. the first part of contains some bogus stuff... Everything between <section> and <para> looks bogus (or at least generates strangeness in the PDF). I wonder if might help if we keep the HTML that is downloaded from the site so we can see, is this a wiki issue or XSL issue etc. As right now I've no idea! :)",documentation_debt,low_quality_documentation,"Mon, 25 Jun 2007 10:46:04 +0000","Mon, 12 May 2008 08:01:38 +0000","Tue, 26 Jun 2007 04:11:56 +0000",62752,"sometimes we get some strange stuff inserted into the generated docbook. e.g. the first part of book-architecture.xml contains some bogus stuff... Everything between <section> and <para> looks bogus (or at least generates strangeness in the PDF). I wonder if might help if we keep the HTML that is downloaded from the site so we can see, is this a wiki issue or XSL issue etc. As right now I've no idea!",0.0687,-0.0113,negative
camel,6620,description,"I got an exception saying that an authMethod value is required. So, I went to the docs and there is no mention of authMethod or the acceptable values. I had to search the code find the AuthMethod enum to know what value is acceptable and to find the authMethod parameter name.",documentation_debt,outdated_documentation,"Thu, 8 Aug 2013 18:52:34 +0000","Fri, 9 Aug 2013 01:03:08 +0000","Fri, 9 Aug 2013 01:03:08 +0000",22234,"I got an exception saying that an authMethod value is required. So, I went to the docs and there is no mention of authMethod or the acceptable values. I had to search the code find the AuthMethod enum to know what value is acceptable and to find the authMethod parameter name.",0.0,0.0,neutral
camel,6896,description,"I've been writing a URI Builder for NetBeans using the Property Sheet api. Everything works great with the API that was added in 2.12, but it would be really nice to be able to grab parameter documentation through this api as well. For instance, with the property sheet, I can set a Short Description of what each endpoint does, and the user would be able to go from there.",documentation_debt,outdated_documentation,"Thu, 24 Oct 2013 21:01:37 +0000","Sat, 29 Nov 2014 09:21:35 +0000","Sat, 29 Nov 2014 09:21:35 +0000",34604398,"I've been writing a URI Builder for NetBeans using the Property Sheet api. Everything works great with the ComponentConfiguration API that was added in 2.12, but it would be really nice to be able to grab parameter documentation through this api as well. For instance, with the property sheet, I can set a Short Description of what each endpoint does, and the user would be able to go from there.",0.4209333333,0.4209333333,positive
camel,7412,description,The docs at state This statement however seems to be wrong. I have a demo at that uses the camel-jdbc component in an XA transaction scenario without errors. Can someone please confirm the docs is wrong and I can correct it in the docs? This statement was introduced in,documentation_debt,low_quality_documentation,"Mon, 5 May 2014 14:17:35 +0000","Tue, 6 May 2014 15:12:34 +0000","Tue, 6 May 2014 15:12:34 +0000",89699,"The docs at http://camel.apache.org/jdbc.html state This component can not be used as a Transactional Client. If you need transaction support in your route, you should use the SQL component instead. This statement however seems to be wrong. I have a demo at https://github.com/tmielke/fuse-demos/tree/master/Camel/Camel-JMS-JDBC-XA-TX that uses the camel-jdbc component in an XA transaction scenario without errors. Can someone please confirm the docs is wrong and I can correct it in the docs? This statement was introduced in https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27822683",0.10625,0.1375,negative
camel,7690,description,We have a WAR example for Apache Tomcat etc. We should have a osgi example as well.,documentation_debt,outdated_documentation,"Wed, 13 Aug 2014 07:51:39 +0000","Wed, 13 Aug 2014 09:57:47 +0000","Wed, 13 Aug 2014 09:57:47 +0000",7568,We have a WAR example for Apache Tomcat etc. We should have a osgi example as well.,0.1155,0.1155,neutral
camel,808,description,Just a ticket for a reminder for myself to improve the wiki documentation a bit for these two components.,documentation_debt,low_quality_documentation,"Mon, 11 Aug 2008 08:38:51 +0000","Thu, 23 Oct 2008 04:37:12 +0000","Tue, 12 Aug 2008 12:55:43 +0000",101812,Just a ticket for a reminder for myself to improve the wiki documentation a bit for these two components.,0.4,0.4,neutral
camel,8101,description,Add runCommand to MongoDB Camel component operations list Javadoc of MongoDB driver is there I should update wiki right after the PR is merge.,documentation_debt,outdated_documentation,"Mon, 1 Dec 2014 20:11:20 +0000","Mon, 9 Feb 2015 05:33:20 +0000","Mon, 9 Feb 2015 05:33:20 +0000",5995320,Add runCommand to MongoDB Camel component operations list Javadoc of MongoDB driver is there https://api.mongodb.org/java/2.12/com/mongodb/DB.html#command(com.mongodb.DBObject) I should update wiki right after the PR is merge.,0.527,0.527,neutral
camel,8370,description,"I sent the following to the list earlier today: After digging around in the code, it looks like the HTTP status code is set via the message header line 308). However, there is no mention of this message header in either the camel-netty-http or camel-netty4-http documentation. It would be helpful to add this header to the list of applicable message headers, and also to include an example that demonstrates how to set the response status code and body: Finally, the existing ""Access to Netty types"" example should be modified to be clear that only the request can be accessed in this way.",documentation_debt,low_quality_documentation,"Tue, 17 Feb 2015 22:40:54 +0000","Wed, 18 Feb 2015 06:43:23 +0000","Wed, 18 Feb 2015 06:43:23 +0000",28949,"I sent the following to the users@camel.apache.org list earlier today: Im using the camel-netty4-http (latest 2.14.2-SNAPSHOT) component to create an endpoint that receives an HTTP POST from a device, translates the message from binary to JSON, then sends it along to a kafka topic for further processing. When there are errors in validating or translating the incoming message, I need to be able to return a HTTP response code and response body. The camel-netty4-http documentation has an Access to Netty types section, which says that I should be able to do the following to access the instance of io.netty.handler.codec.http.HttpResponse: HttpResponse response = exchange.getIn(NettyHttpMessage.class).getHttpResponse(); Regardless of where I access the exchange within the route, getHttpResponse() is always returning null. On the ""happy path I can return 200 by calling exchange.getOut().setBody(myResponse), but I have been unable to figure out how to return another response code. Is this a bug? Or is there another way to accomplish what Im trying to do? After digging around in the code, it looks like the HTTP status code is set via the CamelHttpResponseCode message header (org.apache.camel.component.netty4.http.DefaultNettyHttpBinding, line 308). However, there is no mention of this message header in either the camel-netty-http or camel-netty4-http documentation. It would be helpful to add this header to the list of applicable message headers, and also to include an example that demonstrates how to set the response status code and body: Finally, the existing ""Access to Netty types"" example should be modified to be clear that only the request can be accessed in this way.",0.1875,0.11086,neutral
camel,910,description,"First of all I think we need to promote this great example some more. Maybe it should be easier to find on our wiki site. I will post findings in this ticket: #1 Link to EIP book loan broker sample doesnt work #2 I think the 2 parts in the introduction should be listed as bullets (one for JMS, one for webservice) #3 spelling (comman in the sentence below) credit agency , and banks) #4 Maybe the exchange pattern InOnly, InOut is not easily understood by new users and non JBI/ServiceMix end-users. Maybe use terms such as sync/async instead (and list the MEP in parathes) #5 Could ""test-jms"" component name be renamed to jms or activemq or something with non test? #6 multicast().to() ah clever if its really using a multicast ;) I didn't know that we have the .to on multicase. Are you sure its working as expected? and it should not be multicase(""bank1"", ""bank2"", ...) without the to? #7 Use the getHeader with the expected type as 2nd param, to avoid plain java type cast String ssn = #8 The aggregator. I am wondering if we have or should have a counter build in Camel so you can use a build in header instead of ""remebering"" to code this yourself old + 1); will continue...",documentation_debt,low_quality_documentation,"Tue, 16 Sep 2008 17:47:21 +0000","Thu, 23 Oct 2008 04:39:20 +0000","Mon, 22 Sep 2008 11:55:24 +0000",497283,"First of all I think we need to promote this great example some more. Maybe it should be easier to find on our wiki site. I will post findings in this ticket: #1 Link to EIP book loan broker sample doesnt work #2 I think the 2 parts in the introduction should be listed as bullets (one for JMS, one for webservice) #3 spelling (comman in the sentence below) credit agency , and banks) #4 Maybe the exchange pattern InOnly, InOut is not easily understood by new users and non JBI/ServiceMix end-users. Maybe use terms such as sync/async instead (and list the MEP in parathes) #5 Could ""test-jms"" component name be renamed to jms or activemq or something with non test? #6 multicast().to() ah clever if its really using a multicast I didn't know that we have the .to on multicase. Are you sure its working as expected? and it should not be multicase(""bank1"", ""bank2"", ...) without the to? #7 Use the getHeader with the expected type as 2nd param, to avoid plain java type cast String ssn = (String)exchange.getIn().getHeader(Constants.PROPERTY_SSN); #8 The aggregator. I am wondering if we have or should have a counter build in Camel so you can use a build in header instead of ""remebering"" to code this yourself result.setProperty(""aggregated"", old + 1); will continue...",0.1371363636,0.1094666667,neutral
camel,9412,description,fix some typo correct url,documentation_debt,low_quality_documentation,"Thu, 10 Dec 2015 18:17:34 +0000","Mon, 14 Dec 2015 16:55:22 +0000","Fri, 11 Dec 2015 08:00:13 +0000",49359,fix some typo correct url,0.875,0.875,negative
camel,948,description,"e.g. we should include the tutorials, maybe some of the cook book and am sure there's other useful documentation to include - like the recent changes to XPath / XQuery and other languages. Maybe some reformatting is needed too - missing images and so forth. The various sections of the book are here in the wiki here is the entire book...",documentation_debt,low_quality_documentation,"Tue, 30 Sep 2008 06:16:24 +0000","Mon, 16 Feb 2009 05:51:54 +0000","Mon, 20 Oct 2008 13:47:22 +0000",1755058,"e.g. we should include the tutorials, maybe some of the cook book and am sure there's other useful documentation to include - like the recent changes to XPath / XQuery and other languages. Maybe some reformatting is needed too - missing images and so forth. The various sections of the book are here in the wiki http://activemq.apache.org/camel/book.html here is the entire book... http://activemq.apache.org/camel/book-in-one-page.html",0.03333333333,0.03333333333,neutral
camel,9594,description,We should use swagger.json which is the convention name used by swagger api-docs was the old for 1.x spec.,documentation_debt,low_quality_documentation,"Fri, 12 Feb 2016 09:52:20 +0000","Mon, 21 Mar 2016 13:48:34 +0000","Mon, 21 Mar 2016 13:48:34 +0000",3297374,We should use swagger.json which is the convention name used by swagger http://swagger.io/specification/ api-docs was the old for 1.x spec.,0.0,0.0,neutral
camel,3349,description,"The CxfRsEndpoint's getBinding method is not thread safe. At a customer site, I ran into an issue at startup if 2 threads raced to perform sync and async invocation, the code for getBinding (given below) would react in the following way. - Thread 1 would proceed to create a binding object - Thread 2 would mean while still find the binding to be null and proceed to create a new binding - Meanwhile thread one would have its binding and set the Atomic boolean for binding initialized and proceed to set the HeaderStrategy. - Thread 2 meanwhile would overwrite the original binding object and find that Atomic boolean already set and would have no way to associate a object since the flag is up. - In the absence of a copying of ProtocolHeaders etc will throw exceptions on every following request/invocation. public CxfRsBinding getBinding() { if (binding == null) { binding = new if { LOG.debug(""Create default CXF Binding "" + binding); } } if && binding instanceof { } return binding; }",requirement_debt,non-functional_requirements_not_fully_satisfied,"Fri, 19 Nov 2010 22:32:35 +0000","Sun, 24 Apr 2011 09:57:38 +0000","Fri, 19 Nov 2010 22:57:34 +0000",1499,"The CxfRsEndpoint's getBinding method is not thread safe. At a customer site, I ran into an issue at startup if 2 threads raced to perform sync and async invocation, the code for getBinding (given below) would react in the following way. Thread 1 would proceed to create a binding object Thread 2 would mean while still find the binding to be null and proceed to create a new binding Meanwhile thread one would have its binding and set the Atomic boolean for binding initialized and proceed to set the HeaderStrategy. Thread 2 meanwhile would overwrite the original binding object and find that Atomic boolean already set and would have no way to associate a HeaderFilterStrategy object since the flag is up. In the absence of a HeaderFilterStrategy, copying of ProtocolHeaders etc will throw exceptions on every following request/invocation. -------------------------------------------------- public CxfRsBinding getBinding() { if (binding == null) { binding = new DefaultCxfRsBinding(); if (LOG.isDebugEnabled()) { LOG.debug(""Create default CXF Binding "" + binding); } } if (!bindingInitialized.getAndSet(true) && binding instanceof HeaderFilterStrategyAware) { ((HeaderFilterStrategyAware)binding).setHeaderFilterStrategy(getHeaderFilterStrategy()); } return binding; } ------------------------------------------------",-0.17655,0.01659090909,neutral
camel,11171,description,"component has an issue with the usage of {{RAW()}} function in child endpoint configuration. will mishandle the the content of {{RAW()}} , when at some point the escaped ampersand symbol is unescaped, and a wrong set of parameters is used. The attached PR fixed the issues and adds a unit test to verify the behavior before and after the fix.",test_debt,low_coverage,"Wed, 19 Apr 2017 15:55:11 +0000","Thu, 20 Apr 2017 08:02:09 +0000","Thu, 20 Apr 2017 07:49:47 +0000",57276,"camel-zookeeper-master component has an issue with the usage of RAW() function in child endpoint configuration. zookeeper-master://name:sftp://myhost/inbox?password=RAW(BEFORE_AMPERSAND&AFTER_AMPERSAND)&username=jdoe will mishandle the the content of RAW() , when at some point the escaped ampersand symbol is unescaped, and a wrong set of parameters is used. The attached PR fixed the issues and adds a unit test to verify the behavior before and after the fix.",0.04166666667,0.13125,negative
camel,1842,description,"camel-cxf and camel-mail are the popular components in camel, so we need add some OSGi integration test for them.",test_debt,lack_of_tests,"Mon, 20 Jul 2009 12:45:31 +0000","Sun, 7 Feb 2010 09:56:20 +0000","Wed, 29 Jul 2009 09:52:45 +0000",767234,"camel-cxf and camel-mail are the popular components in camel, so we need add some OSGi integration test for them.",0.6,0.6,neutral
camel,2446,description,"I have attached patches of various changes that I have made to context files. The changes fall into following categories: * using p: for properties rather than full xml * using u:list for lists rather than full xml * using in order to reduce amount of xml required * updating files to use camel:camelContext schema rather than <camelContext Please let me know if I need to tweek any of the changes. I have smoke tested the examples, but is there a general unit test that will validate that the context file?",test_debt,lack_of_tests,"Thu, 4 Feb 2010 02:20:01 +0000","Sun, 24 Apr 2011 09:57:31 +0000","Thu, 26 Aug 2010 06:56:20 +0000",17555779,"I have attached patches of various changes that I have made to context files. The changes fall into following categories: using p: for properties rather than full xml using u:list for lists rather than full xml using default-autowire=""byName"" in order to reduce amount of xml required updating files to use camel:camelContext schema rather than <camelContext xmlns=""http://camel.apache.org/schema/spring""> Please let me know if I need to tweek any of the changes. I have smoke tested the examples, but is there a general unit test that will validate that the context file?",0.2438333333,0.1493888889,neutral
camel,3709,description,"When using together with from(Endpoint), the below Exception occurs during the routes building process. Looking at reveals, that the FromDefintion just created has no URI. That causes the comparison to all the interceptFroms' URIs to fail. As far as I can tell, the way to fix this would be to add in the constructor endpoint)}}. Below the stack trace, there is a unit test that demonstrates the issue. Until it if fixed, it can be easily circumvented by adding the commented-out line, and then change to",test_debt,low_coverage,"Wed, 23 Feb 2011 12:12:35 +0000","Tue, 25 Oct 2011 11:35:47 +0000","Thu, 24 Feb 2011 05:21:03 +0000",61708,"When using interceptFrom(String) together with from(Endpoint), the below Exception occurs during the routes building process. Looking at RoutesDefinition.java:217 reveals, that the FromDefintion just created has no URI. That causes the comparison to all the interceptFroms' URIs to fail. As far as I can tell, the way to fix this would be to add setUri(myEndpoint.getEndpointUri()) in the constructor FromDefinition(Endpoint endpoint). Below the stack trace, there is a unit test that demonstrates the issue. Until it if fixed, it can be easily circumvented by adding the commented-out line, and then change to from(""myEndpoint"").",-0.06666666667,-0.05,negative
camel,4602,description,"See nabble By invoking the filter for directories, it allows end users to skip entire directories, which would make the polling faster. And no need to walk down unwanted directories. The logic need to apply for both file/ftp consumers, as well having unit tests to ensure it works.",test_debt,lack_of_tests,"Tue, 1 Nov 2011 10:48:33 +0000","Sun, 4 Mar 2012 12:30:28 +0000","Sun, 4 Mar 2012 10:44:52 +0000",10713379,"See nabble http://camel.465427.n5.nabble.com/Copying-files-from-an-FTP-server-using-search-criteria-tp4882889p4882889.html By invoking the filter for directories, it allows end users to skip entire directories, which would make the polling faster. And no need to walk down unwanted directories. The logic need to apply for both file/ftp consumers, as well having unit tests to ensure it works.",0.2743333333,0.2743333333,neutral
camel,4736,description,"There's been a mail in the mailing list about camel-xstream not being blueprint friendly. Even though in the current trunk, it does work its a nice chance to add an integration test for it.",test_debt,lack_of_tests,"Fri, 2 Dec 2011 16:41:57 +0000","Fri, 2 Dec 2011 17:19:00 +0000","Fri, 2 Dec 2011 17:19:00 +0000",2223,"There's been a mail in the mailing list about camel-xstream not being blueprint friendly. Even though in the current trunk, it does work its a nice chance to add an integration test for it.",0.1321666667,0.1321666667,neutral
camel,5184,description,"When shutting down seda endpoints they take a lille while to shutdown properly. However during testing we dont need to do that, so we could shortcut this and shutdown faster. For example the camel-test kit could tweak that. As well in camel-core. I suspect we can cut down minutes of testing times.",test_debt,expensive_tests,"Tue, 17 Apr 2012 05:55:01 +0000","Sun, 22 Apr 2012 08:46:17 +0000","Wed, 18 Apr 2012 09:59:33 +0000",101072,"When shutting down seda endpoints they take a lille while to shutdown properly. However during testing we dont need to do that, so we could shortcut this and shutdown faster. For example the camel-test kit could tweak that. As well in camel-core. I suspect we can cut down minutes of testing times.",0.02166666667,0.02166666667,neutral
camel,5983,description,"We've got bunch of (negative) tests on the current codebase expecting a thrown {{XYZException}} however they don't realize if the expected exception is *not* thrown, the typical pattern for this is: Which correctly should be:",test_debt,expensive_tests,"Sun, 20 Jan 2013 10:09:23 +0000","Mon, 21 Jan 2013 20:51:45 +0000","Mon, 21 Jan 2013 20:51:45 +0000",124942,"We've got bunch of (negative) tests on the current codebase expecting a thrown XYZException however they don't realize if the expected exception is not thrown, the typical pattern for this is: Which correctly should be:",0.05,0.05,negative
camel,6296,description,"parameters are not supported for HttpClient3 on Camel-Http component at least for version of camel 2.9.4. Here is a patch that provides support for parameters support. There is no test case provided, but a patch.",test_debt,lack_of_tests,"Thu, 18 Apr 2013 09:13:04 +0000","Tue, 17 Sep 2013 07:28:58 +0000","Wed, 22 May 2013 03:18:25 +0000",2916321,"httpConnectionManager.* parameters are not supported for HttpClient3 on Camel-Http component at least for version of camel 2.9.4. Here is a patch that provides support for httpConnectionManager.* parameters support. There is no test case provided, but a patch.",0.0,0.08,negative
camel,6563,description,"When using a route to listen to UDP multicast address , no messages seem to get consumed. No exceptions are observed. Multicast address is defined as addresses in the range of 224.0.0.0 through 239.255.255.255 Input was simple string (e.g. ""Test String"") Example Route: <route <from Found an old topic in the user discussion forum that seems related. Did not find any unit tests in the Camel source code exercising this behavior.",test_debt,lack_of_tests,"Thu, 18 Jul 2013 18:29:49 +0000","Wed, 28 May 2014 20:17:12 +0000","Mon, 22 Jul 2013 09:13:39 +0000",312230,"When using a route to listen to UDP multicast address , no messages seem to get consumed. No exceptions are observed. Multicast address is defined as addresses in the range of 224.0.0.0 through 239.255.255.255 (http://en.wikipedia.org/wiki/Multicast_address) Input was simple string (e.g. ""Test String"") Example Route: <route> <from uri=""netty:udp://225.1.1.1:8001?allowDefaultCodec=false&sync=false&broadcast=true""/> </route> Found an old topic in the user discussion forum that seems related. Did not find any unit tests in the Camel source code exercising this behavior. (http://camel.465427.n5.nabble.com/camel-netty-and-multicast-tt4638622.html)",0.125,0.1038333333,neutral
camel,6826,description,"The unit tests for the camel-hazelcast component use real HazelcastInstance objects, which is very slow. We should use mock objects instead to speed up testing. Testing the integration can be done with a few, select tests, but the majority of the logic can be tested with mocks.",test_debt,expensive_tests,"Fri, 4 Oct 2013 13:51:25 +0000","Sat, 11 Jul 2015 18:27:31 +0000","Sat, 11 Jul 2015 18:27:21 +0000",55744556,"The unit tests for the camel-hazelcast component use real HazelcastInstance objects, which is very slow. We should use mock objects instead to speed up testing. Testing the integration can be done with a few, select tests, but the majority of the logic can be tested with mocks.",-0.03877777778,-0.03877777778,negative
hadoop,12923,description,Some code is used only by tests. Let's relocate them.,architecture_debt,violation_of_modularity,"Mon, 14 Mar 2016 18:43:25 +0000","Tue, 30 Aug 2016 01:17:15 +0000","Mon, 14 Mar 2016 22:58:15 +0000",15290,Some code is used only by tests. Let's relocate them.,0.0,0.0,neutral
hadoop,13386,description,Avro 1.8.x makes generated classes serializable which makes them much easier to use with Spark. It would be great to upgrade Avro to 1.8.x,architecture_debt,using_obsolete_technology,"Mon, 18 Jul 2016 23:23:06 +0000","Sat, 27 Jan 2024 06:37:07 +0000","Sat, 26 Mar 2022 11:32:04 +0000",179410138,Avro 1.8.x makes generated classes serializable which makes them much easier to use with Spark. It would be great to upgrade Avro to 1.8.x Fix CVE-2021-43045,0.190625,0.190625,positive
hadoop,14692,description,We should upgrade Apache RAT to something modern.,architecture_debt,using_obsolete_technology,"Thu, 27 Jul 2017 17:35:36 +0000","Thu, 27 Jul 2017 20:34:13 +0000","Thu, 27 Jul 2017 20:06:42 +0000",9066,We should upgrade Apache RAT to something modern.,0.525,0.525,neutral
hadoop,2850,description,"Currently, the HOD allocation operation does the following distinct steps - allocate a requested number of nodes, [optionally] transfer a hadoop tarball and install it, then provision hadoop by bringing up the appropriate daemons. It would be nice to separate these layers so each of them could be done independently. This would lead to a very great flexibility in users using the cluster. For e.g. one could allocate a certain number of nodes, then have hod bring up one version, then bring it down, then repeat this again and so on.",architecture_debt,violation_of_modularity,"Sun, 17 Feb 2008 10:31:56 +0000","Wed, 18 May 2011 22:03:36 +0000","Wed, 18 May 2011 22:03:36 +0000",102511900,"Currently, the HOD allocation operation does the following distinct steps - allocate a requested number of nodes, [optionally] transfer a hadoop tarball and install it, then provision hadoop by bringing up the appropriate daemons. It would be nice to separate these layers so each of them could be done independently. This would lead to a very great flexibility in users using the cluster. For e.g. one could allocate a certain number of nodes, then have hod bring up one version, then bring it down, then repeat this again and so on.",0.53225,0.53225,neutral
hadoop,6297,description,"The zlib library supports the ability to perform two types of flushes when deflating data. It can perform both a Z_SYNC_FLUSH, which forces all input to be written as output and byte-aligned and resets the Huffman coding, and it also supports a Z_FULL_FLUSH, which does the same thing but additionally resets the compression dictionary. The Hadoop wrapper for the zlib library does not support either of these two methods. Adding support should be fairly trivial. An additional deflate method that takes a fourth ""flush"" parameter, and a modification to the native c code to accept this fourth parameter and pass it along to the zlib library. I can submit a patch for this if desired. It should be noted that the native SUN Java API is likewise missing this functionality, as has been noted for over a decade here:",architecture_debt,using_obsolete_technology,"Tue, 6 Oct 2009 15:01:24 +0000","Mon, 4 Aug 2014 21:17:39 +0000","Mon, 4 Aug 2014 21:17:39 +0000",152345775,"The zlib library supports the ability to perform two types of flushes when deflating data. It can perform both a Z_SYNC_FLUSH, which forces all input to be written as output and byte-aligned and resets the Huffman coding, and it also supports a Z_FULL_FLUSH, which does the same thing but additionally resets the compression dictionary. The Hadoop wrapper for the zlib library does not support either of these two methods. Adding support should be fairly trivial. An additional deflate method that takes a fourth ""flush"" parameter, and a modification to the native c code to accept this fourth parameter and pass it along to the zlib library. I can submit a patch for this if desired. It should be noted that the native SUN Java API is likewise missing this functionality, as has been noted for over a decade here: http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4206909",0.2714285714,0.2714285714,neutral
hadoop,6374,description,The recent change to mapred-queues.xml that causes many mapreduce tests to break unless you delete out of your build tree is bad. We need to make sure that nothing in conf is used in the unit tests. One potential solution is to copy the templates into build/test/conf and use that instead.,architecture_debt,violation_of_modularity,"Sat, 14 Nov 2009 00:29:35 +0000","Tue, 24 Aug 2010 20:40:53 +0000","Thu, 21 Jan 2010 05:58:46 +0000",5894951,The recent change to mapred-queues.xml that causes many mapreduce tests to break unless you delete conf/mapred-queues.xml out of your build tree is bad. We need to make sure that nothing in conf is used in the unit tests. One potential solution is to copy the templates into build/test/conf and use that instead.,-0.1,-0.08,negative
hadoop,8288,description,"Courtesy Philip Su, we found that were not being used at all. The configuration exists but is never used. Its also mentioned in mapred-default.xml and . Also the method in Shell.java is now useless and can be removed.",architecture_debt,using_obsolete_technology,"Tue, 17 Apr 2012 17:35:19 +0000","Fri, 7 Sep 2012 21:01:45 +0000","Thu, 19 Apr 2012 16:15:01 +0000",167982,"Courtesy Philip Su, we found that (mapred.child.ulimit, mapreduce.map.ulimit, mapreduce.reduce.ulimit) were not being used at all. The configuration exists but is never used. Its also mentioned in mapred-default.xml and templates/../mapred-site.xml . Also the method getUlimitMemoryCommand in Shell.java is now useless and can be removed.",-0.08333333333,-0.03846153846,negative
hadoop,10067,description,Compiling for Fedora revels a missing declaration for This is the result of a missing explicit dependency on jsr305.,build_debt,under-declared_dependencies,"Thu, 24 Oct 2013 18:07:53 +0000","Mon, 24 Feb 2014 20:58:13 +0000","Thu, 14 Nov 2013 09:51:17 +0000",1784604,Compiling for Fedora revels a missing declaration for javax.annotation.Nullable. This is the result of a missing explicit dependency on jsr305.,-0.1,-0.0875,negative
hadoop,14634,description,"A long time ago, HADOOP-9342 removed jline from being included in the Hadoop distribution. Since then, more modules have added Zookeeper, and are pulling in jline again. Recommend excluding jline from the main Hadoop pom in order to prevent subsequent additions of Zookeeper dependencies from doing this again.",build_debt,over-declared_dependencies,"Fri, 7 Jul 2017 22:37:41 +0000","Mon, 10 Jul 2017 17:04:34 +0000","Sat, 8 Jul 2017 11:20:57 +0000",45796,"A long time ago, HADOOP-9342 removed jline from being included in the Hadoop distribution. Since then, more modules have added Zookeeper, and are pulling in jline again. Recommend excluding jline from the main Hadoop pom in order to prevent subsequent additions of Zookeeper dependencies from doing this again.",0.07777777778,0.07777777778,neutral
hadoop,16332,description,HADOOP-16085 added a dependency on apache httpcore just to get a constant of an http error code. This is a needless dependency which can only cause problems downstream. replace the external constant with an internal one and remove the new dependency.,build_debt,over-declared_dependencies,"Tue, 28 May 2019 10:16:12 +0000","Thu, 22 Aug 2019 04:06:33 +0000","Tue, 28 May 2019 21:53:34 +0000",41842,HADOOP-16085 added a dependency on apache httpcore just to get a constant of an http error code. This is a needless dependency which can only cause problems downstream. replace the external constant with an internal one and remove the new dependency.,-0.2833333333,-0.2833333333,negative
hadoop,5775,description,"as war target read user-certs.xml and from If a user set this environment and have some files in it, it could potentially cause the unit test to fail if the conf files does not match what the unit test needs.",build_debt,build_others,"Tue, 5 May 2009 23:25:38 +0000","Tue, 24 Aug 2010 20:37:27 +0000","Fri, 26 Jun 2009 23:06:12 +0000",4491634,"as war target read user-certs.xml and user-permissions.xml from $HDFSPROXY_CONF_DIR. If a user set this environment and have some files in it, it could potentially cause the unit test to fail if the conf files does not match what the unit test needs.",-0.4,-0.2,neutral
hadoop,6538,description,"The default value of is ""kerberos"". It makes sense for that to be ""simple"" since not all users have kerberos infrastructure set up, and it is inconvenient to have it set it to ""simple"" manually.",build_debt,build_others,"Thu, 4 Feb 2010 00:53:07 +0000","Tue, 24 Aug 2010 20:41:47 +0000","Thu, 4 Feb 2010 07:46:34 +0000",24807,"The default value of ""hadoop.security.authentication"" is ""kerberos"". It makes sense for that to be ""simple"" since not all users have kerberos infrastructure set up, and it is inconvenient to have it set it to ""simple"" manually.",-0.45,-0.225,neutral
hadoop,6879,description,com.jcraft  jsch 0.1.42 version needs to be included in the build. This is needed to facilitate implementation of some system (Herriot) testcases . Please include this in ivy. jsch is originally located in,build_debt,under-declared_dependencies,"Sat, 24 Jul 2010 07:27:29 +0000","Mon, 12 Dec 2011 06:20:05 +0000","Mon, 4 Oct 2010 05:10:42 +0000",6212593,http://mvnrepository.com/ com.jcraft  jsch 0.1.42 version needs to be included in the build. This is needed to facilitate implementation of some system (Herriot) testcases . Please include this in ivy. jsch is originally located in http://www.jcraft.com/jsch/,0.04,0.04,neutral
hadoop,7161,description,"Best I can tell we never use the ""oro"" dependency, but it's been in ivy.xml forever. Does anyone know any reason we might need it?",build_debt,over-declared_dependencies,"Thu, 3 Mar 2011 19:20:30 +0000","Tue, 30 Aug 2016 01:34:01 +0000","Thu, 21 Jan 2016 17:48:17 +0000",154218467,"Best I can tell we never use the ""oro"" dependency, but it's been in ivy.xml forever. Does anyone know any reason we might need it?",0.2916666667,0.2916666667,neutral
hadoop,8364,description,"Three different compile flags, compile.native, compile.c++, and compile.libhdfs, turn on or off different subcomponent builds, but they are generally all off or all on and there's no evident need for three different ways to do things. Also, in build.xml, jsvc and task-controller are included in targets ""package"" and ""bin-package"" as sub-ant tasks, while librecordio is included as a simple dependency. We should work through these and get them done in one understandable way. This is a matter of maintainability and understandability, and therefore robustness under future changes in build.xml. No substantial change in functionality is proposed.",build_debt,build_others,"Sun, 6 May 2012 21:02:35 +0000","Fri, 15 May 2015 17:58:56 +0000","Fri, 15 May 2015 17:58:56 +0000",95374581,"Three different compile flags, compile.native, compile.c++, and compile.libhdfs, turn on or off different architecture-specific subcomponent builds, but they are generally all off or all on and there's no evident need for three different ways to do things. Also, in build.xml, jsvc and task-controller are included in targets ""package"" and ""bin-package"" as sub-ant tasks, while librecordio is included as a simple dependency. We should work through these and get them done in one understandable way. This is a matter of maintainability and understandability, and therefore robustness under future changes in build.xml. No substantial change in functionality is proposed.",0.01,0.01,neutral
hadoop,10106,description,"INFO IPC Server listener on 8020: readAndProcess from client 10.115.201.46 threw exception Unknown out of band call #-2147483647 This is thrown by a reader thread, so the message should be like INFO Socket Reader #1 for port 8020: readAndProcess from client 10.115.201.46 threw exception Unknown out of band call #-2147483647 Another example is which can also be called by handler thread. When that happend, the thread name should be the handler thread, not the responder thread.",code_debt,multi-thread_correctness,"Sat, 16 Nov 2013 00:41:24 +0000","Mon, 24 Feb 2014 20:58:37 +0000","Mon, 16 Dec 2013 22:14:35 +0000",2669591,"INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: readAndProcess from client 10.115.201.46 threw exception org.apache.hadoop.ipc.RpcServerException: Unknown out of band call #-2147483647 This is thrown by a reader thread, so the message should be like INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 8020: readAndProcess from client 10.115.201.46 threw exception org.apache.hadoop.ipc.RpcServerException: Unknown out of band call #-2147483647 Another example is Responder.processResponse, which can also be called by handler thread. When that happend, the thread name should be the handler thread, not the responder thread.",0.0,0.0,neutral
hadoop,10169,description,"When i looked into a HBase JvmMetric impl, just found this synchronized seems not essential.",code_debt,dead_code,"Tue, 17 Dec 2013 05:37:44 +0000","Thu, 12 May 2016 18:27:56 +0000","Fri, 20 Dec 2013 22:09:01 +0000",318677,"When i looked into a HBase JvmMetric impl, just found this synchronized seems not essential.",0.5,0.5,neutral
hadoop,10214,description,"When i worked at HADOOP-9420, found the unrelated findbugs warning:",code_debt,low_quality_code,"Thu, 9 Jan 2014 04:10:55 +0000","Thu, 12 May 2016 18:23:20 +0000","Thu, 9 Jan 2014 06:38:25 +0000",8850,"When i worked at HADOOP-9420, found the unrelated findbugs warning: https://builds.apache.org/job/PreCommit-HADOOP-Build/3408//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-common.html",-0.6,-0.6,neutral
hadoop,10343,description,in we print logs at info level when we drop responses. This causes lot of noise on the console.,code_debt,low_quality_code,"Thu, 13 Feb 2014 19:05:48 +0000","Thu, 10 Apr 2014 13:11:36 +0000","Thu, 13 Feb 2014 22:03:44 +0000",10676,in LossyRetryInvocationHandler we print logs at info level when we drop responses. This causes lot of noise on the console.,0.0875,0.0875,neutral
hadoop,10353,description,"The class uses a plain {{HashMap}} for caching. When the number of inserted values exceeds the the map's load threshold, it triggers a rehash. During this time, a different thread that performs a get operation on a previously inserted key can obtain a null value instead of the actual value associated with that key. The result is a NPE potentially being thrown when calling protocol)}} concurrently.",code_debt,multi-thread_correctness,"Thu, 20 Feb 2014 18:41:33 +0000","Thu, 4 Sep 2014 01:16:46 +0000","Thu, 27 Feb 2014 19:17:50 +0000",606977,"The FsUrlStreamHandlerFactory class uses a plain HashMap for caching. When the number of inserted values exceeds the the map's load threshold, it triggers a rehash. During this time, a different thread that performs a get operation on a previously inserted key can obtain a null value instead of the actual value associated with that key. The result is a NPE potentially being thrown when calling FsUrlStreamHandlerFactory#createURLStreamHandler(String protocol) concurrently.",-0.0625,-0.0625,neutral
hadoop,10432,description,"The method is private and takes a configuration to fetch a hardcoded property. Having a public method to resolve a verifier based on the provided value will enable getting a verifier based on the verifier constant (DEFAULT, STRICT, STRICT_IE6, ALLOW_ALL).",code_debt,low_quality_code,"Tue, 25 Mar 2014 06:08:09 +0000","Thu, 12 May 2016 18:26:59 +0000","Wed, 9 Apr 2014 19:40:50 +0000",1344761,"The SSFactory.getHostnameVerifier() method is private and takes a configuration to fetch a hardcoded property. Having a public method to resolve a verifier based on the provided value will enable getting a verifier based on the verifier constant (DEFAULT, DEFAULT_AND_LOCALHOST, STRICT, STRICT_IE6, ALLOW_ALL).",0.075,0.006666666667,neutral
hadoop,10485,description,Hadoop-streaming no longer requires many classes in o.a.h.record. This jira removes the dead code.,code_debt,dead_code,"Wed, 9 Apr 2014 04:35:25 +0000","Fri, 13 May 2016 05:11:02 +0000","Wed, 9 Apr 2014 18:13:29 +0000",49084,Hadoop-streaming no longer requires many classes in o.a.h.record. This jira removes the dead code.,-0.3,-0.3,neutral
hadoop,10496,description,"{{FileSink}} opens a file. If the {{MetricsSystem}} is shutdown, then the sink is discarded and the file is never closed, causing a file descriptor leak.",code_debt,low_quality_code,"Sun, 13 Apr 2014 16:29:21 +0000","Thu, 12 May 2016 18:22:38 +0000","Mon, 14 Apr 2014 04:32:51 +0000",43410,"FileSink opens a file. If the MetricsSystem is shutdown, then the sink is discarded and the file is never closed, causing a file descriptor leak.",-0.1,-0.1,neutral
hadoop,10499,description,The Configuration parameter is not used in the authorize() function. It can be removed and callers can be updated. Attaching the simple patch which removes the unused _conf_ parameter and updates the callers. The ProxyUsers is defined as a private audience and so there shouldn't be any external callers.,code_debt,dead_code,"Mon, 14 Apr 2014 17:10:42 +0000","Wed, 3 Sep 2014 20:36:28 +0000","Wed, 16 Apr 2014 23:18:20 +0000",194858,The Configuration parameter is not used in the authorize() function. It can be removed and callers can be updated. Attaching the simple patch which removes the unused conf parameter and updates the callers. The ProxyUsers is defined as a private audience and so there shouldn't be any external callers.,0.125,0.125,neutral
hadoop,10501,description,All the other methods accessing handlers are synchronized methods.,code_debt,multi-thread_correctness,"Mon, 14 Apr 2014 23:36:34 +0000","Sat, 7 Mar 2015 23:17:57 +0000","Sat, 7 Mar 2015 23:17:57 +0000",28251683,All the other methods accessing handlers are synchronized methods.,0.0,0.0,neutral
hadoop,10681,description,"The current implementation of SnappyCompressor spends more time within the java loop of copying from the user buffer into the direct buffer allocated to the compressor impl, than the time it takes to compress the buffers. The bottleneck was found to be java monitor code inside SnappyCompressor. The methods are neatly inlined by the JIT into the parent caller which unfortunately does not flatten out the synchronized blocks. The loop does a write of small byte[] buffers (each IFile key+value). I counted approximately 6 monitor enter/exit blocks per k-v pair written.",code_debt,slow_algorithm,"Wed, 11 Jun 2014 17:41:42 +0000","Thu, 8 Sep 2016 06:49:18 +0000","Sun, 5 Oct 2014 14:49:19 +0000",10012057,"The current implementation of SnappyCompressor spends more time within the java loop of copying from the user buffer into the direct buffer allocated to the compressor impl, than the time it takes to compress the buffers. The bottleneck was found to be java monitor code inside SnappyCompressor. The methods are neatly inlined by the JIT into the parent caller (BlockCompressorStream::write), which unfortunately does not flatten out the synchronized blocks. The loop does a write of small byte[] buffers (each IFile key+value). I counted approximately 6 monitor enter/exit blocks per k-v pair written.",0.005,0.005,neutral
hadoop,1072,description,"extends IOException. It's name should follow the Java naming convention for Exceptions, and thus be",code_debt,low_quality_code,"Tue, 6 Mar 2007 23:18:03 +0000","Thu, 11 Aug 2011 18:57:59 +0000","Thu, 11 Aug 2011 18:57:59 +0000",139865996,"org.apache.hadoop.ipc.RPC$VersionMismatch extends IOException. It's name should follow the Java naming convention for Exceptions, and thus be VersionMismatchException.",0.1,0.03333333333,neutral
hadoop,10748,description,Currently HttpServer2 loads the JspServlet by default. It should be removed as JSP support is no longer required.,code_debt,dead_code,"Tue, 24 Jun 2014 17:56:38 +0000","Fri, 24 Apr 2015 22:49:05 +0000","Wed, 27 Aug 2014 20:31:47 +0000",5538909,Currently HttpServer2 loads the JspServlet by default. It should be removed as JSP support is no longer required.,-0.05,-0.05,negative
hadoop,10752,description,"This patch adds support for hardware crc for ARM's new 64 bit architecture. The patch is completely conditionalized on __arch64__ For the moment I have only done the non pipelined version as the hw I have only has 1 crc execute unit. Some initial benchmarks on terasort give sw crc: 107 sec hw crc: 103 sec The performance improvement is quite small, but this is limited by the fact that I am using early stage hw which is not performant. I have also built it on x86 and I think the change is fairly safe for other architectures because post conditionalization the src is identical on other architectures. This is the first patch I have submitted for Hadoop so I would welcome any feedback and help.",code_debt,slow_algorithm,"Thu, 26 Jun 2014 08:54:13 +0000","Tue, 31 Mar 2015 08:43:07 +0000","Tue, 31 Mar 2015 08:43:06 +0000",24018533,"This patch adds support for hardware crc for ARM's new 64 bit architecture. The patch is completely conditionalized on _arch64_ For the moment I have only done the non pipelined version as the hw I have only has 1 crc execute unit. Some initial benchmarks on terasort give sw crc: 107 sec hw crc: 103 sec The performance improvement is quite small, but this is limited by the fact that I am using early stage hw which is not performant. I have also built it on x86 and I think the change is fairly safe for other architectures because post conditionalization the src is identical on other architectures. This is the first patch I have submitted for Hadoop so I would welcome any feedback and help.",0.2042,0.2042,neutral
hadoop,10822,description,"HttpFS implements HTTP proxyuser support inline in httpfs code. For HADOOP-10698 we need similar functionality for KMS. Not to duplicate code, we should refactor existing code to common. We should also leverage HADOOP-10817.",code_debt,low_quality_code,"Mon, 14 Jul 2014 22:37:26 +0000","Wed, 13 Aug 2014 20:25:48 +0000","Wed, 13 Aug 2014 20:25:48 +0000",2584102,"HttpFS implements HTTP proxyuser support inline in httpfs code. For HADOOP-10698 we need similar functionality for KMS. Not to duplicate code, we should refactor existing code to common. We should also leverage HADOOP-10817.",0.1,0.1,neutral
hadoop,11013,description,"As part of HADOOP-9902, java execution across many different shell bits were consolidated down to (effectively) two routines. Prior to calling those two routines, the CLASSPATH is exported. This export should really be getting handled in the exec function and not in the individual shell bits. Additionally, it would be good if there was: so that bash -x would show the content of the classpath or even a '--debug classpath' option that would echo the classpath to the screen prior to java exec to help with debugging.",code_debt,low_quality_code,"Wed, 27 Aug 2014 22:53:39 +0000","Thu, 12 May 2016 18:22:47 +0000","Thu, 28 Aug 2014 17:36:37 +0000",67378,"As part of HADOOP-9902, java execution across many different shell bits were consolidated down to (effectively) two routines. Prior to calling those two routines, the CLASSPATH is exported. This export should really be getting handled in the exec function and not in the individual shell bits. Additionally, it would be good if there was: so that bash x would show the content of the classpath or even a '-debug classpath' option that would echo the classpath to the screen prior to java exec to help with debugging.",0.1531666667,0.1531666667,neutral
hadoop,11063,description,"Windows has a maximum path length of 260 characters. KMS includes several long class file names. During packaging and creation of the distro, these paths get even longer because of prepending the standard war directory structure and our share/hadoop/etc. structure. The end result is that the final paths are longer than 260 characters, making it impossible to deploy a distro on Windows.",code_debt,low_quality_code,"Thu, 4 Sep 2014 17:13:32 +0000","Mon, 1 Dec 2014 03:09:25 +0000","Thu, 4 Sep 2014 19:12:30 +0000",7138,"Windows has a maximum path length of 260 characters. KMS includes several long class file names. During packaging and creation of the distro, these paths get even longer because of prepending the standard war directory structure and our share/hadoop/etc. structure. The end result is that the final paths are longer than 260 characters, making it impossible to deploy a distro on Windows.",0.00625,0.00625,neutral
hadoop,11117,description,"If something is failing with kerberos login, should fail with useful information. But not all exceptions from the inner code are caught and converted to LoginException. Those exceptions that aren't wrapped have their text and stack trace lost somewhere in the javax code, leaving on the text ""login failed"" and a stack trace of no value whatsoever.",code_debt,low_quality_code,"Mon, 22 Sep 2014 20:42:40 +0000","Wed, 8 Oct 2014 16:11:45 +0000","Wed, 8 Oct 2014 16:11:45 +0000",1366145,"If something is failing with kerberos login, UserGroupInformation.loginUserFromKeytabAndReturnUGI() should fail with useful information. But not all exceptions from the inner code are caught and converted to LoginException. Those exceptions that aren't wrapped have their text and stack trace lost somewhere in the javax code, leaving on the text ""login failed"" and a stack trace of no value whatsoever.",-1.39e-17,-0.0625,negative
hadoop,1130,description,The ClientFinalizer shutdown hook is not used. This can lead to severe memory leaks if you use the DFSClient in a dynamic class loading context (such as in a webapp) as the class is retained in the memory by the shutdown hook. It retains also the current ClassLoader and thus all classes loaded by the ClassLoader. (Threads put in the shutdown hook are never garbage collected).,code_debt,low_quality_code,"Mon, 19 Mar 2007 13:57:16 +0000","Wed, 8 Jul 2009 16:42:21 +0000","Tue, 13 Nov 2007 08:17:42 +0000",20629226,The ClientFinalizer shutdown hook is not used. This can lead to severe memory leaks if you use the DFSClient in a dynamic class loading context (such as in a webapp) as the DFSClient.ClientFinalizer class is retained in the memory by the shutdown hook. It retains also the current ClassLoader and thus all classes loaded by the ClassLoader. (Threads put in the shutdown hook are never garbage collected).,0.0625,0.06,neutral
hadoop,11379,description,"When locally run findbugs 3.0, there are new warnings generated. This Jira aims to address the new warnings in hadoop-auth and",code_debt,low_quality_code,"Tue, 9 Dec 2014 19:07:55 +0000","Fri, 24 Apr 2015 22:48:55 +0000","Tue, 9 Dec 2014 21:09:20 +0000",7285,"When locally run findbugs 3.0, there are new warnings generated. This Jira aims to address the new warnings in hadoop-auth and hadoop-auth-examples.",-0.6,-0.6,neutral
hadoop,11384,description,"When locally run findbugs 3.0, there are new warnings generated. This Jira aims to address the new warnings in hadoop-openstack.",code_debt,low_quality_code,"Tue, 9 Dec 2014 21:25:17 +0000","Wed, 10 Dec 2014 00:53:37 +0000","Wed, 10 Dec 2014 00:53:37 +0000",12500,"When locally run findbugs 3.0, there are new warnings generated. This Jira aims to address the new warnings in hadoop-openstack.",-0.6,-0.6,neutral
hadoop,11421,description,"We should have a drop-in replacement for File#listDir that doesn't hide IOExceptions, and which returns a ChunkedArrayList rather than a single large array.",code_debt,low_quality_code,"Wed, 17 Dec 2014 19:06:07 +0000","Fri, 10 Apr 2015 20:04:24 +0000","Wed, 17 Dec 2014 23:18:56 +0000",15169,"We should have a drop-in replacement for File#listDir that doesn't hide IOExceptions, and which returns a ChunkedArrayList rather than a single large array.",0.0,0.0,neutral
hadoop,11544,description,are no longer used.,code_debt,low_quality_code,"Wed, 4 Feb 2015 02:53:38 +0000","Fri, 10 Apr 2015 20:04:43 +0000","Wed, 4 Feb 2015 12:22:06 +0000",34108,CommonConfigurationKeys.HADOOP_TRACE_SAMPLER* are no longer used.,0.0,0.0,neutral
hadoop,11607,description,"{{S3AFileSystem}} generates INFO level logs in {{open}} and {{rename}}, which are not necessary.",code_debt,low_quality_code,"Tue, 17 Feb 2015 21:30:07 +0000","Fri, 10 Apr 2015 20:04:36 +0000","Fri, 20 Feb 2015 21:59:09 +0000",260942,"S3AFileSystem generates INFO level logs in open and rename, which are not necessary.",0.5,0.5,neutral
hadoop,11690,description,There appears to be an opportunity of code reduction by re-implementing Groups to use the,code_debt,duplicated_code,"Sun, 8 Mar 2015 23:27:19 +0000","Sat, 1 Sep 2018 20:32:21 +0000","Sat, 1 Sep 2018 20:32:21 +0000",109976702,There appears to be an opportunity of code reduction by re-implementing Groups to use the IdMappingServiceProvider.,0.4,0.4,neutral
hadoop,1192,description,"After installing 0.12.2, we notice that the du command takes a much longer time to execute than the previous release. Another problem is that dus prints a negative value.",code_debt,slow_algorithm,"Mon, 2 Apr 2007 20:42:22 +0000","Wed, 8 Jul 2009 16:42:24 +0000","Tue, 3 Apr 2007 18:17:31 +0000",77709,"After installing 0.12.2, we notice that the du command takes a much longer time to execute than the previous release. Another problem is that dus prints a negative value.",-0.2,-0.2,negative
hadoop,11966,description,"HADOOP-11464 reinstated support for running the bash scripts through Cygwin. The logic involves setting a {{cygwin}} flag variable to indicate if the script is executing through Cygwin. The flag is set in all of the interactive scripts: {{hadoop}}, {{hdfs}}, {{yarn}} and {{mapred}}. The flag is not set through hadoop-daemon.sh though. This can cause an erroneous overwrite of {{HADOOP_HOME}} and inside hadoop-config.sh.",code_debt,low_quality_code,"Tue, 12 May 2015 23:08:26 +0000","Fri, 6 Jan 2017 00:48:56 +0000","Wed, 13 May 2015 19:27:51 +0000",73165,"HADOOP-11464 reinstated support for running the bash scripts through Cygwin. The logic involves setting a cygwin flag variable to indicate if the script is executing through Cygwin. The flag is set in all of the interactive scripts: hadoop, hdfs, yarn and mapred. The flag is not set through hadoop-daemon.sh though. This can cause an erroneous overwrite of HADOOP_HOME and JAVA_LIBRARY_PATH inside hadoop-config.sh.",-0.07142857143,-0.07142857143,neutral
hadoop,12368,description,"These are test base classes that need to be subclassed to run, can mark as abstract.",code_debt,low_quality_code,"Mon, 31 Aug 2015 23:44:54 +0000","Tue, 30 Aug 2016 01:23:38 +0000","Tue, 1 Sep 2015 01:19:41 +0000",5687,"These are test base classes that need to be subclassed to run, can mark as abstract.",0.0,0.0,neutral
hadoop,12701,description,Test source files are not checked by checkstyle because Maven checkstyle plugin parameter is *false* by default. Propose to enable checkstyle on test source files in order to improve the quality of unit tests.,code_debt,low_quality_code,"Mon, 11 Jan 2016 01:03:40 +0000","Tue, 30 Aug 2016 01:20:10 +0000","Sat, 7 May 2016 00:21:36 +0000",10106276,Test source files are not checked by checkstyle because Maven checkstyle plugin parameter includeTestSourceDirectory is false by default. Propose to enable checkstyle on test source files in order to improve the quality of unit tests.,0.0625,0.0625,neutral
hadoop,12733,description,The following variables appear to no longer be used.,code_debt,dead_code,"Sat, 23 Jan 2016 07:16:45 +0000","Wed, 4 Jan 2017 23:04:55 +0000","Wed, 4 Jan 2017 05:21:00 +0000",29973855,The following variables appear to no longer be used. io.seqfile.lazydecompress io.seqfile.sorter.recordlimit,0.0,0.0,neutral
hadoop,12829,description,"The implemented in HADOOP-12107 swallows interrupt exceptions. Over in Solr/Sentry land, we run thread leak checkers on our test code, which passed before this change and fails after it. Here's a sample report: And here's an indication that the interrupt is being ignored: This is inconsistent with how other long-running threads in hadoop, i.e. PeerCache respond to being interrupted. The argument for doing this in HADOOP-12107 is given as I'm unclear on what ""spurious wakeup"" means and it is not mentioned in So, I believe this thread should respect interruption.",code_debt,low_quality_code,"Sat, 20 Feb 2016 01:19:42 +0000","Mon, 16 Dec 2019 10:04:45 +0000","Tue, 23 Feb 2016 19:33:42 +0000",324840,"The StatisticsDataReferenceCleaner, implemented in HADOOP-12107 swallows interrupt exceptions. Over in Solr/Sentry land, we run thread leak checkers on our test code, which passed before this change and fails after it. Here's a sample report: And here's an indication that the interrupt is being ignored: This is inconsistent with how other long-running threads in hadoop, i.e. PeerCache respond to being interrupted. The argument for doing this in HADOOP-12107 is given as (https://issues.apache.org/jira/browse/HADOOP-12107?focusedCommentId=14598397&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14598397): Cleaner#run Catch and log InterruptedException in the while loop, such that thread does not die on a spurious wakeup. It's safe since it's a daemon thread. I'm unclear on what ""spurious wakeup"" means and it is not mentioned in https://docs.oracle.com/javase/tutorial/essential/concurrency/interrupt.html: A thread sends an interrupt by invoking interrupt on the Thread object for the thread to be interrupted. For the interrupt mechanism to work correctly, the interrupted thread must support its own interruption. So, I believe this thread should respect interruption.",-0.3218125,0.003514880952,negative
hadoop,12888,description,"HDFS _client_ requires dangerous permission, in particular _execute_ on _all files_ despite only trying to connect to an HDFS cluster. A full list (for both Hadoop 1 and 2) is available here along with the place in code where they occur. While it is understandable for some permissions to be used, requiring {{FilePermission <<ALL FILES To make matters worse, the code is executed to initialize a field so in case the permissions is not granted, the VM fails with which is unrecoverable. Ironically enough, on Windows this problem does not appear since the code simply bypasses it and initializes the field with a fall back value ({{false}}). A quick fix would be to simply take into account that the JVM {{SecurityManager}} might be active and the permission not granted or that the external process fails and use a fall back value. A proper and long-term fix would be to minimize the use of permissions for hdfs client since it is simply not required. A client should be as light as possible and not have the server requirements leaked onto.",code_debt,low_quality_code,"Mon, 29 Feb 2016 22:53:19 +0000","Tue, 30 Aug 2016 01:17:40 +0000","Wed, 16 Mar 2016 14:32:53 +0000",1352374,"HDFS client requires dangerous permission, in particular execute on all files despite only trying to connect to an HDFS cluster. A full list (for both Hadoop 1 and 2) is available here along with the place in code where they occur. While it is understandable for some permissions to be used, requiring FilePermission <<ALL FILES>> execute to simply initialize a class field Shell which in the end is not used (since it's just a client) simply compromises the entire security system. To make matters worse, the code is executed to initialize a field so in case the permissions is not granted, the VM fails with InitializationError which is unrecoverable. Ironically enough, on Windows this problem does not appear since the code simply bypasses it and initializes the field with a fall back value (false). A quick fix would be to simply take into account that the JVM SecurityManager might be active and the permission not granted or that the external process fails and use a fall back value. A proper and long-term fix would be to minimize the use of permissions for hdfs client since it is simply not required. A client should be as light as possible and not have the server requirements leaked onto.",-0.017,-0.014875,neutral
hadoop,12952,description,"The examples for building distributions include how to create one without any documentation. But it includes the javadoc stage in the build, which is very slow. Adding skips that phase, and helps round out the parameters to a build.",code_debt,slow_algorithm,"Tue, 22 Mar 2016 14:51:50 +0000","Tue, 30 Aug 2016 01:17:05 +0000","Wed, 23 Mar 2016 05:17:16 +0000",51926,"The examples for building distributions include how to create one without any documentation. But it includes the javadoc stage in the build, which is very slow. Adding -Dmaven.javadoc.skip=true skips that phase, and helps round out the parameters to a build.",0.1333333333,0.0973,neutral
hadoop,13030,description,{{kms.sh}} currently cannot handle special characters.,code_debt,low_quality_code,"Sat, 16 Apr 2016 01:20:05 +0000","Tue, 30 Aug 2016 01:16:21 +0000","Thu, 28 Apr 2016 00:14:36 +0000",1032871,kms.sh currently cannot handle special characters.,0.0,0.0,negative
hadoop,13158,description,The {{cannedACL}} field of {{S3AFileSystem}} can be {{null}}. The {{toString}} implementation has an unguarded call to so there is a risk of,code_debt,low_quality_code,"Mon, 16 May 2016 18:33:41 +0000","Tue, 30 Aug 2016 01:14:48 +0000","Tue, 17 May 2016 12:20:53 +0000",64032,"The cannedACL field of S3AFileSystem can be null. The toString implementation has an unguarded call to cannedACL.toString(), so there is a risk of NullPointerException.",-0.25625,-0.3416666667,neutral
hadoop,13529,description,1. argument and variant naming 2. abstract utility class 3. add some comments 4. adjust some configuration 5. fix TODO 6. remove unnecessary commets 7. some bug fix 8. add some unit test,code_debt,low_quality_code,"Mon, 22 Aug 2016 01:05:09 +0000","Mon, 6 Nov 2017 07:53:16 +0000","Fri, 26 Aug 2016 04:40:22 +0000",358513,1. argument and variant naming 2. abstract utility class 3. add some comments 4. adjust some configuration 5. fix TODO 6. remove unnecessary commets 7. some bug fix 8. add some unit test,0.0,0.0,neutral
hadoop,1367,description,line 556 The distFrom field of this class appears to be accessed inconsistently with respect to synchronization.,code_debt,multi-thread_correctness,"Mon, 14 May 2007 22:36:04 +0000","Mon, 20 Aug 2007 18:11:57 +0000","Wed, 27 Jun 2007 18:12:46 +0000",3785802,org.apache.hadoop.net.NetworkTopology.java line 556 The distFrom field of this class appears to be accessed inconsistently with respect to synchronization.,0.009,0.0015,neutral
hadoop,14359,description,commons-httpclient dependency was removed in HADOOP-10105 but there are some settings to shade commons-httpclient. Probably they can be safely removed.,code_debt,dead_code,"Thu, 27 Apr 2017 08:06:23 +0000","Wed, 2 Oct 2019 17:16:03 +0000","Mon, 1 May 2017 06:24:10 +0000",339467,commons-httpclient dependency was removed in HADOOP-10105 but there are some settings to shade commons-httpclient. Probably they can be safely removed.,0.1,0.1,neutral
hadoop,1453,description,{{exists(f)}} adds extra namenode interaction that is not really required. Open is a critical DFS call.,code_debt,low_quality_code,"Fri, 1 Jun 2007 23:24:43 +0000","Wed, 8 Jul 2009 16:42:29 +0000","Wed, 20 Jun 2007 21:30:25 +0000",1634742,exists(f) adds extra namenode interaction that is not really required. Open is a critical DFS call.,0.0,0.0,neutral
hadoop,14942,description,"Over in HBASE-18975, we observed the following: came from second line below: in which case jobFS was null. A check against null should be added.",code_debt,low_quality_code,"Wed, 11 Oct 2017 00:41:37 +0000","Fri, 20 Oct 2017 22:01:24 +0000","Fri, 20 Oct 2017 21:29:45 +0000",852488,"Over in HBASE-18975, we observed the following: NullPointerException came from second line below: in which case jobFS was null. A check against null should be added.",0.0,0.0,neutral
hadoop,15486,description,"Whenever a datanode is restarted, the registration call after the restart received by NameNode lands in via requires write lock on This registration thread is getting starved by flood of calls, which are triggered by clients those who were writing to the restarted datanode. The registration call which is waiting for write lock on is holding write lock on causing all the other RPC calls which require the lock on wait. We can make lock fair so that the registration thread will not starve.",code_debt,multi-thread_correctness,"Mon, 21 May 2018 19:28:58 +0000","Mon, 23 Jul 2018 17:14:03 +0000","Wed, 23 May 2018 17:35:51 +0000",166013,"Whenever a datanode is restarted, the registration call after the restart received by NameNode lands in NetworkTopology#add via DatanodeManager#registerDatanode requires write lock on NetworkTopology#netLock. This registration thread is getting starved by flood of FSNamesystem.getAdditionalDatanode calls, which are triggered by clients those who were writing to the restarted datanode. The registration call which is waiting for write lock on NetworkTopology#netLock is holding write lock on FSNamesystem#fsLock, causing all the other RPC calls which require the lock on FSNamesystem#fsLock wait. We can make NetworkTopology#netLock lock fair so that the registration thread will not starve.",0.1666666667,0.1,neutral
hadoop,15742,description,Currently we don't log the info of ipc backoff. It will look good to print this as well so that makes users know if we enable this.,code_debt,low_quality_code,"Tue, 11 Sep 2018 08:04:59 +0000","Tue, 18 Sep 2018 03:14:56 +0000","Tue, 18 Sep 2018 03:12:47 +0000",587268,Currently we don't log the info of ipc backoff. It will look good to print this as well so that makes users know if we enable this.,0.3178333333,0.3178333333,neutral
hadoop,15859,description,"As a follow up to HADOOP-15820, I was doing more testing on ZSTD compression and still encountered segfaults in the JVM in HBase after that fix. I took a deeper look and realized there is still another bug, which looks like it's that we are actually [calling on the ""remaining"" variable on the class itself (instead of an instance of that class) because the Java stub for the native C init() function [is marked leading to memory corruption and a crash during GC later. Initially I thought we would fix this by changing the Java init() method to be non-static, but it looks like the ""remaining"" setInt() call is actually unnecessary anyway, because in reset() we [set ""remaining"" to 0 right after calling the JNI init() So init() doesn't have to be changed to an instance method, we can leave it as static, but remove the JNI init() call's ""remaining"" setInt() call altogether. Furthermore we should probably clean up the class/instance distinction in the C file because that's what led to this confusion. There are some other methods where the distinction is incorrect or ambiguous, we should fix them to prevent this from happening again. I talked to  who further pointed out the ZStandardCompressor also has similar problems and needs to be fixed too.",code_debt,low_quality_code,"Tue, 16 Oct 2018 18:31:51 +0000","Wed, 7 Nov 2018 01:34:32 +0000","Wed, 17 Oct 2018 19:48:06 +0000",90975,"As a follow up to HADOOP-15820, I was doing more testing on ZSTD compression and still encountered segfaults in the JVM in HBase after that fix. I took a deeper look and realized there is still another bug, which looks like it's that we are actually calling setInt() on the ""remaining"" variable on the ZStandardDecompressor class itself (instead of an instance of that class) because the Java stub for the native C init() function is marked static, leading to memory corruption and a crash during GC later. Initially I thought we would fix this by changing the Java init() method to be non-static, but it looks like the ""remaining"" setInt() call is actually unnecessary anyway, because in ZStandardDecompressor.java's reset() we set ""remaining"" to 0 right after calling the JNI init() call. So ZStandardDecompressor.java init() doesn't have to be changed to an instance method, we can leave it as static, but remove the JNI init() call's ""remaining"" setInt() call altogether. Furthermore we should probably clean up the class/instance distinction in the C file because that's what led to this confusion. There are some other methods where the distinction is incorrect or ambiguous, we should fix them to prevent this from happening again. I talked to jlowe who further pointed out the ZStandardCompressor also has similar problems and needs to be fixed too.",0.07601388889,0.05213333333,negative
hadoop,16265,description,"When call getTimeDuration like this: {color:#333333}If ""nn.interval"" is set manually or configured in xml file, 10000 will be retrurned.{color} If not, 10 will be returned while 10000 is expected. The logic is not consistent.",code_debt,low_quality_code,"Thu, 18 Apr 2019 01:57:25 +0000","Mon, 22 Apr 2019 15:32:30 +0000","Mon, 22 Apr 2019 15:22:36 +0000",393911,"When call getTimeDuration like this: conf.getTimeDuration(""nn.interval"", 10, TimeUnit.SECONDS, TimeUnit.MILLISECONDS); If ""nn.interval"" is set manually or configured in xml file, 10000 will be retrurned. If not, 10 will be returned while 10000 is expected. The logic is not consistent.",0.25,0.0,neutral
hadoop,16409,description,"currently requires a qualified URI (e.g. s3a://bucket/path) which is how I see this being used most immediately, but it also make sense for someone to just be able to configure /path, if all of their buckets follow that pattern, or if they're providing configuration already in a bucket-specific context (e.g. job-level configs, etc.) Just need to qualify whatever is passed in to allowAuthoritative to make that work. Also, in HADOOP-16396 Gabor pointed out a few whitepace nits that I neglected to fix before merging.",code_debt,low_quality_code,"Wed, 3 Jul 2019 20:28:55 +0000","Fri, 19 Jul 2019 18:28:30 +0000","Mon, 8 Jul 2019 17:28:05 +0000",421150,"fs.s3a.authoritative.path currently requires a qualified URI (e.g. s3a://bucket/path) which is how I see this being used most immediately, but it also make sense for someone to just be able to configure /path, if all of their buckets follow that pattern, or if they're providing configuration already in a bucket-specific context (e.g. job-level configs, etc.) Just need to qualify whatever is passed in to allowAuthoritative to make that work. Also, in HADOOP-16396 Gabor pointed out a few whitepace nits that I neglected to fix before merging.",-0.052,-0.0208,neutral
hadoop,2148,description,"first verifies that the block is valid and then returns the file name corresponding to the block. Doing that it performs the data-node blockMap lookup twice. Only one lookup is needed here. This is important since the data-node blockMap is big. Another observation is that data-nodes do not need the blockMap at all. File names can be derived from the block IDs, there is no need to hold Block to File mapping in memory.",code_debt,slow_algorithm,"Sat, 3 Nov 2007 00:12:52 +0000","Wed, 8 Jul 2009 16:42:46 +0000","Wed, 19 Mar 2008 22:48:25 +0000",11918133,"FSDataset.getBlockFile() first verifies that the block is valid and then returns the file name corresponding to the block. Doing that it performs the data-node blockMap lookup twice. Only one lookup is needed here. This is important since the data-node blockMap is big. Another observation is that data-nodes do not need the blockMap at all. File names can be derived from the block IDs, there is no need to hold Block to File mapping in memory.",-0.01111111111,-0.009523809524,neutral
hadoop,2851,description,The constructors for the Configuration object contains a superfluous logging message that logs an IOException whenever logging is enabled for the debug level. Basically both constructors have the statement: if { } I can' t see any reason for it to be there and it just ends up leaving bogus IOExceptions in log files. It looks like its an old debug print statement which has accidentally been left in.,code_debt,low_quality_code,"Tue, 19 Feb 2008 16:34:00 +0000","Tue, 28 Jul 2009 01:14:28 +0000","Tue, 28 Jul 2009 01:14:28 +0000",45304828,"The constructors for the Configuration object contains a superfluous logging message that logs an IOException whenever logging is enabled for the debug level. Basically both constructors have the statement: if (LOG.isDebugEnabled()) { LOG.debug(StringUtils.stringifyException(new IOException(""config()""))); } I can' t see any reason for it to be there and it just ends up leaving bogus IOExceptions in log files. It looks like its an old debug print statement which has accidentally been left in.",-0.654,-0.327,negative
hadoop,289,description,- Datanode needs to catch when registering otherwise it goes down the same way as when the namenode is not available (HADOOP-282). - need to be caught for all non-registering requests. The data node should be shutdown in this case. Otherwise it will loop infinitely and consume namenode resources.,code_debt,low_quality_code,"Thu, 8 Jun 2006 09:34:01 +0000","Wed, 8 Jul 2009 16:41:54 +0000","Sat, 10 Jun 2006 00:16:56 +0000",139375,Datanode needs to catch SocketTimeoutException when registering otherwise it goes down the same way as when the namenode is not available (HADOOP-282). UnregisteredDatanodeException need to be caught for all non-registering requests. The data node should be shutdown in this case. Otherwise it will loop infinitely and consume namenode resources.,-0.09733333333,-0.073,neutral
hadoop,2965,description,From internal bug from Arkady.. Map reduce jobs have names. The log files use mysterious names like They should use real job names so that users can find the logs they need.,code_debt,low_quality_code,"Fri, 7 Mar 2008 09:34:51 +0000","Wed, 18 May 2011 22:03:41 +0000","Wed, 18 May 2011 22:03:41 +0000",100873730,From internal bug from Arkady.. ============================= Map reduce jobs have names. The log files use mysterious names like jobfailures.jsp_jobid_job_200712150418_0001_kind_map_cause_failed.html They should use real job names so that users can find the logs they need.,-0.5,-0.35725,negative
hadoop,3377,description,"A minor cleanup. In the TaskRunner, we can read : ""When hadoop moves to JDK1.5, replace with String#replace"" This patch do so, removing ~30 lines.",code_debt,low_quality_code,"Tue, 13 May 2008 08:14:50 +0000","Wed, 8 Jul 2009 16:52:47 +0000","Fri, 16 May 2008 00:15:30 +0000",230440,"A minor cleanup. In the TaskRunner, we can read : ""When hadoop moves to JDK1.5, replace [TaskRunner.replaceAll] with String#replace"" This patch do so, removing ~30 lines.",0.0,0.0,neutral
hadoop,3491,description,catches a general Exception and logs them no matter what. It should explicitly catch the and exit gracefully.,code_debt,low_quality_code,"Thu, 5 Jun 2008 00:15:38 +0000","Wed, 8 Jul 2009 16:43:08 +0000","Sat, 7 Jun 2008 00:00:23 +0000",171885,ResolutionMonitor.run() catches a general Exception and logs them no matter what. It should explicitly catch the InterruptedException and exit gracefully.,-0.1,-0.06666666667,neutral
hadoop,3501,description,"As of HADOOP-2095, InMemoryFileSystem is no longer used. Its design was optimized for a particular application and it is thus not a good general-purpose RAM-based FileSystem implementation, so it ought to be removed.",code_debt,dead_code,"Thu, 5 Jun 2008 16:47:42 +0000","Thu, 13 Jan 2011 04:52:23 +0000","Fri, 6 Jun 2008 20:14:41 +0000",98819,"As of HADOOP-2095, InMemoryFileSystem is no longer used. Its design was optimized for a particular application and it is thus not a good general-purpose RAM-based FileSystem implementation, so it ought to be removed.",-0.388,-0.388,negative
hadoop,3560,description,Archvies creates a bunch of empty part files sometimes which are not necessary.,code_debt,low_quality_code,"Fri, 13 Jun 2008 20:20:56 +0000","Wed, 8 Jul 2009 16:41:31 +0000","Tue, 17 Jun 2008 05:15:34 +0000",291278,Archvies creates a bunch of empty part files sometimes which are not necessary.,0.15,0.15,negative
hadoop,3849,description,"The 'notify' done by causes the to immediately run to the JobTracker On a 3500 node cluster, I saw that each TaskTracker calls multiple times per-second. This caused the JobTracker's RPC queues to back-up resulting in each RPC spending more than 120s in the queue - leading to shuffle proceeding very very slowly.",code_debt,slow_algorithm,"Tue, 29 Jul 2008 04:58:34 +0000","Wed, 8 Jul 2009 16:52:56 +0000","Tue, 29 Jul 2008 17:52:13 +0000",46419,"The 'notify' done by FetchStatus.getMapEvents causes the MapEventsFetcherThread to immediately run to the JobTracker (getTaskCompletionEvents). On a 3500 node cluster, I saw that each TaskTracker calls JobTracker.getTaskCompletionEvents multiple times per-second. This caused the JobTracker's RPC queues to back-up resulting in each RPC spending more than 120s in the queue - leading to shuffle proceeding very very slowly.",0.2,0.08,neutral
hadoop,3957,description,There are a few javac warning in DistCp and TestCopyFiles.,code_debt,low_quality_code,"Fri, 15 Aug 2008 00:06:03 +0000","Wed, 26 Sep 2012 13:59:09 +0000","Mon, 18 Aug 2008 20:54:41 +0000",334118,There are a few javac warning in DistCp and TestCopyFiles.,-0.6,-0.6,neutral
hadoop,4436,description,"Consider a bucket with the following object names: * / * /foo * foo//bar NativeS3FileSystem treats an object named ""/"" as a directory. Doing an ""fs -lsr"" causes an infinite loop. I suggest we change NativeS3FileSystem to handle these by ignoring any such ""invalid"" names. Thoughts?",code_debt,low_quality_code,"Fri, 17 Oct 2008 01:06:08 +0000","Tue, 30 Jun 2015 07:22:34 +0000","Fri, 16 Jan 2015 13:32:33 +0000",197209585,"Consider a bucket with the following object names: / /foo foo//bar NativeS3FileSystem treats an object named ""/"" as a directory. Doing an ""fs -lsr"" causes an infinite loop. I suggest we change NativeS3FileSystem to handle these by ignoring any such ""invalid"" names. Thoughts?",-0.2603333333,-0.2603333333,neutral
hadoop,4576,description,"The UI for capacity scheduler displays 'pending tasks' counts. However the capacity scheduler does not update these counts to be the actual values for optimization purposes, for e.g. to avoid walking all pending jobs on all heartbeats. Hence this information is not very accurate. Also, while 'running tasks' counts are useful to compare against capacities and limits, 'pending tasks' counts do not add too much user value. A better count to display would be the number of running and pending jobs.",code_debt,low_quality_code,"Mon, 3 Nov 2008 05:38:44 +0000","Wed, 8 Jul 2009 16:40:30 +0000","Fri, 5 Dec 2008 05:14:19 +0000",2763335,"The UI for capacity scheduler displays 'pending tasks' counts. However the capacity scheduler does not update these counts to be the actual values for optimization purposes, for e.g. to avoid walking all pending jobs on all heartbeats. Hence this information is not very accurate. Also, while 'running tasks' counts are useful to compare against capacities and limits, 'pending tasks' counts do not add too much user value. A better count to display would be the number of running and pending jobs.",-0.03666666667,-0.03666666667,negative
hadoop,4884,description,The tool tip of Time series chart for Chukwa is formatting date as: day/month/year hour:minute:second The date format should change to: year/month/day hour:minute:second,code_debt,low_quality_code,"Tue, 16 Dec 2008 19:13:15 +0000","Wed, 8 Jul 2009 16:40:49 +0000","Thu, 8 Jan 2009 08:26:43 +0000",1948408,The tool tip of Time series chart for Chukwa is formatting date as: day/month/year hour:minute:second The date format should change to: year/month/day hour:minute:second,0.0,0.0,neutral
hadoop,4985,description,"In FSDirectory, quite a few methods are unnecessary declared with ""throws IOException"". In some cases, it can just be removed without causing any compilation problem. In some other cases, it can be replaced with a specific subclass like",code_debt,low_quality_code,"Wed, 7 Jan 2009 02:10:41 +0000","Wed, 12 Aug 2020 23:02:16 +0000","Mon, 12 Jan 2009 21:18:36 +0000",500875,"In FSDirectory, quite a few methods are unnecessary declared with ""throws IOException"". In some cases, it can just be removed without causing any compilation problem. In some other cases, it can be replaced with a specific subclass like QuotaExceededException.",0.1333333333,0.1333333333,neutral
hadoop,5076,description,In the log4j appender always rewrite the file instead of append to the log file. This should be changed to append to ensure the metrics log file is streamed correctly.,code_debt,low_quality_code,"Sat, 17 Jan 2009 01:42:28 +0000","Wed, 8 Jul 2009 16:40:47 +0000","Tue, 10 Feb 2009 02:40:06 +0000",2077058,"In Log4jMetricsContext, the log4j appender always rewrite the file instead of append to the log file. This should be changed to append to ensure the metrics log file is streamed correctly.",0.175,0.175,neutral
hadoop,5097,description,"There is another static FSNamesystem variable, fsn, declared in JspHelper. We should remove it.",code_debt,dead_code,"Wed, 21 Jan 2009 22:37:59 +0000","Tue, 24 Aug 2010 20:35:10 +0000","Mon, 2 Feb 2009 18:43:39 +0000",1022740,"There is another static FSNamesystem variable, fsn, declared in JspHelper. We should remove it.",-0.4375,-0.4375,negative
hadoop,537,description,"It produces the following: BUILD FAILED Execute failed: CreateProcess: make clean error=2 Besides, I would propose to have clean-* target for every compile-* target in build.xml. Some people probably don't build libhdfs or contrib, so why should they clean it.",code_debt,low_quality_code,"Fri, 15 Sep 2006 00:43:20 +0000","Wed, 8 Jul 2009 17:05:59 +0000","Mon, 25 Sep 2006 23:08:59 +0000",944739,"It produces the following: BUILD FAILED Hadoop\build.xml:496: Execute failed: java.io.IOException: CreateProcess: make clean error=2 Besides, I would propose to have clean-* target for every compile-* target in build.xml. Some people probably don't build libhdfs or contrib, so why should they clean it.",0.1066666667,-0.04444444444,negative
hadoop,561,description,"one replica of a file should be written locally if possible. That's currently not the case. Copying a 1GB file using hadoop dfs -cp running on one of the cluster nodes, all the blocks were written to remote nodes, as seen by fsck -files -blocks -locations on the newly created file. as long as there is sufficient space locally, a local copy has significant performance benefits.",code_debt,slow_algorithm,"Tue, 26 Sep 2006 16:50:35 +0000","Wed, 8 Jul 2009 16:42:04 +0000","Fri, 20 Oct 2006 17:33:19 +0000",2076164,"one replica of a file should be written locally if possible. That's currently not the case. Copying a 1GB file using hadoop dfs -cp running on one of the cluster nodes, all the blocks were written to remote nodes, as seen by fsck -files -blocks -locations on the newly created file. as long as there is sufficient space locally, a local copy has significant performance benefits.",0.05208333333,0.05208333333,neutral
hadoop,564,description,"Minor nit, but it seems that we should choose a protocol name that is likely not to conflict with other distributed FS projects. HDFS seems less likely to. Right now this will be trivial to change. Just wanted to socialize this before doing the search and replace. PS right now the dfs: usage is not consistent and has crept into a couple of new features the y! team is working on (caching of files and distcp).",code_debt,low_quality_code,"Thu, 28 Sep 2006 03:42:53 +0000","Wed, 8 Jul 2009 16:42:03 +0000","Wed, 21 Feb 2007 22:36:13 +0000",12682400,"Minor nit, but it seems that we should choose a protocol name that is likely not to conflict with other distributed FS projects. HDFS seems less likely to. Right now this will be trivial to change. Just wanted to socialize this before doing the search and replace. PS right now the dfs: usage is not consistent and has crept into a couple of new features the y! team is working on (caching of files and distcp).",0.3423333333,0.3423333333,neutral
hadoop,5824,description,Operation OP_READ_METADATA exists on the datanode streaming interface. But it is not used by any client code and is currently not protected by access token. Should it be removed?,code_debt,dead_code,"Wed, 13 May 2009 21:53:52 +0000","Mon, 3 Feb 2014 10:40:35 +0000","Thu, 14 May 2009 18:13:11 +0000",73159,Operation OP_READ_METADATA exists on the datanode streaming interface. But it is not used by any client code and is currently not protected by access token. Should it be removed?,0.05,0.05,negative
hadoop,6182,description,As of now rats tool shows 111 RA warnings [rat:report] Summary [rat:report] - [rat:report] Notes: 18 [rat:report] Binaries: 118 [rat:report] Archives: 33 [rat:report] Standards: 942 [rat:report] [rat:report] Apache Licensed: 820 [rat:report] Generated Documents: 11 [rat:report] [rat:report] JavaDocs are generated and so license header is optional [rat:report] Generated files do not required license headers [rat:report] [rat:report] 111 Unknown Licenses [rat:report] [rat:report],code_debt,low_quality_code,"Mon, 10 Aug 2009 05:55:27 +0000","Tue, 24 Aug 2010 20:39:20 +0000","Mon, 17 Aug 2009 04:27:18 +0000",599511,As of now rats tool shows 111 RA warnings [rat:report] Summary [rat:report] ------- [rat:report] Notes: 18 [rat:report] Binaries: 118 [rat:report] Archives: 33 [rat:report] Standards: 942 [rat:report] [rat:report] Apache Licensed: 820 [rat:report] Generated Documents: 11 [rat:report] [rat:report] JavaDocs are generated and so license header is optional [rat:report] Generated files do not required license headers [rat:report] [rat:report] 111 Unknown Licenses [rat:report] [rat:report] *******************************,0.0125,0.0125,neutral
hadoop,6435,description,"The public RPC.waitForProxy() method waits for Long.MAX_VALUE before giving up, ignores all interrupt requests. This is excessive. The version of the method that is package scoped should be made public. Interrupt swallowing is covered in HADOOP-6221 and can be done as a separate patch",code_debt,low_quality_code,"Fri, 11 Dec 2009 17:58:55 +0000","Thu, 2 May 2013 02:29:27 +0000","Wed, 23 Dec 2009 19:36:38 +0000",1042663,"The public RPC.waitForProxy() method waits for Long.MAX_VALUE before giving up, ignores all interrupt requests. This is excessive. The version of the method that is package scoped should be made public. Interrupt swallowing is covered in HADOOP-6221 and can be done as a separate patch",-0.07222222222,-0.07222222222,negative
hadoop,6556,description,Cleanup some of the methods of FileContext,code_debt,low_quality_code,"Thu, 11 Feb 2010 00:44:07 +0000","Wed, 26 Apr 2017 08:46:27 +0000","Wed, 26 Apr 2017 08:46:27 +0000",227347340,Cleanup some of the methods of FileContext,0.0,0.0,neutral
hadoop,6639,description,"When FileSystem cache is enabled, FileSystem.get(..) will call which is a synchronized method. If the lookup fails, a new instance will be initialized. Depends on the FileSystem subclass implementation, the initialization may take a long time. In such case, the FileSystem.Cache lock will be hold and all calls to FileSystem.get(..) by other threads will be blocked for a long time. In particular, the initialization may take a long time since there are retries. It is even worst if the socket timeout is set to a large value.",code_debt,slow_algorithm,"Wed, 17 Mar 2010 22:58:43 +0000","Wed, 17 Mar 2010 23:17:35 +0000","Wed, 17 Mar 2010 23:17:35 +0000",1132,"When FileSystem cache is enabled, FileSystem.get(..) will call FileSystem.Cache.get(..), which is a synchronized method. If the lookup fails, a new instance will be initialized. Depends on the FileSystem subclass implementation, the initialization may take a long time. In such case, the FileSystem.Cache lock will be hold and all calls to FileSystem.get(..) by other threads will be blocked for a long time. In particular, the DistributedFileSystem initialization may take a long time since there are retries. It is even worst if the socket timeout is set to a large value.",-0.1333333333,-0.1090909091,neutral
hadoop,6644,description,util.Shell method name - should use common naming convention,code_debt,low_quality_code,"Fri, 19 Mar 2010 06:23:43 +0000","Fri, 28 May 2010 11:23:01 +0000","Wed, 26 May 2010 18:56:50 +0000",5920387,util.Shell getGROUPS_FOR_USER_COMMAND method name - should use common naming convention,0.0,0.0,neutral
hadoop,6658,description,"Packages, classes and methods that are marked with the or annotation should not appear in the public Javadoc. Developer Javadoc generated using the ""javadoc-dev"" ant target should continue to generate Javadoc for all program elements.",code_debt,low_quality_code,"Wed, 24 Mar 2010 04:14:18 +0000","Wed, 28 Mar 2018 20:17:32 +0000","Thu, 22 Apr 2010 20:48:54 +0000",2565276,"Packages, classes and methods that are marked with the InterfaceAudience.Private or InterfaceAudience.LimitedPrivate annotation should not appear in the public Javadoc. Developer Javadoc generated using the ""javadoc-dev"" ant target should continue to generate Javadoc for all program elements.",-0.3125,-0.15625,neutral
hadoop,6709,description,"To make the FileSystem API in the 0.21 release (which should be treated as a minor release) compatible with 0.20 the deprecated methods removed in HADOOP-4779 need to be re-instated. They should still be marked as deprecated, however.",code_debt,low_quality_code,"Thu, 15 Apr 2010 23:39:00 +0000","Fri, 9 Dec 2016 10:45:39 +0000","Thu, 29 Apr 2010 19:57:04 +0000",1196284,"To make the FileSystem API in the 0.21 release (which should be treated as a minor release) compatible with 0.20 the deprecated methods removed in HADOOP-4779 need to be re-instated. They should still be marked as deprecated, however.",0.453,0.453,neutral
hadoop,6727,description,"HADOOP-6537 added to the throws clause and java docs of FileContext public APIs. FileContext fully resolves symbolic links, exception is only used internally by FSLinkResolver. I'll attach a patch which updates the APIs and javadoc.",code_debt,low_quality_code,"Tue, 27 Apr 2010 23:42:05 +0000","Tue, 24 Aug 2010 20:42:57 +0000","Mon, 3 May 2010 17:39:29 +0000",496644,"HADOOP-6537 added UnresolvedLinkException to the throws clause and java docs of FileContext public APIs. FileContext fully resolves symbolic links, UnresolvedLinkException exception is only used internally by FSLinkResolver. I'll attach a patch which updates the APIs and javadoc.",0.1333333333,0.1333333333,neutral
hadoop,6839,description,"Develop a new method for getting the user list. Method signature is public ArrayList<StringAdd new attribute in system-test.xml file for getting userlist path. For submitting the jobs as different user a proxy user id is needed. So,get the available users from a userlist and then pass the user as proxy instead of hardcoding user id in a test.",code_debt,low_quality_code,"Fri, 25 Jun 2010 08:24:21 +0000","Thu, 2 May 2013 02:29:31 +0000","Wed, 14 Jul 2010 18:47:11 +0000",1678970,"Develop a new method for getting the user list. Method signature is public ArrayList<String> getHadoopMultiUsersList() throws IOException; Add new attribute in system-test.xml file for getting userlist path. For submitting the jobs as different user a proxy user id is needed. So,get the available users from a userlist and then pass the user as proxy instead of hardcoding user id in a test.",0.0,0.0,neutral
hadoop,7786,description,"HADOOP-4952 added a couple HDFS-specific configuration values to common (the block size and the replication factor) that conflict with the HDFS values (eg have the wrong defaults, wrong key name), are not used by common or hdfs and should be removed. After removing these I noticed the rest of FsConfig is only used once outside a test, and isn't tagged as a public API, I think we can remove it entirely.",code_debt,low_quality_code,"Fri, 6 May 2011 04:48:15 +0000","Mon, 12 Dec 2011 06:19:17 +0000","Tue, 1 Nov 2011 17:12:53 +0000",15510278,"HADOOP-4952 added a couple HDFS-specific configuration values to common (the block size and the replication factor) that conflict with the HDFS values (eg have the wrong defaults, wrong key name), are not used by common or hdfs and should be removed. After removing these I noticed the rest of FsConfig is only used once outside a test, and isn't tagged as a public API, I think we can remove it entirely.",-0.06,-0.06,negative
hadoop,7985,description,"I use this command ""mvn -Pdist -P-cbuild -DskipTests install"" to build. Without ANY changes in code, running this command takes 1:32. It seems to me this is too long. Investigate if this time can be reduced drastically.",code_debt,slow_algorithm,"Thu, 19 Jan 2012 21:14:41 +0000","Thu, 15 Aug 2013 19:02:33 +0000","Thu, 15 Aug 2013 19:02:33 +0000",49585672,"I use this command ""mvn -Pdist -P-cbuild -Dmaven.javadoc.skip -DskipTests install"" to build. Without ANY changes in code, running this command takes 1:32. It seems to me this is too long. Investigate if this time can be reduced drastically.",0.0,0.0,negative
hadoop,8124,description,The Syncable.sync() was deprecated in 0.21. We should remove it.,code_debt,dead_code,"Thu, 1 Mar 2012 00:09:47 +0000","Thu, 12 May 2016 18:22:41 +0000","Thu, 1 Mar 2012 23:53:06 +0000",85399,The Syncable.sync() was deprecated in 0.21. We should remove it.,0.1666666667,0.1666666667,negative
hadoop,8222,description,"it would be nice if the jsvc pid file were properly namespaced in /var/run to avoid collisions with other jsvc instances jsvc uses /var/run/jsvc.pid for the datanode pid file. If that's configurable, it should be configured: /var/run/jsvc.pid is sorely not namespaced. if [ = ""true"" ]; then -errfile &1 -outfile else # Even though we are trying to run a non-detached datanode, # jsvc will not write to stdout/stderr, so we have to pipe # it and tail the logfile. -errfile &1 -outfile $log_path"" echo Non-detached jsvc output piping to: $log_path touch $log_path tail -f $log_path & fi And the relevant argument is '-pidfile'",code_debt,low_quality_code,"Thu, 29 Mar 2012 01:39:11 +0000","Mon, 30 Jun 2014 23:04:24 +0000","Fri, 30 Mar 2012 01:03:23 +0000",84252,"it would be nice if the jsvc pid file were properly namespaced in /var/run to avoid collisions with other jsvc instances jsvc uses /var/run/jsvc.pid for the datanode pid file. If that's configurable, it should be configured: /var/run/jsvc.pid is sorely not namespaced. if [ ""$_HADOOP_DAEMON_DETACHED"" = ""true"" ]; then _JSVC_FLAGS=""-pidfile $_HADOOP_DAEMON_PIDFILE -errfile &1 -outfile $_HADOOP_DAEMON_OUT"" else Even though we are trying to run a non-detached datanode, jsvc will not write to stdout/stderr, so we have to pipe it and tail the logfile. log_path=/tmp/jsvc_${COMMAND}.$$ _JSVC_FLAGS=""-nodetach -errfile &1 -outfile $log_path"" echo Non-detached jsvc output piping to: $log_path touch $log_path tail -f $log_path & fi And the relevant argument is '-pidfile' (http://linux.die.net/man/1/jsvc).",0.0223,0.004666666667,neutral
hadoop,8283,description,Tests in projects other than common need to be able to change whether the token service uses an ip or a host.,code_debt,low_quality_code,"Mon, 16 Apr 2012 15:32:16 +0000","Fri, 7 Sep 2012 21:01:44 +0000","Mon, 16 Apr 2012 16:12:57 +0000",2441,Tests in projects other than common need to be able to change whether the token service uses an ip or a host.,0.094,0.094,neutral
hadoop,8303,description,Clean up a bunch of existing javac warnings in hadoop-streaming module.,code_debt,low_quality_code,"Sun, 22 Apr 2012 17:44:54 +0000","Thu, 12 May 2016 18:27:52 +0000","Sun, 2 Dec 2012 05:02:52 +0000",19307878,Clean up a bunch of existing javac warnings in hadoop-streaming module.,-0.1,-0.1,neutral
hadoop,8341,description,Now that the precommit build can test hadoop-tools we need to fix or filter the many findbugs warnings that are popping up in there.,code_debt,low_quality_code,"Tue, 1 May 2012 20:09:22 +0000","Thu, 12 May 2016 18:25:56 +0000","Tue, 8 May 2012 13:24:35 +0000",580513,Now that the precommit build can test hadoop-tools we need to fix or filter the many findbugs warnings that are popping up in there.,-0.6,-0.6,neutral
hadoop,8358,description,"Looks easy to fix, and we should avoid using old config params that we ourselves deprecated.",code_debt,low_quality_code,"Fri, 4 May 2012 07:44:56 +0000","Thu, 11 Oct 2012 17:45:09 +0000","Mon, 28 May 2012 15:42:38 +0000",2102262,"Looks easy to fix, and we should avoid using old config params that we ourselves deprecated.",0.046,0.046,positive
hadoop,8405,description,"The ZKFC code wasn't previously terminating the ZK connection in all cases where it should (eg after a failed startup or after formatting ZK). This didn't cause a problem for CLI usage, since the process exited afterwards, but caused the test results to get clouded with a lot of ""Reconecting to ZK"" messages, which make the logs hard to read.",code_debt,low_quality_code,"Wed, 16 May 2012 22:43:46 +0000","Thu, 17 May 2012 00:39:07 +0000","Thu, 17 May 2012 00:39:07 +0000",6921,"The ZKFC code wasn't previously terminating the ZK connection in all cases where it should (eg after a failed startup or after formatting ZK). This didn't cause a problem for CLI usage, since the process exited afterwards, but caused the test results to get clouded with a lot of ""Reconecting to ZK"" messages, which make the logs hard to read.",0.2,0.2,neutral
hadoop,8548,description,Precommit builds show an incorrect link for javac warnings. Note that 'trunk' appears twice. Do we need $(basename $BASEDIR) in the following? Other places don't have it.,code_debt,low_quality_code,"Mon, 2 Jul 2012 18:11:46 +0000","Thu, 12 May 2016 18:25:38 +0000","Mon, 2 Jul 2012 19:36:05 +0000",5059,Precommit builds show an incorrect link for javac warnings. Note that 'trunk' appears twice. Do we need $(basename $BASEDIR) in the following? Other places don't have it.,-0.15,-0.15,negative
hadoop,8819,description,Should use && instead of & in a few places in,code_debt,low_quality_code,"Sun, 16 Sep 2012 16:31:48 +0000","Thu, 12 May 2016 18:22:15 +0000","Mon, 17 Sep 2012 17:11:22 +0000",88774,"Should use && instead of & in a few places in FTPFileSystem,FTPInputStream,S3InputStream,ViewFileSystem,ViewFs.",0.0,0.0,neutral
hadoop,8866,description,"does O(N) calls LinkedList#get() in a loop, rather than using an iterator. This makes query O(N^2), rather than O(N).",code_debt,slow_algorithm,"Tue, 25 Sep 2012 22:41:01 +0000","Fri, 15 Feb 2013 13:11:50 +0000","Sat, 29 Sep 2012 01:00:52 +0000",267591,"SampleQuantiles#query() does O(N) calls LinkedList#get() in a loop, rather than using an iterator. This makes query O(N^2), rather than O(N).",0.0,0.0,neutral
hadoop,8929,description,"The new SampleQuantiles class is useful in the context of benchmarks, but currently there is no way to print it out outside the context of a metrics sink. It would be nice to have a convenient way to stringify it for logging, etc. Also: - made it Comparable and changed the HashMap to TreeMap so that the printout is in ascending percentile order. Given that this map is always very small, and snapshot() is only called once a minute or so, the runtime/memory differences between treemap and hashmap should be negligible. - changed the behavior to return null instead of throw, because all the catching, etc, got pretty ugly. In implementing toString, I figured I'd clean up the other behavior along the way.",code_debt,low_quality_code,"Mon, 15 Oct 2012 19:46:19 +0000","Thu, 12 May 2016 18:22:35 +0000","Tue, 16 Oct 2012 06:07:18 +0000",37259,"The new SampleQuantiles class is useful in the context of benchmarks, but currently there is no way to print it out outside the context of a metrics sink. It would be nice to have a convenient way to stringify it for logging, etc. Also: made it Comparable and changed the HashMap to TreeMap so that the printout is in ascending percentile order. Given that this map is always very small, and snapshot() is only called once a minute or so, the runtime/memory differences between treemap and hashmap should be negligible. changed the behavior to return null instead of throw, because all the catching, etc, got pretty ugly. In implementing toString, I figured I'd clean up the other behavior along the way.",0.325,0.2708333333,neutral
hadoop,8985,description,"Currently .proto files use java_package to specify java packages in proto files, but namespace are not specified for other languages such as cpp, this causes name collision in cpp. we can add namespace declarations to avoid this. In Java, the package specifier is used as the Java package, unless you explicitly provide a option java_package in your .proto file. So the original java package will not be affected. About namespace name, how about in cpp) for all common sub-project proto files, and in cpp) for all hdfs sub-project proto files?",code_debt,low_quality_code,"Fri, 26 Oct 2012 05:53:55 +0000","Wed, 3 Sep 2014 23:11:06 +0000","Sat, 27 Oct 2012 18:48:14 +0000",132859,"Currently .proto files use java_package to specify java packages in proto files, but namespace are not specified for other languages such as cpp, this causes name collision in cpp. we can add namespace declarations to avoid this. In Java, the package specifier is used as the Java package, unless you explicitly provide a option java_package in your .proto file. So the original java package will not be affected. About namespace name, how about ""hadoop.common""(hadoop::common in cpp) for all common sub-project proto files, and ""hadoop.hdfs""(hadoop::hdfs in cpp) for all hdfs sub-project proto files?",-0.05714285714,-0.04444444444,neutral
hadoop,9278,description,fails on Windows due to invalid HAR URI and file handle leak. We need to change the tests to use valid HAR URIs and fix the file handle leak.,code_debt,low_quality_code,"Mon, 4 Feb 2013 23:00:17 +0000","Thu, 12 May 2016 18:22:52 +0000","Tue, 5 Feb 2013 21:26:06 +0000",80749,TestHarFileSystemBasics fails on Windows due to invalid HAR URI and file handle leak. We need to change the tests to use valid HAR URIs and fix the file handle leak.,-0.25,-0.25,negative
hadoop,9419,description,"I recently found a bug in the gpl compression libraries that was causing map tasks for a particular job to OOM. Now granted it does not make a lot of sense for a job to use the LzopCodec for map output compression over the LzoCodec, but arguably other codecs could be doing similar things and causing the same sort of memory leaks. I propose that we do a sanity check when creating a new If the codec newly created object does not match the value from getType... it should turn off caching for that Codec.",code_debt,low_quality_code,"Tue, 19 Mar 2013 18:34:28 +0000","Tue, 19 Mar 2013 21:20:44 +0000","Tue, 19 Mar 2013 21:20:44 +0000",9976,"I recently found a bug in the gpl compression libraries that was causing map tasks for a particular job to OOM. https://github.com/omalley/hadoop-gpl-compression/issues/3 Now granted it does not make a lot of sense for a job to use the LzopCodec for map output compression over the LzoCodec, but arguably other codecs could be doing similar things and causing the same sort of memory leaks. I propose that we do a sanity check when creating a new decompressor/compressor. If the codec newly created object does not match the value from getType... it should turn off caching for that Codec.",0.07716666667,0.06575,negative
hadoop,10225,description,Right now Maven javadoc and sources artifacts do not accompany Hadoop releases within Maven central. This means that one needs to checkout source code to DEBUG aspects of the codebase... this is not user friendly. The build script(s) should be amended to accommodate publication of javadoc and sources artifacts alongside pom and jar artifacts. Some history on this conversation can be seen below,design_debt,non-optimal_design,"Sun, 12 Jan 2014 14:05:14 +0000","Thu, 10 Jan 2019 18:37:42 +0000","Thu, 10 Jan 2019 18:37:42 +0000",157609948,Right now Maven javadoc and sources artifacts do not accompany Hadoop releases within Maven central. This means that one needs to checkout source code to DEBUG aspects of the codebase... this is not user friendly. The build script(s) should be amended to accommodate publication of javadoc and sources artifacts alongside pom and jar artifacts. Some history on this conversation can be seen below http://s.apache.org/7qR,-0.0245,-0.0245,negative
hadoop,1034,description,"Only IOException is catched and logged (in warn). Every Throwable should be logged in error. Eg: a RuntimeException occurs in the writeBlock() method. The exception will not be logged, but simply ignored. The socket is closed silently and nothing and an non understandable exception will be thrown in the DFSClient sending the block....",design_debt,non-optimal_design,"Fri, 23 Feb 2007 13:45:50 +0000","Wed, 8 Jul 2009 16:42:18 +0000","Fri, 23 Feb 2007 20:27:58 +0000",24128,"Only IOException is catched and logged (in warn). Every Throwable should be logged in error. Eg: a RuntimeException occurs in the writeBlock() method. The exception will not be logged, but simply ignored. The socket is closed silently and nothing and an non understandable exception will be thrown in the DFSClient sending the block....",-0.28,-0.28,neutral
hadoop,10979,description,It would make adding common options to hadoop_usage output easier if some entries were auto-populated. This is similar to what happens in FsShell and other parts of the Java code.,design_debt,non-optimal_design,"Tue, 19 Aug 2014 13:39:50 +0000","Thu, 12 May 2016 18:23:14 +0000","Thu, 16 Jul 2015 23:58:54 +0000",28635544,It would make adding common options to hadoop_usage output easier if some entries were auto-populated. This is similar to what happens in FsShell and other parts of the Java code.,0.0,0.0,neutral
hadoop,11523,description,"In current WASB (Windows Azure Storage - Blob) implementation, when rename operation succeeds, WASB will update the parent folder's ""last modified time"" property. By default we do not acquire lease on this folder when updating its property and simply pass ""null"" to it. In HBase scenario, when doing distributed log splitting, there might be a case that multiple processes from different region servers will access the same folder, and randomly we will see this exception in regionserver's log, which makes log splitting fail. So we should acquire the lease when updating the folder property rather than pass ""null"" to it.",design_debt,non-optimal_design,"Thu, 29 Jan 2015 18:57:37 +0000","Thu, 22 Oct 2015 00:48:25 +0000","Fri, 30 Jan 2015 01:08:39 +0000",22262,"In current WASB (Windows Azure Storage - Blob) implementation, when rename operation succeeds, WASB will update the parent folder's ""last modified time"" property. By default we do not acquire lease on this folder when updating its property and simply pass ""null"" to it. In HBase scenario, when doing distributed log splitting, there might be a case that multiple processes from different region servers will access the same folder, and randomly we will see this exception in regionserver's log, which makes log splitting fail. So we should acquire the lease when updating the folder property rather than pass ""null"" to it.",-0.0875,-0.0875,neutral
hadoop,12496,description,"hadoop-aws jar still depends on the very old 1.7.4 version of aws-java-sdk. In newer versions of SDK, there is incompatible API changes that leads to the following error when trying to use the S3A class and newer versions of sdk presents. This is because S3A is calling the method with ""int"" as the parameter type while the new SDK is expecting ""long"". This makes it impossible to use kinesis + s3a in the same process. It would be very helpful to upgrade hadoop-awas's aws-sdk version. $iwC$$iwC.<init at $iwC.<init at <init at .<init at .<clinit at .<init at .<clinit at $print(<console at Method)",design_debt,non-optimal_design,"Wed, 21 Oct 2015 01:48:25 +0000","Tue, 22 Dec 2015 21:12:16 +0000","Wed, 21 Oct 2015 10:11:49 +0000",30204,"hadoop-aws jar still depends on the very old 1.7.4 version of aws-java-sdk. In newer versions of SDK, there is incompatible API changes that leads to the following error when trying to use the S3A class and newer versions of sdk presents. This is because S3A is calling the method with ""int"" as the parameter type while the new SDK is expecting ""long"". This makes it impossible to use kinesis + s3a in the same process. It would be very helpful to upgrade hadoop-awas's aws-sdk version. java.lang.NoSuchMethodError: com.amazonaws.services.s3.transfer.TransferManagerConfiguration.setMultipartUploadThreshold(I)V at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:285) at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2653) at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92) at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687) at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371) at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295) at org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:130) at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:114) at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:104) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:29) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:34) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:36) at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:38) at $iwC$$iwC$$iwC$$iwC.<init>(<console>:40) at $iwC$$iwC$$iwC.<init>(<console>:42) at $iwC$$iwC.<init>(<console>:44) at $iwC.<init>(<console>:46) at <init>(<console>:48) at .<init>(<console>:52) at .<clinit>(<console>) at .<init>(<console>:7) at .<clinit>(<console>) at $print(<console>) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065) at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340) at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840) at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871) at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819) at org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:655) at org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:620) at org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:613) at org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57) at org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93) at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:276) at org.apache.zeppelin.scheduler.Job.run(Job.java:170) at org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:118) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)",0.06508333333,0.03305116279,neutral
hadoop,1251,description,I need a way to get the InputSplit from a Mapper. This is effectively a stop gap until we get context objects and can fix this much better. *smile* I introduce a static method in the TaskTracker named getInputSplit() that returns the InputSplit.,design_debt,non-optimal_design,"Thu, 12 Apr 2007 05:54:52 +0000","Thu, 2 May 2013 02:29:04 +0000","Mon, 16 Apr 2007 22:57:10 +0000",406938,I need a way to get the InputSplit from a Mapper. This is effectively a stop gap until we get context objects and can fix this much better. smile I introduce a static method in the TaskTracker named getInputSplit() that returns the InputSplit.,-0.001388888889,-0.001388888889,negative
hadoop,12520,description,"The hadoop-azure tests support execution against the live Azure Storage service if the developer specifies the key to an Azure Storage account. The configuration works by overwriting the file. This can be an error-prone process. The azure-test.xml file is checked into revision control to show an example. There is a risk that the tester could overwrite azure-test.xml containing the keys and then accidentally commit the keys to revision control. This would leak the keys to the world for potential use by an attacker. This issue proposes to use XInclude to isolate the keys into a separate file, ignored by git, which will never be committed to revision control. This is very similar to the setup already used by hadoop-aws for integration testing.",design_debt,non-optimal_design,"Tue, 27 Oct 2015 21:59:08 +0000","Tue, 3 Jan 2017 11:15:57 +0000","Wed, 28 Oct 2015 05:52:58 +0000",28430,"The hadoop-azure tests support execution against the live Azure Storage service if the developer specifies the key to an Azure Storage account. The configuration works by overwriting the src/test/resources/azure-test.xml file. This can be an error-prone process. The azure-test.xml file is checked into revision control to show an example. There is a risk that the tester could overwrite azure-test.xml containing the keys and then accidentally commit the keys to revision control. This would leak the keys to the world for potential use by an attacker. This issue proposes to use XInclude to isolate the keys into a separate file, ignored by git, which will never be committed to revision control. This is very similar to the setup already used by hadoop-aws for integration testing.",0.048,0.03909090909,neutral
hadoop,12806,description,"Trying to use the hadoop fs s3a library in AWS lambda with temporary credentials but it's not possible because of the way the is defined under Specifically the following code is used to initialise the credentials chain The above works fine when the EC2 metadata endpoint is available (i.e. running on an EC2 instance) however it doesn't work properly when the environment variables are used to define credentials as it happens in AWS Lambda. Amazon suggests to use the in AWS Lambda. To summarise and suggest an alternative I think that the could be used instead of the and that would cover the following cases: {panel} * Environment Variables - AWS_ACCESS_KEY_ID and (RECOMMENDED since they are recognized by all the AWS SDKs and CLI except for .NET), or AWS_ACCESS_KEY and AWS_SECRET_KEY (only recognized by Java SDK) * Java System Properties - aws.accessKeyId and aws.secretKey * Credential profiles file at the default location shared by all AWS SDKs and the AWS CLI * Instance profile credentials delivered through the Amazon EC2 metadata service {panel} If you think that the above change would be useful I could investigate more about what the required changes would be and submit a patch.",design_debt,non-optimal_design,"Mon, 15 Feb 2016 14:26:49 +0000","Mon, 19 Feb 2018 17:16:24 +0000","Mon, 19 Feb 2018 17:16:24 +0000",63514175,Trying to use the hadoop fs s3a library in AWS lambda with temporary credentials but it's not possible because of the way the AWSCredentialsProviderChain is defined under https://github.com/apache/hadoop/blob/29ae25801380b94442253c4202dee782dc4713f5/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java Specifically the following code is used to initialise the credentials chain The above works fine when the EC2 metadata endpoint is available (i.e. running on an EC2 instance) however it doesn't work properly when the environment variables are used to define credentials as it happens in AWS Lambda. Amazon suggests to use the EnvironmentVariableCredentialsProvider in AWS Lambda. To summarise and suggest an alternative I think that the DefaultAWSCredentialsProviderChain could be used instead of the InstanceProfileCredentialsProvider and that would cover the following cases: If you think that the above change would be useful I could investigate more about what the required changes would be and submit a patch.,0.1944444444,0.2166666667,neutral
hadoop,12811,description,The HBase's HMaster port number conflicts with Hadoop kms port number. Both uses 16000. There might be use cases user need kms and HBase present on the same cluster. The HBase is able to encrypt its HFiles but user might need KMS to encrypt other HDFS directories. Users would have to manually override the default port of either application on their cluster. It would be nice to have different default ports so kms and HBase could naturally coexist.,design_debt,non-optimal_design,"Wed, 17 Feb 2016 00:12:23 +0000","Thu, 12 May 2016 18:22:07 +0000","Thu, 14 Apr 2016 18:36:47 +0000",4991064,The HBase's HMaster port number conflicts with Hadoop kms port number. Both uses 16000. There might be use cases user need kms and HBase present on the same cluster. The HBase is able to encrypt its HFiles but user might need KMS to encrypt other HDFS directories. Users would have to manually override the default port of either application on their cluster. It would be nice to have different default ports so kms and HBase could naturally coexist.,-0.00825,-0.00825,neutral
hadoop,12946,description,"Ensure at most one JVMPauseMonitor thread is running per JVM when there are multiple JVMPauseMonitor instances, e.g., in mini clusters. This will prevent redundant GC pause log messages while still maintaining one monitor thread running. This is a different way to fix HADOOP-12855.",design_debt,non-optimal_design,"Sun, 20 Mar 2016 18:14:55 +0000","Tue, 10 Jan 2017 01:41:24 +0000","Tue, 10 Jan 2017 01:41:24 +0000",25514789,"Ensure at most one JVMPauseMonitor thread is running per JVM when there are multiple JVMPauseMonitor instances, e.g., in mini clusters. This will prevent redundant GC pause log messages while still maintaining one monitor thread running. This is a different way to fix HADOOP-12855.",0.08016666667,0.08016666667,neutral
hadoop,13365,description,"While we are mucking with all of the _OPTS variables, this is a good time to convert them to arrays so that filesystems with spaces in them can be used.",design_debt,non-optimal_design,"Mon, 11 Jul 2016 15:06:56 +0000","Sat, 1 Sep 2018 20:06:01 +0000","Sat, 1 Sep 2018 20:06:01 +0000",67582745,"While we are mucking with all of the _OPTS variables, this is a good time to convert them to arrays so that filesystems with spaces in them can be used.",0.625,0.625,neutral
hadoop,13770,description,"creates a bash shell command to verify if the system supports bash. However, its error message is misleading, and the logic should be updated. If the shell command throws an IOException, it does not imply the bash did not run successfully. If the shell command process was interrupted, its internal logic throws an which is a subclass of IOException. An example of it appeared in a recent jenkins job The test logic in starts a thread, wait it for 1 second, and interrupt the thread, expecting the thread to terminate. However, the method swallowed the interrupt, and therefore failed. The original design is not desirable, as it swallowed a potential interrupt, causing to fail. Unfortunately, Java does not allow this static method to throw exception. We should removed the static member variable, so that the method can throw the interrupt exception. The node manager should call the static method, instead of using the static member variable. This fix has an associated benefit: the tests could run faster, because it will no longer need to spawn a bash process when it uses a Shell static method variable (which happens quite often for checking what operating system Hadoop is running on)",design_debt,non-optimal_design,"Wed, 16 Dec 2015 22:28:18 +0000","Wed, 2 Oct 2019 17:14:23 +0000","Fri, 28 Oct 2016 15:11:22 +0000",27362584,"Shell.checkIsBashSupported() creates a bash shell command to verify if the system supports bash. However, its error message is misleading, and the logic should be updated. If the shell command throws an IOException, it does not imply the bash did not run successfully. If the shell command process was interrupted, its internal logic throws an InterruptedIOException, which is a subclass of IOException. An example of it appeared in a recent jenkins job https://builds.apache.org/job/PreCommit-HADOOP-Build/8257/testReport/org.apache.hadoop.ipc/TestRPCWaitForProxy/testInterruptedWaitForProxy/ The test logic in TestRPCWaitForProxy.testInterruptedWaitForProxy starts a thread, wait it for 1 second, and interrupt the thread, expecting the thread to terminate. However, the method Shell.checkIsBashSupported swallowed the interrupt, and therefore failed. The original design is not desirable, as it swallowed a potential interrupt, causing TestRPCWaitForProxy.testInterruptedWaitForProxy to fail. Unfortunately, Java does not allow this static method to throw exception. We should removed the static member variable, so that the method can throw the interrupt exception. The node manager should call the static method, instead of using the static member variable. This fix has an associated benefit: the tests could run faster, because it will no longer need to spawn a bash process when it uses a Shell static method variable (which happens quite often for checking what operating system Hadoop is running on)",-0.3013030303,-0.2529,negative
hadoop,13991,description,"NativeS3FileSystem does not support any retry management for failed uploading to S3. If due to socket timeout or any other network exception, file uploading to S3 bucket fails, then uploading fails and temporary file gets deleted. Connection reset Source) Source) This can be solved by using asynchronous retry management. We have made following modifications to NativeS3FileSystem to add the retry management, which is working fine in our product system, without any uploading failure:",design_debt,non-optimal_design,"Sat, 14 Jan 2017 18:41:34 +0000","Fri, 24 Feb 2017 14:07:36 +0000","Fri, 24 Feb 2017 14:07:36 +0000",3525962,"NativeS3FileSystem does not support any retry management for failed uploading to S3. If due to socket timeout or any other network exception, file uploading to S3 bucket fails, then uploading fails and temporary file gets deleted. java.net.SocketException: Connection reset at java.net.SocketInputStream.read(SocketInputStream.java:196) at java.net.SocketInputStream.read(SocketInputStream.java:122) at org.jets3t.service.S3Service.putObject(S3Service.java:2265) at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.storeFile(Jets3tNativeFileSystemStore.java:122) at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.fs.s3native.$Proxy8.storeFile(Unknown Source) at org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsOutputStream.close(NativeS3FileSystem.java:284) at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72) at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106) at org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.close(CBZip2OutputStream.java:737) at org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream.close(BZip2Codec.java:336) at org.apache.flume.sink.hdfs.HDFSCompressedDataStream.close(HDFSCompressedDataStream.java:155) at org.apache.flume.sink.hdfs.BucketWriter$3.call(BucketWriter.java:312) at org.apache.flume.sink.hdfs.BucketWriter$3.call(BucketWriter.java:308) at org.apache.flume.sink.hdfs.BucketWriter$9$1.run(BucketWriter.java:679) at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50) at org.apache.flume.sink.hdfs.BucketWriter$9.call(BucketWriter.java:676) This can be solved by using asynchronous retry management. We have made following modifications to NativeS3FileSystem to add the retry management, which is working fine in our product system, without any uploading failure:",0.06666666667,0.002279202279,negative
hadoop,1488,description,"During the last rework of the shuffle, a lot of the stages of the shuffle had auto-progress threads added, leading to system lockups when the shuffle stalls. We need to add Progressables to the sort and fetching interfaces so that if any task gets stuck, it will eventually be killed by the framework.",design_debt,non-optimal_design,"Wed, 13 Jun 2007 17:13:54 +0000","Wed, 8 Jul 2009 16:52:18 +0000","Thu, 28 Jun 2007 21:11:23 +0000",1310249,"During the last rework of the shuffle, a lot of the stages of the shuffle had auto-progress threads added, leading to system lockups when the shuffle stalls. We need to add Progressables to the sort and fetching interfaces so that if any task gets stuck, it will eventually be killed by the framework.",0.2,0.2,negative
hadoop,15577,description,Hello! distcp through 3.1.0 appears to copy files and then rename them into their final/destination filename. added support for more efficient S3 committers that do not use renames. Please update distcp to use these efficient committers and no renames. Thanks!,design_debt,non-optimal_design,"Mon, 2 Jul 2018 22:14:48 +0000","Tue, 3 Jul 2018 14:20:20 +0000","Tue, 3 Jul 2018 14:20:20 +0000",57932,Hello! distcp through 3.1.0 appears to copy files and then rename them into their final/destination filename. https://issues.apache.org/jira/browse/HADOOP-13786added support for more efficient S3 committers that do not use renames. Please update distcp to use these efficient committers and no renames. Thanks!,0.2,0.2,positive
hadoop,1586,description,"Currently, in the loop of MAX_RETRIES (set to three) attempts are made to report progress/ping or All attempt failures are counted as critical. Here I am proposing a variant - treat only ConnectException exceptions are critical and treat the others as non-critical. The other exception could be the in the case of the two RPCs. The reason why I am proposing this is that since HADOOP-1462 went in, I have been seeing quite a few unexpected 65 deaths, and with some logging it appears that they happen, most of the time, due to the in the progress RPC call (before HADOOP-1462, the return value of progress would not be checked). And when the hack described above was put in, things improved considerably. One argument that one might make against the above proposal is that the tasktracker could be faulty, when a task is not able to successfully invoke an RPC on it even though it is able to connect. If this is indeed the case, even in the current scheme of things, the only resort is to restart the tasktracker (either manually, or, the JobTracker asks it to reinitialize), and in both the cases, normal behavior of the protocol will ensure that the child task will die (since the reinited tasktracker is going to return false for the progress/ping calls).",design_debt,non-optimal_design,"Tue, 10 Jul 2007 11:55:01 +0000","Wed, 8 Jul 2009 16:52:18 +0000","Thu, 26 Jul 2007 17:49:18 +0000",1403657,"Currently, in the loop of Task.startCommunicationThread, MAX_RETRIES (set to three) attempts are made to report progress/ping (TaskUmbilicalProtocol.progress or TaskUmbilicalProtocol.ping). All attempt failures are counted as critical. Here I am proposing a variant - treat only ConnectException exceptions are critical and treat the others as non-critical. The other exception could be the SocketTimeoutException in the case of the two RPCs. The reason why I am proposing this is that since HADOOP-1462 went in, I have been seeing quite a few unexpected 65 deaths, and with some logging it appears that they happen, most of the time, due to the SocketTimeoutException in the progress RPC call (before HADOOP-1462, the return value of progress would not be checked). And when the hack described above was put in, things improved considerably. One argument that one might make against the above proposal is that the tasktracker could be faulty, when a task is not able to successfully invoke an RPC on it even though it is able to connect. If this is indeed the case, even in the current scheme of things, the only resort is to restart the tasktracker (either manually, or, the JobTracker asks it to reinitialize), and in both the cases, normal behavior of the protocol will ensure that the child task will die (since the reinited tasktracker is going to return false for the progress/ping calls).",0.03333333333,0.05757575758,neutral
hadoop,16013,description,"sets up a {{Timer}} to schedule a decay of the weights it tracks: However this Timer is not set up as a daemon thread. I have seen this cause my JVM to refuse to exit when running, for example, with FairCallQueue enabled.",design_debt,non-optimal_design,"Tue, 18 Dec 2018 17:55:55 +0000","Tue, 5 Nov 2019 01:19:02 +0000","Fri, 11 Jan 2019 20:58:17 +0000",2084542,"DecayRpcScheduler sets up a Timer to schedule a decay of the weights it tracks: However this Timer is not set up as a daemon thread. I have seen this cause my JVM to refuse to exit when running, for example, NNThroughputBenchmark with FairCallQueue enabled.",-0.2,-0.2,negative
hadoop,16461,description,"The lock now has a ShutdownHook creation, which ends up doing which ends up doing a ""new Configuration()"" within the locked section. This indirectly hurts the cache hit scenarios as well, since if the lock on this is held, then the other section cannot be entered either. slowing down the RawLocalFileSystem when there are other threads creating HDFS FileSystem objects at the same time.",design_debt,non-optimal_design,"Wed, 24 Jul 2019 21:53:03 +0000","Tue, 1 Oct 2019 00:06:38 +0000","Fri, 26 Jul 2019 10:35:00 +0000",132117,"https://github.com/apache/hadoop/blob/2546e6ece240924af2188bb39b3954a4896e4a4f/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java#L3388 The lock now has a ShutdownHook creation, which ends up doing https://github.com/apache/hadoop/blob/2546e6ece240924af2188bb39b3954a4896e4a4f/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ShutdownHookManager.java#L205 which ends up doing a ""new Configuration()"" within the locked section. This indirectly hurts the cache hit scenarios as well, since if the lock on this is held, then the other section cannot be entered either. https://github.com/apache/tez/blob/master/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/sort/impl/TezSpillRecord.java#L65 slowing down the RawLocalFileSystem when there are other threads creating HDFS FileSystem objects at the same time.",-0.05883333333,-0.05883333333,neutral
hadoop,16504,description,"Because default value is too small, TCP's ListenDrop indicator along with the rpc request large. The upper limit of the system's semi-join queue is 65636 and maximum number of fully connected queues is 1024. I think this default value should be adjusted.",design_debt,non-optimal_design,"Sat, 10 Aug 2019 16:46:31 +0000","Wed, 2 Oct 2019 17:16:03 +0000","Thu, 15 Aug 2019 22:21:35 +0000",452104,"Because ipc.server.listen.queue.size default value is too small, TCP's ListenDrop indicator along with the rpc request large. The upper limit of the system's semi-join queue is 65636 and maximum number of fully connected queues is 1024. I think this default value should be adjusted.",-0.125,-0.05357142857,negative
hadoop,16607,description,"Hive deployments can use a JCEKs file to store secrets, which it sets up To be readable only by the Hive user, listing it under When it tries to create an S3A FS instance as another user, via a doAs{} clause, the S3A FS getPassword() call fails on the subsequent -even if the secret it is looking for is in the XML file or, as in the case of encryption settings, or session key undefined. I can you point the blame at hive for this -it's the one with a forbidden JCEKS file on the provider path, but I think it is easiest to fix in S3AUtils than in hive, and safer then changing Configuration. ABFS is likely to see the same problem. I propose an option to set the fallback policy. I initially thought about always handling this: Catching the exception, attempting to downgrade to Reading XML and if that fails rethrowing the caught exception. However, this will do the wrong thing if the option is completely undefined, As is common with the encryption settings. I don't want to simply default to log and continue here though, as it may be a legitimate failure -such as when you really do want to read secrets from such a source. Issue: what fallback policies? * fail: fail fast. today's policy; the default. * ignore: log and continue We could try and be clever in future. To get away with that, we would have to declare which options were considered compulsory and re-throw the caught Exception if no value was found in the XML file. That can be a future enhancement -but it is why I want the policy to be an enumeration, rather than a simple boolean. Tests: should be straightforward; set to a non-existent file and expected to be processed according to the settings.",design_debt,non-optimal_design,"Wed, 25 Sep 2019 14:15:33 +0000","Mon, 18 Nov 2019 19:42:53 +0000","Fri, 11 Oct 2019 15:26:36 +0000",1386663,"Hive deployments can use a JCEKs file to store secrets, which it sets up To be readable only by the Hive user, listing it under hadoop.credential.providers. When it tries to create an S3A FS instance as another user, via a doAs{} clause, the S3A FS getPassword() call fails on the subsequent AccessDeniedException -even if the secret it is looking for is in the XML file or, as in the case of encryption settings, or session key undefined. I can you point the blame at hive for this -it's the one with a forbidden JCEKS file on the provider path, but I think it is easiest to fix in S3AUtils than in hive, and safer then changing Configuration. ABFS is likely to see the same problem. I propose an option to set the fallback policy. I initially thought about always handling this: Catching the exception, attempting to downgrade to Reading XML and if that fails rethrowing the caught exception. However, this will do the wrong thing if the option is completely undefined, As is common with the encryption settings. I don't want to simply default to log and continue here though, as it may be a legitimate failure -such as when you really do want to read secrets from such a source. Issue: what fallback policies? fail: fail fast. today's policy; the default. ignore: log and continue We could try and be clever in future. To get away with that, we would have to declare which options were considered compulsory and re-throw the caught Exception if no value was found in the XML file. That can be a future enhancement -but it is why I want the policy to be an enumeration, rather than a simple boolean. Tests: should be straightforward; set hadoop.credential.providers to a non-existent file and expected to be processed according to the settings.",-0.00205952381,-0.00151754386,neutral
hadoop,2208,description,"Currently, We have counter updates from task tracker to job tracker on every heartbeat. Both counter name and the values are updated for every heartbeat. This can be improved by sending names and values for the first time and only the values after that. The frequency can be reduced by doing update only when the counters got changed.",design_debt,non-optimal_design,"Thu, 15 Nov 2007 04:47:40 +0000","Wed, 8 Jul 2009 16:52:35 +0000","Thu, 3 Jan 2008 08:16:17 +0000",4246117,"Currently, We have counter updates from task tracker to job tracker on every heartbeat. Both counter name and the values are updated for every heartbeat. This can be improved by sending names and values for the first time and only the values after that. The frequency can be reduced by doing update only when the counters got changed.",-0.0125,-0.0125,neutral
hadoop,2897,description,"Hod currently allows the user to specify and run a hadoop script after allocation, and deallocating as soon as the script is done. For e.g. hod -m 3 -z ~/hadoop.script allocates 3 nodes, and runs ~/hadoop.script, then deallocates This is a convenient way single line wrapper around 4-5 commands that users have to write themselves. We have this because: - hod 0.3 does not provide an easy way to combine these into a single operation, because of the HOD shell. - even in hod 0.4, users have to carefully write some error checking code to make sure their cluster is allocated successfully, before running the script and their HADOOP_CONF_DIR should be set correctly. - users can free up clusters as soon as they are done. The requirements make sense. But having this as part of the core hod interface seems incorrect. The interface should be an orthogonal set of commands that each just do one thing well. The script option should be converted to a simple wrapper that can be part of the hod project. This way, users can enjoy the benefits of not having to write such a script themselves, while the hod codebase can still be clean. One disadvantage if we change this is that users will need to remember one more command. But given hod 0.4 is a new interface anyway, it is better to address now, rather than later. And we can alleviate this a bit by making sure options are consistently named between hod and the wrapper script.",design_debt,non-optimal_design,"Tue, 26 Feb 2008 10:55:37 +0000","Sat, 8 Mar 2008 19:51:07 +0000","Mon, 3 Mar 2008 05:54:18 +0000",500321,"Hod currently allows the user to specify and run a hadoop script after allocation, and deallocating as soon as the script is done. For e.g. hod -m 3 -z ~/hadoop.script allocates 3 nodes, and runs ~/hadoop.script, then deallocates This is a convenient way single line wrapper around 4-5 commands that users have to write themselves. We have this because: hod 0.3 does not provide an easy way to combine these into a single operation, because of the HOD shell. even in hod 0.4, users have to carefully write some error checking code to make sure their cluster is allocated successfully, before running the script and their HADOOP_CONF_DIR should be set correctly. users can free up clusters as soon as they are done. The requirements make sense. But having this as part of the core hod interface seems incorrect. The interface should be an orthogonal set of commands that each just do one thing well. The script option should be converted to a simple wrapper that can be part of the hod project. This way, users can enjoy the benefits of not having to write such a script themselves, while the hod codebase can still be clean. One disadvantage if we change this is that users will need to remember one more command. But given hod 0.4 is a new interface anyway, it is better to address now, rather than later. And we can alleviate this a bit by making sure options are consistently named between hod and the wrapper script.",0.1542051282,0.1347833333,neutral
hadoop,3087,description,"JobInfo object is not refreshed in loadHistory.jsp if same job is accessed again. In loadhistory.jsp, the jobInfo object is stored as session attribute. And if the same job is accessed again, the object is not refreshed. This becomes a problem if first time the object accessed was during job running and it doesnt refresh even after completing the job.",design_debt,non-optimal_design,"Tue, 25 Mar 2008 07:04:48 +0000","Wed, 8 Jul 2009 16:52:41 +0000","Thu, 27 Mar 2008 12:36:32 +0000",192704,"JobInfo object is not refreshed in loadHistory.jsp if same job is accessed again. In loadhistory.jsp, the jobInfo object is stored as session attribute. And if the same job is accessed again, the object is not refreshed. This becomes a problem if first time the object accessed was during job running and it doesnt refresh even after completing the job.",-0.3518888889,-0.3518888889,neutral
hadoop,3286,description,"Gridmix jobs use time suffix to differentiate output dir names. The current time granularity of second may not be sufficient, causing multiple jobs having the same output dir.",design_debt,non-optimal_design,"Mon, 21 Apr 2008 01:51:44 +0000","Wed, 21 May 2008 20:06:03 +0000","Wed, 23 Apr 2008 01:05:53 +0000",170049,"Gridmix jobs use time suffix to differentiate output dir names. The current time granularity of second may not be sufficient, causing multiple jobs having the same output dir.",0.0625,0.0625,neutral
hadoop,3654,description,"This is being added as a bugrep so that other people can find it, and the workaround 1. On my machine TestFileSystem will hang, even overnight -even though the build was set with a timeout. 2. halting the build left a JVM running; it was not being killed. 3. Under the IDE, the main thread appears hung in the native library call to get a stack trace, somewhere inside Log4J 4. the IDE could not halt the build, and could not be shut down cleanly either The fix for this problem was to edit and switch to a log4J log pattern that did not print the line of the code %-5p %c %x - %m%n Given that working out a stack trace can be an expensive call, and that it can apparently hang some JVMs, perhaps it should not be the default.",design_debt,non-optimal_design,"Fri, 27 Jun 2008 14:59:25 +0000","Thu, 29 Apr 2010 08:07:15 +0000","Thu, 24 Dec 2009 12:44:28 +0000",47079903,"This is being added as a bugrep so that other people can find it, and the workaround 1. On my machine TestFileSystem will hang, even overnight -even though the build was set with a timeout. 2. halting the build left a JVM running; it was not being killed. 3. Under the IDE, the main thread appears hung in the native library call to get a stack trace, somewhere inside Log4J 4. the IDE could not halt the build, and could not be shut down cleanly either The fix for this problem was to edit conf/log4j.properties and switch to a log4J log pattern that did not print the line of the code log4j.appender.console.layout.ConversionPattern=%-4r %-5p %c %x - %m%n Given that working out a stack trace can be an expensive call, and that it can apparently hang some JVMs, perhaps it should not be the default.",0.1782857143,0.1975833333,neutral
hadoop,3925,description,The JobTracker can be prone to a denial-of-service attack if a user submits a job that has a very large number of tasks. This has happened once in our cluster. It would be nice to have a configuration setting that limits the maximum tasks that a single job can have.,design_debt,non-optimal_design,"Fri, 8 Aug 2008 06:13:55 +0000","Wed, 8 Jul 2009 16:52:57 +0000","Thu, 20 Nov 2008 07:59:21 +0000",8991926,The JobTracker can be prone to a denial-of-service attack if a user submits a job that has a very large number of tasks. This has happened once in our cluster. It would be nice to have a configuration setting that limits the maximum tasks that a single job can have.,0.03888888889,0.03888888889,negative
hadoop,400,description,"The job tracker tries not to run tasks that have previously failed on a node on that node again, but it doesn't strictly prevent it. I propose to change the rule so that when pollForNewTask is called by a TaskTracker, the JobTracker will only assign it a task that has failed on that TaskTracker, if and only if it has already failed on the entire cluster. Thus, for ""normal"" clusters with more than 4 TaskTrackers, you will be guaranteed that it will run on 4 different TaskTrackers. For small clusters, it will run on every TaskTracker in the cluster at least once. Does that sound reasonable to everyone?",design_debt,non-optimal_design,"Fri, 28 Jul 2006 20:33:32 +0000","Wed, 8 Jul 2009 16:51:51 +0000","Wed, 9 Aug 2006 13:46:03 +0000",1012351,"The job tracker tries not to run tasks that have previously failed on a node on that node again, but it doesn't strictly prevent it. I propose to change the rule so that when pollForNewTask is called by a TaskTracker, the JobTracker will only assign it a task that has failed on that TaskTracker, if and only if it has already failed on the entire cluster. Thus, for ""normal"" clusters with more than 4 TaskTrackers, you will be guaranteed that it will run on 4 different TaskTrackers. For small clusters, it will run on every TaskTracker in the cluster at least once. Does that sound reasonable to everyone?",0.0484,0.0484,neutral
hadoop,4997,description,"This is a temporary work around issues discussed in HADOOP-4663. The proposal is to remove all the files under tmp directory, thus bringing the behavior back to 0.17. The main cost is that sync() will not be supported. This is incompatible with 0.18.x, but not with 0.17 because of this reason.",design_debt,non-optimal_design,"Thu, 8 Jan 2009 20:21:26 +0000","Wed, 8 Jul 2009 16:43:29 +0000","Fri, 16 Jan 2009 23:56:50 +0000",704124,"This is a temporary work around issues discussed in HADOOP-4663. The proposal is to remove all the files under tmp directory, thus bringing the behavior back to 0.17. The main cost is that sync() will not be supported. This is incompatible with 0.18.x, but not with 0.17 because of this reason.",0.13,0.13,negative
hadoop,508,description,"Some of my applications using Hadoop DFS receive wrong data after certain random seeks. After some investigation I believe (without looking at source code of that it basically boils down to the fact that the method read(byte[] b, int off, int len), when called with an external buffer larger than the internal buffer, reads into the external buffer directly without using the internal buffer anymore, but without invalidating the internal buffer by setting the variable 'count' to 0 such that a subsequent seek to an offset which is closer to the 'position' of the Positioncache than the internal buffersize will put the current position into the internal buffer containing outdated data from somewhere else.",design_debt,non-optimal_design,"Tue, 5 Sep 2006 20:48:16 +0000","Wed, 8 Jul 2009 16:42:02 +0000","Wed, 27 Sep 2006 22:01:27 +0000",1905191,"Some of my applications using Hadoop DFS receive wrong data after certain random seeks. After some investigation I believe (without looking at source code of java.io.BufferedInputStream) that it basically boils down to the fact that the method read(byte[] b, int off, int len), when called with an external buffer larger than the internal buffer, reads into the external buffer directly without using the internal buffer anymore, but without invalidating the internal buffer by setting the variable 'count' to 0 such that a subsequent seek to an offset which is closer to the 'position' of the Positioncache than the internal buffersize will put the current position into the internal buffer containing outdated data from somewhere else.",-0.1895,-0.16775,neutral
hadoop,551,description,"In the patch for HADOOP-423, I made the web ui and the job client cli format the percentage done consistently as 0.00 - 100.00%. This has the side effect of making the job client print a lot more lines to the user's console (up to 20k!).",design_debt,non-optimal_design,"Tue, 19 Sep 2006 22:57:35 +0000","Fri, 6 Oct 2006 21:49:09 +0000","Wed, 20 Sep 2006 21:46:31 +0000",82136,"In the patch for HADOOP-423, I made the web ui and the job client cli format the percentage done consistently as 0.00 - 100.00%. This has the side effect of making the job client print a lot more lines to the user's console (up to 20k!).",0.28125,0.28125,neutral
hadoop,5561,description,"The default configuration for the ant task javadoc-dev does not specify a maxmemory and, after churning for a while, fails with an OOM exception:",design_debt,non-optimal_design,"Mon, 23 Mar 2009 23:49:25 +0000","Tue, 24 Aug 2010 20:36:43 +0000","Thu, 26 Mar 2009 00:25:57 +0000",174992,"The default configuration for the ant task javadoc-dev does not specify a maxmemory and, after churning for a while, fails with an OOM exception:",-0.45,-0.45,negative
hadoop,5657,description,"While TestReduceFetch verifies the reduce semantics for reducing from in-memory segments, it does not validate the data it reads. Data corrupted during the merge will not be detected.",design_debt,non-optimal_design,"Sat, 11 Apr 2009 00:16:20 +0000","Tue, 24 Aug 2010 20:37:05 +0000","Wed, 29 Apr 2009 04:11:21 +0000",1569301,"While TestReduceFetch verifies the reduce semantics for reducing from in-memory segments, it does not validate the data it reads. Data corrupted during the merge will not be detected.",-0.4,-0.4,negative
hadoop,6198,description,"There's an avoidable overhead in listing/globbing items with some property (e.g. owned by user foo, only files, files larger than _n_ bytes, etc.). Internally, the Path is extracted from a FileStatus object and passed to the PathFilter; simply passing the FileStatus object would allow one to filter on the information in the status object.",design_debt,non-optimal_design,"Tue, 18 Aug 2009 05:46:36 +0000","Thu, 24 Jul 2014 19:57:11 +0000","Thu, 24 Jul 2014 19:57:11 +0000",155657435,"There's an avoidable overhead in listing/globbing items with some property (e.g. owned by user foo, only files, files larger than n bytes, etc.). Internally, the Path is extracted from a FileStatus object and passed to the PathFilter; simply passing the FileStatus object would allow one to filter on the information in the status object.",0.06425,0.06425,neutral
hadoop,6364,description,There was a bit of talk about HDFS-34 last night at Apachecon. One of the points brought up was the difficulty in assuming that the 'hostname' was externally available. Perhaps what needs to happen instead is to follow what httpd and a few other apps do. That is provide a way for the service to know what hostname to advertise itself as and use in all communications. This could also help solve the multi-nic problems (HADOOP-6210) faced when using hadoop in a HA environment.,design_debt,non-optimal_design,"Thu, 5 Nov 2009 17:30:50 +0000","Thu, 22 Mar 2012 06:02:21 +0000","Wed, 2 Nov 2011 18:22:22 +0000",62815892,There was a bit of talk about HDFS-34 last night at Apachecon. One of the points brought up was the difficulty in assuming that the 'hostname' was externally available. Perhaps what needs to happen instead is to follow what httpd and a few other apps do. That is provide a way for the service to know what hostname to advertise itself as and use in all communications. This could also help solve the multi-nic problems (HADOOP-6210) faced when using hadoop in a HA environment.,-0.1279,-0.1279,neutral
hadoop,6589,description,"Currently when authentication fails, RPC server simply closes the connection. Sending certain error messages back to the client may help user debug the problem. Of course, those error messages that are sent back shouldn't compromise system security.",design_debt,non-optimal_design,"Mon, 22 Feb 2010 19:52:34 +0000","Tue, 24 Aug 2010 20:42:15 +0000","Sat, 27 Feb 2010 06:17:38 +0000",383104,"Currently when authentication fails, RPC server simply closes the connection. Sending certain error messages back to the client may help user debug the problem. Of course, those error messages that are sent back shouldn't compromise system security.",-0.2833333333,-0.2833333333,neutral
hadoop,659,description,"I see two types of replications that should be accelerated compared to all others. 1. Blocks that have only one remaining copy (but are required to have higher replication). 2. Blocks that have less than 1/3 of their replicas in place. The latter occurs when map/reduce sets replication of certain files to 10, and we want it happen fast to achieve better performance on the tasks. So I think we should distinguish two major groups of under-replicated blocks: first-priority (having only 1 copy or less than 1/3 of required replicas), and the rest. The name-node places first-priority blocks into the beginning of the neededReplication list, and the rest are placed at the end. That way the first-priority blocks will be replicated first and then the others.",design_debt,non-optimal_design,"Tue, 31 Oct 2006 20:29:31 +0000","Wed, 8 Jul 2009 16:42:06 +0000","Thu, 25 Jan 2007 22:59:13 +0000",7439382,"I see two types of replications that should be accelerated compared to all others. 1. Blocks that have only one remaining copy (but are required to have higher replication). 2. Blocks that have less than 1/3 of their replicas in place. The latter occurs when map/reduce sets replication of certain files to 10, and we want it happen fast to achieve better performance on the tasks. So I think we should distinguish two major groups of under-replicated blocks: first-priority (having only 1 copy or less than 1/3 of required replicas), and the rest. The name-node places first-priority blocks into the beginning of the neededReplication list, and the rest are placed at the end. That way the first-priority blocks will be replicated first and then the others.",-0.02222222222,-0.02222222222,neutral
hadoop,7358,description,"When a server implementation throws an exception handling an RPC, the Handler thread catches it and logs it before responding with the exception over the IPC channel. This is currently done at INFO level. I'd like to propose that, if the exception is an unchecked exception, it should be logged at WARN level instead. This can help alert operators when they might be hitting some kind of bug.",design_debt,non-optimal_design,"Fri, 3 Jun 2011 21:45:39 +0000","Mon, 16 Mar 2015 19:22:44 +0000","Thu, 24 Nov 2011 01:57:03 +0000",14962284,"When a server implementation throws an exception handling an RPC, the Handler thread catches it and logs it before responding with the exception over the IPC channel. This is currently done at INFO level. I'd like to propose that, if the exception is an unchecked exception, it should be logged at WARN level instead. This can help alert operators when they might be hitting some kind of bug.",-0.08125,-0.08125,neutral
hadoop,7620,description,"Currently, the default profile doesn't generate the javadoc, which gives the developer a false sense of security. Leaving the forrest stuff in the doc profile makes sense.",design_debt,non-optimal_design,"Fri, 9 Sep 2011 18:01:17 +0000","Sun, 26 Apr 2015 01:26:52 +0000","Sun, 26 Apr 2015 01:26:52 +0000",114420335,"Currently, the default profile doesn't generate the javadoc, which gives the developer a false sense of security. Leaving the forrest stuff in the doc profile makes sense.",-0.175,-0.175,negative
hadoop,8209,description,"In 1.x DNs currently refuse to connect to NNs if their build *revision* (ie svn revision) do not match. TTs refuse to connect to JTs if their build *version* (version, revision, user, and source checksum) do not match. This prevents rolling upgrades, which is intentional, see the discussion in HADOOP-5203. The primary motivation in that jira was (1) it's difficult to guarantee every build on a large cluster got deployed correctly, builds don't get rolled back to old versions by accident etc, and (2) mixed versions can lead to execution problems that are hard to debug. However there are also cases when users know they two builds are compatible, eg when deploying a new build which contains the same contents as the previous one, plus a critical security patch that does not affect compatibility. Currently deploying a 1 line patch requires taking down the entire cluster (or trying to work around the issue by lying about the build revision or checksum, yuck). These users would like to be able to perform a rolling upgrade. In order to support this, let's add an option that is off by default, but, when enabled, makes the DN and TT version check just check for an exact version match (eg ""1.0.2"") but ignore the build revision (DN) and the source checksum (TT). Two builds still need to match the major, minor, and point numbers, but nothing else.",design_debt,non-optimal_design,"Sun, 25 Mar 2012 22:17:38 +0000","Wed, 24 Oct 2012 19:12:09 +0000","Thu, 12 Apr 2012 22:06:37 +0000",1554539,"In 1.x DNs currently refuse to connect to NNs if their build revision (ie svn revision) do not match. TTs refuse to connect to JTs if their build version (version, revision, user, and source checksum) do not match. This prevents rolling upgrades, which is intentional, see the discussion in HADOOP-5203. The primary motivation in that jira was (1) it's difficult to guarantee every build on a large cluster got deployed correctly, builds don't get rolled back to old versions by accident etc, and (2) mixed versions can lead to execution problems that are hard to debug. However there are also cases when users know they two builds are compatible, eg when deploying a new build which contains the same contents as the previous one, plus a critical security patch that does not affect compatibility. Currently deploying a 1 line patch requires taking down the entire cluster (or trying to work around the issue by lying about the build revision or checksum, yuck). These users would like to be able to perform a rolling upgrade. In order to support this, let's add an option that is off by default, but, when enabled, makes the DN and TT version check just check for an exact version match (eg ""1.0.2"") but ignore the build revision (DN) and the source checksum (TT). Two builds still need to match the major, minor, and point numbers, but nothing else.",0.09700285714,0.09700285714,neutral
hadoop,829,description,"In the existing implementation, the Namenode has a data structure called DatanodeDescriptor. It is used to serialize contents of the Datanode while writing it to the fsimage. The same serialization method is used to send a Datanode object to the Datanode and/or the Client. This introduces the shortcoming that one cannot introduce non-persistent fields in the DatanodeDescriptor. One solution is to separate out the following two functionality into two separate classes: 1. The fields from a Datanode that are part of the ClientProtocol and/or DatanodeProcotol. 2. The fields from a Datanode that are stored in the fsImage.",design_debt,non-optimal_design,"Fri, 15 Dec 2006 05:10:13 +0000","Wed, 8 Jul 2009 16:42:13 +0000","Mon, 18 Dec 2006 20:59:35 +0000",316162,"In the existing implementation, the Namenode has a data structure called DatanodeDescriptor. It is used to serialize contents of the Datanode while writing it to the fsimage. The same serialization method is used to send a Datanode object to the Datanode and/or the Client. This introduces the shortcoming that one cannot introduce non-persistent fields in the DatanodeDescriptor. One solution is to separate out the following two functionality into two separate classes: 1. The fields from a Datanode that are part of the ClientProtocol and/or DatanodeProcotol. 2. The fields from a Datanode that are stored in the fsImage.",-0.17025,-0.17025,neutral
hadoop,8304,description,DNSToSwitchMapping now has only one API to resolve a host list: public List<String> names). But the two major caller: and are taking single host name but have to wrapper it to an single entry ArrayList. This is not necessary especially the host has been cached before.,design_debt,non-optimal_design,"Mon, 23 Apr 2012 10:35:52 +0000","Wed, 23 May 2012 20:15:50 +0000","Tue, 8 May 2012 21:14:32 +0000",1334320,DNSToSwitchMapping now has only one API to resolve a host list: public List<String> resolve(List<String> names). But the two major caller: RackResolver.resolve() and DatanodeManager.resolveNetworkLocation() are taking single host name but have to wrapper it to an single entry ArrayList. This is not necessary especially the host has been cached before.,0.4833333333,0.4,neutral
hadoop,8316,description,"HADOOP-7633 made hdfs, mr and security audit logging on by default (INFO level) in log4j.properties used for the packages, this then got copied over to the non-packaging log4j.properties in HADOOP-8216 (which made them consistent). Seems like we should keep with the v1.x setting which is disabled (WARNING level) by default. There's a performance overhead to audit logging, and HADOOP-7633 provided not rationale (just ""We should add the audit logs as part of default confs"") as to why they were enabled for the packages.",design_debt,non-optimal_design,"Thu, 26 Apr 2012 02:34:24 +0000","Thu, 11 Oct 2012 17:45:05 +0000","Fri, 11 May 2012 19:25:54 +0000",1356690,"HADOOP-7633 made hdfs, mr and security audit logging on by default (INFO level) in log4j.properties used for the packages, this then got copied over to the non-packaging log4j.properties in HADOOP-8216 (which made them consistent). Seems like we should keep with the v1.x setting which is disabled (WARNING level) by default. There's a performance overhead to audit logging, and HADOOP-7633 provided not rationale (just ""We should add the audit logs as part of default confs"") as to why they were enabled for the packages.",-0.2583333333,-0.2583333333,neutral
hadoop,8395,description,"Text from Display set of Shell commands (hadoop fs -text), has a strict subclass check for a loaded key class to be a subclass of WritableComparable. The sequence file writer itself has no such checks (one can create sequence files with just plain writable keys, comparable is needed for sequence file's sorter alone, which not all of them use always), and hence its not reasonable for Text command to carry it either. We should relax the check and simply just check for ""Writable"", not",design_debt,non-optimal_design,"Fri, 11 May 2012 19:58:28 +0000","Thu, 12 May 2016 18:23:00 +0000","Sat, 12 May 2012 06:04:19 +0000",36351,"Text from Display set of Shell commands (hadoop fs -text), has a strict subclass check for a sequence-file-header loaded key class to be a subclass of WritableComparable. The sequence file writer itself has no such checks (one can create sequence files with just plain writable keys, comparable is needed for sequence file's sorter alone, which not all of them use always), and hence its not reasonable for Text command to carry it either. We should relax the check and simply just check for ""Writable"", not ""WritableComparable"".",0.1453333333,0.1453333333,neutral
hadoop,8895,description,TokenRenewer is a fully abstract class. Making it an interface will allow classes extending other classes to implement the interface.,design_debt,non-optimal_design,"Sat, 6 Oct 2012 03:55:55 +0000","Mon, 3 Nov 2014 18:33:59 +0000","Tue, 9 Oct 2012 18:33:46 +0000",311871,TokenRenewer is a fully abstract class. Making it an interface will allow classes extending other classes to implement the interface.,0.1,0.1,neutral
hadoop,8912,description,Source code in hadoop-common repo has a bunch of files that have CRLF endings. With more development happening on windows there is a higher chance of more CRLF files getting into the source tree. I would like to avoid that by creating .gitattributes file which prevents sources from having CRLF entries in text files. I am adding a couple of links here to give more primer on what exactly is the issue and how we are trying to fix it. # # This issue for adding .gitattributes file to the tree.,design_debt,non-optimal_design,"Wed, 10 Oct 2012 18:09:30 +0000","Wed, 3 Sep 2014 23:07:19 +0000","Fri, 12 Oct 2012 05:02:44 +0000",125594,Source code in hadoop-common repo has a bunch of files that have CRLF endings. With more development happening on windows there is a higher chance of more CRLF files getting into the source tree. I would like to avoid that by creating .gitattributes file which prevents sources from having CRLF entries in text files. I am adding a couple of links here to give more primer on what exactly is the issue and how we are trying to fix it. http://git-scm.com/docs/gitattributes#_checking_out_and_checking_in http://stackoverflow.com/questions/170961/whats-the-best-crlf-handling-strategy-with-git This issue for adding .gitattributes file to the tree.,-0.07142857143,-0.07142857143,negative
hadoop,922,description,A seek on a DFSInputStream causes causes the next read to re-open the socket connection to the datanode and fetch the remainder of the block all over again. This is not optimal. A small read followed by a small positive seek could re-utilize the data already fetched from the datanode as part of the previous read.,design_debt,non-optimal_design,"Wed, 24 Jan 2007 00:13:09 +0000","Wed, 8 Jul 2009 16:42:15 +0000","Tue, 30 Jan 2007 22:47:37 +0000",599668,A seek on a DFSInputStream causes causes the next read to re-open the socket connection to the datanode and fetch the remainder of the block all over again. This is not optimal. A small read followed by a small positive seek could re-utilize the data already fetched from the datanode as part of the previous read.,-0.2976666667,-0.2976666667,negative
hadoop,9259,description,the teardown code in assumes that {{fs!=null}} and that it's OK to throw an exception if the delete operation fails. Better to check the {{fs}} value and catch and convert an exception in the {{fs.delete()}} operation to a {{LOG.error()}} instead. This will stop failures in teardown becoming a distraction from the root causes of the problem (that your FileSystem is broken),design_debt,non-optimal_design,"Wed, 16 Jan 2013 10:53:53 +0000","Thu, 18 Aug 2016 18:35:36 +0000","Mon, 25 Mar 2013 13:13:56 +0000",5883603,the teardown code in FileSystemContractBaseTest assumes that fs!=null and that it's OK to throw an exception if the delete operation fails. Better to check the fs value and catch and convert an exception in the fs.delete() operation to a LOG.error() instead. This will stop failures in teardown becoming a distraction from the root causes of the problem (that your FileSystem is broken),0.02397222222,0.02397222222,neutral
hadoop,9336,description,"Querying is synch'ed and inefficient for short-lived RPC requests. Since the connection already contains the UGI, there should be a means to query it directly and avoid a call to",design_debt,non-optimal_design,"Tue, 26 Feb 2013 20:22:15 +0000","Thu, 12 May 2016 18:21:47 +0000","Thu, 28 Feb 2013 22:07:17 +0000",179102,"Querying UGI.getCurrentUser is synch'ed and inefficient for short-lived RPC requests. Since the connection already contains the UGI, there should be a means to query it directly and avoid a call to UGI.getCurrentUser.",-0.475,-0.2375,negative
hadoop,95,description,"Dfs needs a validation operation similiar to fsck, so that we get to know the files that are corrupted and which data blocks are missing. Dfs namenode also needs to log more specific information such as which block is replication or is deleted. So when something goes wrong, we have a clue what has happened.",design_debt,non-optimal_design,"Sat, 18 Mar 2006 09:31:35 +0000","Wed, 8 Jul 2009 16:41:49 +0000","Wed, 18 Oct 2006 18:31:16 +0000",18521981,"Dfs needs a validation operation similiar to fsck, so that we get to know the files that are corrupted and which data blocks are missing. Dfs namenode also needs to log more specific information such as which block is replication or is deleted. So when something goes wrong, we have a clue what has happened.",-0.2833333333,-0.2833333333,neutral
hadoop,9674,description,"This problem was originally mentioned in discussion on HADOOP-8980. When calling initialization of the server's internal {{Listener}} and {{Reader}} threads happens in the background. This initialization is not guaranteed to complete by the time the caller returns from This may be misleading to a caller that expects the server has been fully initialized. This problem sometimes manifests as a test failure in This test looks at the stack frames of all running threads, expecting to find the {{Listener}} and {{Reader}} threads, but sometimes it doesn't find them.",design_debt,non-optimal_design,"Thu, 27 Jun 2013 04:38:37 +0000","Thu, 12 May 2016 18:21:58 +0000","Sun, 30 Jun 2013 17:36:27 +0000",305870,"This problem was originally mentioned in discussion on HADOOP-8980. When calling RPC#Server#start, initialization of the server's internal Listener and Reader threads happens in the background. This initialization is not guaranteed to complete by the time the caller returns from RPC#Server#start. This may be misleading to a caller that expects the server has been fully initialized. This problem sometimes manifests as a test failure in TestRPC#testStopsAllThreads. This test looks at the stack frames of all running threads, expecting to find the Listener and Reader threads, but sometimes it doesn't find them.",-0.0125,-0.2583333333,negative
hadoop,9748,description,"EnsureInitialized is always sync'ed on the class, when it should only sync if it actually has to initialize.",design_debt,non-optimal_design,"Thu, 18 Jul 2013 17:40:12 +0000","Thu, 12 May 2016 18:27:32 +0000","Fri, 19 Jul 2013 14:08:36 +0000",73704,"EnsureInitialized is always sync'ed on the class, when it should only sync if it actually has to initialize.",0.5,0.5,neutral
hadoop,10139,description,"The document should be understandable to a newcomer because the first place he will go is ""setup a single node"".",documentation_debt,low_quality_documentation,"Mon, 2 Dec 2013 18:54:30 +0000","Thu, 4 Sep 2014 01:16:47 +0000","Thu, 30 Jan 2014 19:31:21 +0000",5099811,"The document should be understandable to a newcomer because the first place he will go is ""setup a single node"".",0.5,0.5,neutral
hadoop,10423,description,"As discussed on the dev mailing lists and MAPREDUCE-4052, we need to update the text of the compatibility policy to discuss a new client combined with an old server.",documentation_debt,outdated_documentation,"Mon, 24 Mar 2014 20:59:54 +0000","Thu, 4 Sep 2014 01:16:46 +0000","Mon, 24 Mar 2014 23:28:26 +0000",8912,"As discussed on the dev mailing lists and MAPREDUCE-4052, we need to update the text of the compatibility policy to discuss a new client combined with an old server.",0.0,0.0,neutral
hadoop,10602,description,"Multiple pages of our documentation have ""Go Back"" links that are broken, because they point to an incorrect relative path.",documentation_debt,low_quality_documentation,"Tue, 13 May 2014 18:44:34 +0000","Thu, 12 May 2016 18:27:29 +0000","Thu, 29 May 2014 17:36:21 +0000",1378307,"Multiple pages of our documentation have ""Go Back"" links that are broken, because they point to an incorrect relative path.",0.15,0.15,negative
hadoop,10915,description,"I've found 4 dead links on the main page of chukwa component. I think a good idea is generate the html with last build , than all links will be updated automatically with packages",documentation_debt,low_quality_documentation,"Thu, 31 Jul 2014 13:55:59 +0000","Thu, 31 Jul 2014 15:44:59 +0000","Thu, 31 Jul 2014 15:44:59 +0000",6540,"I've found 4 dead links on the main page of chukwa component. I think a good idea is generate the html with last build , than all links will be updated automatically with packages",0.088,0.088,neutral
hadoop,11103,description,"RemoteException has a number of undocumented behaviors * has no javadocs on getClassName. Reading the source, the String returned is the classname of the wrapped remote exception. * String) is equivalent to calling String, null) * Constructors allow null for all arguments * Some of the test code doesn't check for correct error codes to correspond with the wrapped exception type * methods don't document when they might return null",documentation_debt,outdated_documentation,"Wed, 17 Sep 2014 19:51:26 +0000","Tue, 30 Aug 2016 01:31:55 +0000","Tue, 19 May 2015 09:13:47 +0000",21043341,"RemoteException has a number of undocumented behaviors o.a.h.ipc.RemoteException has no javadocs on getClassName. Reading the source, the String returned is the classname of the wrapped remote exception. RemoteException(String, String) is equivalent to calling RemoteException(String, String, null) Constructors allow null for all arguments Some of the test code doesn't check for correct error codes to correspond with the wrapped exception type methods don't document when they might return null",-0.03055555556,-0.02291666667,neutral
hadoop,11313,description,"is a good tool to check whether native libraries are loaded correctly. We don't have any docs about this, so we should add it to",documentation_debt,low_quality_documentation,"Tue, 18 Nov 2014 07:50:04 +0000","Fri, 24 Apr 2015 22:49:02 +0000","Sun, 7 Dec 2014 04:14:26 +0000",1628662,"NativeLibraryChecker is a good tool to check whether native libraries are loaded correctly. We don't have any docs about this, so we should add it to NativeLibraries.apt.vm.",0.319,0.2126666667,neutral
hadoop,11844,description,at least on 2.6.0 points to an invalid link to rumen. Need to verify and potentially fix this link in newer releases.,documentation_debt,low_quality_documentation,"Fri, 17 Apr 2015 14:32:33 +0000","Fri, 8 May 2015 17:00:56 +0000","Fri, 8 May 2015 17:00:56 +0000",1823303,SchedulerLoadSimulator at least on 2.6.0 points to an invalid link to rumen. Need to verify and potentially fix this link in newer releases.,0.0,0.0,negative
hadoop,12452,description,The sample code in Tracing.md have some problems. * compilation error by not importing Tracer. * generic options are not reflected because Tracer is initialized before ToolRunner#run. * it may be confusing to use FsShell in example because it has embedded Tracer now.,documentation_debt,outdated_documentation,"Wed, 30 Sep 2015 13:36:25 +0000","Tue, 30 Aug 2016 01:22:49 +0000","Mon, 5 Oct 2015 18:36:22 +0000",449997,The sample code in Tracing.md have some problems. compilation error by not importing Tracer. generic options are not reflected because Tracer is initialized before ToolRunner#run. it may be confusing to use FsShell in example because it has embedded Tracer now.,-0.2474,-0.2474,negative
hadoop,12458,description,Spotted this typo in the code while working on a separate YARN issue. E.g Checked in the whole project. Found a few occurrences of the typo in code/comment. The JIRA is meant to help fix those typos.,documentation_debt,low_quality_documentation,"Sat, 3 Oct 2015 03:15:58 +0000","Tue, 30 Aug 2016 01:22:46 +0000","Sat, 3 Oct 2015 13:10:24 +0000",35666,Spotted this typo in the code while working on a separate YARN issue. E.g DEFAULT_RM_NODEMANAGER_CONNECT_RETIRES Checked in the whole project. Found a few occurrences of the typo in code/comment. The JIRA is meant to help fix those typos.,0.25,0.125,negative
hadoop,13039,description,The RPC server enforces a maximum length on incoming messages. Messages larger than the maximum are rejected immediately. The maximum length can be tuned by setting configuration property but this is not documented in core-site.xml.,documentation_debt,outdated_documentation,"Tue, 19 Apr 2016 22:33:30 +0000","Fri, 6 Jan 2017 01:01:38 +0000","Wed, 27 Apr 2016 03:49:39 +0000",623769,"The RPC server enforces a maximum length on incoming messages. Messages larger than the maximum are rejected immediately. The maximum length can be tuned by setting configuration property ipc.maximum.data.length, but this is not documented in core-site.xml.",0.365625,0.2982142857,neutral
hadoop,14314,description,"Unfortunately, Oracle took down opensolaris.org, so the link is dead. The only replacement I could find with a quick search was this PDF:",documentation_debt,outdated_documentation,"Mon, 17 Apr 2017 21:38:32 +0000","Thu, 23 Aug 2018 13:21:29 +0000","Thu, 23 Aug 2018 13:07:10 +0000",42564518,"Unfortunately, Oracle took down opensolaris.org, so the link is dead. The only replacement I could find with a quick search was this PDF: http://cuddletech.com/opensolaris/osdevref.pdf",-0.3736666667,-0.3736666667,negative
hadoop,15569,description,"The S3A assumed role doc is now where we document the permissions needed to work with buckets # detail the permissions you need for s3guard user and admin ops # and what you need for SSE-KMS This involves me working them out, so presumably get some new stack traces also: fix any errors noted in the doc",documentation_debt,low_quality_documentation,"Thu, 28 Jun 2018 19:19:44 +0000","Tue, 10 Jul 2018 16:58:49 +0000","Tue, 10 Jul 2018 16:58:49 +0000",1028345,"The S3A assumed role doc is now where we document the permissions needed to work with buckets detail the permissions you need for s3guard user and admin ops and what you need for SSE-KMS This involves me working them out, so presumably get some new stack traces also: fix any errors noted in the doc",0.185,0.185,neutral
hadoop,16291,description,Fix some errors in the HDFS Permissions doc. Noticed this when reviewing HADOOP-16251. The FS Permissions seems to mark a lot of permissions as Not Applicable (N/A) when that is not the case. In particular getFileInfo (getFileStatus) checks READ permission bit as it should.,documentation_debt,low_quality_documentation,"Thu, 2 May 2019 20:59:59 +0000","Fri, 3 May 2019 16:34:41 +0000","Fri, 3 May 2019 16:28:04 +0000",70085,"Fix some errors in the HDFS Permissions doc. Noticed this when reviewing HADOOP-16251. The FS Permissions documentation seems to mark a lot of permissions as Not Applicable (N/A) when that is not the case. In particular getFileInfo (getFileStatus) checks READ permission bit here, as it should.",-0.225,-0.225,negative
hadoop,2205,description,HADOOP-1917 changed but did not regenerate and to reflect those changes.,documentation_debt,outdated_documentation,"Wed, 14 Nov 2007 16:06:37 +0000","Wed, 28 Nov 2007 22:32:47 +0000","Thu, 15 Nov 2007 09:09:40 +0000",61383,"HADOOP-1917 changed src/docs/src/documentation/content/xdocs/site.xml, but did not regenerate docs/hdfs_design.html and docs/mailing_lists.html to reflect those changes.",0.0,0.125,neutral
hadoop,3505,description,"There's a couple HOD limitations that really trip up the unwary. Two I've encountered are that hod can't take relative paths on the command line, and that if you pass hod a tarball with a modified /conf, then the cluster will fail mysteriously to be initialized. I don't see any references to either in the documentation, and it'd be great to write this down.",documentation_debt,low_quality_documentation,"Thu, 5 Jun 2008 22:07:49 +0000","Fri, 22 Aug 2008 19:50:40 +0000","Sat, 21 Jun 2008 00:27:40 +0000",1304391,"There's a couple HOD limitations that really trip up the unwary. Two I've encountered are that hod can't take relative paths on the command line, and that if you pass hod a tarball with a modified /conf, then the cluster will fail mysteriously to be initialized. I don't see any references to either in the documentation, and it'd be great to write this down.",-0.082,-0.082,negative
hadoop,4066,description,stop having to duplicate changes in both the readme and wiki This is a change to the README only and as such should not require any QA or anything and obviously no unit tests :),documentation_debt,low_quality_documentation,"Thu, 4 Sep 2008 00:54:33 +0000","Wed, 8 Jul 2009 17:05:05 +0000","Thu, 4 Sep 2008 22:20:22 +0000",77149,stop having to duplicate changes in both the readme and wiki This is a change to the README only and as such should not require any QA or anything and obviously no unit tests,0.1,-0.2,neutral
hadoop,4182,description,"When Text input data is used with streaming, every line is expected to end with a newline. Hadoop results are undefined if input files do not end in a newline. (The results will depend on how files are assigned to mappers.) Example: In streaming if mapper = xargs cat reducer = cat and the input is a two line, where each line is symbolic link in HDFS link1\n link2\n EOF link1 points to a file which contains This is line1EOF link2 points to a file which contains This is line2EOF Now running a streaming job such that, there is only one split, will produce results: This is line1This is line2\t\n But if there were two splits, the result will be This is line1\t\n This is line2\t\n So in summary, the output depends on the factor that how many mappers were invoked. As a caution, it should be recorded in Streaming wiki that users always put a new line at the end of each line to get away with such problems.",documentation_debt,low_quality_documentation,"Tue, 16 Sep 2008 05:36:36 +0000","Wed, 8 Jul 2009 17:05:24 +0000","Tue, 23 Sep 2008 16:58:13 +0000",645697,"When Text input data is used with streaming, every line is expected to end with a newline. Hadoop results are undefined if input files do not end in a newline. (The results will depend on how files are assigned to mappers.) Example: In streaming if mapper = xargs cat reducer = cat and the input is a two line, where each line is symbolic link in HDFS link1\n link2\n EOF link1 points to a file which contains This is line1EOF link2 points to a file which contains This is line2EOF Now running a streaming job such that, there is only one split, will produce results: This is line1This is line2\t\n But if there were two splits, the result will be This is line1\t\n This is line2\t\n So in summary, the output depends on the factor that how many mappers were invoked. As a caution, it should be recorded in Streaming wiki that users always put a new line at the end of each line to get away with such problems.",-0.18,-0.18,neutral
hadoop,4603,description,"A default installation as outlined in the docs won't start on Solaris 10 x86. The ""whoami"" utility is in path ""/usr/ucb"" on Solaris 10, which isn't in the standard PATH environment variable unless the user has added that specifically. The documentation should reflect this. Solaris 10 also seemed to throw NPEs if you didn't explicitly set the IP address to bind the servers to. Simply overriding the IP address fixes the problem.",documentation_debt,outdated_documentation,"Thu, 6 Nov 2008 21:21:29 +0000","Mon, 21 Jul 2014 16:56:19 +0000","Mon, 21 Jul 2014 16:56:19 +0000",179955290,"A default installation as outlined in the docs won't start on Solaris 10 x86. The ""whoami"" utility is in path ""/usr/ucb"" on Solaris 10, which isn't in the standard PATH environment variable unless the user has added that specifically. The documentation should reflect this. Solaris 10 also seemed to throw NPEs if you didn't explicitly set the IP address to bind the servers to. Simply overriding the IP address fixes the problem.",0.1025,0.1025,neutral
hadoop,4611,description,"The documentation for the Tool interface will not work out of the box. It seems to have taken the Sort() implementation in examples, but has ripped out some important information. 1) args[1] and args[2] should probably be args[0] and args[1], as most MapReduce tasks don't take the first argument that examples.jar takes 2) int run() needs to actually return an int 3) and are deprecated. 4) the call to ToolRunner.run() in main() should take ""new MyApp()"" instead of ""Sort()"" as an argument More generally, a working implementation of Tool in the docs would be handy.",documentation_debt,low_quality_documentation,"Fri, 7 Nov 2008 06:37:30 +0000","Sat, 19 Jul 2014 00:04:56 +0000","Sat, 19 Jul 2014 00:04:56 +0000",179688446,"The documentation for the Tool interface will not work out of the box. It seems to have taken the Sort() implementation in examples, but has ripped out some important information. 1) args[1] and args[2] should probably be args[0] and args[1], as most MapReduce tasks don't take the first argument that examples.jar takes 2) int run() needs to actually return an int 3) JobConf.setInputPath() and JobConf.setOutputPath() are deprecated. 4) the call to ToolRunner.run() in main() should take ""new MyApp()"" instead of ""Sort()"" as an argument More generally, a working implementation of Tool in the docs would be handy.",0.07916666667,0.059375,negative
hadoop,524,description,Documentation for contrib modules like streaming and the small jobs benchmark does not appear in the Javadoc.,documentation_debt,low_quality_documentation,"Tue, 12 Sep 2006 22:40:35 +0000","Sat, 3 Feb 2007 03:26:58 +0000","Thu, 4 Jan 2007 19:05:10 +0000",9836675,Documentation for contrib modules like streaming and the small jobs benchmark does not appear in the Javadoc.,0.0,0.0,neutral
hadoop,6134,description,"New Hadoop Common Site Set up site, initial pass. May need to add more content. May need to update some links.",documentation_debt,low_quality_documentation,"Wed, 8 Jul 2009 23:29:45 +0000","Fri, 17 Jul 2009 15:54:54 +0000","Fri, 17 Jul 2009 15:54:54 +0000",750309,"New Hadoop Common Site Set up site, initial pass. May need to add more content. May need to update some links.",0.1666666667,0.1666666667,neutral
hadoop,6536,description,"deletes contents of sym-linked directory when we pass a symlink. If this is the behavior, it should be documented as so. Or it should be changed not to delete the contents of the sym-linked directory.",documentation_debt,low_quality_documentation,"Wed, 3 Feb 2010 05:54:15 +0000","Mon, 12 Dec 2011 06:18:39 +0000","Tue, 20 Jul 2010 05:23:08 +0000",14426933,"FileUtil.fullyDelete(dir) deletes contents of sym-linked directory when we pass a symlink. If this is the behavior, it should be documented as so. Or it should be changed not to delete the contents of the sym-linked directory.",0.0,0.0,neutral
hadoop,6885,description,There are a couple java docs warnings in Groups and,documentation_debt,low_quality_documentation,"Wed, 28 Jul 2010 01:46:25 +0000","Mon, 12 Dec 2011 06:19:06 +0000","Wed, 18 Aug 2010 21:51:40 +0000",1886715,There are a couple java docs warnings in Groups and RefreshUserMappingsProtocol.,-0.6,-0.6,neutral
hadoop,6936,description,has links to : both of which are 404 as of time of filing this issue.,documentation_debt,low_quality_documentation,"Wed, 1 Sep 2010 19:38:57 +0000","Sun, 12 Jun 2011 18:55:25 +0000","Sun, 12 Jun 2011 18:52:36 +0000",24534819,http://wiki.apache.org/hadoop/FAQ#A12//s has links to : http://hadoop.apache.org/core/docs/current/hadoop-default.html#dfs.replication.min http://hadoop.apache.org/common/docs/current/hadoop-default.html#dfs.safemode.threshold.pct both of which are 404 as of time of filing this issue.,0.0,0.0,negative
hadoop,7634,description,The cluster setup docs indicate task-controller.cfg must be owned by the user running TaskTracker but the code checks for root. We should update the docs to reflect the real requirement.,documentation_debt,outdated_documentation,"Wed, 14 Sep 2011 02:47:30 +0000","Wed, 17 Oct 2012 18:27:26 +0000","Mon, 19 Sep 2011 16:18:46 +0000",480676,The cluster setup docs indicate task-controller.cfg must be owned by the user running TaskTracker but the code checks for root. We should update the docs to reflect the real requirement.,0.1666666667,0.1666666667,neutral
hadoop,7919,description,"The following only resides in core-default.xml and doesn't look like its used anywhere at all. At least a grep of the prop name and parts of it does not give me back anything at all. These settings are now configurable via generic Log4J opts, via the shipped log4j.properties file in the distributions.",documentation_debt,low_quality_documentation,"Tue, 13 Dec 2011 20:34:20 +0000","Mon, 5 Mar 2012 02:49:05 +0000","Mon, 2 Jan 2012 06:28:51 +0000",1677271,"The following only resides in core-default.xml and doesn't look like its used anywhere at all. At least a grep of the prop name and parts of it does not give me back anything at all. These settings are now configurable via generic Log4J opts, via the shipped log4j.properties file in the distributions.",-0.1,-0.1,negative
hadoop,8741,description,"Hi, The links from the cluster setup pages to the configuration files are broken. Read-only default configuration should be The same holds for the three configuration : core, hdfs and mapred.",documentation_debt,low_quality_documentation,"Tue, 28 Aug 2012 14:27:13 +0000","Wed, 24 May 2017 09:57:48 +0000","Wed, 24 May 2017 09:57:47 +0000",149455834,"Hi, The links from the cluster setup pages to the configuration files are broken. http://hadoop.apache.org/common/docs/stable/cluster_setup.html Read-only default configuration http://hadoop.apache.org/common/docs/current/core-default.html should be http://hadoop.apache.org/common/docs/r1.0.3/core-default.html The same holds for the three configuration : core, hdfs and mapred.",-0.35,-0.35,negative
hadoop,9267,description,It's not friendly for new users when the command line scripts don't show usage instructions when passed the defacto Unix usage flags. Imagine this sequence of commands: Same applies for the `hdfs` script.,documentation_debt,low_quality_documentation,"Thu, 31 Jan 2013 03:29:16 +0000","Thu, 12 May 2016 18:21:53 +0000","Fri, 22 Feb 2013 18:38:00 +0000",1955324,It's not friendly for new users when the command line scripts don't show usage instructions when passed the defacto Unix usage flags. Imagine this sequence of commands: Same applies for the `hdfs` script.,-0.172,-0.172,negative
hadoop,10427,description,The {{KeyProvider}} API should be thread-safe so it can be used safely in server apps.,requirement_debt,non-functional_requirements_not_fully_satisfied,"Tue, 25 Mar 2014 05:25:48 +0000","Thu, 12 May 2016 18:26:58 +0000","Wed, 9 Apr 2014 19:43:52 +0000",1347484,The KeyProvider API should be thread-safe so it can be used safely in server apps.,0.225,0.225,neutral
hadoop,3394,description,"Here is the current prototype implementation of a distributed free text index using Hadoop based on Doug Cutting's design: There has also been some discussion about this on the Hadoop Wiki: This work is not finished, so it is not intended for inclusion yet. For a description of the contribution and its current status see the report in doc/index.html in the attached archive that gives some details of the implementation. This work was designed as a contrib contribution. However, as there are at least two other projects (Bailey and Katta) with similar goals it seemed a good idea to make this code available for discussion.",requirement_debt,requirement_partially_implemented,"Thu, 15 May 2008 16:13:13 +0000","Thu, 9 Jun 2011 11:26:41 +0000","Thu, 9 Jun 2011 11:26:41 +0000",96750808,"Here is the current prototype implementation of a distributed free text index using Hadoop based on Doug Cutting's design: http://www.mail-archive.com/general@lucene.apache.org/msg00338.html There has also been some discussion about this on the Hadoop Wiki: http://wiki.apache.org/hadoop/DistributedLucene This work is not finished, so it is not intended for inclusion yet. For a description of the contribution and its current status see the report in doc/index.html in the attached archive that gives some details of the implementation. This work was designed as a contrib contribution. However, as there are at least two other projects (Bailey and Katta) with similar goals it seemed a good idea to make this code available for discussion.",0.2802,0.2802,neutral
hadoop,9309,description,"Checking for Snappy support calls native method This method has not been implemented for Windows in hadoop.dll, so it throws",requirement_debt,requirement_partially_implemented,"Thu, 14 Feb 2013 21:18:45 +0000","Thu, 21 Feb 2013 18:58:39 +0000","Thu, 21 Feb 2013 18:45:01 +0000",595576,"Checking for Snappy support calls native method NativeCodeLoader#buildSupportsSnappy. This method has not been implemented for Windows in hadoop.dll, so it throws UnsatisfiedLinkError.",0.2,0.1333333333,neutral
hadoop,10374,description,"There are valid use cases for accessing the InterfaceAudience annotations programatically. In HBase, we are writing a unit test to check whether every class in the client packages are annotated with one of the annotations. Plus, we are also thinking about a golden file to containing all public method sigs, so that we can ensure public facing API-compatibility from a unit test. Related: HBASE-8546, HBASE-10462, HBASE-8275",test_debt,lack_of_tests,"Thu, 27 Feb 2014 21:45:33 +0000","Thu, 4 Sep 2014 01:16:46 +0000","Thu, 27 Feb 2014 23:34:08 +0000",6515,"There are valid use cases for accessing the InterfaceAudience annotations programatically. In HBase, we are writing a unit test to check whether every class in the client packages are annotated with one of the annotations. Plus, we are also thinking about a golden file to containing all public method sigs, so that we can ensure public facing API-compatibility from a unit test. Related: HBASE-8546, HBASE-10462, HBASE-8275",0.1231875,0.1231875,neutral
hadoop,10729,description,"We have ProtocolInfo specified in protocol interface with version info, but we don't have unit test to verify if/how it works. We should have tests to track this annotation work as expectation.",test_debt,lack_of_tests,"Fri, 20 Jun 2014 01:56:56 +0000","Fri, 25 Oct 2019 20:25:47 +0000","Tue, 8 Dec 2015 21:45:09 +0000",46381693,"We have ProtocolInfo specified in protocol interface with version info, but we don't have unit test to verify if/how it works. We should have tests to track this annotation work as expectation.",0.0,0.0,neutral
hadoop,11730,description,"s3n attempts to read again when it encounters IOException during read. But the current logic does not reopen the connection, thus, it ends up with no-op, and committing the wrong(truncated) output. Here's a stack trace as an example. It seems this is a regression, which was introduced by the following optimizations. Also, test cases should be reviewed so that it covers this scenario.",test_debt,low_coverage,"Thu, 19 Mar 2015 23:22:57 +0000","Sat, 31 Oct 2020 00:26:01 +0000","Thu, 23 Apr 2015 20:45:10 +0000",3014533,"s3n attempts to read again when it encounters IOException during read. But the current logic does not reopen the connection, thus, it ends up with no-op, and committing the wrong(truncated) output. Here's a stack trace as an example. 2015-03-13 20:17:24,835 [TezChild] INFO org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor - Starting output org.apache.tez.mapreduce.output.MROutput@52008dbd to vertex scope-12 2015-03-13 20:17:24,866 [TezChild] DEBUG org.jets3t.service.impl.rest.httpclient.HttpMethodReleaseInputStream - Released HttpMethod as its response data stream threw an exception org.apache.http.ConnectionClosedException: Premature end of Content-Length delimited message body (expected: 296587138; received: 155648 at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:184) at org.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:138) at org.jets3t.service.io.InterruptableInputStream.read(InterruptableInputStream.java:78) at org.jets3t.service.impl.rest.httpclient.HttpMethodReleaseInputStream.read(HttpMethodReleaseInputStream.java:146) at org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsInputStream.read(NativeS3FileSystem.java:145) at java.io.BufferedInputStream.read1(BufferedInputStream.java:273) at java.io.BufferedInputStream.read(BufferedInputStream.java:334) at java.io.DataInputStream.read(DataInputStream.java:100) at org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:180) at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216) at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174) at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185) at org.apache.pig.builtin.PigStorage.getNext(PigStorage.java:259) at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:204) at org.apache.tez.mapreduce.lib.MRReaderMapReduce.next(MRReaderMapReduce.java:116) at org.apache.pig.backend.hadoop.executionengine.tez.plan.operator.POSimpleTezLoad.getNextTuple(POSimpleTezLoad.java:106) at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:307) at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNextTuple(POForEach.java:246) at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:307) at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter.getNextTuple(POFilter.java:91) at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:307) at org.apache.pig.backend.hadoop.executionengine.tez.plan.operator.POStoreTez.getNextTuple(POStoreTez.java:117) at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.runPipeline(PigProcessor.java:313) at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.run(PigProcessor.java:192) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) 2015-03-13 20:17:24,867 [TezChild] INFO org.apache.hadoop.fs.s3native.NativeS3FileSystem - Received IOException while reading 'user/hadoop/tsato/readlarge/input/cloudian-s3.log.20141119', attempting to reopen. 2015-03-13 20:17:24,867 [TezChild] DEBUG org.jets3t.service.impl.rest.httpclient.HttpMethodReleaseInputStream - Released HttpMethod as its response data stream is fully consumed 2015-03-13 20:17:24,868 [TezChild] INFO org.apache.tez.dag.app.TaskAttemptListenerImpTezDag - Commit go/no-go request from attempt_1426245338920_0001_1_00_000004_0 2015-03-13 20:17:24,868 [TezChild] INFO org.apache.tez.dag.app.dag.impl.TaskImpl - attempt_1426245338920_0001_1_00_000004_0 given a go for committing the task output. It seems this is a regression, which was introduced by the following optimizations. https://issues.apache.org/jira/browse/HADOOP-10589 https://issues.apache.org/jira/browse/HADOOP-10457 Also, test cases should be reviewed so that it covers this scenario.",-0.005,0.00404290429,negative
hadoop,12485,description,Don't prefer ipv4 stack and instead use ipv6 if it's there. This should mean that tests actually bind on ipv6 if it's there. It should mean that tests running against this branch are more likely to be representative of what would be running in production.,test_debt,lack_of_tests,"Fri, 16 Oct 2015 00:51:26 +0000","Tue, 29 Dec 2015 21:55:21 +0000","Tue, 29 Dec 2015 21:55:21 +0000",6469435,Don't prefer ipv4 stack and instead use ipv6 if it's there. This should mean that tests actually bind on ipv6 if it's there. It should mean that tests running against this branch are more likely to be representative of what would be running in production.,0.0,0.0,neutral
hadoop,13051,description,"On {{branch-2}}, the below is the (incorrect) behaviour today, where paths with special characters get dropped during globStatus calls: Whereas trunk has the right behaviour, subtly fixed via the pattern library change of HADOOP-12436: (I've placed a ^M explicitly to indicate presence of the intentional hidden character) We should still add a simple test-case to cover this situation for future regressions.",test_debt,lack_of_tests,"Fri, 22 Apr 2016 12:55:57 +0000","Thu, 12 May 2016 18:27:42 +0000","Thu, 5 May 2016 21:23:12 +0000",1153635,"On branch-2, the below is the (incorrect) behaviour today, where paths with special characters get dropped during globStatus calls: Whereas trunk has the right behaviour, subtly fixed via the pattern library change of HADOOP-12436: (I've placed a ^M explicitly to indicate presence of the intentional hidden character) We should still add a simple test-case to cover this situation for future regressions.",0.2026666667,0.2026666667,neutral
hadoop,3077,description,"HADOOP-2899 and HADOOP-2936 introduced minor problems in their unit tests that should be fixed. As per HADOOP-2899: There's an incorrect hardcoded error message. Also, a test data uses a user name which should be changed. As per HADOOP-2936: Temporary testing directories are being left behind after some tests run.",test_debt,low_coverage,"Mon, 24 Mar 2008 05:22:30 +0000","Wed, 18 May 2011 22:03:39 +0000","Wed, 18 May 2011 22:03:39 +0000",99420069,"HADOOP-2899 and HADOOP-2936 introduced minor problems in their unit tests that should be fixed. As per HADOOP-2899: There's an incorrect hardcoded error message. Also, a test data uses a user name which should be changed. As per HADOOP-2936: Temporary testing directories are being left behind after some tests run.",-0.325,-0.325,negative
hadoop,6730,description,"Thanks to Eli, He noticed that there is no test for FileContext#Copy operation. On further investigation with the help of Sanjay we found that there is bug in *FileStatus dstFs = should be in try...catch block.",test_debt,low_coverage,"Wed, 28 Apr 2010 18:50:09 +0000","Mon, 12 Dec 2011 06:19:03 +0000","Sat, 1 May 2010 21:09:36 +0000",267567,"Thanks to Eli, He noticed that there is no test for FileContext#Copy operation. On further investigation with the help of Sanjay we found that there is bug in FileContext#checkDest. FileStatus dstFs = getFileStatus(dst); should be in try...catch block.",0.2791666667,0.209375,negative
hadoop,7118,description,"In HADOOP-7082 I stupidly introduced a regression whereby will throw an NPE if it is called before any .get() call is made, since the properties member is not initialized. This is causing a failure in on my box, but doesn't appear to trigger any failures in the non-contrib tests since .get() is usually called first. This JIRA is to fix the bug and add a unit test for writeXml in common (apparently it never had a unit test)",test_debt,lack_of_tests,"Tue, 25 Jan 2011 22:51:30 +0000","Mon, 12 Dec 2011 06:19:06 +0000","Wed, 26 Jan 2011 07:08:28 +0000",29818,"In HADOOP-7082 I stupidly introduced a regression whereby Configuration.writeXml will throw an NPE if it is called before any .get() call is made, since the properties member is not initialized. This is causing a failure in TestCapacitySchedulerWithJobTracker on my box, but doesn't appear to trigger any failures in the non-contrib tests since .get() is usually called first. This JIRA is to fix the bug and add a unit test for writeXml in common (apparently it never had a unit test)",-0.08,-0.06666666667,negative
hadoop,7946,description,"Hi friends, I am writing the map-reduce using scripting languages(Ruby).I am planning to write unit test cases in ruby.In ruby i can test the input and expected output by using local file system but i am feeling it is not right way.Is there any way to test real map-reduce running on hdfs . Can you suggest me any testing framework is there for scripting languages.",test_debt,lack_of_tests,"Mon, 2 Jan 2012 10:07:58 +0000","Mon, 2 Jan 2012 12:30:51 +0000","Mon, 2 Jan 2012 11:48:18 +0000",6020,"Hi friends, I am writing the map-reduce using scripting languages(Ruby).I am planning to write unit test cases in ruby.In ruby i can test the input and expected output by using local file system but i am feeling it is not right way.Is there any way to test real map-reduce running on hdfs . Can you suggest me any testing framework is there for scripting languages.",-0.0327,-0.0327,neutral
hadoop,9791,description,We've seen historically that paths longer than 260 chars can cause things not to work on Windows if not properly handled. Filing a tracking Jira to add a native io test case with long paths for new FileUtil access check methods added with HADOOP-9413.,test_debt,low_coverage,"Tue, 30 Jul 2013 02:20:21 +0000","Thu, 12 May 2016 18:27:41 +0000","Thu, 19 Sep 2013 07:10:36 +0000",4423815,We've seen historically that paths longer than 260 chars can cause things not to work on Windows if not properly handled. Filing a tracking Jira to add a native io test case with long paths for new FileUtil access check methods added with HADOOP-9413.,-0.3,-0.3,negative
hbase,22837,description,"Currently, explanation about *Custom WAL Directory* configuration is a sub-topic of *Bulk Loading,* chapter, yet this subject has not much relation with bulk loading at all. It should rather be moved to a sub-section of the *Write Ahead Log (WAL)* chapter.",architecture_debt,violation_of_modularity,"Mon, 12 Aug 2019 18:42:08 +0000","Thu, 15 Aug 2019 08:12:57 +0000","Tue, 13 Aug 2019 09:34:02 +0000",53514,"Currently, explanation aboutCustom WAL Directoryconfiguration is a sub-topic ofBulk Loading,chapter, yet this subject has not much relation with bulk loading at all. It should rather be moved to a sub-section of theWrite Ahead Log (WAL)chapter.",0.0,0.0,neutral
hbase,414,description,Let's move all the client classes into the o.a.h.h.client package. Files to move: * HTable * HBaseAdmin * HConnection * HConnectionManager Is there anything else I am missing? Obviously a lot of the tests will get moved too.,architecture_debt,violation_of_modularity,"Wed, 6 Feb 2008 06:50:37 +0000","Fri, 22 Aug 2008 21:13:07 +0000","Fri, 15 Feb 2008 00:29:31 +0000",754734,Let's move all the client classes into the o.a.h.h.client package. Files to move: HTable HBaseAdmin HConnection HConnectionManager Is there anything else I am missing? Obviously a lot of the tests will get moved too.,-0.1333333333,-0.1333333333,neutral
hbase,8103,description,Site and info plugins are too old. The site plugin is in the wrong place. Can't generate reports w/o update and move of location.,architecture_debt,using_obsolete_technology,"Thu, 14 Mar 2013 05:52:38 +0000","Fri, 5 Apr 2013 01:00:48 +0000","Thu, 14 Mar 2013 05:54:55 +0000",137,Site and info plugins are too old. The site plugin is in the wrong place. Can't generate reports w/o update and move of location.,-0.08333333333,-0.08333333333,negative
hbase,15732,description,"{{hbase-rsgroup}} is a new module that does not appear in the assembly. The binary tarball still contains the jars through dependencies, but we need the test-jar as well for running the  can you take a quick look.",build_debt,build_others,"Thu, 28 Apr 2016 18:54:15 +0000","Fri, 29 Apr 2016 03:53:41 +0000","Fri, 29 Apr 2016 00:15:36 +0000",19281,"hbase-rsgroup is a new module that does not appear in the assembly. The binary tarball still contains the jars through dependencies, but we need the test-jar as well for running the IntegrationTestRSGroup. toffer can you take a quick look.",0.228,0.152,neutral
hbase,22038,description,"When building the hbase c++ client with Dockerfile, it fails, because of the url resources not found. But this patch just solve the problem in temporary, cos when some dependent libraries are removed someday, the failure will appear again. Maybe a base docker image which contains all these dependencies maintained by us is required in the long run.",build_debt,build_others,"Tue, 12 Mar 2019 04:16:03 +0000","Thu, 23 Jun 2022 20:38:12 +0000","Wed, 20 Mar 2019 03:35:10 +0000",688747,"When building the hbase c++ client with Dockerfile, it fails, because of the url resources not found. But this patch just solve the problem in temporary, cos when some dependent libraries are removed someday, the failure will appear again. Maybe a base docker image which contains all these dependencies maintained by us is required in the long run.",-0.01233333333,-0.01233333333,negative
hbase,9479,description,"Right now, HBase contains so many dependencies, that using the most basic HBase functionality such as HConnection in a larger application is unreasonably hard. For example, trying to include HBase connectivity in a Spring web app leads to hundreds of JarClassLoader errors such as: Why is this all bundled together? Why not have an ""hbase-client"" or ""hbase-client-dev"" package which is friendly for creating applications? I have spent 2+ days attempting to run a web service which is backed by HBase with no luck. I have created several stack overflow questions: The use of BeanUtils is also known to have a very bad issue: ""The three jars contain wrong classes"" Why is this so difficult? How do I include what I need to make an HBase app. So far I have tried using Maven, but this approach is draconian, and I have not succeeded. Am I Pwned?",build_debt,build_others,"Mon, 9 Sep 2013 20:35:23 +0000","Thu, 16 Jun 2022 18:04:00 +0000","Mon, 9 Sep 2013 21:22:21 +0000",2818,"Right now, HBase contains so many dependencies, that using the most basic HBase functionality such as HConnection in a larger application is unreasonably hard. For example, trying to include HBase connectivity in a Spring web app leads to hundreds of JarClassLoader errors such as: Why is this all bundled together? Why not have an ""hbase-client"" or ""hbase-client-dev"" package which is friendly for creating applications? I have spent 2+ days attempting to run a web service which is backed by HBase with no luck. I have created several stack overflow questions: http://stackoverflow.com/questions/18703903/java-massive-class-collision http://stackoverflow.com/questions/18690582/how-to-create-jetty-spring-app-with-hbase-connection The use of BeanUtils is also known to have a very bad issue: ""The three jars contain wrong classes"" https://issues.apache.org/jira/browse/BEANUTILS-398 Why is this so difficult? How do I include what I need to make an HBase app. So far I have tried using Maven, but this approach is draconian, and I have not succeeded. Am I Pwned?",-0.1365625,-0.1365625,negative
hbase,10001,description,"We have a mockup to test only the client. If we want to include the network, without beeing limited by the i/o, we don't have much tools. This coprocessor helps to test this. I put it in the main code as to make it usable without adding a jar... I don't think it's possible avoid the WAL writes in the coprocessors. It would be great to have it to simplify the test with any kind of client (i.e. w/o changing the durability).",code_debt,complex_code,"Tue, 19 Nov 2013 15:13:42 +0000","Wed, 18 Dec 2013 22:53:27 +0000","Wed, 20 Nov 2013 11:40:51 +0000",73629,"We have a mockup to test only the client. If we want to include the network, without beeing limited by the i/o, we don't have much tools. This coprocessor helps to test this. I put it in the main code as to make it usable without adding a jar... I don't think it's possible avoid the WAL writes in the coprocessors. It would be great to have it to simplify the test with any kind of client (i.e. w/o changing the durability).",0.28,0.28,neutral
hbase,10562,description,"See parent. This test was removed from 0.95.2. While investigating while it fails in 0.94 with Hadoop2 I found it is extremely slow and uses up a *lot* of threads. , any input?",code_debt,multi-thread_correctness,"Tue, 18 Feb 2014 02:05:41 +0000","Wed, 26 Feb 2014 04:45:39 +0000","Tue, 18 Feb 2014 02:24:17 +0000",1116,"See parent. This test was removed from 0.95.2. While investigating while it fails in 0.94 with Hadoop2 I found it is extremely slow and uses up a lot of threads. stack, any input?",-0.1333333333,-0.1,negative
hbase,10631,description,"There is an extra seek(0) on FileLink open, that we can skip",code_debt,low_quality_code,"Fri, 28 Feb 2014 01:21:25 +0000","Thu, 14 Aug 2014 20:20:07 +0000","Fri, 28 Feb 2014 09:21:20 +0000",28795,"There is an extra seek(0) on FileLink open, that we can skip",0.0,0.0,neutral
hbase,11621,description,Daryn proposed the following change in HDFS-6773: With this change in runtime for TestAdmin went from 8:35 min to 7 min,code_debt,slow_algorithm,"Wed, 30 Jul 2014 22:36:24 +0000","Fri, 6 Apr 2018 17:51:13 +0000","Thu, 31 Jul 2014 16:46:45 +0000",65421,"Daryn proposed the following change in HDFS-6773: With this change in HBaseTestingUtility#startMiniDFSCluster(), runtime for TestAdmin went from 8:35 min to 7 min",0.0,0.0,neutral
hbase,12106,description,Test annotation interfaces used to be under then moved to We should move them to,code_debt,low_quality_code,"Fri, 26 Sep 2014 21:11:49 +0000","Fri, 6 Apr 2018 17:54:44 +0000","Tue, 7 Oct 2014 07:42:21 +0000",901832,Test annotation interfaces used to be under hbase-common/src/test then moved to hbase-annotations/src/main. We should move them to hbase-annotations/src/test.,0.0,0.0,neutral
hbase,12833,description,TestShell is erring out (timeout) consistently for me. Culprit is OOM cannot create native thread. It looks to me like test_table.rb and hbase/table.rb are made for leaking connections. table calls for every table but provides no close() method to clean it up. test_table creates a new table with every test.,code_debt,low_quality_code,"Fri, 9 Jan 2015 20:40:05 +0000","Fri, 6 Apr 2018 17:55:48 +0000","Fri, 16 Jan 2015 19:55:32 +0000",602127,TestShell is erring out (timeout) consistently for me. Culprit is OOM cannot create native thread. It looks to me like test_table.rb and hbase/table.rb are made for leaking connections. table calls ConnectionFactory.createConnection() for every table but provides no close() method to clean it up. test_table creates a new table with every test.,0.1464285714,0.128125,negative
hbase,12905,description,"while working on HBASE-12898, hadoop-annotations showed up as an undeclared dependency. Looks like a hand full of hadoop InterfaceAudience annotations made it in through backports.",code_debt,low_quality_code,"Thu, 22 Jan 2015 17:14:03 +0000","Fri, 20 Nov 2015 11:56:05 +0000","Thu, 22 Jan 2015 18:49:23 +0000",5720,"while working on HBASE-12898, hadoop-annotations showed up as an undeclared dependency. Looks like a hand full of hadoop InterfaceAudience annotations made it in through backports.",0.3,0.3,neutral
hbase,1298,description,"is a key in my ""userdata"" table which happens to be the start key for a region named lists a link to which is incorrect because the "" + "" character is not properly URI Encoded. This results in a misleading user error message: ""This region is no longer available. It may be due to a split, a merge or the name changed. "" manually escaping the "" + "" character as ""%2B"" produces the correct output. A quick skim of master.jsp suggests it has a similar problem: it doesn't URI Encode table names when constructing links to table.jsp",code_debt,low_quality_code,"Mon, 30 Mar 2009 17:43:35 +0000","Sun, 13 Sep 2009 22:24:31 +0000","Fri, 3 Apr 2009 14:18:40 +0000",333305,"""UAZAAAAAZNaGnEKI+gC"" is a key in my ""userdata"" table which happens to be the start key for a region named ""userdata,UAZAAAAAZNaGnEKI+gC,1238170268268"" ""/table.jsp?name=userdata"" lists a link to ""/regionhistorian.jsp?regionname=userdata,UAZAAAAAZNaGnEKI+gC,1238170268268"" which is incorrect because the "" + "" character is not properly URI Encoded. This results in a misleading user error message: ""This region is no longer available. It may be due to a split, a merge or the name changed. "" manually escaping the "" + "" character as ""%2B"" produces the correct output. A quick skim of master.jsp suggests it has a similar problem: it doesn't URI Encode table names when constructing links to table.jsp",0.1160714286,0.07386363636,negative
hbase,1337,description,When comparing timestamps in the KeyValues tests have shown that it is faster to do a byte[] compare on the timestamps rather than converting to longs and then do the long compare. In the case where you have one byte[] and one long that you need to compare it is better to convert both to long and do the long compare. Some numbers: Compare byte[] to byte[] timer 1042 ns Compare bytes2long to bytes2long timer 3143 ns Compare long to long timer 281 ns bytes2long timer 1328 ns long2bytes timer 1349 ns,code_debt,slow_algorithm,"Wed, 22 Apr 2009 20:57:30 +0000","Sat, 11 Jun 2022 20:35:55 +0000","Wed, 17 Jun 2009 16:09:38 +0000",4821128,When comparing timestamps in the KeyValues tests have shown that it is faster to do a byte[] compare on the timestamps rather than converting to longs and then do the long compare. In the case where you have one byte[] and one long that you need to compare it is better to convert both to long and do the long compare. Some numbers: Compare byte[] to byte[] timer 1042 ns Compare bytes2long to bytes2long timer 3143 ns Compare long to long timer 281 ns bytes2long timer 1328 ns long2bytes timer 1349 ns,0.1666666667,0.1666666667,neutral
hbase,13395,description,"This class is marked as deprecated, probably can remove it, and if any methods from this specific class are in active use, need to decide what to do on callers' side. Should be able to replace with just Table interface usage.",code_debt,dead_code,"Fri, 3 Apr 2015 09:37:09 +0000","Thu, 16 Jun 2022 18:18:32 +0000","Sat, 25 Mar 2017 16:09:04 +0000",62404315,"This class is marked as deprecated, probably can remove it, and if any methods from this specific class are in active use, need to decide what to do on callers' side. Should be able to replace with just Table interface usage.",0.444,0.444,neutral
hbase,1359,description,"If you see I removed and ip or something for security reasons Once I truncate the table, hbase freaks out for about 10 seconds and all the thrift servers die. Thrift server log: 2009-04-02 12:09:08,971 INFO Retrying connect to server: /:60020. Already tried 0 time(s). You see this a bunch of times and then it times out The hbase shell 13:01:08 INFO Quorum servers: Truncating t2; it may take a while Disabling table... 09/04/30 13:01:19 INFO client.HBaseAdmin: Disabled t2 0 row(s) in 10.3417 seconds Dropping table... 09/04/30 13:01:19 INFO client.HBaseAdmin: Deleted t2 0 row(s) in 0.1592 seconds Creating table... 0 row(s) in 14.7567 seconds undefined local variable or method `lsit' for #<Object:0x3bbe9a50 from (hbase):3 undefined local variable or method `lsit' for #<Object:0x3bbe9a50 from (hbase):4 null from `proc essRow' from `metaScan' from `metaScan' from `list Tables' from `listTables' from `invoke0' from `invoke' from `invoke' from `invoke' from ndling' from `invoke' from `call' from `cacheAndCal l' from `call' from `interpret' from `interpret' ... 113 levels... from `call' from `call ' from `call' from `cacheAndCal l' from `call' from `__file_ _' from `__file__ ' from `load' from `runScript' from `runNormally' from `runFromMain' from `run' from `run' from `main' from `list' from",code_debt,slow_algorithm,"Thu, 30 Apr 2009 17:39:21 +0000","Sun, 13 Sep 2009 22:24:34 +0000","Mon, 20 Jul 2009 22:03:21 +0000",7014240,"If you see **** I removed and ip or something for security reasons Once I truncate the table, hbase freaks out for about 10 seconds and all the thrift servers die. Thrift server log: 2009-04-02 12:09:08,971 INFO org.apache.hadoop.ipc.HBaseClass: Retrying connect to server: /*****:60020. Already tried 0 time(s). You see this a bunch of times and then it times out The hbase shell nhbase(main):001:0> truncate 't2' 09/04/30 13:01:08 INFO zookeeper.ZooKeeperWrapper: Quorum servers: **** Truncating t2; it may take a while Disabling table... 09/04/30 13:01:19 INFO client.HBaseAdmin: Disabled t2 0 row(s) in 10.3417 seconds Dropping table... 09/04/30 13:01:19 INFO client.HBaseAdmin: Deleted t2 0 row(s) in 0.1592 seconds Creating table... 0 row(s) in 14.7567 seconds hbase(main):002:0> lsit NameError: undefined local variable or method `lsit' for #<Object:0x3bbe9a50> from (hbase):3 hbase(main):003:0> lsit NameError: undefined local variable or method `lsit' for #<Object:0x3bbe9a50> from (hbase):4 hbase(main):004:0> list NativeException: java.lang.NullPointerException: null from org/apache/hadoop/hbase/client/HConnectionManager.java:344:in `proc essRow' from org/apache/hadoop/hbase/client/MetaScanner.java:64:in `metaScan' from org/apache/hadoop/hbase/client/MetaScanner.java:29:in `metaScan' from org/apache/hadoop/hbase/client/HConnectionManager.java:351:in `list Tables' from org/apache/hadoop/hbase/client/HBaseAdmin.java:121:in `listTables' from sun/reflect/NativeMethodAccessorImpl.java:-2:in `invoke0' from sun/reflect/NativeMethodAccessorImpl.java:39:in `invoke' from sun/reflect/DelegatingMethodAccessorImpl.java:25:in `invoke' from java/lang/reflect/Method.java:597:in `invoke' from org/jruby/javasupport/JavaMethod.java:298:in `invokeWithExceptionHa ndling' from org/jruby/javasupport/JavaMethod.java:259:in `invoke' from org/jruby/java/invokers/InstanceMethodInvoker.java:36:in `call' from org/jruby/runtime/callsite/CachingCallSite.java:260:in `cacheAndCal l' from org/jruby/runtime/callsite/CachingCallSite.java:75:in `call' from org/jruby/ast/CallNoArgNode.java:61:in `interpret' from org/jruby/ast/ForNode.java:101:in `interpret' ... 113 levels... from org/jruby/internal/runtime/methods/DynamicMethod.java:226:in `call' from org/jruby/internal/runtime/methods/CompiledMethod.java:216:in `call ' from org/jruby/internal/runtime/methods/CompiledMethod.java:71:in `call' from org/jruby/runtime/callsite/CachingCallSite.java:260:in `cacheAndCal l' from org/jruby/runtime/callsite/CachingCallSite.java:75:in `call' from home/fds/ts/hadoop/hbase/bin/$dot_dot/bin/hirb.rb:441:in `_file _' from home/fds/ts/hadoop/hbase/bin/$dot_dot/bin/hirb.rb:-1:in `_file_ ' from home/fds/ts/hadoop/hbase/bin/$dot_dot/bin/hirb.rb:-1:in `load' from org/jruby/Ruby.java:564:in `runScript' from org/jruby/Ruby.java:467:in `runNormally' from org/jruby/Ruby.java:340:in `runFromMain' from org/jruby/Main.java:214:in `run' from org/jruby/Main.java:100:in `run' from org/jruby/Main.java:84:in `main' from /home/fds/ts/hadoop/hbase/bin/../bin/hirb.rb:300:in `list' from (hbase):5hbase(main):005:0> hbase(main):006:0*",-0.2135,-0.04650649351,negative
hbase,14494,description,"noticed that the grant shell command was missing commas in-between the method arguments in the usage/help messages. After taking a look at some of the others, it's not alone. I'll make a pass over the shell commands and try to find some more help messages which tell the user to run a bad command.",code_debt,low_quality_code,"Fri, 25 Sep 2015 21:44:37 +0000","Tue, 16 Jan 2018 12:31:51 +0000","Thu, 1 Oct 2015 19:07:03 +0000",508946,"enis noticed that the grant shell command was missing commas in-between the method arguments in the usage/help messages. After taking a look at some of the others, it's not alone. I'll make a pass over the shell commands and try to find some more help messages which tell the user to run a bad command.",0.2222222222,0.2222222222,negative
hbase,14677,description,"BucketCache implementation of freeBlock has a bad performance / concurrency and affects bulk block evictions from cache on a file close (after split, compactions or region moves).",code_debt,slow_algorithm,"Thu, 22 Oct 2015 17:46:57 +0000","Thu, 22 Oct 2015 20:33:03 +0000","Thu, 22 Oct 2015 17:49:09 +0000",132,"BucketCache implementation of freeBlock has a bad performance / concurrency and affects bulk block evictions from cache on a file close (after split, compactions or region moves).",-0.4,-0.4,negative
hbase,15490,description,"Currently there're two in our branch-1 code base (one in package, the other in and both are in use. This is a regression of HBASE-14969 and only exists in branch-1. We should remove the one in and change the default compaction throughput controller back to to keep compatible with previous branch-1 version Thanks  for pointing out the issue.",code_debt,duplicated_code,"Sat, 19 Mar 2016 18:11:32 +0000","Mon, 21 Mar 2016 10:51:03 +0000","Sun, 20 Mar 2016 08:20:29 +0000",50937,"Currently there're two CompactionThroughputControllerFactory in our branch-1 code base (one in o.a.h.h.regionserver.compactions package, the other in o.a.h.h.regionserver.throttle) and both are in use. This is a regression of HBASE-14969 and only exists in branch-1. We should remove the one in o.a.h.h.regionserver.compactions, and change the default compaction throughput controller back to NoLimitThroughputController to keep compatible with previous branch-1 version Thanks ghelmling for pointing out the issue.",0.05833333333,0.02916666667,neutral
hbase,15704,description,This class is in backup package (as well as backup/examples classes) but is not backup - related. Remove examples classes from a codebase,code_debt,low_quality_code,"Mon, 25 Apr 2016 19:09:20 +0000","Fri, 1 Jul 2022 20:14:53 +0000","Thu, 24 Aug 2017 20:32:02 +0000",41995362,This class is in backup package (as well as backup/examples classes) but is not backup - related. Remove examples classes from a codebase,0.3155,0.3155,negative
hbase,1655,description,"A discussion on the HBase user mailing list led to some suggested improvements for the class. I will be submitting a patch that contains the following changes to HTablePool: * Remove constructors that were not used. * Change access to remaining contstructor from public to private to enforce use of the static factory method getPool. * Change internal map from TreeMap to HashMap because I couldn't see any reason it needed to be sorted. * Remove HBaseConfiguration and tableName member variables since they aren't really properties of the pool itself. They are associated with the HTable that should get instantiated when one is requested from the pool, but not already there.",code_debt,dead_code,"Tue, 14 Jul 2009 19:52:59 +0000","Wed, 12 Aug 2020 23:02:36 +0000","Fri, 17 Jul 2009 19:26:30 +0000",257611,"A discussion on the HBase user mailing list (http://markmail.org/thread/7leeha56ny5mwecg) led to some suggested improvements for the org.apache.hadoop.hbase.client.HTablePool class. I will be submitting a patch that contains the following changes to HTablePool: Remove constructors that were not used. Change access to remaining contstructor from public to private to enforce use of the static factory method getPool. Change internal map from TreeMap to HashMap because I couldn't see any reason it needed to be sorted. Remove HBaseConfiguration and tableName member variables since they aren't really properties of the pool itself. They are associated with the HTable that should get instantiated when one is requested from the pool, but not already there.",-0.1458333333,-0.07954545455,neutral
hbase,17184,description,"Initially I only wanted to fix a badly worded Log message but found a bunch of code convention violations etc. so I decided to clean up the class. It is only whitespace changes with the exception of the one log message (""too many"").",code_debt,low_quality_code,"Mon, 28 Nov 2016 10:00:06 +0000","Sat, 3 Dec 2016 00:49:12 +0000","Wed, 30 Nov 2016 20:58:05 +0000",212279,"Initially I only wanted to fix a badly worded Log message but found a bunch of code convention violations etc. so I decided to clean up the class. It is only whitespace changes with the exception of the one log message (""too many"").",0.01875,0.01875,negative
hbase,17383,description,Currently we get this log Here the global offheap memstore size is greater than the blocking size. The memstore heap overhead need not be included in this log unless the higher water mark breach is only due to the heap overhead.,code_debt,low_quality_code,"Wed, 28 Dec 2016 10:14:44 +0000","Thu, 22 Mar 2018 07:28:21 +0000","Thu, 22 Mar 2018 07:28:21 +0000",38783617,Currently we get this log Here the global offheap memstore size is greater than the blocking size. The memstore heap overhead need not be included in this log unless the higher water mark breach is only due to the heap overhead.,-0.125,-0.125,neutral
hbase,17480,description,"HBASE-14551 moves the split region logic to the master-side. With the code in HBASE-14551, the split transaction code from region server side became un-used. There is no need to keep region_server-side split region code. We should remove them to avoid code duplication.",code_debt,dead_code,"Tue, 17 Jan 2017 23:08:57 +0000","Fri, 17 Jun 2022 18:05:45 +0000","Thu, 19 Jan 2017 17:05:56 +0000",151019,"HBASE-14551 moves the split region logic to the master-side. With the code in HBASE-14551, the split transaction code from region server side became un-used. There is no need to keep region_server-side split region code. We should remove them to avoid code duplication.",0.075,0.075,neutral
hbase,18085,description,"Parallel purge in ObjectPool is meaningless and will cause contention issue since has synchronization (source code shown below) We observed threads blocking on the purge method while using offheap bucket cache, and we could easily reproduce this by testing the 100% cache hit case in bucket cache with enough reading threads. We propose to add a purgeLock and use tryLock to avoid parallel purge.",code_debt,multi-thread_correctness,"Fri, 19 May 2017 19:04:08 +0000","Wed, 1 Aug 2018 06:24:22 +0000","Wed, 24 May 2017 07:57:01 +0000",391973,"Parallel purge in ObjectPool is meaningless and will cause contention issue since ReferenceQueue#poll has synchronization (source code shown below) We observed threads blocking on the purge method while using offheap bucket cache, and we could easily reproduce this by testing the 100% cache hit case in bucket cache with enough reading threads. We propose to add a purgeLock and use tryLock to avoid parallel purge.",-0.1166666667,-0.1166666667,negative
hbase,18092,description,Removing a peer does not clean up the associated metrics and state from walsById map in the,code_debt,low_quality_code,"Mon, 22 May 2017 18:47:28 +0000","Wed, 1 Aug 2018 06:22:22 +0000","Fri, 9 Jun 2017 18:15:45 +0000",1553297,Removing a peer does not clean up the associated metrics and state from walsById map in the ReplicationSourceManager.,-0.4,-0.4,negative
hbase,18501,description,"We need to do some cleanup for the *public* class as much as possible. Otherwise, the HTD and HCD may linger in the code base for a long time.",code_debt,low_quality_code,"Wed, 2 Aug 2017 09:59:59 +0000","Wed, 21 Mar 2018 22:12:12 +0000","Sat, 26 Aug 2017 12:09:25 +0000",2081366,"We need to do some cleanup for the public class as much as possible. Otherwise, the HTD and HCD may linger in the code base for a long time.",0.0,0.0,neutral
hbase,19183,description,Currently the modules hbase-checkstyle and hbase-error-prone define the groupId redundantly. Remove the groupId from these POMs.,code_debt,low_quality_code,"Sat, 4 Nov 2017 21:59:42 +0000","Tue, 7 Nov 2017 12:10:59 +0000","Tue, 7 Nov 2017 07:22:19 +0000",206557,Currently the modules hbase-checkstyle and hbase-error-prone define the groupId redundantly. Remove the groupId from these POMs.,-0.2,-0.2,negative
hbase,19373,description,Fix the remaining Checkstyle error regarding line length in the *hbase-annotations* module.,code_debt,low_quality_code,"Wed, 29 Nov 2017 11:11:15 +0000","Fri, 17 Jun 2022 18:29:54 +0000","Fri, 8 Dec 2017 22:26:20 +0000",818105,Fix the remaining Checkstyle error regarding line length in the hbase-annotations module.,-0.4,-0.4,neutral
hbase,19478,description,Currently issues one Get per WAL file: This is rather inefficient considering the number of WAL files in production can get quite large. We should use multi-get to reduce the number of calls to backup table (which normally resides on another server).,code_debt,slow_algorithm,"Sun, 10 Dec 2017 17:10:15 +0000","Mon, 1 Jan 2018 17:45:29 +0000","Mon, 1 Jan 2018 14:55:59 +0000",1892744,Currently BackupLogCleaner#getDeletableFiles() issues one Get per WAL file: This is rather inefficient considering the number of WAL files in production can get quite large. We should use multi-get to reduce the number of calls to backup table (which normally resides on another server).,-0.375,-0.375,negative
hbase,1990,description,"Consider the following client code... byte b[] = result.getValue( ); put.add( Bytes.toBytes( ""value"") ); ... the requirement to supply family and qualifiers as bytes causes code to get cluttered and verbose. At worst, it scares peoples un-necessarily about HBase development, and at best, developers inevitably will get tired of doing all this casting and then add their own wrapper classes around the HBase client to make their code more readable. I would like to see something like this in the API... byte b[] = result.getValue( ""family""), ""qualifier"" ); put.add( ""family"", ""qualifer"", Bytes.toBytes( ""value"") ); ... where the Hbase client can perform the required Bytes.toBytes() conversion behind the scenes.",code_debt,complex_code,"Wed, 18 Nov 2009 21:48:59 +0000","Sat, 11 Jun 2022 23:15:15 +0000","Mon, 12 May 2014 00:40:37 +0000",141274298,"Consider the following client code... byte b[] = result.getValue( Bytes.toBytes(""family""), Bytes.toBytes(""qualifier"") ); put.add( Bytes.toBytes(""family""), Bytes.toBytes(""qualifer""), Bytes.toBytes( ""value"") ); ... the requirement to supply family and qualifiers as bytes causes code to get cluttered and verbose. At worst, it scares peoples un-necessarily about HBase development, and at best, developers inevitably will get tired of doing all this casting and then add their own wrapper classes around the HBase client to make their code more readable. I would like to see something like this in the API... byte b[] = result.getValue( ""family""), ""qualifier"" ); put.add( ""family"", ""qualifer"", Bytes.toBytes( ""value"") ); ... where the Hbase client can perform the required Bytes.toBytes() conversion behind the scenes.",-0.0575,-0.04107142857,negative
hbase,20975,description,"Find this one when investigating HBASE-20921, too. Here is some code from executeRollback in You can see my comments in the code above, reuseLock can cause the procedure executing(rollback) without a lock. Though I haven't found any bugs introduced by this issue, it is indeed a potential bug need to fix. I think we can just remove the reuseLock logic. Acquire and release lock every time. Find another case that during rolling back, the procedure's lock may not be released properly: see comment:",code_debt,multi-thread_correctness,"Mon, 30 Jul 2018 07:56:06 +0000","Tue, 14 Aug 2018 11:44:47 +0000","Mon, 13 Aug 2018 12:24:30 +0000",1225704,"Find this one when investigating HBASE-20921, too. Here is some code from executeRollback in ProcedureExecutor.java. You can see my comments in the code above, reuseLock can cause the procedure executing(rollback) without a lock. Though I haven't found any bugs introduced by this issue, it is indeed a potential bug need to fix. I think we can just remove the reuseLock logic. Acquire and release lock every time. Find another case that during rolling back, the procedure's lock may not be released properly: see comment: https://issues.apache.org/jira/browse/HBASE-20975?focusedCommentId=16565123&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16565123",-0.08333333333,-0.0625,neutral
hbase,21173,description,"After HBASE-21138, some test methods still have the duplicate HRegion#close.So open this issue to remove the duplicate close",code_debt,duplicated_code,"Sat, 8 Sep 2018 14:28:39 +0000","Fri, 1 Feb 2019 20:12:31 +0000","Tue, 11 Sep 2018 09:52:38 +0000",242639,"After HBASE-21138, some test methods still have the duplicate HRegion#close.So open this issue to remove the duplicate close",0.1405,0.1405,negative
hbase,21599,description,and Pretty trivial stuff to clean up now.,code_debt,low_quality_code,"Thu, 13 Dec 2018 22:05:35 +0000","Thu, 3 Jan 2019 18:09:05 +0000","Thu, 3 Jan 2019 18:09:05 +0000",1800210,and Pretty trivial stuff to clean up now.,0.3,0.3,negative
hbase,22193,description,"Now the default config is Integer.MAX_VALUE. The ITBLL failed to open the region as HBASE-22163 and retry 170813 to reopen. After I fixed the problem and restart master, I found it need take a long time to init the old procedure logs because there are too many old logs... Code in",code_debt,low_quality_code,"Tue, 9 Apr 2019 03:21:48 +0000","Tue, 30 Apr 2019 13:15:19 +0000","Sat, 13 Apr 2019 03:17:46 +0000",345358,"Now the default config isInteger.MAX_VALUE.  The ITBLL failed to open the region asHBASE-22163 and retry 170813 to reopen. After I fixed the problem and restart master, I found it need take a long time to init the old procedure logs because there are too many old logs... Code inWALProcedureStore,java.",-0.2125,-0.2125,negative
hbase,22203,description,The DemoClient.java currently uses a not consistent formatting and should be reformatted.,code_debt,low_quality_code,"Wed, 10 Apr 2019 15:09:06 +0000","Sun, 28 Apr 2019 00:52:16 +0000","Fri, 12 Apr 2019 07:49:08 +0000",146402,The DemoClient.java currently uses a not consistent formatting and should be reformatted.,0.0,0.0,negative
hbase,22228,description,ThrottlingException was deprecated in 2.0.0 and should be removed in 3.0.0.,code_debt,dead_code,"Fri, 12 Apr 2019 20:55:47 +0000","Sat, 13 Apr 2019 13:25:47 +0000","Sat, 13 Apr 2019 08:50:40 +0000",42893,ThrottlingException was deprecated in 2.0.0 and should be removed in 3.0.0.,0.0,0.0,negative
hbase,2241,description,"This is a quick workaround until we do a better balancer. Taking a region off line when cluster is under load is bad news. Latency goes up as we wait on regions to come up in new locations. The load balancer should only cut in if the cluster is way out of alignment. I'd argue that 10% deviance from the avg. is not good enough reason moving regions around when cluster is under load. Balancer already has a knack for cutting in at most inopportune moments: during cluster startup, when new node is added to a small cluster, or moving a region just after its been opened on a node. We'll need to do a better balancer but meantime lets just allow that region loading can be sloppier, say 20% or 30% off the average before balancer cuts in.",code_debt,slow_algorithm,"Fri, 19 Feb 2010 19:53:51 +0000","Fri, 12 Oct 2012 06:14:57 +0000","Fri, 19 Feb 2010 20:06:24 +0000",753,"This is a quick workaround until we do a better balancer. Taking a region off line when cluster is under load is bad news. Latency goes up as we wait on regions to come up in new locations. The load balancer should only cut in if the cluster is way out of alignment. I'd argue that 10% deviance from the avg. is not good enough reason moving regions around when cluster is under load. Balancer already has a knack for cutting in at most inopportune moments: during cluster startup, when new node is added to a small cluster, or moving a region just after its been opened on a node. We'll need to do a better balancer but meantime lets just allow that region loading can be sloppier, say 20% or 30% off the average before balancer cuts in.",-0.1010416667,-0.1010416667,negative
hbase,2247,description,"hbase-2180 added pread whenever we do a get -- random-read -- and kept the old sync+position+read for when scanning. In between is the case of small scans. Small scans of 0-100 or so rows where the cells are small will likely fit a single hfile blocksize, especially if its the default 64k. We should recognize small scans and flip to random-read to satisfy (somehow). It'll up the performance a bit.",code_debt,slow_algorithm,"Mon, 22 Feb 2010 23:02:02 +0000","Sat, 11 Jun 2022 23:07:17 +0000","Sun, 26 Jan 2014 20:04:33 +0000",123886951,"hbase-2180 added pread whenever we do a get  random-read  and kept the old sync+position+read for when scanning. In between is the case of small scans. Small scans of 0-100 or so rows where the cells are small will likely fit a single hfile blocksize, especially if its the default 64k. We should recognize small scans and flip to random-read to satisfy (somehow). It'll up the performance a bit.",0.2048333333,0.2048333333,neutral
hbase,22832,description,The method was deprecated in and should be removed for 3.0.0.,code_debt,dead_code,"Sat, 10 Aug 2019 21:43:03 +0000","Thu, 15 Aug 2019 08:12:53 +0000","Sun, 11 Aug 2019 19:43:14 +0000",79211,The method getHTableDescriptor was deprecated in HFileOutputFormat2 and should be removed for 3.0.0.,0.0,0.0,negative
hbase,23087,description,The class is IA.Private and it has not been released yet so let's just remove this method to keep the class clean.,code_debt,dead_code,"Sat, 28 Sep 2019 11:44:32 +0000","Fri, 17 Jun 2022 18:42:49 +0000","Sun, 29 Sep 2019 01:30:29 +0000",49557,The class is IA.Private and it has not been released yet so let's just remove this method to keep the class clean.,0.2,0.2,neutral
hbase,23646,description,In {{hbase-rest}} Checkstyle reports a lot of violations. The remaining violations in the tests should be fixed.,code_debt,low_quality_code,"Sun, 5 Jan 2020 23:31:38 +0000","Wed, 11 Mar 2020 03:34:11 +0000","Mon, 20 Jan 2020 21:22:13 +0000",1288235,In hbase-rest Checkstyle reports a lot of violations. The remaining violations in the tests should be fixed.,-0.4,-0.4,negative
hbase,23,description,"Currently, looking at a cluster under load, you'll often trip over the disorientating listing of more than one region with a null end key (Was seen by Billy yesterday and psaab today). UI should list out all of its attributes. Also sort region listings by server address so easier finding servers.",code_debt,low_quality_code,"Wed, 21 Nov 2007 23:00:46 +0000","Fri, 22 Aug 2008 21:13:04 +0000","Wed, 14 May 2008 19:08:23 +0000",15106057,"Currently, looking at a cluster under load, you'll often trip over the disorientating listing of more than one region with a null end key (Was seen by Billy yesterday and psaab today). UI should list out all of its attributes. Also sort region listings by server address so easier finding servers.",0.1666666667,0.1666666667,negative
hbase,2555,description,"Now that we have HFile, it seems that the constant is no longer useful. I've grepped around the codebase and couldn't find it used elsewhere. Perhaps kept around in case folks decide to store their HBase data into a MapFile?",code_debt,dead_code,"Sun, 16 May 2010 21:36:30 +0000","Fri, 20 Nov 2015 12:42:41 +0000","Sun, 16 May 2010 22:47:17 +0000",4247,"Now that we have HFile, it seems that the constant http://hadoop.apache.org/hbase/docs/r0.20.4/api/org/apache/hadoop/hbase/HColumnDescriptor.html#MAPFILE_INDEX_INTERVAL is no longer useful. I've grepped around the codebase and couldn't find it used elsewhere. Perhaps kept around in case folks decide to store their HBase data into a MapFile?",-0.1666666667,-0.1666666667,negative
hbase,3057,description,"In we spin up cluster, create three tables, shut it down, start it back up, and ensure we still have three regions. A subtle race condition during the first shutdown makes it so the flush of META doesn't finish so when we start back up there are no user regions. I'm not sure if there are reasons the ordering is as such, but the is the section of code in CloseRegionHandler around line 118: We remove from the online map of regions before actually closing. But what the main run() loop in the RS is waiting on to determine when it can shut down is that the online region map is empty. Any reason not to swap these two and do the close before removing from online regions?",code_debt,multi-thread_correctness,"Thu, 30 Sep 2010 18:44:50 +0000","Fri, 20 Nov 2015 12:43:07 +0000","Thu, 30 Sep 2010 18:54:09 +0000",559,"In TestRestartCluster.testClusterRestart() we spin up cluster, create three tables, shut it down, start it back up, and ensure we still have three regions. A subtle race condition during the first shutdown makes it so the flush of META doesn't finish so when we start back up there are no user regions. I'm not sure if there are reasons the ordering is as such, but the is the section of code in CloseRegionHandler around line 118: We remove from the online map of regions before actually closing. But what the main run() loop in the RS is waiting on to determine when it can shut down is that the online region map is empty. Any reason not to swap these two and do the close before removing from online regions?",0.01313333333,0.01094444444,neutral
hbase,3571,description,"This lock in CompactSplitThread doesn't do anything best as I can tell: If two instances of CompactSplitThread, it would not prevent the two threads contending since its local to the instance. Remove it.",code_debt,dead_code,"Fri, 25 Feb 2011 20:25:53 +0000","Sun, 12 Jun 2022 18:12:04 +0000","Mon, 20 Jun 2011 16:39:06 +0000",9922393,"This lock in CompactSplitThread doesn't do anything best as I can tell: If two instances of CompactSplitThread, it would not prevent the two threads contending since its local to the instance. Remove it.",-0.1458333333,-0.1458333333,negative
hbase,369,description,"The client api (as defined by HTable and HBaseAdmin are not insignificantly different from HRegionInterface (wire protocol). They could be made much more similar by implementing a couple of new Writable classes (e.g., SortedMapWritable).",code_debt,low_quality_code,"Sat, 11 Aug 2007 23:19:34 +0000","Mon, 4 Feb 2008 18:41:56 +0000","Thu, 16 Aug 2007 22:53:30 +0000",430436,"The client api (as defined by HTable and HBaseAdmin are not insignificantly different from HRegionInterface (wire protocol). They could be made much more similar by implementing a couple of new Writable classes (e.g., SortedMapWritable).",0.25,0.25,neutral
hbase,4250,description,"* Did some restructuring of sections * Added section for common patch feedback (based on my own experiences in a recent patch). ** I ran these past Todd & JD at this week's hackathon. * mentioning ReviewBoard in patch submission process ** this didn't appear anywhere before. I still need to add more on tips on using it, but this is a start.",code_debt,low_quality_code,"Wed, 24 Aug 2011 21:09:06 +0000","Sun, 12 Jun 2022 19:18:23 +0000","Wed, 24 Aug 2011 21:11:14 +0000",128,"Did some restructuring of sections Added section for common patch feedback (based on my own experiences in a recent patch). I ran these past Todd & JD at this week's hackathon. mentioning ReviewBoard in patch submission process this didn't appear anywhere before. I still need to add more on tips on using it, but this is a start.",0.07025,0.07025,neutral
hbase,4303,description,"Currently it's outputting: REGION = Notice the missing quotes around tableName, etc.",code_debt,low_quality_code,"Tue, 30 Aug 2011 19:59:19 +0000","Fri, 20 Nov 2015 11:52:24 +0000","Tue, 30 Aug 2011 21:57:41 +0000",7102,"Currently it's outputting: REGION => {NAME => '.META.,,1 TableName => ', STARTKEY => '', ENDKEY => '', ENCODED => 1028785192,} Notice the missing quotes around tableName, etc.",-0.4,-0.2,neutral
hbase,4859,description,"See description at HBASE-3553. We had a patch ready for this in HBASE-3620 but never applied it publicly. Testing showed massive speedup in HBCK, especially when RegionServers were down or had long response times.",code_debt,slow_algorithm,"Wed, 23 Nov 2011 23:37:21 +0000","Fri, 20 Nov 2015 11:52:31 +0000","Sat, 10 Dec 2011 00:40:24 +0000",1386183,"See description at HBASE-3553. We had a patch ready for this in HBASE-3620 but never applied it publicly. Testing showed massive speedup in HBCK, especially when RegionServers were down or had long response times.",0.07766666667,0.07766666667,neutral
hbase,5039,description,book.xml * Arch chapter/Regions. clearing up a little more in region assignment * FAQ. Adding an architecture section. * MapReduce chapter. Fixed nit that Ian Varley brought to my attention on RDBMS summary.,code_debt,low_quality_code,"Thu, 15 Dec 2011 15:14:55 +0000","Sun, 12 Jun 2022 19:57:55 +0000","Thu, 15 Dec 2011 15:17:26 +0000",151,book.xml Arch chapter/Regions. clearing up a little more in region assignment FAQ. Adding an architecture section. MapReduce chapter. Fixed nit that Ian Varley brought to my attention on RDBMS summary.,0.0,0.0,neutral
hbase,5110,description,"The HLog class (method has unnecessary if check in a loop. static byte [][] long oldestWALseqid, final Map<byte [], Long // This method is static so it can be unit tested the easier. List<byte [] for (Map.Entry<byte [], Long if <= oldestWALseqid) { if (regions == null) regions = new ArrayList<byte [] } } return regions == null? null: regions.toArray(new byte [][] } The following change is suggested static byte [][] long oldestWALseqid, final Map<byte [], Long // This method is static so it can be unit tested the easier. List<byte [] for (Map.Entry<byte [], Long if <= oldestWALseqid) { } } return regions.size() == 0? null: regions.toArray(new byte [][] }",code_debt,low_quality_code,"Fri, 30 Dec 2011 19:01:01 +0000","Sun, 12 Jun 2022 20:02:21 +0000","Tue, 21 Apr 2015 00:50:03 +0000",104305742,"The HLog class (method findMemstoresWithEditsEqualOrOlderThan) has unnecessary if check in a loop. static byte [][] findMemstoresWithEditsEqualOrOlderThan(final long oldestWALseqid, final Map<byte [], Long> regionsToSeqids) { // This method is static so it can be unit tested the easier. List<byte []> regions = null; for (Map.Entry<byte [], Long> e: regionsToSeqids.entrySet()) { if (e.getValue().longValue() <= oldestWALseqid) { if (regions == null) regions = new ArrayList<byte []>(); regions.add(e.getKey()); } } return regions == null? null: regions.toArray(new byte [][] {HConstants.EMPTY_BYTE_ARRAY}); } The following change is suggested static byte [][] findMemstoresWithEditsEqualOrOlderThan(final long oldestWALseqid, final Map<byte [], Long> regionsToSeqids) { // This method is static so it can be unit tested the easier. List<byte []> regions = new ArrayList<byte []>(); for (Map.Entry<byte [], Long> e: regionsToSeqids.entrySet()) { if (e.getValue().longValue() <= oldestWALseqid) { regions.add(e.getKey()); } } return regions.size() == 0? null: regions.toArray(new byte [][] {HConstants.EMPTY_BYTE_ARRAY} ); }",-0.07193939394,-0.04960087719,neutral
hbase,5282,description,"When debugging hbck, found that the code responsible for this exception can leak open file handles.",code_debt,low_quality_code,"Thu, 26 Jan 2012 13:05:35 +0000","Fri, 12 Oct 2012 05:35:00 +0000","Thu, 26 Jan 2012 23:50:18 +0000",38683,"When debugging hbck, found that the code responsible for this exception can leak open file handles.",0.1,0.1,negative
hbase,5466,description,"Having upgraded to CDH3U3 version of hbase we found we had a zookeeper connection leak, tracking it down we found that closing the connection will only close the zookeeper connection if all calls to get the connection have been closed, there is incCount and decCount in the HConnection class, When a table is opened it makes a call to the metascanner class which opens a connection to the meta table, this table never gets closed. This caused the count in the HConnection class to never return to zero meaning that the zookeeper connection will not close when we close all the tables or calling true);",code_debt,low_quality_code,"Thu, 23 Feb 2012 21:03:09 +0000","Fri, 12 Oct 2012 05:35:03 +0000","Fri, 24 Feb 2012 01:00:23 +0000",14234,"Having upgraded to CDH3U3 version of hbase we found we had a zookeeper connection leak, tracking it down we found that closing the connection will only close the zookeeper connection if all calls to get the connection have been closed, there is incCount and decCount in the HConnection class, When a table is opened it makes a call to the metascanner class which opens a connection to the meta table, this table never gets closed. This caused the count in the HConnection class to never return to zero meaning that the zookeeper connection will not close when we close all the tables or calling HConnectionManager.deleteConnection(config, true);",-0.4225,-0.09933333333,neutral
hbase,5595,description,Fix this ugly exception that shows when running 0.92.1 when on local filesystem:,code_debt,low_quality_code,"Fri, 16 Mar 2012 17:59:37 +0000","Fri, 20 Nov 2015 11:53:56 +0000","Sun, 18 Mar 2012 19:45:56 +0000",179179,Fix this ugly exception that shows when running 0.92.1 when on local filesystem:,-1.0,-1.0,negative
hbase,5958,description,"ByteString.copyFrom makes a copy of a byte array in case it is changed in other thread. In most case, we don't need to worry about that. We should avoid copying the bytes for performance issue.",code_debt,slow_algorithm,"Tue, 8 May 2012 17:34:38 +0000","Mon, 13 Jun 2022 16:35:55 +0000","Fri, 8 Jun 2012 16:28:02 +0000",2674404,"ByteString.copyFrom makes a copy of a byte array in case it is changed in other thread. In most case, we don't need to worry about that. We should avoid copying the bytes for performance issue.",0.02825,0.02825,neutral
hbase,7604,description,"the static method getReferencedPath() contains almost the same code done internally by the FileLink, and is only used by ExportSnapshot. Remove that code and use the class directly",code_debt,duplicated_code,"Thu, 17 Jan 2013 16:39:29 +0000","Mon, 23 Sep 2013 18:31:08 +0000","Thu, 17 Jan 2013 19:51:44 +0000",11535,"the static method getReferencedPath() contains almost the same code done internally by the FileLink, and is only used by ExportSnapshot. Remove that code and use the class directly",-0.4375,-0.4375,negative
hbase,8904,description,* Clean up the Long overflow (oops) * Clean up the usage of tableName vs tableNameBytes * Add in more than just max time. * Add in tracing.,code_debt,low_quality_code,"Tue, 9 Jul 2013 17:44:59 +0000","Mon, 23 Sep 2013 19:22:31 +0000","Tue, 9 Jul 2013 23:46:09 +0000",21670,Clean up the Long overflow (oops) Clean up the usage of tableName vs tableNameBytes Add in more than just max time. Add in tracing.,0.06666666667,0.06666666667,neutral
hbase,9433,description,OpenRegionHandler uses the timeout monitor checking period as the timeout. It should share the same default as in AssignmentManager.,code_debt,low_quality_code,"Wed, 4 Sep 2013 15:28:53 +0000","Fri, 20 Nov 2015 11:54:24 +0000","Wed, 4 Sep 2013 23:58:32 +0000",30579,OpenRegionHandler uses the timeout monitor checking period as the timeout. It should share the same default as in AssignmentManager.,-0.075,-0.075,neutral
hbase,9493,description,"To make reading code easier, we should rename the CellUtil#get*Array methods to to CellUtil#clone*. This eliminates the possibly confusion between the semantics of these methods (which allocate-copy) and that of Cell#get*Array (which essentially just returns a pointer).",code_debt,low_quality_code,"Tue, 10 Sep 2013 20:44:08 +0000","Fri, 20 Nov 2015 11:53:02 +0000","Tue, 10 Sep 2013 23:10:15 +0000",8767,"To make reading code easier, we should rename the CellUtil#get*Array methods (Value,Qualifier,Family,Row) to to CellUtil#clone*. This eliminates the possibly confusion between the semantics of these methods (which allocate-copy) and that of Cell#get*Array (which essentially just returns a pointer).",0.0,0.0,neutral
hbase,10083,description,"When RegionServer failed to load a bloom block from HDFS due to any timeout or other reasons, it threw out the exception and disable the entire bloom filter for this HFile. This behavior does not make too much sense, especially for the compound bloom filter. Instead of disabling the bloom filter for the entire file, it could just return a potentially false positive result (true) and keep the bloom filter available.",design_debt,non-optimal_design,"Thu, 5 Dec 2013 00:47:55 +0000","Fri, 17 Jun 2022 04:51:04 +0000","Mon, 25 Sep 2017 19:05:11 +0000",120161836,"When RegionServer failed to load a bloom block from HDFS due to any timeout or other reasons, it threw out the exception and disable the entire bloom filter for this HFile. This behavior does not make too much sense, especially for the compound bloom filter. Instead of disabling the bloom filter for the entire file, it could just return a potentially false positive result (true) and keep the bloom filter available.",0.1733333333,0.1733333333,negative
hbase,1012,description,Ning Li up on list has stated that getting blocks using hdfs though the block is local takes almost the same amount of time as accesing the block over the network. See if can do something smarter when the data is known to be local short-circuiting hdfs if we can in a subclass of DFSClient (George Porter suggestion).,design_debt,non-optimal_design,"Wed, 19 Nov 2008 20:59:26 +0000","Sat, 11 Jun 2022 20:28:55 +0000","Fri, 24 Feb 2012 19:12:41 +0000",102982395,Ning Li up on list has stated that getting blocks using hdfs though the block is local takes almost the same amount of time as accesing the block over the network. See if can do something smarter when the data is known to be local short-circuiting hdfs if we can in a subclass of DFSClient (George Porter suggestion).,0.2125,0.2125,neutral
hbase,1054,description,"I had some tables refuse to find their indexes even though they were defined and had been updated. Scanning .META. I see that some regions from the table don't have the indexes... Oddly, during the .META. scan, I would see that I had 3 entries per table. (I have very little data in each table, def not splitting yet) But when I visited the UI it showed just one region per table... This patch also addes some toStrings that helped in diagnostics, and a new null check that I found I needed in IndexedTableScanner",design_debt,non-optimal_design,"Wed, 10 Dec 2008 00:42:51 +0000","Sun, 13 Sep 2009 22:26:34 +0000","Wed, 10 Dec 2008 16:23:55 +0000",56464,"I had some tables refuse to find their indexes even though they were defined and had been updated. Scanning .META. I see that some regions from the table don't have the indexes... Oddly, during the .META. scan, I would see that I had 3 entries per table. (I have very little data in each table, def not splitting yet) But when I visited the UI it showed just one region per table... This patch also addes some toStrings that helped in diagnostics, and a new null check that I found I needed in IndexedTableScanner",0.007142857143,0.007142857143,neutral
hbase,10968,description,"Here is related code: context was dereferenced first, leaving the null check ineffective.",design_debt,non-optimal_design,"Sat, 12 Apr 2014 01:10:49 +0000","Sat, 21 Feb 2015 23:30:17 +0000","Sat, 12 Apr 2014 01:56:36 +0000",2747,"Here is related code: context was dereferenced first, leaving the null check ineffective.",-0.583,-0.583,neutral
hbase,11011,description,"On load we already have a StoreFileInfo and we create it from the path, this will result in an extra fs.getFileStatus() call. In we do a fs.exists() and later a fs.getFileStatus() to create the StoreFileInfo, we can avoid the exists.",design_debt,non-optimal_design,"Thu, 17 Apr 2014 02:06:05 +0000","Sat, 21 Feb 2015 23:35:09 +0000","Fri, 18 Apr 2014 17:19:58 +0000",141233,"On load we already have a StoreFileInfo and we create it from the path, this will result in an extra fs.getFileStatus() call. In completeCompactionMarker() we do a fs.exists() and later a fs.getFileStatus() to create the StoreFileInfo, we can avoid the exists.",-0.04,-0.04,neutral
hbase,11229,description,"See parent issue. Small changes in the hit percentage can have large implications, even when movement is inside a single percent: i.e. going from 99.11 to 99.87 percent. As is, percents are ints. Make them doubles.",design_debt,non-optimal_design,"Wed, 21 May 2014 22:05:09 +0000","Sat, 21 Feb 2015 23:32:31 +0000","Thu, 22 May 2014 18:44:45 +0000",74376,"See parent issue. Small changes in the hit percentage can have large implications, even when movement is inside a single percent: i.e. going from 99.11 to 99.87 percent. As is, percents are ints. Make them doubles.",0.0,0.0,neutral
hbase,11511,description,"We used to write COMPLETE_FLUSH event to WAL until it got removed in 0.96 in issue HBASE-7329. For secondary region replicas, it is important to share the data files with the primary region. So we should reintroduce the flush wal markers so that the secondary region replicas can pick up the newly flushed files from the WAL and start serving data from those. A design doc which explains the context a bit better can be found in HBASE-11183.",design_debt,non-optimal_design,"Mon, 14 Jul 2014 18:14:43 +0000","Fri, 6 Apr 2018 17:51:19 +0000","Tue, 15 Jul 2014 21:52:15 +0000",99452,"We used to write COMPLETE_FLUSH event to WAL until it got removed in 0.96 in issue HBASE-7329. For secondary region replicas, it is important to share the data files with the primary region. So we should reintroduce the flush wal markers so that the secondary region replicas can pick up the newly flushed files from the WAL and start serving data from those. A design doc which explains the context a bit better can be found in HBASE-11183.",0.2,0.2,neutral
hbase,11612,description,Can be removed now from 0.98 onwards? The javadoc says it should be removed on a major release after 0.96. It does a full Meta scan which takes quite some time especially if there are around 1M regions.,design_debt,non-optimal_design,"Wed, 30 Jul 2014 00:16:23 +0000","Fri, 17 Jun 2022 05:52:59 +0000","Wed, 30 Jul 2014 00:23:17 +0000",414,Can MetaMigrationConvertingToPB be removed now from 0.98 onwards? The javadoc says it should be removed on a major release after 0.96. It does a full Meta scan which takes quite some time especially if there are around 1M regions.,0.0,0.0,neutral
hbase,12017,description,"Now that Connection and ConnectionFactory are in place, internal code should use them instead of HTable constructors as per HBASE-11825.",design_debt,non-optimal_design,"Thu, 18 Sep 2014 15:27:55 +0000","Wed, 4 Apr 2018 23:05:46 +0000","Thu, 27 Nov 2014 16:03:35 +0000",6050140,"Now that Connection and ConnectionFactory are in place, internal code should use them instead of HTable constructors as per HBASE-11825.",0.0,0.0,neutral
hbase,12059,description,Different versions of hadoop have different annotations. We can smooth this out by providing our own.,design_debt,non-optimal_design,"Tue, 23 Sep 2014 01:04:57 +0000","Fri, 6 Apr 2018 17:54:27 +0000","Wed, 24 Sep 2014 03:18:32 +0000",94415,Different versions of hadoop have different annotations. We can smooth this out by providing our own.,0.1875,0.1875,neutral
hbase,12211,description,"In the hbase-daemon.sh file line 199, it has the content: The variable loglog is defined as: For my understanding, this information should be printed to the ""logout"" variable; we should not mix this ""ulimit"" information with the actual log printed by hbase java program.",design_debt,non-optimal_design,"Thu, 9 Oct 2014 02:15:21 +0000","Fri, 17 Jun 2022 17:40:25 +0000","Thu, 6 Jul 2017 19:00:08 +0000",86546687,"In the hbase-daemon.sh file line 199, it has the content: echo ""`ulimit -a`"" >> $loglog 2>&1 The variable loglog is defined as: logout=$HBASE_LOG_DIR/$HBASE_LOG_PREFIX.out loggc=$HBASE_LOG_DIR/$HBASE_LOG_PREFIX.gc loglog=""${HBASE_LOG_DIR}/${HBASE_LOGFILE}"" For my understanding, this information should be printed to the ""logout"" variable; we should not mix this ""ulimit"" information with the actual log printed by hbase java program.",0.25,0.125,neutral
hbase,12238,description,"Let me fix a few innocuous exceptions that show on startup (saw testing 0.99.1), even when regular -- will throw people off. Here is one: More to follow...",design_debt,non-optimal_design,"Mon, 13 Oct 2014 02:23:59 +0000","Fri, 6 Apr 2018 17:55:53 +0000","Thu, 30 Oct 2014 04:54:55 +0000",1477856,"Let me fix a few innocuous exceptions that show on startup (saw testing 0.99.1), even when regular  will throw people off. Here is one: More to follow...",-0.5,-0.5,negative
hbase,12293,description,"In trying to solve HBASE-12285, it was pointed out that tests are writing too much to output again. At best, this is a sloppy practice and, at worst, it leaves us open to builds breaking when our test tools can't handle the flood. If  would be willing give me a little bit of mentoring on how he dealt with this problem a few years back, I'd be happy to add it to my plate.",design_debt,non-optimal_design,"Sun, 19 Oct 2014 19:53:26 +0000","Fri, 17 Jun 2022 17:48:21 +0000","Wed, 29 Oct 2014 18:05:26 +0000",857520,"In trying to solve HBASE-12285, it was pointed out that tests are writing too much to output again. At best, this is a sloppy practice and, at worst, it leaves us open to builds breaking when our test tools can't handle the flood. If nkeywal would be willing give me a little bit of mentoring on how he dealt with this problem a few years back, I'd be happy to add it to my plate.",-0.07222222222,-0.07222222222,negative
hbase,12464,description,"meta table region assignment could reach to the 'FAILED_OPEN' state, which makes the region not available unless the target region server shutdown or manual resolution. This is undesirable state for meta tavle region. Here is the sequence how this could happen (the code is in Step 1: Master detects a region server (RS1) that hosts one meta table region is down, it changes the meta region state from 'online' to 'offline' Step 2: In a loop (with configuable maximumAttempts count, default is 10, and minimal is 1), AssignmentManager tries to find a RS to host the meta table region. If there is no RS available, it would loop forver by resetting the loop count (BUG#1 from this logic - a small bug) Step 3: Once a new RS is found (RS2), inside the same loop as Step 2, AssignmentManager tries to assign the meta region to RS2 (OFFLINE, RS1 = Based on the document ( ), this is by design - ""17. For regions in FAILED_OPEN or FAILED_CLOSE states , the master tries to close them again when they are reassigned by an operator via HBase Shell."". However, this is bad design, espcially for meta table region (it is arguable that the design is good for regular table - for this ticket, I am more focus on fixing the meta region availablity issue). I propose 2 possible fixes: Fix#1 (band-aid change): in Step 3, just like Step 2, if the region is a meta table region, reset the loop count so that it would not leave the loop with meta table region in FAILED_OPEN state. Fix#2 (more involved): if a region is in FAILED_OPEN state, we should provide a way to automatically trigger after a short period of time (leaving any region in FAILED_OPEN state or other states like 'FAILED_CLOSE' is undesirable, should have some way to retrying and auto-heal the region). I think at least for 1.0.0, Fix#1 is good enough. We can open a task-type of JIRA for Fix#2 in future release.",design_debt,non-optimal_design,"Wed, 12 Nov 2014 18:12:22 +0000","Fri, 6 Apr 2018 17:55:10 +0000","Thu, 20 Nov 2014 22:02:39 +0000",705017,"meta table region assignment could reach to the 'FAILED_OPEN' state, which makes the region not available unless the target region server shutdown or manual resolution. This is undesirable state for meta tavle region. Here is the sequence how this could happen (the code is in AssignmentManager#assign()): Step 1: Master detects a region server (RS1) that hosts one meta table region is down, it changes the meta region state from 'online' to 'offline' Step 2: In a loop (with configuable maximumAttempts count, default is 10, and minimal is 1), AssignmentManager tries to find a RS to host the meta table region. If there is no RS available, it would loop forver by resetting the loop count (BUG#1 from this logic - a small bug) Step 3: Once a new RS is found (RS2), inside the same loop as Step 2, AssignmentManager tries to assign the meta region to RS2 (OFFLINE, RS1 => PENDING_OPEN, RS2). If for some reason that opening the region in RS2 failed (eg. the target RS2 is not ready to serve - ServerNotRunningYetException), AssignmentManager would change the state from (PENDING_OPEN, RS2) to (FAILED_OPEN, RS2). then it would retry (and even change the RS server to go to). The retry is up to maximumAttempts. Once the maximumAttempts is reached, the meta region will be in the 'FAILED_OPEN' state, unless either (1). RS2 shutdown to trigger region assignment again or (2). it is reassigned by an operator via HBase Shell. Based on the document ( http://hbase.apache.org/book/regions.arch.html ), this is by design - ""17. For regions in FAILED_OPEN or FAILED_CLOSE states , the master tries to close them again when they are reassigned by an operator via HBase Shell."". However, this is bad design, espcially for meta table region (it is arguable that the design is good for regular table - for this ticket, I am more focus on fixing the meta region availablity issue). I propose 2 possible fixes: Fix#1 (band-aid change): in Step 3, just like Step 2, if the region is a meta table region, reset the loop count so that it would not leave the loop with meta table region in FAILED_OPEN state. Fix#2 (more involved): if a region is in FAILED_OPEN state, we should provide a way to automatically trigger AssignmentManager::assign() after a short period of time (leaving any region in FAILED_OPEN state or other states like 'FAILED_CLOSE' is undesirable, should have some way to retrying and auto-heal the region). I think at least for 1.0.0, Fix#1 is good enough. We can open a task-type of JIRA for Fix#2 in future release.",0.02106060606,0.007477777778,neutral
hbase,1263,description,"As some of us have been discussing, allowing the client to manually set the timestamp of a put breaks the general semantics of versioning and I'd like to see it removed as part of HBASE-880 (a more appropriate place to debate that). However, one trick being used when you don't want the overhead of versions on a frequently updated column (which are only cleared on compactions even if set to 1), was to use the same timestamp. Since that would create an identical key it would just overwrite the value not create a new version. It's a very common use-case, and this hack is being used as part of the committed increment ops from Rather than making a special optimization for counters, an optimization on single-version families that never stores more than one version of a column.",design_debt,non-optimal_design,"Sun, 15 Mar 2009 07:08:10 +0000","Sat, 11 Jun 2022 20:35:01 +0000","Wed, 16 Jul 2014 21:45:45 +0000",168446255,"As some of us have been discussing, allowing the client to manually set the timestamp of a put breaks the general semantics of versioning and I'd like to see it removed as part of HBASE-880 (a more appropriate place to debate that). However, one trick being used when you don't want the overhead of versions on a frequently updated column (which are only cleared on compactions even if set to 1), was to use the same timestamp. Since that would create an identical key it would just overwrite the value not create a new version. It's a very common use-case, and this hack is being used as part of the committed increment ops from HBASE-868/HBASE-1252. Rather than making a special optimization for counters, an optimization on single-version families that never stores more than one version of a column.",0.334375,0.2675,negative
hbase,13710,description,HttpServer makes use of Hadoop's ReflectionUtil instead of our own. AFAICT it's using 1 extra method. Just copy that one over to our own ReflectionUtil.,design_debt,non-optimal_design,"Tue, 19 May 2015 05:58:23 +0000","Fri, 24 Jun 2022 18:50:45 +0000","Thu, 28 May 2015 05:37:17 +0000",776334,HttpServer makes use of Hadoop's ReflectionUtil instead of our own. AFAICT it's using 1 extra method. Just copy that one over to our own ReflectionUtil.,0.0,0.0,neutral
hbase,13905,description,The reruns failed because the test is not idempotent. Perhaps we should have the test startup clean up it's workspace before starting.,design_debt,non-optimal_design,"Mon, 15 Jun 2015 17:09:18 +0000","Mon, 31 Aug 2015 22:39:39 +0000","Tue, 16 Jun 2015 00:42:25 +0000",27187,The reruns failed because the test is not idempotent. Perhaps we should have the test startup clean up it's workspace before starting.,0.0,0.0,negative
hbase,14517,description,"In production env, regionservers may be removed from the cluster for hardware problems and rejoined the cluster after the repair. There is a potential risk that the version of rejoined regionserver may diff from others because the cluster has been upgraded through many versions. To solve this, we can show the all regionservers' version in the server list of master's status page, and highlight the regionserver when its version is different from the master's version, similar to HDFS-3245 Suggestions are welcome~",design_debt,non-optimal_design,"Wed, 30 Sep 2015 05:30:06 +0000","Fri, 1 Jul 2022 21:38:21 +0000","Fri, 9 Oct 2015 22:15:05 +0000",837899,"In production env, regionservers may be removed from the cluster for hardware problems and rejoined the cluster after the repair. There is a potential risk that the version of rejoined regionserver may diff from others because the cluster has been upgraded through many versions. To solve this, we can show the all regionservers' version in the server list of master's status page, and highlight the regionserver when its version is different from the master's version, similar to HDFS-3245 Suggestions are welcome~",-0.11,-0.11,neutral
hbase,14604,description,"The code in MoveCoseFunction: It uses cluster.numRegions + META_MOVE_COST_MULT as the max value when scale moveCost to [0,1]. But this should use maxMoves as the max value when cluster have a lot of regions. Assume a cluster have 10000 regions, maxMoves is 2500, it only scale moveCost to [0, 0.25]. Improve moveCost by use maxMoves) as the max cost, so it can scale moveCost to [0,1].",design_debt,non-optimal_design,"Wed, 14 Oct 2015 09:54:05 +0000","Tue, 20 Oct 2015 15:20:36 +0000","Tue, 20 Oct 2015 09:40:16 +0000",517571,"The code in MoveCoseFunction: It uses cluster.numRegions + META_MOVE_COST_MULT as the max value when scale moveCost to [0,1]. But this should use maxMoves as the max value when cluster have a lot of regions. Assume a cluster have 10000 regions, maxMoves is 2500, it only scale moveCost to [0, 0.25]. Improve moveCost by use Math.min(cluster.numRegions, maxMoves) as the max cost, so it can scale moveCost to [0,1].",0.08,0.05714285714,neutral
hbase,1492,description,Make the region split size a table configuration parameter. It would help make a smaller but higher concurrent table split sooner and improve scalability on smaller data sets.,design_debt,non-optimal_design,"Sat, 6 Jun 2009 08:54:43 +0000","Sun, 13 Sep 2009 22:24:41 +0000","Sat, 6 Jun 2009 20:17:01 +0000",40938,Make the region split size a table configuration parameter. It would help make a smaller but higher concurrent table split sooner and improve scalability on smaller data sets.,0.2,0.2,neutral
hbase,15617,description,"When running in regionserver mode the Canary is expected to probe for service health one time per regionserver during a probe interval. Each time the canary chore fires, we create a which uses (via to enumerate over all table descriptors, find the locations for each table, then assemble the list of regionservers to probe from this result. The list may not contain all live regionservers, if there is a regionserver up but for some reason not serving any regions. To ensure we have the complete list of live regionservers I think it would be better to use and enumerate the live server list returned in the result.",design_debt,non-optimal_design,"Fri, 8 Apr 2016 16:55:56 +0000","Thu, 19 May 2016 17:52:21 +0000","Thu, 19 May 2016 02:21:41 +0000",3489945,"When running in regionserver mode the Canary is expected to probe for service health one time per regionserver during a probe interval. Each time the canary chore fires, we create a RegionServerMonitor, which uses filterRegionServerByName (via getAllRegionServerByName) to enumerate over all table descriptors, find the locations for each table, then assemble the list of regionservers to probe from this result. The list may not contain all live regionservers, if there is a regionserver up but for some reason not serving any regions. To ensure we have the complete list of live regionservers I think it would be better to use Admin#getClusterStatus and enumerate the live server list returned in the result.",0.0875,0.0875,neutral
hbase,15640,description,"I was baffled looking at L1 Cache because it was stuck at 100k blocks and only 'loading' 3G of data according to the UI. When I looked in log, all the numbers looked write in the Cache summary reported in log. Turns out our buckecache UI template will cut off calculating stats at a limit --which is probably what we want -- but the fact that it has done this should be more plain.",design_debt,non-optimal_design,"Tue, 12 Apr 2016 23:28:31 +0000","Thu, 9 Nov 2017 00:43:41 +0000","Wed, 20 Apr 2016 21:28:00 +0000",683969,"I was baffled looking at L1 Cache because it was stuck at 100k blocks and only 'loading' 3G of data according to the UI. When I looked in log, all the numbers looked write in the Cache summary reported in log. Turns out our buckecache UI template will cut off calculating stats at a limit --which is probably what we want  but the fact that it has done this should be more plain.",-0.09444444444,-0.09444444444,negative
hbase,15835,description,"When a MiniCluster is being started with the method (most typically in the context of JUnit testing), if a local HBase instance is already running (or for that matter, another thread with another MiniCluster is already running), the startup will fail with a RuntimeException saying ""HMasterAddress already in use"", referring explicitly to contention for the same default master info port (16010). This problem most recently came up in conjunction with HBASE-14876 and its sub-JIRAs (development of new HBase-oriented Maven archetypes), but this is apparently a known issue to veteran developers, who tend to set up the @BeforeClass sections of their test modules with code similar to the following: A comprehensive solution modeled on this should be put directly into HBaseTestUtility's main constructor, using one of the following options: OPTION 1 (always force random port assignment): OPTION 2 (always force random port assignment if user has not explicitly defined alternate port):",design_debt,non-optimal_design,"Mon, 16 May 2016 08:54:16 +0000","Fri, 1 Jul 2022 20:27:59 +0000","Wed, 9 May 2018 14:58:37 +0000",62489061,"When a MiniCluster is being started with the HBaseTestUtility#startMiniCluster method (most typically in the context of JUnit testing), if a local HBase instance is already running (or for that matter, another thread with another MiniCluster is already running), the startup will fail with a RuntimeException saying ""HMasterAddress already in use"", referring explicitly to contention for the same default master info port (16010). This problem most recently came up in conjunction with HBASE-14876 and its sub-JIRAs (development of new HBase-oriented Maven archetypes), but this is apparently a known issue to veteran developers, who tend to set up the @BeforeClass sections of their test modules with code similar to the following: A comprehensive solution modeled on this should be put directly into HBaseTestUtility's main constructor, using one of the following options: OPTION 1 (always force random port assignment): OPTION 2 (always force random port assignment if user has not explicitly defined alternate port):",0.03,0.03,neutral
hbase,16273,description,Currently store file size is reported in MB and index & bloom filter in kb. There are two problems here. 1) They are calculated from bytes by dividing by 1024 / 1024 and then converted to an int. So for small regions it always reports 0. 2) And this conversion is done per store so even if there are multiple stores and all of them combined would be larger than 1 MB it'd be reported as 0 I'd suggest to count them in bytes and then when displaying them on the RegionServer UI page decide on an appropriate unit to use.,design_debt,non-optimal_design,"Fri, 22 Jul 2016 13:46:06 +0000","Fri, 22 Jul 2016 21:13:33 +0000","Fri, 22 Jul 2016 21:13:33 +0000",26847,Currently store file size is reported in MB and index & bloom filter in kb. There are two problems here. 1) They are calculated from bytes by dividing by 1024 / 1024 and then converted to an int. So for small regions it always reports 0. 2) And this conversion is done per store so even if there are multiple stores and all of them combined would be larger than 1 MB it'd be reported as 0 I'd suggest to count them in bytes and then when displaying them on the RegionServer UI page decide on an appropriate unit to use.,0.07,0.07,negative
hbase,16817,description,"Current we write length header before it is more efficient to write length header inside it, so we only to calculate the length only once.",design_debt,non-optimal_design,"Wed, 12 Oct 2016 14:38:08 +0000","Thu, 13 Oct 2016 12:26:13 +0000","Thu, 13 Oct 2016 12:26:13 +0000",78485,"Current we write length header before KeyValueUtil#oswrite, it is more efficient to write length header inside it, so we only to calculate the length only once.",0.0,0.0,neutral
hbase,17101,description,"As described in the doc (see HBASE-15531), we would like to start with user tables for favored nodes. This task ensures FN does not apply to system tables. System tables are in memory and won't benefit from favored nodes. Since we also maintain FN information for user regions in meta, it helps to keep implementation simpler by ignoring system tables for the first iterations.",design_debt,non-optimal_design,"Tue, 15 Nov 2016 18:35:53 +0000","Thu, 23 Jun 2022 19:12:57 +0000","Tue, 31 Jan 2017 18:59:41 +0000",6654228,"As described in the doc (see HBASE-15531), we would like to start with user tables for favored nodes. This task ensures FN does not apply to system tables. System tables are in memory and won't benefit from favored nodes. Since we also maintain FN information for user regions in meta, it helps to keep implementation simpler by ignoring system tables for the first iterations.",0.064,0.064,neutral
hbase,17338,description,"We have only data size and heap overhead being tracked globally. Off heap memstore works with off heap backed MSLAB pool. But a cell, when added to memstore, not always getting copied to MSLAB. Append/Increment ops doing an upsert, dont use MSLAB. Also based on the Cell size, we sometimes avoid MSLAB copy. But now we track these cell data size also under the global memstore data size which indicated off heap size in case of off heap memstore. For global checks for flushes (against lower/upper watermark levels), we check this size against max off heap memstore size. We do check heap overhead against global heap memstore size (Defaults to 40% of xmx) But for such cells the data size also should be accounted under the heap overhead.",design_debt,non-optimal_design,"Mon, 19 Dec 2016 13:18:49 +0000","Fri, 1 Jul 2022 21:33:56 +0000","Fri, 10 Mar 2017 05:43:36 +0000",6971087,"We have only data size and heap overhead being tracked globally. Off heap memstore works with off heap backed MSLAB pool. But a cell, when added to memstore, not always getting copied to MSLAB. Append/Increment ops doing an upsert, dont use MSLAB. Also based on the Cell size, we sometimes avoid MSLAB copy. But now we track these cell data size also under the global memstore data size which indicated off heap size in case of off heap memstore. For global checks for flushes (against lower/upper watermark levels), we check this size against max off heap memstore size. We do check heap overhead against global heap memstore size (Defaults to 40% of xmx) But for such cells the data size also should be accounted under the heap overhead.",-0.0375,-0.0375,neutral
hbase,1863,description,"does not support read/write of unknown Writable object (will throw in addition, writing a known Writable object, e.g., HColumnDescriptor, will write the code twice. furthermore, it may be useful to change addToMap from private to public. not causing any problem with hbase, but will be nice to have the above corrected, especially part of the code is already there.",design_debt,non-optimal_design,"Wed, 23 Sep 2009 18:33:22 +0000","Sat, 11 Jun 2022 21:03:14 +0000","Sat, 18 May 2013 20:28:48 +0000",115178126,"o.a.h.h.i.HbaseObjectWritable does not support read/write of unknown Writable object (will throw UnsupportedIoerationException); in addition, writing a known Writable object, e.g., HColumnDescriptor, will write the code twice. furthermore, it may be useful to change addToMap from private to public. not causing any problem with hbase, but will be nice to have the above corrected, especially part of the code is already there.",0.3263888889,0.3263888889,negative
hbase,18646,description,The default procedure timeout of 60 sec and pool size (1) may be not optimal for large deployements,design_debt,non-optimal_design,"Mon, 21 Aug 2017 22:32:06 +0000","Fri, 1 Jul 2022 20:12:05 +0000","Fri, 1 Sep 2017 17:18:52 +0000",931606,The default procedure timeout of 60 sec and pool size (1) may be not optimal for large deployements,-0.75,-0.75,negative
hbase,1897,description,"Presumption is that WAL is a sequencefile. I just spent some time looking again at my old buddy SF and its kinda heavy-duty for our needs. Do we need the sync bytes it writes into the stream every time you call a sync? Maybe it'd help recovering logs of edits? We don't need the compression by record or block, metadata info, etc. Its also currently unsuited because its sync actually doesn't do a sync on the backing stream: We should move all of our HLog stuff into a wal package and rename the classes as WAL, WALEdit, etc. Splitting code and replay code should reference a WAL.Reader rather than a etc.",design_debt,non-optimal_design,"Fri, 9 Oct 2009 04:48:03 +0000","Sat, 11 Jun 2022 21:03:37 +0000","Fri, 9 Oct 2009 18:38:09 +0000",49806,"Presumption is that WAL is a sequencefile. I just spent some time looking again at my old buddy SF and its kinda heavy-duty for our needs. Do we need the sync bytes it writes into the stream every time you call a sync? Maybe it'd help recovering logs of edits? We don't need the compression by record or block, metadata info, etc. Its also currently unsuited because its sync actually doesn't do a sync on the backing stream: We should move all of our HLog stuff into a wal package and rename the classes as WAL, WALEdit, etc. Splitting code and replay code should reference a WAL.Reader rather than a SequenceFile.Reader, etc.",0.1380952381,0.1208333333,neutral
hbase,19073,description,"- Remove the configuration - Keep following interface since they nicely separate ZK based implementation: ProcedureMemberRpcs - Replace CSM (interface) + BCSM (unnecessary middle hierarchy) with single CSM interface. - Don't pass whole CSM object around (with server in it which gives acess to pretty much everything), only pass the relevant dependencies. Discussion thread on dev@ mailing list.",design_debt,non-optimal_design,"Mon, 23 Oct 2017 22:47:02 +0000","Wed, 1 Aug 2018 06:22:22 +0000","Wed, 25 Oct 2017 03:05:27 +0000",101905,"Remove the configuration hbase.coordinated.state.manager.class Keep following interface since they nicely separate ZK based implementation: SplitLogWorkerCoordination, SplitLogManagerCoordination, ProcedureCoordinatorRpcs, ProcedureMemberRpcs Replace CSM (interface) + BCSM (unnecessary middle hierarchy) with single CSM interface. Don't pass whole CSM object around (with server in it which gives acess to pretty much everything), only pass the relevant dependencies. Discussion thread on dev@ mailing list. http://mail-archives.apache.org/mod_mbox/hbase-dev/201710.mbox/%3CCAAjhxrqjOg90Fdi73kZZe_Gxtrqq8ff%2B%3DAj_epptO_XO812Abg%40mail.gmail.com%3E",0.1905,0.047625,neutral
hbase,19633,description,"In the previous implementation, we can not always cleanly remove all the replication queues when removing a peer since the removing work is done by RS and if an RS is crashed then some queues may left there forever. That's why we need to check if there are already some queues for a newly created peer since we may reuse the peer id and causes problem. With the new procedure based replication peer modification, I think we can do it cleanly. After the are done on all RSes, we can make sure that no RS will create queue for this peer again, then we can iterate over all the queues for all Rses and do another round of clean up.",design_debt,non-optimal_design,"Tue, 26 Dec 2017 13:32:49 +0000","Fri, 24 Jun 2022 19:14:30 +0000","Tue, 2 Jan 2018 02:01:39 +0000",563330,"In the previous implementation, we can not always cleanly remove all the replication queues when removing a peer since the removing work is done by RS and if an RS is crashed then some queues may left there forever. That's why we need to check if there are already some queues for a newly created peer since we may reuse the peer id and causes problem. With the new procedure based replication peer modification, I think we can do it cleanly. After the RefreshPeerProcedures are done on all RSes, we can make sure that no RS will create queue for this peer again, then we can iterate over all the queues for all Rses and do another round of clean up.",-0.025,-0.025,neutral
hbase,1964,description,"When a hadoop/hbase cluster is under heavy load it will inevitably reach a tipping point where data is lost or corrupted. A graceful method is needed to put the cluster into safe mode until more resources can be added or the load on the cluster has been reduced. St.Ack has suggested the following short-term task: ""Meantime, it should be possible to have a cron run a script that checks cluster resources from time-to-time -- e.g. how full hdfs is, how much each regionserver is carrying -- and when it determines the needle is in the red, flip the cluster to be read-only.""",design_debt,non-optimal_design,"Mon, 9 Nov 2009 19:16:05 +0000","Sat, 11 Jun 2022 23:10:39 +0000","Sat, 13 Apr 2013 01:10:01 +0000",108021236,"When a hadoop/hbase cluster is under heavy load it will inevitably reach a tipping point where data is lost or corrupted. A graceful method is needed to put the cluster into safe mode until more resources can be added or the load on the cluster has been reduced. St.Ack has suggested the following short-term task: ""Meantime, it should be possible to have a cron run a script that checks cluster resources from time-to-time  e.g. how full hdfs is, how much each regionserver is carrying  and when it determines the needle is in the red, flip the cluster to be read-only.""",0.01388888889,0.01388888889,neutral
hbase,1968,description,"From a Trend dev team: Their further analysis explains in detail the scenario, which I will summarize here: 1) An invalid put is added to the writeBuffer by put(Put put). It will trigger a once it goes to the region server. 2) At some point, the buffer is flushed. 3) When the invalid put is processed, an exception is thrown. The finally clause of flushCommits() removes all successful puts from the writebuffer list but the failed put remains at the top. This entry becomes an immovable blocker which prevents any subsequent entry from being processed. 4) Subsequent puts will add more entries to the write buffer until the buffer limit is reached, compounding the problem by allowing more edits to be queued which can never be processed. A workaround could be for the client to call getWriteBuffer() -- on trunk -- and remove the entry at the head of the list manually, but without the patch on this issue, the client cannot get access to the list on branch.",design_debt,non-optimal_design,"Tue, 10 Nov 2009 04:53:49 +0000","Fri, 12 Oct 2012 06:13:23 +0000","Tue, 10 Nov 2009 04:56:59 +0000",190,"From a Trend dev team: When insert rows into one table by calling the method public synchronized void put(final Put put), if the column family of one row does not exist, the insert operation will failed and throw NoSuchColumnFamilyException.. We observed that all the following insert operation will fails even though all of them have valid column family. That is one exception of insert operation can cause failure of all the following insert operation. Their further analysis explains in detail the scenario, which I will summarize here: 1) An invalid put is added to the writeBuffer by put(Put put). It will trigger a NoSuchColumnFamilyException once it goes to the region server. 2) At some point, the buffer is flushed. 3) When the invalid put is processed, an exception is thrown. The finally clause of flushCommits() removes all successful puts from the writebuffer list but the failed put remains at the top. This entry becomes an immovable blocker which prevents any subsequent entry from being processed. 4) Subsequent puts will add more entries to the write buffer until the buffer limit is reached, compounding the problem by allowing more edits to be queued which can never be processed. A workaround could be for the client to call getWriteBuffer()  on trunk  and remove the entry at the head of the list manually, but without the patch on this issue, the client cannot get access to the list on branch.",0.103125,0.0025,neutral
hbase,19862,description,"We have temporary (added in HBASE-19007) and concept of CoreCoprocessors which require that whichever they get, it should also implement This test builds mock RegionCpEnv for TokenProvider but it falls short of what's expected and results in following exceptions in test logs Patch adds the missing interface to the mock. Also, uses Mockito to mock the interfaces rather the crude way.",design_debt,non-optimal_design,"Thu, 25 Jan 2018 22:01:27 +0000","Wed, 21 Mar 2018 22:23:45 +0000","Fri, 26 Jan 2018 08:27:33 +0000",37566,"We have temporary HasRegionServerServices (added in HBASE-19007) and concept of CoreCoprocessors which require that whichever *CoprocessorEnvironment they get, it should also implement HasRegionServerServices. This test builds mock RegionCpEnv for TokenProvider (RegionCoprocessor), but it falls short of what's expected and results in following exceptions in test logs Patch adds the missing interface to the mock. Also, uses Mockito to mock the interfaces rather the crude way.",-0.35,-0.2333333333,neutral
hbase,19969,description,"Some file system operations are not fault tolerant during merge. We delete backup data in a backup file system, then copy new data over to backup destination. Deletes can be partial, copy can fail as well",design_debt,non-optimal_design,"Fri, 9 Feb 2018 22:57:42 +0000","Wed, 28 Mar 2018 02:50:38 +0000","Sat, 10 Mar 2018 17:49:55 +0000",2487133,"Some file system operations are not fault tolerant during merge. We delete backup data in a backup file system, then copy new data over to backup destination. Deletes can be partial, copy can fail as well",-0.09483333333,-0.09483333333,negative
hbase,20302,description,"when catalogJanitor is disabled it has various condition for which it can be true to disable CatalogJanitor and sends this message ""CatalogJanitor disabled! Not running scan."" Since this is an async thread, it is difficult to identify what is the exact reason for it disable. We could log all conditions alongwith disabled! Not running scan."") for better debugging.",design_debt,non-optimal_design,"Wed, 28 Mar 2018 11:54:56 +0000","Fri, 1 Feb 2019 20:05:01 +0000","Thu, 29 Mar 2018 17:45:03 +0000",107407,"when catalogJanitor is disabled it has various condition for which it can be true to disableCatalogJanitor and sends this message ""CatalogJanitor disabled! Not running scan."" Since this is an async thread, it is difficult to identify what is the exact reason for it disable. We could log all conditions alongwithLOG.warn(""CatalogJanitor disabled! Not running scan."") for better debugging.",0.1969166667,0.1116428571,negative
hbase,20595,description,"Regionserver groups needs to specially handle what it calls ""special tables"", tables upon which core or other modular functionality depends. They need to be excluded from normal rsgroup processing during bootstrap to avoid circular dependencies or errors due to insufficiently initialized state. I think we also want to ensure that such tables are always given a rsgroup assignment with nonzero servers. (IIRC another issue already raises that point, we can link it later.) Special tables include: * The system tables in the 'hbase:' namespace * The ACL table if the AccessController coprocessor is installed * The Labels table if the coprocessor is installed * The Quotas table if the FS quotas feature is active Either we need a facility where ""special tables"" can be registered, which should be in core. Or, we institute a blanket rule that core and all extensions that need a ""special table"" must put them into the 'hbase:' namespace, so the test will return TRUE for all, and then rsgroups simply needs to test for that.",design_debt,non-optimal_design,"Wed, 16 May 2018 21:37:08 +0000","Mon, 22 Apr 2019 16:10:14 +0000","Wed, 23 May 2018 18:58:44 +0000",595296,"Regionserver groups needs to specially handle what it calls ""special tables"", tables upon which core or other modular functionality depends. They need to be excluded from normal rsgroup processing during bootstrap to avoid circular dependencies or errors due to insufficiently initialized state. I think we also want to ensure that such tables are always given a rsgroup assignment with nonzero servers. (IIRC another issue already raises that point, we can link it later.) Special tables include: The system tables in the 'hbase:' namespace The ACL table if the AccessController coprocessor is installed The Labels table if the VisibilityController coprocessor is installed The Quotas table if the FS quotas feature is active Either we need a facility where ""special tables"" can be registered, which should be in core. Or, we institute a blanket rule that core and all extensions that need a ""special table"" must put them into the 'hbase:' namespace, so the TableName#isSystemTable() test will return TRUE for all, and then rsgroups simply needs to test for that.",0.1026666667,0.1026666667,neutral
hbase,2085,description,Some references in toString() converted from StringBuffer to StringBuilder as concurrency is probably not needed in those contexts as the references do not get out of scope.,design_debt,non-optimal_design,"Thu, 31 Dec 2009 20:44:43 +0000","Fri, 20 Nov 2015 13:01:56 +0000","Thu, 31 Dec 2009 22:34:35 +0000",6592,Some references in toString() converted from StringBuffer to StringBuilder as concurrency is probably not needed in those contexts as the references do not get out of scope.,0.0,0.0,neutral
hbase,20990,description,"In AMv2, we batch open/close region operations and call RS with executeProcedures API. But, in this API, if one of the region's operations throws an exception, all the operations in the batch will receive the same exception. Actually, some of the operations in the batch is executing normally in the RS. I think we should try catch exceptions respectively, and call remoteCallFailed or remoteCallCompleted in respectively. Otherwise, there will be some very strange behave. Such as this one: The AssignProcedure failed with a what??? It is very strange, actually, the AssignProcedure successes on the RS, another CloseRegion operation failed in the operation batch was causing the exception. To correct this, we need to modify the response of executeProcedures API, which is the proto, to return infos(status, exceptions) per operation. This issue alone won't cause much trouble, so not so hurry to change the behave here, but indeed we need to consider this one when we want do some reconstruct to AMv2.",design_debt,non-optimal_design,"Wed, 1 Aug 2018 03:40:40 +0000","Thu, 21 Nov 2019 13:59:28 +0000","Thu, 21 Nov 2019 13:59:28 +0000",41249928,"In AMv2, we batch open/close region operations and call RS with executeProcedures API. But, in this API, if one of the region's operations throws an exception, all the operations in the batch will receive the same exception. Actually, some of the operations in the batch is executing normally in the RS. I think we should try catch exceptions respectively, and call remoteCallFailed or remoteCallCompleted in RegionTransitionProcedure respectively. Otherwise, there will be some very strange behave. Such as this one: The AssignProcedure failed with a NotServingRegionException, what??? It is very strange, actually, the AssignProcedure successes on the RS, another CloseRegion operation failed in the operation batch was causing the exception. To correct this, we need to modify the response of executeProcedures API, which is the ExecuteProceduresResponse proto, to return infos(status, exceptions) per operation. This issue alone won't cause much trouble, so not so hurry to change the behave here, but indeed we need to consider this one when we want do some reconstruct to AMv2.",-0.04492592593,-0.04492592593,neutral
hbase,21238,description,Correct way of handling error condition is through return value of run method.,design_debt,non-optimal_design,"Wed, 26 Sep 2018 15:17:05 +0000","Tue, 16 Oct 2018 13:29:39 +0000","Mon, 15 Oct 2018 20:17:39 +0000",1659634,Correct way of handling error condition is through return value of run method.,0.2375,0.2375,neutral
hbase,21816,description,"User may get confused, to understanding our HBase configurations which are loaded for replication. Sometimes, User may place source and destination cluster conf under ""/etc/hbase/conf"" directory. It will create uncertainty because our log points that all the configurations are co-located. Existing Logs, But it should be something like, This jira only to change the log-line, no issue with the functionality.",design_debt,non-optimal_design,"Fri, 1 Feb 2019 02:03:31 +0000","Sun, 12 May 2019 19:44:01 +0000","Fri, 8 Feb 2019 05:16:04 +0000",616353,"User may get confused, to understanding our HBase configurations which are loaded for replication. Sometimes,User mayplace source and destination clusterconf under ""/etc/hbase/conf"" directory. It willcreate uncertainty because our log pointsthat all the configurations are co-located.  Existing Logs, But it should be something like,  Thisjira only to change the log-line, no issuewith the functionality.",-0.1,-0.1,neutral
hbase,22933,description,"The old implementation is a bit strange, the isStuck method is like this: It can only return true when there are ongoing procedures. But if we have a procedure, then the procedure will try to reassign region. Scheduling a new procedure does not make sense here, at least for branch-2.2+. I suggest we just remove the related code, since the default retry number for assigning a region is Integer.MAX_VALUE. And even if user set this to small value and finally the region is left in FAILED_OPEN state without a procedure, HBCK2 is used to deal with this, it is not necessary to deal it automatically.",design_debt,non-optimal_design,"Tue, 27 Aug 2019 13:34:03 +0000","Mon, 2 Sep 2019 09:03:19 +0000","Fri, 30 Aug 2019 03:21:32 +0000",222449,"The old implementation is a bit strange, the isStuck method is like this: It can only return true when there are ongoing procedures. But if we have a procedure, then the procedure will try to reassign region. Scheduling a new procedure does not make sense here, at least for branch-2.2+. I suggest we just remove the related code, since the default retry number for assigning a region is Integer.MAX_VALUE. And even if user set this to small value and finally the region is left in FAILED_OPEN state without a procedure, HBCK2 is used to deal with this, it is not necessary to deal it automatically.",-0.03764285714,-0.03764285714,negative
hbase,23061,description,We are using Jackson to emit JSON in at least one place in common and client. We don't need all of Jackson and all the associated trouble just to do that. Use a suitably licensed JSON library with no known vulnerability. This will avoid problems downstream because we are trying to avoid having them pull in a vulnerable Jackson via us so Jackson is a 'provided' scope transitive dependency of client and its in-project dependencies (like common). Here's where I am referring to:,design_debt,non-optimal_design,"Sat, 21 Sep 2019 00:53:27 +0000","Fri, 24 Jun 2022 17:55:18 +0000","Sat, 21 Sep 2019 03:28:56 +0000",9329,We are using Jackson to emit JSON in at least one place in common and client. We don't need all of Jackson and all the associated trouble just to do that. Use a suitably licensed JSON library with no known vulnerability. This will avoid problems downstream because we are trying to avoid having them pull in a vulnerable Jackson via us so Jackson is a 'provided' scope transitive dependency of client and its in-project dependencies (like common). Here's where I am referring to: org.apache.hadoop.hbase.util.JsonMapper.<clinit>(JsonMapper.java:37) at org.apache.hadoop.hbase.client.Operation.toJSON(Operation.java:70) at org.apache.hadoop.hbase.client.Operation.toString(Operation.java:96),0.1749,0.03363461538,neutral
hbase,23651,description,"HBASE-17178 Add region balance throttling, but it can not be disabled, sometimes we need no throttle and balance the cluster as fast as possible.",design_debt,non-optimal_design,"Mon, 6 Jan 2020 07:51:00 +0000","Fri, 10 Jan 2020 09:21:20 +0000","Wed, 8 Jan 2020 11:08:04 +0000",184624,"HBASE-17178 Add region balance throttling, but it can not be disabled, sometimes we need no throttle and balance the cluster as fast as possible.",0.0,0.0,neutral
hbase,2969,description,"Considering that the method _getTable(String)_ in is invoked by multiple threads, it may happen that while _'queue == null'_ is true, it is possible to have a queue mapped to that name into the tables map when queue)'_ is executed. However, I don't think it will cause any harm because the overwritten queue will eventually be garbage-collected. :-)",design_debt,non-optimal_design,"Wed, 8 Sep 2010 17:58:53 +0000","Fri, 20 Nov 2015 12:43:50 +0000","Wed, 8 Sep 2010 22:42:04 +0000",16991,"Considering that the method getTable(String) in org.apache.hadoop.hbase.client.HTablePool is invoked by multiple threads, it may happen that while 'queue == null' is true, it is possible to have a queue mapped to that name into the tables map when 'tables.put(tableName, queue)' is executed. However, I don't think it will cause any harm because the overwritten queue will eventually be garbage-collected.",0.2164444444,0.0685,neutral
hbase,3324,description,"The block cache has lots of configuration parameters but they aren't using Configuration like they should. It would also be nice to have a better way of doing hit ratios, like a rolling window.",design_debt,non-optimal_design,"Thu, 9 Dec 2010 08:44:19 +0000","Sun, 12 Jun 2022 00:16:59 +0000","Sun, 18 May 2014 04:08:54 +0000",108501875,"The block cache has lots of configuration parameters but they aren't using Configuration like they should. It would also be nice to have a better way of doing hit ratios, like a rolling window.",0.23125,0.23125,negative
hbase,3510,description,"The IPC readers come out of a thread pool but have no name, which is annoying.",design_debt,non-optimal_design,"Mon, 7 Feb 2011 20:44:11 +0000","Fri, 20 Nov 2015 12:43:36 +0000","Mon, 7 Feb 2011 22:02:23 +0000",4692,"The IPC readers come out of a thread pool but have no name, which is annoying.",-1.0,-1.0,negative
hbase,3528,description,"Our maven build exports a lot of deps, and they flow to clients who depend on us. for example we shouldnt export etc. Clients should be able to depend on any version of the above libraries or NOT depend on them.",design_debt,non-optimal_design,"Sat, 12 Feb 2011 06:44:16 +0000","Sun, 12 Jun 2022 17:29:05 +0000","Mon, 29 Dec 2014 19:06:16 +0000",122386920,"Our maven build exports a lot of deps, and they flow to clients who depend on us. for example we shouldnt export thrift,protobuf,jetty,jruby,tomcat,jersey,guava, etc. Clients should be able to depend on any version of the above libraries or NOT depend on them.",-0.344,0.344,neutral
hbase,3625,description,"Currently the surefire plugin configuration defines the following exclusion: AFAICT the '{{/$**}}' does not resolve to anything meaningful. Adding support to exclude one or more tests via Maven property, i.e.",design_debt,non-optimal_design,"Fri, 11 Mar 2011 07:22:17 +0000","Fri, 20 Nov 2015 12:41:35 +0000","Tue, 15 Mar 2011 01:11:43 +0000",323366,"Currently the surefire plugin configuration defines the following exclusion: AFAICT the '**/*$*' does not resolve to anything meaningful. Adding support to exclude one or more tests via Maven property, i.e. '-Dtest.exclude=<TESTCLASS>' would be useful.",-0.1541666667,-0.05277777778,negative
hbase,3673,description,"In the case of medium-to-large sized HTable pools, the amount of time the client spends blocking on the underlying map and queue data structures turns out to be quite significant. Using an efficient wait-free implementation of maps and queues might serve to reduce the contention on the pool. In particular, I was wondering if we should replace the synchronized map with a concurrent hash map, and linked list with a concurrent linked queue.",design_debt,non-optimal_design,"Fri, 18 Mar 2011 22:20:04 +0000","Fri, 20 Nov 2015 12:40:39 +0000","Thu, 24 Mar 2011 22:28:49 +0000",518925,"In the case of medium-to-large sized HTable pools, the amount of time the client spends blocking on the underlying map and queue data structures turns out to be quite significant. Using an efficient wait-free implementation of maps and queues might serve to reduce the contention on the pool. In particular, I was wondering if we should replace the synchronized map with a concurrent hash map, and linked list with a concurrent linked queue.",0.06666666667,0.06666666667,neutral
hbase,3807,description,Currently the metrics are a mix of MB and bytes. Its confusing.,design_debt,non-optimal_design,"Thu, 21 Apr 2011 06:28:51 +0000","Fri, 20 Nov 2015 12:43:21 +0000","Wed, 10 Aug 2011 19:43:28 +0000",9638077,Currently the metrics are a mix of MB and bytes. Its confusing.,-0.2185,-0.2185,negative
hbase,3840,description,A common user error (and even hbase dev error) is to pass a vanilla Hadoop Configuration into HBase methods that expect to see all of the relevant hbase defaults from hbase-default.xml. This often results in NPE or issues locating ZK. We should add a method like which ensures that the conf has incorporated hbase-default.xml. We can do this by checking for existence of,design_debt,non-optimal_design,"Sun, 1 May 2011 05:56:34 +0000","Sun, 12 Jun 2022 18:18:23 +0000","Wed, 5 Sep 2012 18:08:27 +0000",42639113,A common user error (and even hbase dev error) is to pass a vanilla Hadoop Configuration into HBase methods that expect to see all of the relevant hbase defaults from hbase-default.xml. This often results in NPE or issues locating ZK. We should add a method like HBaseConfiguration.verify(conf) which ensures that the conf has incorporated hbase-default.xml. We can do this by checking for existence of hbase.defaults.for.version.,-0.07222222222,-0.04333333333,negative
hbase,4016,description,"We wanted to use an int (32-bit) atomic counter and we initialize it with a certain value when the row is created. Later, we increment the counter using This call results in one of two outcomes. 1. The call succeeds, but the column value now is a long (64-bit) and is corrupt (by additional data that was in the buffer read). 2. Throws offset (65547) + length (8) exceed the capacity of the array: 65551 Source) Based on our incorrect usage of counters (initializing it with a 32 bit value and later using it as a counter), I would expect that we fail consistently with mode 2 rather than silently corrupting data with mode 1. However, the exception is thrown only rarely and I am not sure what determines the case to be executed. I am wondering if this has something to do with flush. Here is a HRegion unit test that can reproduce this problem. We modified our code to initialize the counter with a 64 bit value. But, I was also wondering if something has to change in to handle inconsistent counter sizes gracefully without corrupting existing data. Please let me know if you need additional information.",design_debt,non-optimal_design,"Tue, 21 Jun 2011 23:07:46 +0000","Fri, 20 Nov 2015 11:53:05 +0000","Tue, 28 Jun 2011 23:36:32 +0000",606526,"We wanted to use an int (32-bit) atomic counter and we initialize it with a certain value when the row is created. Later, we increment the counter using HTable.incrementColumnValue(). This call results in one of two outcomes. 1. The call succeeds, but the column value now is a long (64-bit) and is corrupt (by additional data that was in the buffer read). 2. Throws IOException/IllegalArgumentException. Java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: offset (65547) + length (8) exceed the capacity of the array: 65551 at org.apache.hadoop.hbase.util.Bytes.explainWrongLengthOrOffset(Bytes.java:502) at org.apache.hadoop.hbase.util.Bytes.toLong(Bytes.java:480) at org.apache.hadoop.hbase.regionserver.HRegion.incrementColumnValue(HRegion.java:3139) at org.apache.hadoop.hbase.regionserver.HRegionServer.incrementColumnValue(HRegionServer.java:2468) at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570) at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1039) Based on our incorrect usage of counters (initializing it with a 32 bit value and later using it as a counter), I would expect that we fail consistently with mode 2 rather than silently corrupting data with mode 1. However, the exception is thrown only rarely and I am not sure what determines the case to be executed. I am wondering if this has something to do with flush. Here is a HRegion unit test that can reproduce this problem. http://paste.lisp.org/display/122822 We modified our code to initialize the counter with a 64 bit value. But, I was also wondering if something has to change in HRegion.incrementColumnValue() to handle inconsistent counter sizes gracefully without corrupting existing data. Please let me know if you need additional information.",0.09166666667,0.01486486486,neutral
hbase,4311,description,"When refactoring TestHBaseFsckRepair to add more hbck test cases, I noticed that uses an existing table with empty region, adds more regions, and then attempts to remove the region. The region remains in meta and is causes hbck to report at inconsistency. Ideally these test table generation utility functions should generate clean tables.",design_debt,non-optimal_design,"Wed, 31 Aug 2011 16:31:47 +0000","Sun, 12 Jun 2022 19:20:18 +0000","Thu, 12 Apr 2012 16:49:37 +0000",19441070,"When refactoring TestHBaseFsckRepair to add more hbck test cases, I noticed that HBaseTestingUtility.createMultiRegions uses an existing table with empty region, adds more regions, and then attempts to remove the region. The region remains in meta and is causes hbck to report at inconsistency. Ideally these test table generation utility functions should generate clean tables.",0.06666666667,0.05,negative
hbase,4341,description,"This's the reason of why did get failure . In this test, one case was timeout and cause the whole test process got killed. [logs] Here's the related logs(From [Analysis] One region was opened during the RS's stopping. This is method of HRS#onlineRegions is a ConcurrentHashMap. So walk down this map may not get all the data if some entries are been added during the traverse. Once one region was missed, it can't be closed anymore. And this regionserver will not be stopped normally. Then the following logs occurred:",design_debt,non-optimal_design,"Wed, 7 Sep 2011 02:42:48 +0000","Fri, 20 Nov 2015 11:54:46 +0000","Thu, 8 Sep 2011 15:39:20 +0000",132992,"This's the reason of why did ""https://builds.apache.org/job/hbase-0.90/282"" get failure . In this test, one case was timeout and cause the whole test process got killed. [logs] Here's the related logs(From org.apache.hadoop.hbase.mapreduce.TestTableMapReduce-output.txt): [Analysis] One region was opened during the RS's stopping. This is method of ""HRS#closeAllRegions"": HRS#onlineRegions is a ConcurrentHashMap. So walk down this map may not get all the data if some entries are been added during the traverse. Once one region was missed, it can't be closed anymore. And this regionserver will not be stopped normally. Then the following logs occurred:",-0.2115,-0.128,negative
hbase,4459,description,"There are about 90 classes/codes in HbaseObjectWritable currently and Byte.MAX_VALUE is 127. In addition, anyone wanting to add custom classes but not break compatibility might want to leave a gap before using codes and that's difficult in such limited space. Eventually we should get rid of this pattern that makes compatibility difficult (better client/server protocol handshake) but we should probably at least bump this to a short for 0.94.",design_debt,non-optimal_design,"Thu, 22 Sep 2011 18:46:58 +0000","Fri, 20 Nov 2015 11:55:50 +0000","Thu, 20 Oct 2011 18:23:23 +0000",2417785,"There are about 90 classes/codes in HbaseObjectWritable currently and Byte.MAX_VALUE is 127. In addition, anyone wanting to add custom classes but not break compatibility might want to leave a gap before using codes and that's difficult in such limited space. Eventually we should get rid of this pattern that makes compatibility difficult (better client/server protocol handshake) but we should probably at least bump this to a short for 0.94.",0.0625,0.0625,neutral
hbase,467,description,"Per Jim's suggestions on HBase-419, let's move all the store-related files into a subpackage. It should either be o.a.h.h.store or If we push it down another level, I think that we should make sure we never use any of those files outside of",design_debt,non-optimal_design,"Sun, 24 Feb 2008 05:57:43 +0000","Fri, 22 Aug 2008 21:35:04 +0000","Sun, 24 Feb 2008 22:10:11 +0000",58348,"Per Jim's suggestions on HBase-419, let's move all the store-related files into a subpackage. It should either be o.a.h.h.store or o.a.h.h.regionserver.store. If we push it down another level, I think that we should make sure we never use any of those files outside of o.a.h.h.regionserver.",-0.146,-0.073,neutral
hbase,506,description,"Every so often, we find ourselves trying to debug a problem that happens in HTable where we exhaust all our retries trying to contact the region server hosting the region we want to operate on. Oftentimes the last exception that comes out is something like which should just never be the case. As a way to improve our debugging capabilities, when we decide to throw an exception out of ServerCallable, let's show not just the last exception but all the exceptions that caused all the retries in the first place. This will help us understand the sequence of events that led to us running out of retries.",design_debt,non-optimal_design,"Wed, 12 Mar 2008 00:20:42 +0000","Fri, 22 Aug 2008 21:13:11 +0000","Sat, 15 Mar 2008 21:27:18 +0000",335196,"Every so often, we find ourselves trying to debug a problem that happens in HTable where we exhaust all our retries trying to contact the region server hosting the region we want to operate on. Oftentimes the last exception that comes out is something like WrongRegionException, which should just never be the case. As a way to improve our debugging capabilities, when we decide to throw an exception out of ServerCallable, let's show not just the last exception but all the exceptions that caused all the retries in the first place. This will help us understand the sequence of events that led to us running out of retries.",0.1,0.1,negative
hbase,5333,description,"Currently if the cannot keep up with the writeload, we block writers up to milliseconds (default is 90000). Would be nice if there was a concept of a soft ""backpressure"" that slows writing clients gracefully *before* we reach this condition. From the log: ""2012-02-04 00:00:06,963 WARN Region <table",design_debt,non-optimal_design,"Sat, 4 Feb 2012 01:04:30 +0000","Sun, 12 Jun 2022 20:17:27 +0000","Thu, 23 Aug 2012 04:31:10 +0000",17378800,"Currently if the memstore/flush/compaction cannot keep up with the writeload, we block writers up to hbase.hstore.blockingWaitTime milliseconds (default is 90000). Would be nice if there was a concept of a soft ""backpressure"" that slows writing clients gracefully before we reach this condition. From the log: ""2012-02-04 00:00:06,963 WARN org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Region <table>,,1328313512779.c2761757621ddf8fb78baf5288d71271. has too many store files; delaying flush up to 90000ms""",-0.07916666667,-0.04895833333,negative
hbase,5523,description,"A Delete at time T marks a Put at time T as deleted. In parent I invented special logic that insert a virtual millisecond into the tr if the encountered KV is a delete marker. This was so that there is a way to specify a timerange that would allow to see the put but not the delete: Discussed this today with a coworker and he convinced me that this is very confusing and also not needed. When we have a Delete and Put at the same time T, there *is* not timerange that can include the Put but not the Delete. So I will change the code to this (and fix the tests): It's easier to understand, and does not lead to strange scenarios when the TS is used as a controlled counter. Needs to be done before 0.94 goes out.",design_debt,non-optimal_design,"Mon, 5 Mar 2012 22:43:26 +0000","Tue, 26 Feb 2013 08:12:56 +0000","Tue, 6 Mar 2012 06:22:22 +0000",27536,"A Delete at time T marks a Put at time T as deleted. In parent I invented special logic that insert a virtual millisecond into the tr if the encountered KV is a delete marker. This was so that there is a way to specify a timerange that would allow to see the put but not the delete: Discussed this today with a coworker and he convinced me that this is very confusing and also not needed. When we have a Delete and Put at the same time T, there is not timerange that can include the Put but not the Delete. So I will change the code to this (and fix the tests): It's easier to understand, and does not lead to strange scenarios when the TS is used as a controlled counter. Needs to be done before 0.94 goes out.",0.05483333333,0.05483333333,neutral
hbase,6009,description,"The additions to add backup masters to ClusterStatus are technically incompatible between clients and servers. Older clients will basically not read the extra bits that the newer server pushes for the backup masters, thus screwing up the serialization for the next blob in the pipe. For the Writable, we can add a total size field for ClusterStatus at the beginning, or we can have start and end markers. I can make a patch for either approach; interested in whatever folks have to suggest. Would be good to get this in soon to limit the damage to 0.92.1 (don't know if we can get this in in time for 0.94.0). Either change will make us forward-compatible starting with when the change goes in, but will not fix the backwards incompatibility, which we will have to mark with a release note as there have already been releases with this change. Hopefully we can do this in a cleaner way when wire compat rolls around in 0.96.",design_debt,non-optimal_design,"Wed, 16 May 2012 00:17:14 +0000","Mon, 13 Jun 2022 16:39:46 +0000","Mon, 25 Jun 2012 21:56:10 +0000",3533936,"The additions to add backup masters to ClusterStatus are technically incompatible between clients and servers. Older clients will basically not read the extra bits that the newer server pushes for the backup masters, thus screwing up the serialization for the next blob in the pipe. For the Writable, we can add a total size field for ClusterStatus at the beginning, or we can have start and end markers. I can make a patch for either approach; interested in whatever folks have to suggest. Would be good to get this in soon to limit the damage to 0.92.1 (don't know if we can get this in in time for 0.94.0). Either change will make us forward-compatible starting with when the change goes in, but will not fix the backwards incompatibility, which we will have to mark with a release note as there have already been releases with this change. Hopefully we can do this in a cleaner way when wire compat rolls around in 0.96.",0.2191190476,0.2191190476,neutral
hbase,6050,description,The scenario is like this There if the regiondir doesnot exist we tend to create and then add the recovered.edits. Because of this HBCK thinks it to be an orphan region because we have the regiondir but with no regioninfo. Ideally cluster is fine but we it is misleading.,design_debt,non-optimal_design,"Fri, 18 May 2012 15:41:10 +0000","Tue, 26 Feb 2013 08:16:18 +0000","Sun, 27 May 2012 16:38:55 +0000",781065,The scenario is like this -> A region is getting splitted. -> The master is still not processed the split . -> Region server goes down. -> Split log manager starts splitting the logs and creates the recovered.edits in the splitlog path. -> CJ starts and deletes the entry from META and also just completes the deletion of the region dir. -> in hlogSplitter on final step we rename the recovered.edits to come under the regiondir. There if the regiondir doesnot exist we tend to create and then add the recovered.edits. Because of this HBCK thinks it to be an orphan region because we have the regiondir but with no regioninfo. Ideally cluster is fine but we it is misleading.,-0.04375,-0.001714285714,negative
hbase,6093,description,"Many applications run with maxVersions=1 and do not care about timestamps, or they will specify one timestamp per row as a normal KeyValue rather than per-cell. Then, DataBlockEncoders like those in HBASE-4218 and HBASE-4676 often encode timestamps as diffs from the previous or diffs from the minimum timestamp in the block. If all timestamps in a block are the same, they will all compress to basically <= 8 bytes total per block. This can be 10% to 25% space savings for some schemas, and that savings is realized both on disk and in block cache. We could add a ColumnFamily setting If true, then all timestamps are modified during a flush/compaction to the currentTimeMillis() at the start of the flush/compaction. If all timestamps are made identical in a file, then the encoder will be able to eliminate them. The simplest use case is probably that where all inserts are type=Put, there are no overwrites, and there are no deletes. As use cases get more complex, then so does the implementation. For example, what happens when there is a Put and a Delete of the same cell in the same memstore? Maybe for a flush at t=flushStartTime, the Put gets timestamp=t, and the Delete gets timestamp=t+1. Or maybe HBASE-4241 could take care of this problem.",design_debt,non-optimal_design,"Fri, 25 May 2012 06:15:44 +0000","Mon, 13 Jun 2022 16:34:39 +0000","Sat, 11 Apr 2015 01:33:27 +0000",90789463,"Many applications run with maxVersions=1 and do not care about timestamps, or they will specify one timestamp per row as a normal KeyValue rather than per-cell. Then, DataBlockEncoders like those in HBASE-4218 and HBASE-4676 often encode timestamps as diffs from the previous or diffs from the minimum timestamp in the block. If all timestamps in a block are the same, they will all compress to basically <= 8 bytes total per block. This can be 10% to 25% space savings for some schemas, and that savings is realized both on disk and in block cache. We could add a ColumnFamily setting flattenTimestamps=[true/false]. If true, then all timestamps are modified during a flush/compaction to the currentTimeMillis() at the start of the flush/compaction. If all timestamps are made identical in a file, then the encoder will be able to eliminate them. The simplest use case is probably that where all inserts are type=Put, there are no overwrites, and there are no deletes. As use cases get more complex, then so does the implementation. For example, what happens when there is a Put and a Delete of the same cell in the same memstore? Maybe for a flush at t=flushStartTime, the Put gets timestamp=t, and the Delete gets timestamp=t+1. Or maybe HBASE-4241 could take care of this problem.",0.04796969697,0.05951388889,neutral
hbase,6151,description,"See, for example: The HRegionServer calls HBaseServer: but the server can start accepting RPCs once the threads have been started, but if they do, they throw until openServer runs. We should probably 1) Catch the remote exception and retry on the master 2) Look into whether the start() behavior of HBaseServer makes any sense. Why would you start accepting RPCs only to throw back",design_debt,non-optimal_design,"Sat, 2 Jun 2012 00:51:46 +0000","Mon, 13 Jun 2022 16:37:09 +0000","Fri, 13 Jul 2012 00:48:01 +0000",3542175,"See, for example: The HRegionServer calls HBaseServer: but the server can start accepting RPCs once the threads have been started, but if they do, they throw ServerNotRunningException until openServer runs. We should probably 1) Catch the remote exception and retry on the master 2) Look into whether the start() behavior of HBaseServer makes any sense. Why would you start accepting RPCs only to throw back ServerNotRunningException?",0.3,0.3,neutral
hbase,6244,description,"Now, the RowResultGenerator and the will fit the column family if the request doesn't contain any column info. The will cost 10+ milliseconds in our hbase cluster each request. We can remove these code because the server will auto add the columns.",design_debt,non-optimal_design,"Wed, 20 Jun 2012 15:16:18 +0000","Tue, 26 Feb 2013 08:15:58 +0000","Wed, 20 Jun 2012 21:16:08 +0000",21590,"Now, the RowResultGenerator and the ScanerResultGenerator will fit the column family if the request doesn't contain any column info. The table.getTableDescriptor() will cost 10+ milliseconds in our hbase cluster each request. We can remove these code because the server will auto add the columns.",0.1943333333,0.14575,neutral
hbase,6969,description,"While not an official part of the HBase API, it'd be nice if HBaseTestingUtility allowed for consumer to inject their own instance of the when executing tests. Currently there is no way to control how the ZK instance is started which we've had to hack around when there were ZK version conflicts (3.3 vs 3.4). Or if you want to control which specific port it starts on vs random. Allowing consumers to inject an instance (unstarted) gives them the freedom to implement their own without having to fork the entire HBaseTestingUtility class",design_debt,non-optimal_design,"Wed, 10 Oct 2012 02:34:56 +0000","Tue, 14 Jun 2022 22:00:52 +0000","Sat, 11 Apr 2015 00:23:59 +0000",78875343,"While not an official part of the HBase API, it'd be nice if HBaseTestingUtility allowed for consumer to inject their own instance of the MiniZookeeperCluster when executing tests. Currently there is no way to control how the ZK instance is started which we've had to hack around when there were ZK version conflicts (3.3 vs 3.4). Or if you want to control which specific port it starts on vs random. Allowing consumers to inject an instance (unstarted) gives them the freedom to implement their own without having to fork the entire HBaseTestingUtility class",0.090625,0.090625,neutral
hbase,7212,description,"This is a simplified version of what was proposed in HBASE-6573. Instead of claiming to be a 2pc or 3pc implementation (which implies logging at each actor, and recovery operations) this is just provides a best effort global barrier mechanism called a Procedure. Users need only to implement a methods to acquireBarrier, to act when insideBarrier, and to releaseBarrier that use the ExternalException cooperative error checking mechanism. Globally consistent snapshots require the ability to quiesce writes to a set of region servers before a the snapshot operation is executed. Also if any node fails, it needs to be able to notify them so that they abort. The first cut of other online snapshots don't need the fully barrier but may still use this for its error propagation mechanisms. This version removes the extra layer incurred in the previous implementation due to the use of generics, separates the coordinator and members, and reduces the amount of inheritance used in favor of composition.",design_debt,non-optimal_design,"Thu, 22 Nov 2012 19:08:26 +0000","Mon, 23 Sep 2013 18:30:35 +0000","Sat, 29 Dec 2012 06:18:52 +0000",3150626,"This is a simplified version of what was proposed in HBASE-6573. Instead of claiming to be a 2pc or 3pc implementation (which implies logging at each actor, and recovery operations) this is just provides a best effort global barrier mechanism called a Procedure. Users need only to implement a methods to acquireBarrier, to act when insideBarrier, and to releaseBarrier that use the ExternalException cooperative error checking mechanism. Globally consistent snapshots require the ability to quiesce writes to a set of region servers before a the snapshot operation is executed. Also if any node fails, it needs to be able to notify them so that they abort. The first cut of other online snapshots don't need the fully barrier but may still use this for its error propagation mechanisms. This version removes the extra layer incurred in the previous implementation due to the use of generics, separates the coordinator and members, and reduces the amount of inheritance used in favor of composition.",0.1198928571,0.1198928571,neutral
hbase,76,description,"Chatting with Jim while looking at profiler outputs, we should make an effort at purging the servers of the Text type so HRegionServer doesn't ever have to deal in Characters and the concomitant encode/decode to UTF-8. Toward this end, we'd make changes like moving HStoreKey to have four rather than 3 data members: column family, column family qualifier, row + timestamp done as a basic Writable -- -- and a long rather than a Text column, Text row and a timestamp long. This would save on our having to do the relatively expensive 'find' of the column family separator inside in extractFamily (",design_debt,non-optimal_design,"Mon, 7 Jan 2008 22:16:02 +0000","Fri, 22 Aug 2008 21:13:05 +0000","Thu, 15 May 2008 22:03:00 +0000",11144818,"Chatting with Jim while looking at profiler outputs, we should make an effort at purging the servers of the Text type so HRegionServer doesn't ever have to deal in Characters and the concomitant encode/decode to UTF-8. Toward this end, we'd make changes like moving HStoreKey to have four rather than 3 data members: column family, column family qualifier, row + timestamp done as a basic Writable  ImmutableBytesWritable?  and a long rather than a Text column, Text row and a timestamp long. This would save on our having to do the relatively expensive 'find' of the column family separator inside in extractFamily (>10% of CPU scanning). Chatting about it, we could effect the change without change in the public client API; clients could continue to take Text type for row and column and then client-side, the convertion to HStoreKey could be done before crossing the wire to the server.",0.1333333333,0.08,neutral
hbase,8089,description,"This proposal outlines an improvement to HBase that provides for a set of types, above and beyond the existing ""byte-bucket"" strategy. This is intended to reduce user-level duplication of effort, provide better support for 3rd-party integration, and provide an overall improved experience for developers using HBase.",design_debt,non-optimal_design,"Wed, 13 Mar 2013 16:18:34 +0000","Thu, 16 Jun 2022 05:49:38 +0000","Sat, 11 Jun 2022 18:11:53 +0000",291779599,"This proposal outlines an improvement to HBase that provides for a set of types, above and beyond the existing ""byte-bucket"" strategy. This is intended to reduce user-level duplication of effort, provide better support for 3rd-party integration, and provide an overall improved experience for developers using HBase.",0.496,0.496,positive
hbase,8608,description,"Since we enabled blooms, there is a bunch of bloom spew.... We also do a bunch of logging around new file open....",design_debt,non-optimal_design,"Fri, 24 May 2013 00:09:52 +0000","Mon, 23 Sep 2013 19:08:34 +0000","Mon, 10 Jun 2013 22:00:16 +0000",1547424,"Since we enabled blooms, there is a bunch of bloom spew.... We also do a bunch of logging around new file open....",0.0,0.0,neutral
hbase,8665,description,"Note that this can be solved by bumping up the number of compaction threads but still it seems like this priority ""inversion"" should be dealt with. There's a store with 1 big file and 3 flushes (1 2 3 4) sitting around and minding its own business when it decides to compact. Compaction (2 3 4) is created and put in queue, it's low priority, so it doesn't get out of the queue for some time - other stores are compacting. Meanwhile more files are flushed and at (1 2 3 4 5 6 7) it decides to compact (5 6 7). This compaction now has higher priority than the first one. After that if the load is high it enters vicious cycle of compacting and compacting files as they arrive, with store being blocked on and off, with the (2 3 4) compaction staying in queue for up to ~20 minutes (that I've seen). I wonder why we do thing thing where we queue compaction and compact separately. Perhaps we should take snapshot of all store priorities, then do select in order and execute the first compaction we find. This will need starvation safeguard too but should probably be better. Btw, exploring compaction policy may be more prone to this, as it can select files from the middle, not just beginning, which, given the treatment of already selected files that was not changed from the old ratio-based one (all files with lower seqNums than the ones selected are also ineligible for further selection), will make more files ineligible (e.g. imagine with 10 blocking files, with 8 present (1-8), (6 7 8) being selected and getting stuck). Today I see the case that would also apply to old policy, but yesterday I saw file distribution something like this: 4,5g, 2,1g, 295,9m, 113,3m, 68,0m, 67,8m, 1,1g, 295,1m, 100,4m, unfortunately w/o enough logs to figure out how it resulted.",design_debt,non-optimal_design,"Thu, 30 May 2013 23:21:38 +0000","Thu, 5 May 2022 02:11:33 +0000","Wed, 19 Jun 2013 00:42:56 +0000",1646478,"Note that this can be solved by bumping up the number of compaction threads but still it seems like this priority ""inversion"" should be dealt with. There's a store with 1 big file and 3 flushes (1 2 3 4) sitting around and minding its own business when it decides to compact. Compaction (2 3 4) is created and put in queue, it's low priority, so it doesn't get out of the queue for some time - other stores are compacting. Meanwhile more files are flushed and at (1 2 3 4 5 6 7) it decides to compact (5 6 7). This compaction now has higher priority than the first one. After that if the load is high it enters vicious cycle of compacting and compacting files as they arrive, with store being blocked on and off, with the (2 3 4) compaction staying in queue for up to ~20 minutes (that I've seen). I wonder why we do thing thing where we queue compaction and compact separately. Perhaps we should take snapshot of all store priorities, then do select in order and execute the first compaction we find. This will need starvation safeguard too but should probably be better. Btw, exploring compaction policy may be more prone to this, as it can select files from the middle, not just beginning, which, given the treatment of already selected files that was not changed from the old ratio-based one (all files with lower seqNums than the ones selected are also ineligible for further selection), will make more files ineligible (e.g. imagine with 10 blocking files, with 8 present (1-8), (6 7 8) being selected and getting stuck). Today I see the case that would also apply to old policy, but yesterday I saw file distribution something like this: 4,5g, 2,1g, 295,9m, 113,3m, 68,0m, 67,8m, 1,1g, 295,1m, 100,4m, unfortunately w/o enough logs to figure out how it resulted.",0.1573151515,0.1573151515,neutral
hbase,8816,description,"Introducing an optional parameter 'num_tables' into LoadTestTool. When it's specified with positive integer n, LoadTestTool will load n tables parallely. -tn parameter value becomes table name prefix. Tables are created with name in format <tn The motivation is to add a handy way to load multiple tables concurrently. In addition, we could use this option to test resource leakage of long running clients.",design_debt,non-optimal_design,"Fri, 28 Jun 2013 04:40:38 +0000","Wed, 21 Aug 2013 00:08:49 +0000","Sun, 4 Aug 2013 01:33:55 +0000",3185597,"Introducing an optional parameter 'num_tables' into LoadTestTool. When it's specified with positive integer n, LoadTestTool will load n tables parallely. -tn parameter value becomes table name prefix. Tables are created with name in format <tn>_1...<tn>_n. A sample command line ""-tn test -num_tables 2"" will create & load tables:""test_1"" and ""test_2"" The motivation is to add a handy way to load multiple tables concurrently. In addition, we could use this option to test resource leakage of long running clients.",0.1955,0.1955,neutral
hbase,9052,description,"If a region is split/merged, before it's removed from meta, you can still assign it from the HBase shell. It's better to prevent this from happening.",design_debt,non-optimal_design,"Fri, 26 Jul 2013 23:48:30 +0000","Mon, 23 Sep 2013 19:22:36 +0000","Tue, 30 Jul 2013 16:56:23 +0000",320873,"If a region is split/merged, before it's removed from meta, you can still assign it from the HBase shell. It's better to prevent this from happening.",0.2155,0.2155,negative
hbase,9668,description,The Scan objects used to construct AggregateRequest's for aggregates supported by AggregationClient qualify as small scan because response from each region is small. We should utilize small scan for better performance.,design_debt,non-optimal_design,"Fri, 27 Sep 2013 01:53:26 +0000","Thu, 16 Jun 2022 18:12:40 +0000","Fri, 27 Sep 2013 02:55:22 +0000",3716,The Scan objects used to construct AggregateRequest's for aggregates supported by AggregationClient qualify as small scan because response from each region is small. We should utilize small scan for better performance.,0.45,0.45,neutral
hbase,980,description,"Profiling, I learned that the HBASE-975 makes things worse rather than better. For every Reader opened -- one is opened per store file when we open a region as well as a Reader per file when we compact and then another Reader whenever a Scanner is opened -- the change adds about 4 seeks and at least in the case of compacting and scanning, to no benefit. Even where it is of benefit, when going against HalfMapFiles or when many Store files and we're testing to see if row is in file, it looks like the number of seeks saved are miniscule -- definetly not something that would show up in timings. This issue is about undoing the get of first and last key on open of a store file, the heart of HBASE-975 (975 included a bunch of cleanup refactoring. That'll stay). Profiling seeks, I did notice that we do an extra seek during a get, a reset that takes us to the start of the file. Then internally to getClosest, the core of our get, we're also doing a seek to closest index. Let me try undoing the extra seek and see if it breaks things.",design_debt,non-optimal_design,"Tue, 4 Nov 2008 06:46:48 +0000","Sun, 13 Sep 2009 22:26:31 +0000","Tue, 4 Nov 2008 08:29:14 +0000",6146,"Profiling, I learned that the HBASE-975 makes things worse rather than better. For every Reader opened  one is opened per store file when we open a region as well as a Reader per file when we compact and then another Reader whenever a Scanner is opened  the change adds about 4 seeks and at least in the case of compacting and scanning, to no benefit. Even where it is of benefit, when going against HalfMapFiles or when many Store files and we're testing to see if row is in file, it looks like the number of seeks saved are miniscule  definetly not something that would show up in timings. This issue is about undoing the get of first and last key on open of a store file, the heart of HBASE-975 (975 included a bunch of cleanup refactoring. That'll stay). Profiling seeks, I did notice that we do an extra seek during a get, a reset that takes us to the start of the file. Then internally to getClosest, the core of our get, we're also doing a seek to closest index. Let me try undoing the extra seek and see if it breaks things.",0.1453541667,0.1453541667,negative
hbase,9950,description,"We have a replication setup with the same table and column family being present in multiple data centers. Currently, all of them have exactly the same data, but each cluster doesn't need all the data. Rows need to be present in only x out of the total y clusters. This information varies at the row level and thus more granular replication cannot be achieved by setting up cluster level replication. Adding row level replication should solve this.",design_debt,non-optimal_design,"Mon, 11 Nov 2013 22:26:36 +0000","Thu, 16 Jun 2022 18:32:26 +0000","Tue, 14 May 2019 00:38:34 +0000",173585518,"We have a replication setup with the same table and column family being present in multiple data centers. Currently, all of them have exactly the same data, but each cluster doesn't need all the data. Rows need to be present in only x out of the total y clusters. This information varies at the row level and thus more granular replication cannot be achieved by setting up cluster level replication. Adding row level replication should solve this.",0.04,0.04,neutral
hbase,10074,description,"Region count description is in config section; region size description is in architecture sections; both of these have a lot of good technical details, but imho we could do better in terms of admin-centric advice. Currently, there's a nearly-empty capacity section; I'd like to rewrite it to consolidate capacity sizing information, and some basic configuration pertaining to it.",documentation_debt,low_quality_documentation,"Tue, 3 Dec 2013 22:47:13 +0000","Mon, 16 Dec 2013 18:46:36 +0000","Thu, 5 Dec 2013 23:22:28 +0000",174915,"Region count description is in config section; region size description is in architecture sections; both of these have a lot of good technical details, but imho we could do better in terms of admin-centric advice. Currently, there's a nearly-empty capacity section; I'd like to rewrite it to consolidate capacity planning/sizing/region sizing information, and some basic configuration pertaining to it.",0.219,0.219,neutral
hbase,10081,description,"Discussed in HBASE-7091. It's not critical, but a little bit surprising, as the comments in bin/hbase doesn't say anything about this. If you create your own hbase-env then it's not an issue...",documentation_debt,outdated_documentation,"Wed, 4 Dec 2013 17:50:48 +0000","Fri, 17 Jun 2022 04:50:59 +0000","Wed, 22 May 2019 01:31:31 +0000",172309243,"Discussed in HBASE-7091. It's not critical, but a little bit surprising, as the comments in bin/hbase doesn't say anything about this. If you create your own hbase-env then it's not an issue...",0.0,0.0,neutral
hbase,10334,description,The links to RS's seems to be broken in table.jsp after HBASE-9892.,documentation_debt,low_quality_documentation,"Tue, 14 Jan 2014 04:03:25 +0000","Sat, 21 Feb 2015 23:31:51 +0000","Tue, 14 Jan 2014 23:23:46 +0000",69621,The links to RS's seems to be broken in table.jsp after HBASE-9892.,-0.1,-0.1,negative
hbase,10864,description,We should really be more careful about spelling qualifier,documentation_debt,low_quality_documentation,"Fri, 28 Mar 2014 17:42:56 +0000","Sat, 21 Feb 2015 23:38:18 +0000","Sat, 29 Mar 2014 03:40:27 +0000",35851,We should really be more careful about spelling qualifier,0.4,0.4,negative
hbase,11421,description,"HTable#batch() APIs javadoc says * @param actions list of Get, Put, Delete, Increment, Append, RowMutations objects But we can not pass RowMutations in batch. Small patch to correct the doc.",documentation_debt,low_quality_documentation,"Fri, 27 Jun 2014 11:36:39 +0000","Sat, 21 Feb 2015 23:29:51 +0000","Fri, 27 Jun 2014 17:02:42 +0000",19563,"HTable#batch() APIs javadoc says @param actions list of Get, Put, Delete, Increment, Append, RowMutations objects But we can not pass RowMutations in batch. Small patch to correct the doc.",0.4375,0.4375,negative
hbase,11730,description,"New development goes against trunk and is backported as desired to existing release branches. From what I have seen on the jira, it looks like each branch's release manager makes the call on backporting a particular issue. We should document both this norm and who the relevant release manager is for each branch. In the current docs, I'd suggest adding the RM list to the ""Codelines"" section (18.11.1) and add a brief explanation of pinging the RM as a new section after ""submitting a patch again"" (18.12.6). Post HBASE-4593, the note about pinging a prior branch RM should just go as a bullet in the ""patch workflow.""",documentation_debt,low_quality_documentation,"Wed, 13 Aug 2014 15:12:15 +0000","Fri, 6 Apr 2018 17:55:46 +0000","Thu, 18 Sep 2014 02:52:28 +0000",3066013,"New development goes against trunk and is backported as desired to existing release branches. From what I have seen on the jira, it looks like each branch's release manager makes the call on backporting a particular issue. We should document both this norm and who the relevant release manager is for each branch. In the current docs, I'd suggest adding the RM list to the ""Codelines"" section (18.11.1) and add a brief explanation of pinging the RM as a new section after ""submitting a patch again"" (18.12.6). Post HBASE-4593, the note about pinging a prior branch RM should just go as a bullet in the ""patch workflow.""",0.18,0.18,neutral
hbase,12400,description,"The refguide has bits of code in it. The code does 'new HTable' to get a table instance. Rather, it should be promoting the new style where we get a Connection and then do a getTable on it. Ditto for references to 'new HBaseAdmin'. See ConnectionFactory for new style. See also package-info.java in Client for updated example. Misty, if you are game for this one, I can help w/ how it should look.",documentation_debt,outdated_documentation,"Fri, 31 Oct 2014 21:19:58 +0000","Fri, 6 Apr 2018 17:54:34 +0000","Wed, 26 Nov 2014 17:31:28 +0000",2232690,"The refguide has bits of code in it. The code does 'new HTable' to get a table instance. Rather, it should be promoting the new style where we get a Connection and then do a getTable on it. Ditto for references to 'new HBaseAdmin'. See ConnectionFactory for new style. See also package-info.java in Client for updated example. Misty, if you are game for this one, I can help w/ how it should look.",0.09375,0.09375,neutral
hbase,13582,description,the ref guide currently points to HTrace at its old location. update it to point at the ASF project. Should also verify that the usage example is still correct.,documentation_debt,outdated_documentation,"Tue, 28 Apr 2015 15:39:49 +0000","Fri, 24 Jun 2022 18:23:50 +0000","Tue, 19 May 2015 23:06:06 +0000",1841177,the ref guide currently points to HTrace at its old location. update it to point at the ASF project. Should also verify that the usage example is still correct.,0.1926666667,0.1926666667,neutral
hbase,13629,description,Parent introduced a typo. should be,documentation_debt,low_quality_documentation,"Wed, 6 May 2015 05:19:16 +0000","Fri, 24 Jun 2022 18:45:02 +0000","Wed, 6 May 2015 05:33:09 +0000",833,Parent introduced a typo. should be,0.0,0.0,negative
hbase,13924,description,"The description in the following is wrong: The is *not* used for coprocessors, but only for filters, comparators, and exceptions. Fix.",documentation_debt,low_quality_documentation,"Wed, 17 Jun 2015 12:58:31 +0000","Fri, 24 Jun 2022 19:24:29 +0000","Mon, 10 Aug 2015 04:08:38 +0000",4633807,"The description in the following is wrong: The DynamicClassLoader is not used for coprocessors, but only for filters, comparators, and exceptions. Fix.",-0.125,-0.125,negative
hbase,16856,description,"A very small bug, a typo in exception message: It should print currentSequence and syncFutureSequence, but print two syncFutureSequence",documentation_debt,low_quality_documentation,"Mon, 17 Oct 2016 06:22:14 +0000","Tue, 18 Oct 2016 02:49:44 +0000","Mon, 17 Oct 2016 14:24:16 +0000",28922,"A very small bug, a typo in exception message: It should print currentSequence and syncFutureSequence, but print two syncFutureSequence",0.0,0.0,negative
hbase,17918,description,"It looks like HBASE-9465 addresses one of the major flaws in our existing replication (namely that order of delivery is not assured). All I see in the reference guide is a note on Instead we should cover this in the replication section, especially given that we call out the order of delivery limitation.",documentation_debt,low_quality_documentation,"Fri, 14 Apr 2017 16:43:32 +0000","Thu, 16 Jun 2022 18:03:11 +0000","Tue, 10 Apr 2018 01:45:51 +0000",31136539,"It looks like HBASE-9465 addresses one of the major flaws in our existing replication (namely that order of delivery is not assured). All I see in the reference guide is a note on hbase.serial.replication.waitingMs. Instead we should cover this in the replication section, especially given that we call out the order of delivery limitation.",-0.447,-0.149,negative
hbase,22206,description,"The dist.apache.org server is only intended for use by developers in staging releases. It must not be used on public download pages. Please use www.apache.org/dist (for KEYS, hashes and sigs) and the mirror system instead. The current download page has lots of references to dist.a.o; please replace thes.",documentation_debt,low_quality_documentation,"Wed, 10 Apr 2019 21:43:14 +0000","Mon, 22 Apr 2019 22:37:35 +0000","Mon, 22 Apr 2019 14:12:44 +0000",1009770,"The dist.apache.org server is only intended for use by developers in staging releases. It must not be used on public download pages. Please use www.apache.org/dist (for KEYS, hashes and sigs) and the mirror system instead. The current download page has lots of references to dist.a.o; please replace thes.",0.1555555556,0.1555555556,neutral
hbase,2621,description,"There's a bad link in the HFile javadoc; should point to HBASE-61, but instead point to HBASE-3315. Also cleaned up the reference to TFile.",documentation_debt,low_quality_documentation,"Fri, 28 May 2010 09:38:24 +0000","Fri, 20 Nov 2015 12:40:31 +0000","Fri, 28 May 2010 18:23:37 +0000",31513,"There's a bad link in the HFile javadoc; should point to HBASE-61, but instead point to HBASE-3315. Also cleaned up the reference to TFile.",-0.3,-0.3,negative
hbase,396,description,"Currently javadoc for hbase contrib does not show up anywhere at Below is some discussion from hadoop-dev list on how to do the hbase contrib javadoc build. From: Doug Cutting Re: javadoc for hbase on apache.org I'd vote for including it as we have other contrib documentation, as a separate section in the main javadoc tree. Doug Michael Stack wrote: > St.Ack",documentation_debt,low_quality_documentation,"Mon, 21 May 2007 18:42:25 +0000","Mon, 4 Feb 2008 18:42:03 +0000","Mon, 21 May 2007 23:05:49 +0000",15804,"Currently javadoc for hbase contrib does not show up anywhere at lucene.apache.org/hadoop. Below is some discussion from hadoop-dev list on how to do the hbase contrib javadoc build. From: Doug Cutting <cutting@apache.org> Subject: Re: javadoc for hbase on apache.org I'd vote for including it as we have other contrib documentation, as a separate section in the main javadoc tree. Doug Michael Stack wrote: > Any chance of having the hbase javadoc show somewhere up on > lucene.apache.org/hadoop? > > It looks like other contribs  streaming and datajoin  have their > javadoc produced as part of the general hadoop javadoc target up in the > root build.xml. I could submit a patch like the below that adds hbase > but perhaps folks have other ideas such as a 'javadoc-contrib' target in > the root build.xml that calls down into subtargets under src/contrib? > > Thanks, > St.Ack",-0.05,0.02557142857,neutral
hbase,4804,description,I was going through the 0.92 CHANGES and found are a few entries in CHANGES.txt where jira numbers don't match up descriptions.,documentation_debt,low_quality_documentation,"Wed, 16 Nov 2011 23:18:06 +0000","Fri, 20 Nov 2015 11:52:12 +0000","Wed, 16 Nov 2011 23:24:47 +0000",401,I was going through the 0.92 CHANGES and found are a few entries in CHANGES.txt where jira numbers don't match up descriptions.,0.0,0.0,negative
hbase,6264,description,"In section 6.9: In section 9.2: There is ""are are"" twice. I'm not 100% sure what's the best way to propose a fix, so I have included the patch below. If that's fine, I will probably propose some other corrections. JM Index:  (rvision 1352979) +++ (copie de travail) @@ -828,7 +828,7 @@ Secondary Indexes and Alternate Query Paths </title <para- A common example on the dist-list is where a row-key is of the format ""user-timestamp"" but there are are reporting requirements on activity across users for certain + A common example on the dist-list is where a row-key is of the format ""user-timestamp"" but there are reporting requirements on activity across users for certain time ranges. Thus, selecting by user is easy because it is in the lead position of the key, but time is not. </para <para@@ -1324,7 +1324,7 @@ <section <title- <para+ <para of the HBase shell's <code </para <section",documentation_debt,low_quality_documentation,"Sat, 23 Jun 2012 20:01:33 +0000","Mon, 23 Sep 2013 18:31:39 +0000","Thu, 30 Aug 2012 23:10:43 +0000",5886550,"In section 6.9: http://hbase.apache.org/book/secondary.indexes.html In section 9.2: http://hbase.apache.org/book/arch.catalog.html There is ""are are"" twice. I'm not 100% sure what's the best way to propose a fix, so I have included the patch below. If that's fine, I will probably propose some other corrections. JM Index: branches/0.94/src/docbkx/book.xml ===================================================================  branches/0.94/src/docbkx/book.xml (rvision 1352979) +++ branches/0.94/src/docbkx/book.xml (copie de travail) @@ -828,7 +828,7 @@ Secondary Indexes and Alternate Query Paths </title> <para>This section could also be titled ""what if my table rowkey looks like <emphasis>this</emphasis> but I also want to query my table like <emphasis>that</emphasis>."" A common example on the dist-list is where a row-key is of the format ""user-timestamp"" but there are are reporting requirements on activity across users for certain + A common example on the dist-list is where a row-key is of the format ""user-timestamp"" but there are reporting requirements on activity across users for certain time ranges. Thus, selecting by user is easy because it is in the lead position of the key, but time is not. </para> <para>There is no single answer on the best way to handle this because it depends on... @@ -1324,7 +1324,7 @@ <section xml:id=""arch.catalog""> <title>Catalog Tables</title> <para>The catalog tables ROOT and .META. exist as HBase tables. They are are filtered out + <para>The catalog tables ROOT and .META. exist as HBase tables. They are filtered out of the HBase shell's <code>list</code> command, but they are in fact tables just like any other. </para> <section xml:id=""arch.catalog.root"">",0.03683333333,-0.0202,neutral
hbase,9131,description,HBASE-7404 added the bucket cache but its configuration settings are currently undocumented. Without documentation developers would be the only ones aware of the feature. Specifically documentation about slide 23 from would be great to add!,documentation_debt,outdated_documentation,"Mon, 5 Aug 2013 23:28:26 +0000","Thu, 16 Jun 2022 17:50:38 +0000","Wed, 18 Jun 2014 03:37:06 +0000",27317320,HBASE-7404 added the bucket cache but its configuration settings are currently undocumented. Without documentation developers would be the only ones aware of the feature. Specifically documentation about slide 23 from http://www.slideshare.net/cloudera/operations-session-4 would be great to add!,-0.004333333333,-0.004333333333,neutral
hbase,9742,description,"The [security in the HBase book only talks about using Kerberos. There is a simple user access mode too that is not documented. This should be documented for development systems and HBase installs where security is not an issue, but they want to prevent user mistakes. I've added a section to the security chapter that talks about simple user access. The new section makes it very clear it is not secure.",documentation_debt,low_quality_documentation,"Thu, 10 Oct 2013 17:47:49 +0000","Fri, 20 Nov 2015 11:53:29 +0000","Fri, 11 Oct 2013 04:15:29 +0000",37660,"The security section in the HBase book only talks about using Kerberos. There is a simple user access mode too that is not documented. This should be documented for development systems and HBase installs where security is not an issue, but they want to prevent user mistakes. I've added a section to the security chapter that talks about simple user access. The new section makes it very clear it is not secure.",-0.05333333333,-0.05333333333,negative
hbase,991,description,"The examples in package doc. are old making mention of the long deprecated Text, etc. Update them.",documentation_debt,outdated_documentation,"Tue, 11 Nov 2008 06:24:39 +0000","Sun, 13 Sep 2009 22:26:32 +0000","Mon, 17 Nov 2008 02:28:58 +0000",504259,"The examples in package doc. are old making mention of the long deprecated Text, etc. Update them.",0.0,0.0,negative
hbase,17259,description,"I'm noticing that while I have create and update APIs for quotas, I missed the remove functionality. Need to add public API for that and some tests.",requirement_debt,requirement_partially_implemented,"Mon, 5 Dec 2016 18:01:55 +0000","Fri, 6 Apr 2018 04:00:24 +0000","Mon, 13 Feb 2017 17:30:07 +0000",6046092,"I'm noticing that while I have create and update APIs for quotas, I missed the remove functionality. Need to add public API for that and some tests.",-0.2,-0.2,negative
hbase,17808,description,"FastPath for the FIFO rpcscheduler was introduced in HBASE-16023. But it is not implemented for RW queues. In this issue, I use in RW queues. So anyone who want to isolate their read/write requests can also benefit from the fastpath. I haven't test the performance yet. But since I haven't change any of the core implemention of it should have the same performance in HBASE-16023.",requirement_debt,requirement_partially_implemented,"Mon, 20 Mar 2017 09:13:13 +0000","Wed, 15 Dec 2021 02:48:27 +0000","Wed, 15 Dec 2021 02:48:27 +0000",149535314,"FastPath for the FIFO rpcscheduler was introduced in HBASE-16023. But it is not implemented for RW queues. In this issue, I use FastPathBalancedQueueRpcExecutor in RW queues. So anyone who want to isolate their read/write requests can also benefit from the fastpath. I haven't test the performance yet. But since I haven't change any of the core implemention of FastPathBalancedQueueRpcExecutor, it should have the same performance in HBASE-16023.",0.074,0.074,neutral
hbase,3696,description,"LocalFileSystem in Hadoop doesn't currently implement sync(), so when we're running in that case, we don't have any durability. This isn't a huge deal since it isn't a realistic deployment scenario, but it's probably worth documenting. It caused some confusion for a user when a table disappeared after killing a standalone instance that was hosting its data in the local FS.",requirement_debt,requirement_partially_implemented,"Thu, 24 Mar 2011 01:10:36 +0000","Sun, 12 Jun 2022 00:55:50 +0000","Wed, 28 May 2014 04:08:13 +0000",100321057,"LocalFileSystem in Hadoop doesn't currently implement sync(), so when we're running in that case, we don't have any durability. This isn't a huge deal since it isn't a realistic deployment scenario, but it's probably worth documenting. It caused some confusion for a user when a table disappeared after killing a standalone instance that was hosting its data in the local FS.",-0.1125,-0.1125,negative
hbase,10008,description,"is flakey on Jenkins, this test has failed due to timeout a couple times for me on precommit. Here's the stacktrace.",test_debt,flaky_test,"Wed, 20 Nov 2013 02:44:57 +0000","Fri, 20 Nov 2015 11:53:20 +0000","Thu, 21 Nov 2013 17:28:03 +0000",139386,"TestNamespaceCommands is flakey on Jenkins, this test has failed due to timeout a couple times for me on precommit. Here's the stacktrace.",-0.2,-0.2,negative
hbase,15192,description,"fails intermittently due to failed assertion on cleaned merge region count: Before calling the test does: newcount1 is not cleared at the beginning of the loop. This means that if the check for newcount1 <= 1 doesn't pass the first iteration, it wouldn't pass in subsequent iterations. After timeout is exhausted, is called. However, there is a chance that has been called by the Chore already (during the wait period), leaving the cleaned count 0 and failing the test.",test_debt,flaky_test,"Sat, 30 Jan 2016 00:05:31 +0000","Wed, 24 Feb 2016 06:39:27 +0000","Mon, 8 Feb 2016 23:34:56 +0000",862165,"TestRegionMergeTransactionOnCluster#testCleanMergeReference fails intermittently due to failed assertion on cleaned merge region count: Before calling CatalogJanitor#scan(), the test does: newcount1 is not cleared at the beginning of the loop. This means that if the check for newcount1 <= 1 doesn't pass the first iteration, it wouldn't pass in subsequent iterations. After timeout is exhausted, admin.runCatalogScan() is called. However, there is a chance that CatalogJanitor#scan() has been called by the Chore already (during the wait period), leaving the cleaned count 0 and failing the test.",-0.2,-0.16,negative
hbase,20100,description,"Failed in the nightly in interesting way. A subprocedure of enable table is assigning regions. The test is doing a kill of the procedure store to ensure we can handle all state transtions even in face of failure. In this case, the kill of PE framework came in the finish of the assign procedure AFTER we'd updated hbase:meta and the internal AM state moving Region from OPENING to OPEN. Rerunning this step on restoration of the AMv2 WAL Store results in....",test_debt,low_coverage,"Tue, 27 Feb 2018 19:59:19 +0000","Tue, 7 May 2019 16:08:43 +0000","Thu, 1 Mar 2018 16:55:27 +0000",161768,"Failed in the nightly in interesting way. https://builds.apache.org/job/HBase%20Nightly/job/branch-2/398/ A subprocedure of enable table is assigning regions. The test is doing a kill of the procedure store to ensure we can handle all state transtions even in face of failure. In this case, the kill of PE framework came in the finish of the assign procedure AFTER we'd updated hbase:meta and the internal AM state moving Region from OPENING to OPEN. Rerunning this step on restoration of the AMv2 WAL Store results in....",-0.07333333333,-0.07333333333,neutral
hbase,22424,description,"When running rsgroup test class folder to run all the UTs together, and will flaky. Because TestRSGroupsAdmin1, TestRSGroupsAdmin2 and TestRSGroupsBalance are all extends TestRSGroupsBase, which has a static variable INIT, controlling the initialize of 'master 'group and the number of rs in 'default' rsgroup. Output errors of is shown in HBASE-22420, and will encounter NPE in because `master` group has not been added.",test_debt,flaky_test,"Wed, 15 May 2019 08:47:37 +0000","Fri, 17 May 2019 19:59:26 +0000","Fri, 17 May 2019 01:18:34 +0000",145857,"When running rsgroup test class folder to run all the UTs together, TestRSGroupsAdmin2.testMoveServersAndTables and TestRSGroupsBalance.testGroupBalance will flaky. Because TestRSGroupsAdmin1, TestRSGroupsAdmin2 and TestRSGroupsBalance are all extends TestRSGroupsBase, which has a static variable INIT, controlling the initialize of 'master 'group and the number of rs in 'default' rsgroup. Output errors of TestRSGroupsBalance.testGroupBalance is shown in HBASE-22420, and TestRSGroupsAdmin2.testMoveServersAndTables will encounter NPE in ```rsGroupAdmin.getRSGroupInfo(""master"").containsServer(server.getAddress())``` because `master` group has not been added.",-0.03958333333,0.043125,neutral
hbase,23789,description,"We can't find the balancer rules we just read in the test in high load conditions Test then goes on to fail with: Instead, have tests write rules to local test dir.",test_debt,low_coverage,"Tue, 4 Feb 2020 05:49:24 +0000","Tue, 7 Apr 2020 21:39:02 +0000","Wed, 5 Feb 2020 00:42:09 +0000",67965,"We can't find the balancer rules we just read in the HeterogeneousRegionCountCostFunction test in high load conditions Test then goes on to fail with: Instead, have tests write rules to local test dir.",0.4,0.4,negative
hbase,23867,description,Test has this on it: @Test // Test is flakey. TODO: Fix! Let me cut it down. It runs for a while. It fails nearly every run since HBASE-23865 Up flakey history from 5 to 10 which upped the flakey test so it ran with more fork count.,test_debt,flaky_test,"Wed, 19 Feb 2020 06:38:28 +0000","Tue, 7 Apr 2020 20:38:07 +0000","Wed, 19 Feb 2020 06:42:59 +0000",271,Test has this on it: @Test // Test is flakey. TODO: Fix! Let me cut it down. It runs for a while. It fails nearly every run since HBASE-23865 Up flakey history from 5 to 10 which upped the flakey test so it ran with more fork count.,-0.1292,-0.1292,negative
hbase,4740,description,"Running more frequently seems to show that it has become flaky. It is difficult to tell if this is because of a unrecoverable failure or a recoverable failure. To make this visiable from test and for users, we need to make a change to bulkload call's interface on HRegionServer. The change should make successful rpcs return true, recoverable failures return false, and unrecoverable failure throw an IOException. This is an RPC change, so it would be really good to get this api right before the final 0.92 goes out.",test_debt,flaky_test,"Thu, 3 Nov 2011 17:58:14 +0000","Fri, 20 Nov 2015 11:53:11 +0000","Tue, 8 Nov 2011 14:40:38 +0000",420144,"Running TestHFileOutputFormat more frequently seems to show that it has become flaky. It is difficult to tell if this is because of a unrecoverable failure or a recoverable failure. To make this visiable from test and for users, we need to make a change to bulkload call's interface on HRegionServer. The change should make successful rpcs return true, recoverable failures return false, and unrecoverable failure throw an IOException. This is an RPC change, so it would be really good to get this api right before the final 0.92 goes out.",0.07381428571,0.07381428571,neutral
hbase,5217,description,"At some point we disabled tests for the thrift server. In addition, it looks like the getRegionInfo no longer functions. I'd like to reenable the tests and add one for getRegionInfo. I had to write this to test my changes in HBASE-2600 anyway. I figured I would break it out. We shouldn't commit it until we have fixed getting the regioninfo from the thriftserver.",test_debt,low_coverage,"Tue, 17 Jan 2012 19:17:36 +0000","Sun, 12 Jun 2022 20:10:28 +0000","Wed, 22 Apr 2015 00:33:57 +0000",102834981,"At some point we disabled tests for the thrift server. In addition, it looks like the getRegionInfo no longer functions. I'd like to reenable the tests and add one for getRegionInfo. I had to write this to test my changes in HBASE-2600 anyway. I figured I would break it out. We shouldn't commit it until we have fixed getting the regioninfo from the thriftserver.",-0.03333333333,-0.03333333333,negative
hbase,7172,description,"fails when run individually (run just that test case from eclipse). I've also noticed that it is flaky on windows. The reason is a rare race condition, which somehow does not happen that much when the whole class is run. The sequence of events is smt like this: - we create 1 log file to split - we call in its own thread. - is waiting in since there are no splitlogworkers, it keep waiting. - we delete the task znode from zk - SplitLogManager receives the zk callback from which will call setDone() and mark the task as success. - However, meanwhile the loops sees that remainingInZK == 0, and calls return concurrently to the above. - on return from fails because the znode delete callback has not completed yet. This race only happens when the last task is deleted from zk, and normally only the SplitLogManager deletes the task znodes after processing it, so I don't think this is a production issue.",test_debt,flaky_test,"Fri, 16 Nov 2012 00:38:37 +0000","Tue, 26 Feb 2013 08:23:00 +0000","Wed, 28 Nov 2012 22:41:14 +0000",1116157,"TestSplitLogManager.testVanishingTaskZNode fails when run individually (run just that test case from eclipse). I've also noticed that it is flaky on windows. The reason is a rare race condition, which somehow does not happen that much when the whole class is run. The sequence of events is smt like this: we create 1 log file to split we call splitLogDistributed() in its own thread. splitLogDistributed() is waiting in waitForSplittingCompletion() since there are no splitlogworkers, it keep waiting. we delete the task znode from zk SplitLogManager receives the zk callback from GetDataAsyncCallback, which will call setDone() and mark the task as success. However, meanwhile the waitForSplittingCompletion() loops sees that remainingInZK == 0, and calls return concurrently to the above. on return from waitForSplittingCompletion(), splitLogDistributed() fails because the znode delete callback has not completed yet. This race only happens when the last task is deleted from zk, and normally only the SplitLogManager deletes the task znodes after processing it, so I don't think this is a production issue.",-0.03,0.01,negative
hbase,8256,description,"To make the Jenkin build more useful, it is good to keep it blue/green. We can mark those flaky tests flaky, and don't run them by default. However, people can still run them. We can also set up a Jekin build just for those flaky tests.",test_debt,flaky_test,"Wed, 3 Apr 2013 18:07:36 +0000","Thu, 16 Jun 2022 05:57:12 +0000","Mon, 27 Jan 2014 18:03:21 +0000",25833345,"To make the Jenkin build more useful, it is good to keep it blue/green. We can mark those flaky tests flaky, and don't run them by default. However, people can still run them. We can also set up a Jekin build just for those flaky tests.",0.35475,0.35475,neutral
impala,1120,description,"We should update our code that fetches and updates column stats to use the new bulk APIs introduced in Hive .13. Instead of fetching a single column at a time, we can now fetch stats for a list of column names.",architecture_debt,using_obsolete_technology,"Tue, 29 Jul 2014 00:38:53 +0000","Tue, 13 Jan 2015 18:17:33 +0000","Tue, 13 Jan 2015 18:17:33 +0000",14578720,"We should update our code that fetches and updates column stats to use the new bulk APIs introduced in Hive .13. Instead of fetching a single column at a time, we can now fetch stats for a list of column names.",0.08333333333,0.08333333333,neutral
impala,4652,description,Kudu's utility library depends on We need to add the most recent version to the toolchain.,architecture_debt,using_obsolete_technology,"Mon, 12 Dec 2016 18:43:32 +0000","Thu, 12 Jan 2017 23:51:13 +0000","Thu, 12 Jan 2017 23:51:13 +0000",2696861,Kudu's utility library depends on crcutil. We need to add the most recent version to the toolchain.,0.0,0.0,neutral
impala,6080,description,One of the parts of this patch was cleanup of how descriptor tables were managed. As described in the commit message: That cleanup should be independent of the larger patch so we could get it in separately.,architecture_debt,violation_of_modularity,"Wed, 18 Oct 2017 23:53:05 +0000","Thu, 16 Nov 2017 22:12:16 +0000","Thu, 16 Nov 2017 22:12:16 +0000",2499551,One of the parts of this patch https://gerrit.cloudera.org/#/c/7065/ was cleanup of how descriptor tables were managed. As described in the commit message: That cleanup should be independent of the larger patch so we could get it in separately.,0.1,0.1,neutral
impala,3252,description,There seems to be a configuration file checked in in git that changes every time we do a build. The file is This file should be removed from git.,build_debt,build_others,"Mon, 28 Mar 2016 17:55:33 +0000","Mon, 11 Jun 2018 20:57:57 +0000","Mon, 11 Jun 2018 20:42:56 +0000",69562043,There seems to be a configuration file checked in in git that changes every time we do a build. The file is thirdparty/hadoop-2.6.0-cdh5.8.0-SNAPSHOT/share/hadoop/kms/tomcat/conf/server.xml. This file should be removed from git.,0.0,0.025,negative
impala,3338,description,"The Impala-lzo build depends on the Impala build scripts and headers, but we don't need to do an entire Impala build to produce the Impala-lzo .so. Factor out the build logic from buildall.sh so it can be built independently if needed.",build_debt,build_others,"Wed, 13 Apr 2016 00:50:25 +0000","Tue, 7 Jun 2016 16:19:39 +0000","Tue, 7 Jun 2016 16:19:39 +0000",4807754,"The Impala-lzo build depends on the Impala build scripts and headers, but we don't need to do an entire Impala build to produce the Impala-lzo .so. Factor out the build logic from buildall.sh so it can be built independently if needed.",0.125,0.125,neutral
impala,8862,description,These get bundled into the containers but are not be invoked at runtime. We should be able to avoid including them as dependencies entirely.,build_debt,over-declared_dependencies,"Wed, 14 Aug 2019 18:39:35 +0000","Fri, 16 Aug 2019 17:47:59 +0000","Fri, 16 Aug 2019 17:47:59 +0000",169704,These get bundled into the containers but are not be invoked at runtime. We should be able to avoid including them as dependencies entirely.,0.122,0.122,neutral
impala,1013,description,This function is very simple and goes through the HS2 interface to retrieve a few functions. I believe this used to be instantaneously and now takes hundreds of seconds making this likely unusable.,code_debt,slow_algorithm,"Wed, 21 May 2014 21:54:09 +0000","Wed, 11 Jun 2014 19:22:37 +0000","Wed, 11 Jun 2014 19:22:37 +0000",1805308,This function is very simple and goes through the HS2 interface to retrieve a few functions. I believe this used to be instantaneously and now takes hundreds of seconds making this likely unusable.,-0.2,-0.2,negative
impala,1414,description,"The following query on a relatively small input takes ~70s with codegen and ~35 minutes without codegen, which seems unreasonably long. We should make sure we're not doing anything crazy in the non-codegen eval path. Executing with codegen: The query profile when running without codegen is attached. I also used perf (linux profiling tool) to collect a profile (sampled stacks) and printed the aggregated callstacks to the attached file The summary without codegen. Most of the time (28mins) is spent in the hash joins:",code_debt,slow_algorithm,"Thu, 23 Oct 2014 18:22:13 +0000","Thu, 29 Sep 2016 20:47:33 +0000","Thu, 29 Sep 2016 20:36:09 +0000",61092836,"The following query on a relatively small input takes ~70s with codegen and ~35 minutes without codegen, which seems unreasonably long. We should make sure we're not doing anything crazy in the non-codegen eval path. Executing with codegen: The query profile when running without codegen is attached. I also used perf (linux profiling tool) to collect a profile (sampled stacks) and printed the aggregated callstacks to the attached file perf-histograms.out. The summary without codegen. Most of the time (28mins) is spent in the hash joins:",0.025,0.01785714286,neutral
impala,1466,description,"The catalog cache is cold. Loading the table metadata when I've a single request is fast (within 2sec). However, when I've 100+ concurrent queries all requesting the same table metadata, the loading time extremely long (in 10s of seconds+). This shouldn't be the case. The first request should load the metadata and all subsequent request (even though they're concurrent) should be a no-op.",code_debt,slow_algorithm,"Tue, 11 Nov 2014 19:24:05 +0000","Thu, 26 Jan 2017 01:03:23 +0000","Thu, 26 Jan 2017 01:03:23 +0000",69658758,"The catalog cache is cold. Loading the table metadata when I've a single request is fast (within 2sec). However, when I've 100+ concurrent queries all requesting the same table metadata, the loading time extremely long (in 10s of seconds+). This shouldn't be the case. The first request should load the metadata and all subsequent request (even though they're concurrent) should be a no-op.",0.0,0.0,negative
impala,1552,description,"When running out of memory in PAGG the error message does not make it clear why we ran out of memory. In the example below we see that the limit is 100MB and the consumption is only 8MB, but we ran out of memory. In this particular case, the reason is that PAGG tries to allocate the initial min mem for each partition and fails (each partition needs ~8MB and we have 32 of them). It would be good to improve the error message with some more useful info, e.g. how much more memory was needed. Backend 0:Memory Limit Exceeded Limit: Limit=100.00 MB Consumption=8.03 MB Fragment Consumption=8.00 KB EXCHANGE_NODE (id=5): Consumption=0 DataStreamRecvr: Consumption=0 Block Manager: Consumption=0 Fragment Consumption=4.01 MB SORT_NODE (id=2): Consumption=0 AGGREGATION_NODE (id=4): Consumption=4.00 MB EXCHANGE_NODE (id=3): Consumption=0 DataStreamRecvr: Consumption=0 DataStreamSender: Consumption=4.00 KB Fragment Consumption=4.01 MB AGGREGATION_NODE (id=1): Consumption=4.00 MB HDFS_SCAN_NODE (id=0): Consumption=0 DataStreamSender: Consumption=4.00 KB",code_debt,low_quality_code,"Wed, 26 Nov 2014 02:12:58 +0000","Wed, 4 Jan 2017 23:58:13 +0000","Tue, 8 Sep 2015 16:37:21 +0000",24762263,"When running out of memory in PAGG the error message does not make it clear why we ran out of memory. In the example below we see that the limit is 100MB and the consumption is only 8MB, but we ran out of memory. In this particular case, the reason is that PAGG tries to allocate the initial min mem for each partition and fails (each partition needs ~8MB and we have 32 of them). It would be good to improve the error message with some more useful info, e.g. how much more memory was needed. Backend 0:Memory Limit Exceeded Query(3b428a0f1e22f611:722a79f624fcb0b5) Limit: Limit=100.00 MB Consumption=8.03 MB Fragment 3b428a0f1e22f611:722a79f624fcb0b6: Consumption=8.00 KB EXCHANGE_NODE (id=5): Consumption=0 DataStreamRecvr: Consumption=0 Block Manager: Consumption=0 Fragment 3b428a0f1e22f611:722a79f624fcb0b7: Consumption=4.01 MB SORT_NODE (id=2): Consumption=0 AGGREGATION_NODE (id=4): Consumption=4.00 MB EXCHANGE_NODE (id=3): Consumption=0 DataStreamRecvr: Consumption=0 DataStreamSender: Consumption=4.00 KB Fragment 3b428a0f1e22f611:722a79f624fcb0b8: Consumption=4.01 MB AGGREGATION_NODE (id=1): Consumption=4.00 MB HDFS_SCAN_NODE (id=0): Consumption=0 DataStreamSender: Consumption=4.00 KB",-0.1704857143,-0.1704857143,negative
impala,1577,description,"The performance of from_utc_timestamp, besides being extremely poor compared to that of other builtin functions such as from_unixtime, depends greatly on the input timezone. Given that evaluating this function can dominate the total query time, it would be good to at least make its performance more consistent, and even better to improve its performance overall.",code_debt,slow_algorithm,"Fri, 5 Dec 2014 01:16:08 +0000","Mon, 9 Jul 2018 16:45:30 +0000","Mon, 9 Jul 2018 16:45:30 +0000",113412562,"The performance of from_utc_timestamp, besides being extremely poor compared to that of other builtin functions such as from_unixtime, depends greatly on the input timezone. Given that evaluating this function can dominate the total query time, it would be good to at least make its performance more consistent, and even better to improve its performance overall.",0.07933333333,0.07933333333,negative
impala,1596,description,Some the builtin decimal functions assume they can always use the val16 union field of a DecimalType. This isn't actually a valid assumption though. We should only initialize and use the smaller fields (val4/val8) for performance if possible.,code_debt,low_quality_code,"Wed, 10 Dec 2014 21:10:54 +0000","Tue, 5 May 2015 22:31:37 +0000","Tue, 5 May 2015 22:31:37 +0000",12619243,Some the builtin decimal functions assume they can always use the val16 union field of a DecimalType. This isn't actually a valid assumption though. We should only initialize and use the smaller fields (val4/val8) for performance if possible.,0.0,0.0,negative
impala,1929,description,"While running the mem leak test with 4 concurrent clients using the release bits an impalad crashed at with the following stack trace. This can happen in case of RIGHT_OUTER, RIGHT_ANTI and FULL_OUTER joins when the hash table of the partition is NULL. The code has a dcheck that the hash_tbl is not null, which we didn't hit because we were using the release bits.",code_debt,low_quality_code,"Tue, 31 Mar 2015 23:44:24 +0000","Sun, 20 Dec 2015 00:05:28 +0000","Sun, 19 Jul 2015 02:18:49 +0000",9426865,"While running the mem leak test with 4 concurrent clients using the release bits an impalad crashed at PHJ::NextSpilledProbeRowBatch() with the following stack trace. This can happen in case of RIGHT_OUTER, RIGHT_ANTI and FULL_OUTER joins when the hash table of the partition is NULL. The code has a dcheck that the hash_tbl is not null, which we didn't hit because we were using the release bits.",0.02822222222,0.02822222222,neutral
impala,1963,description,"We have several data sources that return dates in ISO-8601 format. Following IMPALA-648, we are closer to having these convertable to timestamp without effort, but time zones are still not supported: Neither format with a time zone specifier will parse even though they are valid based on my understanding of ISO-8601 and For now we have worked around this by converting with substring to cut off the time zone identifier, but I'm not sure which time zone the dates will be interpreted in. If I cut off the Z, will my dates be parsed in server local time? Ideally, we should be able to cast dates with time zone specifiers to timestamps with cast easily.",code_debt,low_quality_code,"Wed, 22 Apr 2015 15:03:34 +0000","Fri, 5 Jun 2015 04:47:07 +0000","Fri, 5 Jun 2015 04:47:07 +0000",3764613,"We have several data sources that return dates in ISO-8601 format. Following IMPALA-648, we are closer to having these convertable to timestamp without effort, but time zones are still not supported: Neither format with a time zone specifier will parse even though they are valid based on my understanding of ISO-8601 and http://en.wikipedia.org/wiki/ISO_8601 For now we have worked around this by converting with substring to cut off the time zone identifier, but I'm not sure which time zone the dates will be interpreted in. If I cut off the Z, will my dates be parsed in server local time? Ideally, we should be able to cast dates with time zone specifiers to timestamps with cast easily.",0.09541666667,0.09541666667,neutral
impala,1984,description,"CDH 5.4 Impala 2.2.0 create TABLE EmptySrc ( F1 INT, F2 STRING ) ; create TABLE EmptyTgt ( F1 INT, F2 STRING ) ; insert OVERWRITE EmptyTgt SELECT * FROM EmptySrc ; Query: insert OVERWRITE EmptyTgt SELECT * FROM EmptySrc WARNINGS: Could not list directory: This warning is unnecessary when there are empty tables involved and should be removed.",code_debt,low_quality_code,"Thu, 7 May 2015 20:44:13 +0000","Fri, 15 May 2015 23:21:35 +0000","Fri, 15 May 2015 23:21:35 +0000",700642,"CDH 5.4 Impala 2.2.0 create TABLE EmptySrc ( F1 INT, F2 STRING ) ; create TABLE EmptyTgt ( F1 INT, F2 STRING ) ; insert OVERWRITE EmptyTgt SELECT * FROM EmptySrc ; [cdh54-2.ent.xxx.com:21000] > insert OVERWRITE EmptyTgt SELECT * FROM EmptySrc ; Query: insert OVERWRITE EmptyTgt SELECT * FROM EmptySrc WARNINGS: Could not list directory: hdfs://cdh54-1.ent.xxx.com:8020/user/hive/warehouse/emptytgt This warning is unnecessary when there are empty tables involved and should be removed.",0.3152,0.2508,neutral
impala,2244,description,"This function now loops through each scan range and when there are many, is noticeably slow. For example, an EXPLAIN on a query against 1M blocks and 1000K partitions went from around 450 ms to 700 ms with this change.",code_debt,slow_algorithm,"Mon, 24 Aug 2015 23:08:00 +0000","Thu, 17 Sep 2015 06:17:46 +0000","Thu, 17 Sep 2015 06:17:46 +0000",2012986,"This function now loops through each scan range and when there are many, is noticeably slow. For example, an EXPLAIN on a query against 1M blocks and 1000K partitions went from around 450 ms to 700 ms with this change.",-0.1,-0.1,negative
impala,231,description,"Impala's HBase scan is very slow. For scanning 40000 rows from an colocated HBase table, it took ~5.7sec. The query is: select count(*) from (select * from hbasetbl limit 40000); Majority of the time is spent inside I've done some experiment to figure out where we spent the time by adding a few more timers. Here's the runtime profile: HBASE_SCAN_NODE (id=0):(5s714ms 99.98%) - BytesRead: 4.26 MB - 483.594us - 5s710ms - MemoryUsed: 0.00 - MyOwnTimer1: 1s387ms <-- We should trim this time. - MyOwnTimer2: 2s798ms <-- - MyOwnTimer3: 688.179ms - MyOwnTimer4: 0ns - MyOwnTimer5: 0ns - NumDisksAccessed: 0 - 5.14 MB/sec - RowsReturned: 40.00K (40000) - RowsReturnedRate: 7.00 K/sec - ScanRangesComplete: 0 - 0 - 0ns - 0ns - 0ns - 0ns - 0 - 827.604ms <-- we spent only 800ms on fetching from HBase - 775.05 KB/sec I've attached the code with more timers in the attached file When I increase the limit to 80000, the perf is much worse. Here's the profile: HBASE_SCAN_NODE (id=0):(23s027ms 99.99%) - BytesRead: 8.51 MB - 249.40us - 23s018ms - MemoryUsed: 0.00 - MyOwnTimer1: 5s680ms - MyOwnTimer2: 11s401ms - MyOwnTimer3: 2s829ms - MyOwnTimer4: 0ns - MyOwnTimer5: 0ns - NumDisksAccessed: 0 - 2.76 MB/sec - RowsReturned: 80.00K (80000) - RowsReturnedRate: 3.47 K/sec - ScanRangesComplete: 0 - 0 - 0ns - 0ns - 0ns - 0ns - 0 - 3s085ms - 376.52 KB/sec I didn't see any disk activity. So, I dont' think HBase is going to disk. Also, the read time is only 3sec. The big chuck of time is spent within the block of MyOwnTimer1 and MyOwnTimer2. We spent 5x more time there even though we only scan 2x more rows.",code_debt,slow_algorithm,"Thu, 11 Apr 2013 20:43:39 +0000","Wed, 17 Apr 2013 20:28:22 +0000","Wed, 17 Apr 2013 20:28:12 +0000",517473,"Impala's HBase scan is very slow. For scanning 40000 rows from an colocated HBase table, it took ~5.7sec. The query is: select count from (select * from hbasetbl limit 40000); Majority of the time is spent inside HBaseTableScanner::Next. I've done some experiment to figure out where we spent the time by adding a few more timers. Here's the runtime profile: HBASE_SCAN_NODE (id=0):(5s714ms 99.98%) BytesRead: 4.26 MB HBaseTableScanner.ScanSetup: 483.594us HBaseTableScanner::ResultScanner_next: 5s710ms MemoryUsed: 0.00 MyOwnTimer1: 1s387ms <-- We should trim this time. MyOwnTimer2: 2s798ms <-- MyOwnTimer3: 688.179ms MyOwnTimer4: 0ns MyOwnTimer5: 0ns NumDisksAccessed: 0 PerReadThreadRawHdfsThroughput: 5.14 MB/sec RowsReturned: 40.00K (40000) RowsReturnedRate: 7.00 K/sec ScanRangesComplete: 0 ScannerThreadsInvoluntaryContextSwitches: 0 ScannerThreadsTotalWallClockTime: 0ns MaterializeTupleTime: 0ns ScannerThreadsSysTime: 0ns ScannerThreadsUserTime: 0ns ScannerThreadsVoluntaryContextSwitches: 0 TotalRawHdfsReadTime: 827.604ms <-- we spent only 800ms on fetching from HBase TotalReadThroughput: 775.05 KB/sec I've attached the code with more timers in the attached file HBaseTAbleScanner.Next When I increase the limit to 80000, the perf is much worse. Here's the profile: HBASE_SCAN_NODE (id=0):(23s027ms 99.99%) BytesRead: 8.51 MB HBaseTableScanner.ScanSetup: 249.40us HBaseTableScanner::ResultScanner_next: 23s018ms MemoryUsed: 0.00 MyOwnTimer1: 5s680ms MyOwnTimer2: 11s401ms MyOwnTimer3: 2s829ms MyOwnTimer4: 0ns MyOwnTimer5: 0ns NumDisksAccessed: 0 PerReadThreadRawHdfsThroughput: 2.76 MB/sec RowsReturned: 80.00K (80000) RowsReturnedRate: 3.47 K/sec ScanRangesComplete: 0 ScannerThreadsInvoluntaryContextSwitches: 0 ScannerThreadsTotalWallClockTime: 0ns MaterializeTupleTime: 0ns ScannerThreadsSysTime: 0ns ScannerThreadsUserTime: 0ns ScannerThreadsVoluntaryContextSwitches: 0 TotalRawHdfsReadTime: 3s085ms TotalReadThroughput: 376.52 KB/sec I didn't see any disk activity. So, I dont' think HBase is going to disk. Also, the read time is only 3sec. The big chuck of time is spent within the block of MyOwnTimer1 and MyOwnTimer2. We spent 5x more time there even though we only scan 2x more rows.",0.03755555556,0.04914285714,neutral
impala,2341,description,"Ideally, we should make this query work. Alternatively, the error message should be improved. Stack:",code_debt,low_quality_code,"Mon, 14 Sep 2015 22:36:03 +0000","Wed, 16 Sep 2015 04:52:08 +0000","Wed, 16 Sep 2015 04:52:08 +0000",108965,"Ideally, we should make this query work. Alternatively, the error message should be improved. Stack:",0.2,0.2,neutral
impala,2355,description,"I am running a select query that is fetching only 3 rows but taking 15 mins to provide the resultset. I have gone through the Profile and observed that majority of the time is spend on unregister query or Client fetch wait time. I just want to deep dive to find the exact issue. Can someone please help me in the same. Query Info Duration: 15m, 2s Rows Produced: 3 Bytes Streamed: 182.8 MiB Client Fetch Wait Time: 15.0m Client Fetch Wait Time Percentage: 100 Query Timeline Start execution: 81.43us (81.43us) Planning finished: 27ms (27ms) Ready to start remote fragments: 29ms (2ms) Remote fragments started: 724ms (694ms) Rows available: 1.16s (440ms) First row fetched: 1.76s (594ms) Unregister query: 15.0m (15.0m)",code_debt,slow_algorithm,"Thu, 17 Sep 2015 08:03:36 +0000","Tue, 22 Sep 2015 04:39:24 +0000","Tue, 22 Sep 2015 04:39:24 +0000",419748,"I am running a select query that is fetching only 3 rows but taking 15 mins to provide the resultset. I have gone through the Profile and observed that majority of the time is spend on unregister query or Client fetch wait time. I just want to deep dive to find the exact issue. Can someone please help me in the same. Query Info Duration: 15m, 2s Rows Produced: 3 Bytes Streamed: 182.8 MiB Client Fetch Wait Time: 15.0m Client Fetch Wait Time Percentage: 100 Query Timeline Start execution: 81.43us (81.43us) Planning finished: 27ms (27ms) Ready to start remote fragments: 29ms (2ms) Remote fragments started: 724ms (694ms) Rows available: 1.16s (440ms) First row fetched: 1.76s (594ms) Unregister query: 15.0m (15.0m)",0.4205,0.4205,negative
impala,2457,description,"According to the math behind the PERCENT_RANK() value, a row group containing a single row yields a division by zero error and therefore Impala returns NaN in this case. In this example, there is only 1 'Reptile' row and that line in the result set has a NaN: Most RDBMS examples on the web use data with multiple rows per group, so they don't illustrate what's supposed to happen in this case. But I suspect the right return value in this case might be zero. Cf. ""The first row in any set has a PERCENT_RANK of 0."" That statement suggests the first row (and any tied rows I presume) could be special-cased and no calculation performed. Cf. The analytic example doesn't show all of the relevant output, but there are two DEPARTMENT_IDs in the output that have only one relevant row, and in both those cases the PERCENT_RANK result is shown as 0. I didn't find any PERCENT_RANK() in MySQL to try. I don't have a PostgreSQL instance handy.",code_debt,low_quality_code,"Thu, 1 Oct 2015 00:21:34 +0000","Wed, 7 Oct 2015 00:07:38 +0000","Wed, 7 Oct 2015 00:07:38 +0000",517564,"According to the math behind the PERCENT_RANK() value, a row group containing a single row yields a division by zero error and therefore Impala returns NaN in this case. In this example, there is only 1 'Reptile' row and that line in the result set has a NaN: Most RDBMS examples on the web use data with multiple rows per group, so they don't illustrate what's supposed to happen in this case. But I suspect the right return value in this case might be zero. Cf. http://docs.aws.amazon.com/redshift/latest/dg/r_WF_PERCENT_RANK.html ""The first row in any set has a PERCENT_RANK of 0."" That statement suggests the first row (and any tied rows I presume) could be special-cased and no calculation performed. Cf. http://docs.oracle.com/cd/B19306_01/server.102/b14200/functions109.htm The analytic example doesn't show all of the relevant output, but there are two DEPARTMENT_IDs in the output that have only one relevant row, and in both those cases the PERCENT_RANK result is shown as 0. I didn't find any PERCENT_RANK() in MySQL to try. I don't have a PostgreSQL instance handy.",-0.07198333333,-0.07198333333,neutral
impala,2642,description,"I just noticed this while reading the statestore code: {{OfferUpdate()}} takes if the update queue is full, but in one place that lock has already been taken by the caller: It's not as scary as it sounds, because if the update queue has > 10000 entries there's something wrong anyway, but we should fix this.",code_debt,multi-thread_correctness,"Fri, 6 Nov 2015 00:31:26 +0000","Tue, 6 Mar 2018 18:34:34 +0000","Tue, 6 Mar 2018 18:34:34 +0000",73591388,"I just noticed this while reading the statestore code: OfferUpdate() takes subscribers_lock_ if the update queue is full, but in one place that lock has already been taken by the caller: It's not as scary as it sounds, because if the update queue has > 10000 entries there's something wrong anyway, but we should fix this.",-0.4375,-0.4375,negative
impala,2707,description,"For each input row the aggregation node uses HashTable::Find() followed by HashTable::Insert() if the grouping key isn't already present in the table. Both of these methods probe the hash table to find the same bucket. If we added a FindOrInsert() method to the hash table that returned a modifiable iterator pointing to the bucket, we could save a significant number of hash table probes. There is already a TODO in the code for this, so I'm creating a JIRA to track the issue. This could speed up aggregations with large output size significantly, e.g. TPC-H query 13 (see IMPALA-2470).",code_debt,slow_algorithm,"Tue, 24 Nov 2015 18:00:29 +0000","Wed, 9 Dec 2015 00:19:33 +0000","Mon, 7 Dec 2015 19:34:23 +0000",1128834,"For each input row the aggregation node uses HashTable::Find() followed by HashTable::Insert() if the grouping key isn't already present in the table. Both of these methods probe the hash table to find the same bucket. If we added a FindOrInsert() method to the hash table that returned a modifiable iterator pointing to the bucket, we could save a significant number of hash table probes. There is already a TODO in the partitioned-aggregation-node-ir.cc code for this, so I'm creating a JIRA to track the issue. This could speed up aggregations with large output size significantly, e.g. TPC-H query 13 (see IMPALA-2470).",0.16,0.1333333333,neutral
impala,3103,description,"{{TBloomFilters}} have a 'directory' structure that is a list of individual buckets (buckets are about 64k wide). The total size of the directory can be 1MB or even much more. That leads to a lot of buckets, and very inefficient deserialisation as each bucket has to be allocated on the heap. Instead, the {{TBloomFilter}} representation should use one contiguous string (like the real {{BloomFilter}} does, so that it can be allocated with a single operation (and deserialized with a single copy).",code_debt,slow_algorithm,"Tue, 1 Mar 2016 00:12:14 +0000","Tue, 1 Mar 2016 15:35:11 +0000","Tue, 1 Mar 2016 15:35:11 +0000",55377,"TBloomFilters have a 'directory' structure that is a list of individual buckets (buckets are about 64k wide). The total size of the directory can be 1MB or even much more. That leads to a lot of buckets, and very inefficient deserialisation as each bucket has to be allocated on the heap. Instead, the TBloomFilter representation should use one contiguous string (like the real BloomFilter does, so that it can be allocated with a single operation (and deserialized with a single copy).",-0.1875,-0.1875,neutral
impala,326,description,"The validation for supported encodings for parquet is too strict. We also need to allow RLE and BIT_PACKED in the enums check. Also, the enums should print the strings in the error message, not the numerical values.",code_debt,low_quality_code,"Mon, 29 Apr 2013 16:55:44 +0000","Sun, 20 Dec 2015 00:05:02 +0000","Mon, 6 May 2013 03:44:37 +0000",557333,"The validation for supported encodings for parquet is too strict. We also need to allow RLE and BIT_PACKED in the enums check. Also, the enums should print the strings in the error message, not the numerical values.",0.06666666667,0.06666666667,negative
impala,3276,description,PrepareForRead() allows callers to use it in two different modes. In one of the modes it does not report pin failures. The code was written assuming that it will have reservations. This can result it hitting a DCHECK or going down some strange code paths:,code_debt,low_quality_code,"Wed, 30 Mar 2016 19:27:38 +0000","Tue, 5 Jul 2016 18:10:14 +0000","Tue, 12 Apr 2016 02:27:12 +0000",1061974,PrepareForRead() allows callers to use it in two different modes. In one of the modes it does not report pin failures. The code was written assuming that it will have reservations. This can result it hitting a DCHECK or going down some strange code paths:,-0.2115,-0.2115,neutral
impala,330,description,"Web page requests hold for the duration of the callbacks that render the page. This isn't terrible for short-lived requests, but anything that takes a while to render will block other requests. I tried the obvious thing, and Mongoose started behaving very weirdly (rendering the wrong page, rejecting a lot of concurrent requests).",code_debt,multi-thread_correctness,"Tue, 30 Apr 2013 21:38:18 +0000","Thu, 27 Feb 2014 22:36:34 +0000","Thu, 27 Feb 2014 22:36:34 +0000",26182696,"Web page requests hold path_handlers_lock_ for the duration of the callbacks that render the page. This isn't terrible for short-lived requests, but anything that takes a while to render will block other requests. I tried the obvious thing, and Mongoose started behaving very weirdly (rendering the wrong page, rejecting a lot of concurrent requests).",-0.008333333333,-0.008333333333,negative
impala,3344,description,"The sorter code does not have its internal invariants well-documented and can be tricky to understand when working on it. E.g. what are the valid states for merged versus initial runs, how are end-of-block cases handled. This issue is to clean up the sorter code while documenting important invariants as preparatory work for porting the sorter to the new buffer pool.",code_debt,low_quality_code,"Wed, 13 Apr 2016 17:43:23 +0000","Thu, 8 Sep 2016 16:28:02 +0000","Fri, 3 Jun 2016 04:02:31 +0000",4357148,"The sorter code does not have its internal invariants well-documented and can be tricky to understand when working on it. E.g. what are the valid states for merged versus initial runs, how are end-of-block cases handled. This issue is to clean up the sorter code while documenting important invariants as preparatory work for porting the sorter to the new buffer pool.",-0.03677777778,-0.03677777778,neutral
impala,3742,description,"Inserts into Kudu tables should be partitioned (i.e. rows hashed using the same hash partitioning as the Kudu table) and, at the table sink, sorted on the primary key. This would significantly improve performance. This will require a local sort (IMPALA-2521), and support from Kudu to provide the partitioning.",code_debt,slow_algorithm,"Tue, 14 Jun 2016 20:31:16 +0000","Fri, 1 Sep 2017 21:45:00 +0000","Thu, 4 May 2017 16:00:08 +0000",27977332,"Inserts into Kudu tables should be partitioned (i.e. rows hashed using the same hash partitioning as the Kudu table) and, at the table sink, sorted on the primary key. This would significantly improve performance. This will require a local sort (IMPALA-2521), and support from Kudu to provide the partitioning.",0.4333333333,0.4333333333,neutral
impala,3780,description,"I observed after adding logging that the scanner was issuing many small 1024-byte scan ranges: This appears to be based on the constant in the text scanner, which is used to try and find the end of a field that extends into the next HDFS block. We could avoid this in a couple of ways: * Increase the constant. It's unclear why it's so low: I think the cost of reading additional data is probably negligible, at least up to 10's or 100's of KB or so. * Ramp up the read size, e.g. recursive doubling up to 8MB.",code_debt,slow_algorithm,"Thu, 23 Jun 2016 17:29:42 +0000","Mon, 18 Jul 2016 15:56:38 +0000","Mon, 18 Jul 2016 15:56:38 +0000",2154416,"I observed after adding logging that the scanner was issuing many small 1024-byte scan ranges: This appears to be based on the ""NEXT_BLOCK_READ_SIZE"" constant in the text scanner, which is used to try and find the end of a field that extends into the next HDFS block. We could avoid this in a couple of ways: Increase the constant. It's unclear why it's so low: I think the cost of reading additional data is probably negligible, at least up to 10's or 100's of KB or so. Ramp up the read size, e.g. recursive doubling up to 8MB.",-0.05,-0.06666666667,neutral
impala,4291,description,"For simple queries with one or two functions to codegen, we spent about 23ms or more out of 40ms of codegen time in preparation. We can save some time by not eagerly populating the map of to {{llvm::Function*}} map. This should help with the fixes for IMPALA-3638 when we create the LLVM module unconditionally.",code_debt,slow_algorithm,"Thu, 13 Oct 2016 18:37:43 +0000","Thu, 30 Aug 2018 18:49:49 +0000","Thu, 20 Oct 2016 05:58:34 +0000",559251,"For simple queries with one or two functions to codegen, we spent about 23ms or more out of 40ms of codegen time in preparation. We can save some time by not eagerly populating the map of IRFunction::Type to llvm::Function* map. This should help with the fixes for IMPALA-3638 when we create the LLVM module unconditionally.",0.2666666667,0.2666666667,neutral
impala,4328,description,"The config variable has hostnames that are private to Cloudera. Can you make those environment variables or configured at the command line or something, rather than hardcoding ""Cloudera"" into the script?",code_debt,low_quality_code,"Thu, 20 Oct 2016 18:18:38 +0000","Thu, 27 Jul 2017 02:55:35 +0000","Thu, 27 Jul 2017 02:55:35 +0000",24136617,"The config variable has hostnames that are private to Cloudera. Can you make those environment variables or configured at the command line or something, rather than hardcoding ""Cloudera"" into the script?",0.0,0.0,neutral
impala,4548,description,"As shown in IMPALA-4532, the build async thread can still be running after the query has completed. This may lead use-after-free issue in IMPALA-4532. A more appropriate design is for to wait for the completion of the build thread.",code_debt,multi-thread_correctness,"Mon, 28 Nov 2016 22:32:33 +0000","Fri, 21 Apr 2017 21:24:13 +0000","Fri, 21 Apr 2017 21:20:43 +0000",12437290,"As shown in IMPALA-4532, the build async thread can still be running after the query has completed. This may lead use-after-free issue in IMPALA-4532. A more appropriate design is for BlockingJoinNode::Close() to wait for the completion of the build thread.",0.4103333333,0.4103333333,neutral
impala,4617,description,"Currently Expr.isConstant() and Expr::IsConstant() in the frontend and backend have duplicate logic to do exactly the same analysis. They need to be kept exactly in sync to avoid problems. We should plumb through the value of isConstant() from the frontend to avoid this duplication. We need to be a little careful about how we do this in the frontend: Alex Behn mentioned that storing state in Exprs was risky, and also that naively calling isConstant() for each expr node was problematic since it could result in traversing the Expr tree many times.",code_debt,duplicated_code,"Wed, 7 Dec 2016 21:56:41 +0000","Tue, 31 Jan 2017 18:09:21 +0000","Tue, 31 Jan 2017 18:09:21 +0000",4738360,"Currently Expr.isConstant() and Expr::IsConstant() in the frontend and backend have duplicate logic to do exactly the same analysis. They need to be kept exactly in sync to avoid problems. We should plumb through the value of isConstant() from the frontend to avoid this duplication. We need to be a little careful about how we do this in the frontend: Alex Behn mentioned that storing state in Exprs was risky, and also that naively calling isConstant() for each expr node was problematic since it could result in traversing the Expr tree many times.",-0.05666666667,-0.05666666667,neutral
impala,4713,description,"I'm trying to debug failure queries where an is thrown wrapping a from an ""invalidate metadata <table The error message shows up in impala-shell but it doesn't appear that the Java backtrace is logged anywhere in the logs. This makes it impossible to determine where the NPE has come from.",code_debt,low_quality_code,"Sun, 25 Dec 2016 00:40:46 +0000","Fri, 2 Nov 2018 23:56:49 +0000","Fri, 2 Nov 2018 23:56:49 +0000",58576563,"I'm trying to debug failure queries where an IllegalStateException is thrown wrapping a NullPointerException from an ""invalidate metadata <table>"" query. The error message shows up in impala-shell but it doesn't appear that the Java backtrace is logged anywhere in the logs. This makes it impossible to determine where the NPE has come from.",-0.2166666667,-0.2833333333,negative
impala,4996,description,"Since we will multi-thread query execution at the fragment level, we should rework KuduScanNode to only use a single thread (the one that's executing the fragment).",code_debt,multi-thread_correctness,"Sat, 25 Feb 2017 16:11:23 +0000","Tue, 21 Mar 2017 20:00:28 +0000","Tue, 21 Mar 2017 20:00:28 +0000",2087345,"Since we will multi-thread query execution at the fragment level, we should rework KuduScanNode to only use a single thread (the one that's executing the fragment).",-0.5,-0.5,neutral
impala,5042,description,"Loading metadata for partitions with custom paths is 4x slower compared to partitions without custom paths, the slow down is due to an N2 lookups to check if a partition already exists. The List should ideally be replaced with a Set. From From Java mission control",code_debt,slow_algorithm,"Wed, 8 Mar 2017 08:21:24 +0000","Wed, 22 Mar 2017 18:33:30 +0000","Fri, 17 Mar 2017 17:27:01 +0000",810337,"Loading metadata for partitions with custom paths is 4x slower compared to partitions without custom paths, the slow down is due to an N2 lookups to check if a partition already exists. The List should ideally be replaced with a Set. From https://github.com/apache/incubator-impala/blob/master/fe/src/main/java/org/apache/impala/catalog/HdfsTable.java From Java mission control",-0.09733333333,-0.09733333333,negative
impala,5070,description,Issues like IMPALA-92 with multiple attachments with the same name did not have all of their attachments translate properly.,code_debt,low_quality_code,"Mon, 13 Mar 2017 22:10:27 +0000","Thu, 16 Mar 2017 04:09:38 +0000","Thu, 16 Mar 2017 04:09:19 +0000",194332,Issues like IMPALA-92 with multiple attachments with the same name did not have all of their attachments translate properly.,0.0,0.0,negative
impala,5130,description,"can run concurrently with There is no synchronisation between the two. I saw a crash with this stack on a development branch, which I believe is caused by this: on this line: This method is not currently used in query execution, but we need to fix this before switching on the buffer pool for query execution.",code_debt,multi-thread_correctness,"Tue, 28 Mar 2017 18:59:35 +0000","Thu, 30 Mar 2017 00:21:05 +0000","Thu, 30 Mar 2017 00:21:05 +0000",105690,"MemTracker::EnableReservationReporting() can run concurrently with MemTracker::LogUsage(). There is no synchronisation between the two. I saw a crash with this stack on a development branch, which I believe is caused by this: on this line: This method is not currently used in query execution, but we need to fix this before switching on the buffer pool for query execution.",-0.2,-0.1333333333,negative
impala,5273,description,"Replacing StringCompare (which uses SSE4.2 instructions) with a call to glibc's memcmp results in a memcmp on my machine mainly uses sse4.1's ptest, after detecting at run-time that I have sse4.1 instructions available. The StringCompare benchmark is 5 years old and likely out-of-date by now. To replicate:",code_debt,slow_algorithm,"Tue, 2 May 2017 21:46:55 +0000","Tue, 9 May 2017 16:39:42 +0000","Tue, 9 May 2017 16:39:42 +0000",586367,"Replacing StringCompare (which uses SSE4.2 instructions) with a call to glibc's dynamically-dispatched memcmp results in a >5x improvement for large strings. memcmp on my machine mainly uses sse4.1's ptest, after detecting at run-time that I have sse4.1 instructions available. The StringCompare benchmark is 5 years old and likely out-of-date by now. To replicate:",0.0,0.07742857143,neutral
impala,530,description,"I was experimenting with queries on year / month / day fields stored as strings in a TEXT data file. In addition to the need to CAST them to INT to get correct sorting during ORDER BY, I noticed that the sorting of the CAST-to-INT values was 2x faster than the original string values (this is with about 500M rows): select distinct year, month, day from raw_data_ymd order by year, month, day limit 7500; ... Returned 5124 row(s) in 102.45s select distinct year, month, day from raw_data_ymd order by cast (year as int), cast (month as int), cast (day as int) limit 7500; ... Returned 5124 row(s) in 41.87s But all the strings are very short, 1-2 bytes for the day and month fields, and 4 bytes for the year field. I would expect these could be sorted in about the same time as the corresponding integers, especially since the integer sorting in the above query has the overhead of 3 CASTs per row. I wonder if there is some latent UTF-8 processing slowing down the string sorting even though all the values are ASCII, or if the unknown-in-advance length of the strings makes the string sort harder to optimize. Could there be some special casing when the string data is known to be ASCII (i.e. like currently), and Impala could infer a binary collation? E.g. the strings could be compared 2, 4, or 8 bytes at a time as if they were integers. I first thought of this as an optimization for short string columns where the data is of consistent length, but if the strings are null-terminated and held in a buffer with a size rounded to an even number, I think the technique would apply for strings of different or unknown-in-advance lengths.",code_debt,slow_algorithm,"Tue, 13 Aug 2013 18:11:25 +0000","Fri, 5 Dec 2014 07:28:44 +0000","Fri, 5 Dec 2014 07:28:44 +0000",41347039,"I was experimenting with queries on year / month / day fields stored as strings in a TEXT data file. In addition to the need to CAST them to INT to get correct sorting during ORDER BY, I noticed that the sorting of the CAST-to-INT values was 2x faster than the original string values (this is with about 500M rows): select distinct year, month, day from raw_data_ymd order by year, month, day limit 7500; ... Returned 5124 row(s) in 102.45s select distinct year, month, day from raw_data_ymd order by cast (year as int), cast (month as int), cast (day as int) limit 7500; ... Returned 5124 row(s) in 41.87s But all the strings are very short, 1-2 bytes for the day and month fields, and 4 bytes for the year field. I would expect these could be sorted in about the same time as the corresponding integers, especially since the integer sorting in the above query has the overhead of 3 CASTs per row. I wonder if there is some latent UTF-8 processing slowing down the string sorting even though all the values are ASCII, or if the unknown-in-advance length of the strings makes the string sort harder to optimize. Could there be some special casing when the string data is known to be ASCII (i.e. like currently), and Impala could infer a binary collation? E.g. the strings could be compared 2, 4, or 8 bytes at a time as if they were integers. I first thought of this as an optimization for short string columns where the data is of consistent length, but if the strings are null-terminated and held in a buffer with a size rounded to an even number, I think the technique would apply for strings of different or unknown-in-advance lengths.",0.1458333333,0.1458333333,neutral
impala,5481,description,"One of the {{RowBatch}} c'tors copies the row descriptor into the row batch. This leads to a lot of allocation churn since {{RowDescriptor}} contains some vector members, and since the descriptor is usually the same the copies are unnecessary. Instead, we should consider allocating the {{RowDescriptor}} once from an object pool, and sharing it amongst all row batches that need that descriptor. In some tests, {{RowDescriptor()}} shows up as 20% of the tcmalloc allocation time.",code_debt,slow_algorithm,"Sat, 10 Jun 2017 00:55:46 +0000","Wed, 21 Jun 2017 03:02:29 +0000","Wed, 21 Jun 2017 03:02:28 +0000",958002,"One of the RowBatch c'tors copies the row descriptor into the row batch. This leads to a lot of allocation churn since RowDescriptor contains some vector members, and since the descriptor is usually the same the copies are unnecessary. Instead, we should consider allocating the RowDescriptor once from an object pool, and sharing it amongst all row batches that need that descriptor. In some tests, RowDescriptor() shows up as 20% of the tcmalloc allocation time.",-0.1405,-0.1405,neutral
impala,5618,description,The extra time appears to be in AddRowCustom() via I think allocating the boost::function object is causing the slowdown.,code_debt,slow_algorithm,"Thu, 6 Jul 2017 00:51:03 +0000","Fri, 7 Jul 2017 18:02:02 +0000","Fri, 7 Jul 2017 18:02:02 +0000",148259,The extra time appears to be in AddRowCustom() via PartitionedAggregationNode::ConstructIntermediateTuple(). I think allocating the boost::function object is causing the slowdown.,-0.181,-0.0905,neutral
impala,5849,description,"IMPALA-5800, IMPALA-5775 and IMPALA-5743 added TLS configuration to Impala and Squeasel. Since Impala is often built against different versions of OpenSSL (with different TLS capabilities), we used compile-time definitions to avoid using symbols from OpenSSL 1.0.1 that weren't available. This works great if we can ensure that the machine on which Impala is built is the same environment as the one on which it executes, but we have discovered that the installed version of OpenSSL can vary between minor releases of Linux distributions. It appears possible to write the support for TLS1.1+ in terms of symbols that are available in OpenSSL 1.0.0 only. The only downside is that Impala can't then tell whether or not the runtime supports TLS 1.2, and so the error messages won't be quite as clear. However, the benefit of a single binary and Thrift toolchain dependency for all supported versions of OpenSSL is well worth it.",code_debt,low_quality_code,"Fri, 25 Aug 2017 18:54:25 +0000","Tue, 5 Sep 2017 16:49:06 +0000","Tue, 5 Sep 2017 16:49:06 +0000",942881,"IMPALA-5800, IMPALA-5775 and IMPALA-5743 added TLS configuration to Impala and Squeasel. Since Impala is often built against different versions of OpenSSL (with different TLS capabilities), we used compile-time definitions to avoid using symbols from OpenSSL 1.0.1 that weren't available. This works great if we can ensure that the machine on which Impala is built is the same environment as the one on which it executes, but we have discovered that the installed version of OpenSSL can vary between minor releases of Linux distributions. It appears possible to write the support for TLS1.1+ in terms of symbols that are available in OpenSSL 1.0.0 only. The only downside is that Impala can't then tell whether or not the runtime supports TLS 1.2, and so the error messages won't be quite as clear. However, the benefit of a single binary and Thrift toolchain dependency for all supported versions of OpenSSL is well worth it.",0.1673452381,0.1673452381,neutral
impala,6048,description,"When running 32 concurrent queries from TPCDS a couple of instances from TPC-DS Q78 9 hours to finish and it appeared to be hung. On an idle cluster the query finished in under 5 minutes, profiles attached. When the query ran for long fragments reported +16 hours of network send/receive time The logs show there is a lot of messages like the one below, there are incidents for this log message where a node waited too long from an RPC from itself",code_debt,slow_algorithm,"Thu, 12 Oct 2017 23:54:47 +0000","Tue, 14 May 2019 01:53:18 +0000","Tue, 14 May 2019 01:53:18 +0000",49946311,"When running 32 concurrent queries from TPCDS a couple of instances from TPC-DS Q78 9 hours to finish and it appeared to be hung. On an idle cluster the query finished in under 5 minutes, profiles attached. When the query ran for long fragments reported +16 hours of network send/receive time The logs show there is a lot of messages like the one below, there are incidents for this log message where a node waited too long from an RPC from itself",0.0,0.0,neutral
impala,6077,description,"As a follow-on to IMPALA-6076, we should remove support for the encoding in a compat-breaking release so that we no longer have to maintain this code.",code_debt,dead_code,"Wed, 18 Oct 2017 20:36:54 +0000","Fri, 23 Mar 2018 21:54:15 +0000","Mon, 12 Feb 2018 22:15:10 +0000",10114696,"As a follow-on to IMPALA-6076, we should remove support for the encoding in a compat-breaking release so that we no longer have to maintain this code.",0.4,0.4,neutral
impala,6131,description,Currently we (ab-)use to track the last update time of statistics. Instead we should introduce a separate counter to track the last update. With that we should also remove all occurrences of from and fall back to Hive's default behavior.,code_debt,low_quality_code,"Mon, 30 Oct 2017 23:49:52 +0000","Thu, 2 May 2019 21:48:05 +0000","Mon, 11 Jun 2018 15:23:35 +0000",19323223,Currently we (ab-)use transient_lastDdlTime to track the last update time of statistics. Instead we should introduce a separate counter to track the last update. With that we should also remove all occurrences of catalog_.updateLastDdlTime() from CatalogOpExecutor and fall back to Hive's default behavior.,-0.1666666667,-0.125,neutral
impala,6613,description,"Now that KRPC is on by default, we should rename the environment variable to reflect that.",code_debt,low_quality_code,"Tue, 6 Mar 2018 21:35:21 +0000","Mon, 19 Mar 2018 22:29:42 +0000","Mon, 19 Mar 2018 22:29:42 +0000",1126461,"Now that KRPC is on by default, we should rename the environment variable to reflect that.",-0.5,-0.5,neutral
impala,6666,description,"This is a performance optimization for spill-to-disk when encryption is enabled. If running with an OpenSSL version with AES-GCM support on a CPU that supports the CLMUL instruction, this faster encryption mode will be used.",code_debt,slow_algorithm,"Wed, 14 Mar 2018 20:32:08 +0000","Thu, 5 Apr 2018 22:12:46 +0000","Thu, 5 Apr 2018 22:12:46 +0000",1906838,"This is a performance optimization for spill-to-disk when encryption is enabled. If running with an OpenSSL version with AES-GCM support on a CPU that supports the CLMUL instruction, this faster encryption mode will be used.",0.2,0.2,neutral
impala,6694,description,"It appears that the buffer pool statistics of exec node is sometimes misaligned in the profile. For instance, the aggregation node's buffer pool appears after the exchange node below: cc'ing",code_debt,low_quality_code,"Sat, 17 Mar 2018 01:47:10 +0000","Sat, 31 Mar 2018 23:35:04 +0000","Thu, 29 Mar 2018 22:30:41 +0000",1111411,"It appears that the buffer pool statistics of exec node is sometimes misaligned in the profile. For instance, the aggregation node's buffer pool appears after the exchange node below: cc'ing tarmstrong@cloudera.com], mmokhtar",0.0,0.0,neutral
impala,7682,description,"The method public Set<String> groups, Set<String> users, ActiveRoleSet roleSet) in AuthorizationPolicy needs to be synchronized.",code_debt,multi-thread_correctness,"Tue, 9 Oct 2018 18:24:48 +0000","Fri, 12 Oct 2018 17:11:38 +0000","Thu, 11 Oct 2018 01:40:18 +0000",112530,"The method public Set<String> listPrivileges(Set<String> groups, Set<String> users, ActiveRoleSet roleSet) in AuthorizationPolicy needs to be synchronized.",0.0,0.0,neutral
impala,7748,description,"With IMPALA-110, we can now support multiple count distinct directly, and the appx_count_distinct query option is no longer needed. Users who want the perf improvement from it can always just use the ndv() function directly in their sql. We'll mark this option as deprecated in the docs starting from 3.1. Removing it can be targeted for 4.0",code_debt,dead_code,"Tue, 23 Oct 2018 20:30:01 +0000","Thu, 25 Oct 2018 22:05:47 +0000","Thu, 25 Oct 2018 22:05:47 +0000",178546,"With IMPALA-110, we can now support multiple count distinct directly, and the appx_count_distinct query option is no longer needed. Users who want the perf improvement from it can always just use the ndv() function directly in their sql. We'll mark this option as deprecated in the docs starting from 3.1. Removing it can be targeted for 4.0",0.19275,0.19275,neutral
impala,7808,description,"The analysis steps in {{SelectStmt}} and {{AnalysisContext}} are large and cumbersome. There is ample evidence in the literature that simpler, smaller functions are easier to understand and debug than larger, more complex functions. This ticket requests breaking up the large functions in these two cases into smaller, easier-understood units in preparation for tracking down issues related to missing rewrites of the {{WHERE}} and {{GROUP BY}} clauses. One might argue that large functions perform better by eliminating unnecessary function calls. However, the planner is not performance sensitive, and the dozen extra calls that this change introduce will not change performance given the thousands of calls already made. Experience has shown that the JIT compiler in the JVM actually does a better job optimizing smaller functions, and gives up when functions get to large. So, by creating smaller functions, we may actually allow the JIT compiler to generate better code. And, this refactoring is in support of a possible outcome that the planner can handle rewrites without making multiple passes through the analyzer: that savings will far outweigh the few extra calls this change introduces.",code_debt,complex_code,"Mon, 5 Nov 2018 22:32:04 +0000","Fri, 1 Mar 2019 17:54:10 +0000","Fri, 1 Mar 2019 17:54:09 +0000",10005725,"The analysis steps in SelectStmt and AnalysisContext are large and cumbersome. There is ample evidence in the literature that simpler, smaller functions are easier to understand and debug than larger, more complex functions. This ticket requests breaking up the large functions in these two cases into smaller, easier-understood units in preparation for tracking down issues related to missing rewrites of the WHERE and GROUP BY clauses. One might argue that large functions perform better by eliminating unnecessary function calls. However, the planner is not performance sensitive, and the dozen extra calls that this change introduce will not change performance given the thousands of calls already made. Experience has shown that the JIT compiler in the JVM actually does a better job optimizing smaller functions, and gives up when functions get to large. So, by creating smaller functions, we may actually allow the JIT compiler to generate better code. And, this refactoring is in support of a possible outcome that the planner can handle rewrites without making multiple passes through the analyzer: that savings will far outweigh the few extra calls this change introduces.",0.088,0.088,neutral
impala,7841,description,"IMPALA-7808 started the process of refactoring the Analyzer code for easier debugging. It did so by grouping {{SelectStmt}} code into a nested class, which then allowed breaking up a large function into smaller chunks. This ticket continues that process with two changes: * Follow-on refactoring of {{SelectStmt}} to make some of the newly-created functions simpler. (The first change tried to keep code unchanged as much as possible.) * Apply the same technique to the {{QueryStmt}} base class and to its other subclasses.",code_debt,low_quality_code,"Thu, 8 Nov 2018 22:56:23 +0000","Fri, 1 Mar 2019 18:16:00 +0000","Fri, 1 Mar 2019 18:16:00 +0000",9746377,"IMPALA-7808 started the process of refactoring the Analyzer code for easier debugging. It did so by grouping SelectStmt code into a nested class, which then allowed breaking up a large function into smaller chunks. This ticket continues that process with two changes: Follow-on refactoring of SelectStmt to make some of the newly-created functions simpler. (The first change tried to keep code unchanged as much as possible.) Apply the same technique to the QueryStmt base class and to its other subclasses.",0.0,0.0,neutral
impala,7869,description,suggested reorganising the file to be easier to read on Compile times are also an issue - this file is the longest pole in the Impala compilation at the moment.,code_debt,low_quality_code,"Mon, 19 Nov 2018 16:57:08 +0000","Tue, 27 Nov 2018 02:01:56 +0000","Tue, 27 Nov 2018 02:01:56 +0000",637488,csringhofer suggested reorganising the file to be easier to read on https://gerrit.cloudera.org/#/c/8319/ Compile times are also an issue - this file is the longest pole in the Impala compilation at the moment.,0.0,0.0,negative
impala,8073,description,The error in the log (attached) appears to be a connection to the HMS error. Some initial googling suggested that it might be the server closing the connection because of hitting a connection limit.  could you take a look and see if you have any ideas. I wonder if we're leaking HMS connections in this test somehow?,code_debt,low_quality_code,"Fri, 11 Jan 2019 21:16:14 +0000","Thu, 14 Mar 2019 14:34:58 +0000","Wed, 23 Jan 2019 04:58:59 +0000",978165,The error in the log (attached) appears to be a connection to the HMS error. Some initial googling suggested that it might be the server closing the connection because of hitting a connection limit. fredyw could you take a look and see if you have any ideas. I wonder if we're leaking HMS connections in this test somehow?,0.10825,0.10825,neutral
impala,844,description,"We idiomatically do this for Thrift RPCs: Thrift can throw as well (see e.g. {{recv_*}} for any method). We don't catch it, and therefore can abort on the rare occasion it gets thrown. Instead we should catch {{TException}}.",code_debt,low_quality_code,"Fri, 28 Feb 2014 00:19:53 +0000","Wed, 5 Mar 2014 23:24:00 +0000","Wed, 5 Mar 2014 23:24:00 +0000",515047,"We idiomatically do this for Thrift RPCs: Thrift can throw TApplicationException as well (see e.g. recv_* for any method). We don't catch it, and therefore can abort on the rare occasion it gets thrown. Instead we should catch TException.",0.2103333333,0.2103333333,neutral
impala,8485,description,Running the command *git grep authz-policy* produces the following output: ** authz-policy.ini = % WAREHOUSE These references to the *authz-policy.ini* should be cleaned up as the authorization policy file feature is deprecated as of *IMPALA-7918.*,code_debt,dead_code,"Thu, 2 May 2019 22:39:43 +0000","Fri, 3 May 2019 21:26:14 +0000","Fri, 3 May 2019 20:31:51 +0000",78728,"Running the command git grep authz-policy produces the following output: ** bin/create-test-configuration.sh:generate_config authz-policy.ini.template authz-policy.ini fe/.gitignore:src/test/resources/authz-policy.ini tests/authorization/test_authorization.py:AUTH_POLICY_FILE = ""%s/authz-policy.ini"" % WAREHOUSE These references to the authz-policy.ini should be cleaned up as the authorization policy file feature is deprecated as of IMPALA-7918.",0.0,0.0,neutral
impala,8578,description,"metrics.h and other metric headers are included a lot of places and there is a lot of code in the header that has very few callers. It appears to be pulled into several hundred compilation units, increasing the compile time of each of those and forcing recompilation when the headers are changed. Some ideas: * Move function implementations to .cc files. E.g. ToJson() and ToPrometheus() don't need to be inlined. * Move MetricGroup to its own file * Try to see if we can use forward declarations in more places to avoid including it.",code_debt,low_quality_code,"Wed, 22 May 2019 20:28:03 +0000","Wed, 5 Jun 2019 05:38:32 +0000","Wed, 5 Jun 2019 05:38:32 +0000",1156229,"metrics.h and other metric headers are included a lot of places and there is a lot of code in the header that has very few callers. It appears to be pulled into several hundred compilation units, increasing the compile time of each of those and forcing recompilation when the headers are changed. Some ideas: Move function implementations to .cc files. E.g. ToJson() and ToPrometheus() don't need to be inlined. Move MetricGroup to its own file Try to see if we can use forward declarations in more places to avoid including it.",-0.02857142857,-0.02857142857,neutral
impala,8605,description,"Following on from IMPALA-8538 and IMPALA-1653, it would be nice to clean up some of the session management logic and add some more tests for the HS2 APIs - we have some testing gaps and a few of the invariants are unclear about what operations are allowed.",code_debt,low_quality_code,"Fri, 31 May 2019 00:07:22 +0000","Tue, 11 Jun 2019 23:03:47 +0000","Tue, 11 Jun 2019 23:03:47 +0000",1032985,"Following on from IMPALA-8538 and IMPALA-1653, it would be nice to clean up some of the session management logic and add some more tests for the HS2 APIs - we have some testing gaps and a few of the invariants are unclear about what operations are allowed.",0.3416666667,0.3416666667,neutral
impala,8626,description,"I noticed that the parameterized JDBC tests are passing on the dockerised cluster, which shouldn't be possible because IMPALA-8623 isn't done. The connection strings look identical in both cases: I was looking at related code and saw some misuse of == vs equals() for string comparison here But I don't think that explains what I'm seeing above.",code_debt,low_quality_code,"Wed, 5 Jun 2019 19:19:28 +0000","Tue, 18 Jun 2019 18:54:48 +0000","Tue, 18 Jun 2019 18:54:48 +0000",1121720,"I noticed that the parameterized JDBC tests are passing on the dockerised cluster, which shouldn't be possible because IMPALA-8623 isn't done. https://jenkins.impala.io/job/ubuntu-16.04-dockerised-tests/453/testReport/org.apache.impala.service/JdbcTest/ The connection strings look identical in both cases: I was looking at related code and saw some misuse of == vs equals() for string comparison here https://github.com/apache/impala/blob/master/fe/src/test/java/org/apache/impala/testutil/ImpalaJdbcClient.java#L172 But I don't think that explains what I'm seeing above.",-0.4375,-0.4375,negative
impala,8884,description,It would be useful for debugging I/O performance problems if we had histogram stats for the time taken for various operations so that we could see if there were slow operations on a particular disk (e.g. because of disk failure) or from a particular remote filesystem.,code_debt,low_quality_code,"Thu, 22 Aug 2019 22:09:12 +0000","Wed, 4 Mar 2020 03:10:43 +0000","Thu, 17 Oct 2019 00:52:46 +0000",4761814,It would be useful for debugging I/O performance problems if we had histogram stats for the time taken for various operations so that we could see if there were slow operations on a particular disk (e.g. because of disk failure) or from a particular remote filesystem.,-0.1,-0.1,neutral
impala,902,description,"We use YARN's to check if a user has access to a resource pool. If the user is not on the remote system, (Hadoop) will write a scary looking message and stack to the log. This is not actually a functional problem, but this spew may appear in impalad logs and should be hidden. e.g. when I connect to a remote impalad where I'm not a user on the local system, our logs will show:",code_debt,low_quality_code,"Wed, 19 Mar 2014 17:31:08 +0000","Sun, 20 Dec 2015 00:05:14 +0000","Thu, 5 Jun 2014 18:04:32 +0000",6741204,"We use YARN's AllocationConfiguration.hasAccess() to check if a user has access to a resource pool. If the user is not on the remote system, (Hadoop) ShellBasedUnixGroupsMapping will write a scary looking message and stack to the log. This is not actually a functional problem, but this spew may appear in impalad logs and should be hidden. e.g. when I connect to a remote impalad where I'm not a user on the local system, our logs will show:",-0.05625,-0.045,neutral
impala,92,description,"I'm running the following two queries. The only difference between them is I'm using ""LIKE"" in one case and ""="" in another, though there is no ""%"" in the LIKE, so the effect is the same. I was surprised to see approximately a 10x difference in performance between them. I'm running I've attached the two query profiles. The basic difference is in the execution rate: Obviously I've fixed my query.",code_debt,slow_algorithm,"Sun, 24 Feb 2013 00:48:45 +0000","Thu, 16 Mar 2017 04:03:32 +0000","Fri, 1 Mar 2013 22:09:39 +0000",508854,"I'm running the following two queries. The only difference between them is I'm using ""LIKE"" in one case and ""="" in another, though there is no ""%"" in the LIKE, so the effect is the same. I was surprised to see approximately a 10x difference in performance between them. I'm running I've attached the two query profiles. The basic difference is in the execution rate: Obviously I've fixed my query.",0.0,0.0,neutral
impala,946,description,"I ran a simple experiment to test the performance of various FE steps and noticed that loading the table metadata takes a ""long"" time, roughly linear in the number of columns. I performed the following series of actions for every data point: I ran this test for various N and the table below shows the total query time as reported by the Impala shell (I ran with impala-shell.py -f): A simple script to generate the SQL as above is attached.",code_debt,slow_algorithm,"Fri, 11 Apr 2014 22:34:38 +0000","Fri, 11 Apr 2014 23:43:29 +0000","Fri, 11 Apr 2014 22:55:24 +0000",1246,"I ran a simple experiment to test the performance of various FE steps and noticed that loading the table metadata takes a ""long"" time, roughly linear in the number of columns. I performed the following series of actions for every data point: I ran this test for various N and the table below shows the total query time as reported by the Impala shell (I ran with impala-shell.py -f): A simple script to generate the SQL as above is attached.",0.0,0.0,neutral
impala,9543,description,"Reduce duplicate code in thrift CMakeLists.txt. And if in future, we change hive to version 4 or higher. This can adapt automatically.",code_debt,duplicated_code,"Mon, 23 Mar 2020 09:09:12 +0000","Thu, 9 Apr 2020 02:36:53 +0000","Thu, 9 Apr 2020 02:03:24 +0000",1443252,"Reduce duplicate code in thrift CMakeLists.txt. And if in future, we change hive to version 4 or higher. This can adapt automatically.",0.0,0.0,neutral
impala,101,description,"The query log currently contains the query but not the current database. This makes it very hard to figure out what was run, particularly in our cluster setup where the db encodes the scale factor. We should just add this to the query log.",design_debt,non-optimal_design,"Fri, 1 Mar 2013 21:52:53 +0000","Thu, 11 Apr 2013 19:24:36 +0000","Thu, 11 Apr 2013 19:24:35 +0000",3533502,"The query log currently contains the query but not the current database. This makes it very hard to figure out what was run, particularly in our cluster setup where the db encodes the scale factor. We should just add this to the query log.",-0.06666666667,-0.06666666667,negative
impala,137,description,"As an example: (Build version: Impala v0.7 (06b3f21) built on Wed Mar 13 16:54:46 PDT 2013) [localhost:21000] Query: select * from alltypes limit 10 ERROR: Analysis exception (in select * from alltypes limit 10) Caused by: Unknown table: 'alltypes' ... 2 more The error here is ""Unknown table: 'alltypes'"" but we dump all this stuff that is not user friendly. This might be useful for development but we should have the default not show the stack traces.",design_debt,non-optimal_design,"Thu, 14 Mar 2013 20:47:02 +0000","Mon, 28 Dec 2015 17:43:33 +0000","Thu, 14 Mar 2013 20:49:26 +0000",144,"As an example: (Build version: Impala v0.7 (06b3f21) built on Wed Mar 13 16:54:46 PDT 2013) [localhost:21000] > select * from alltypes limit 10; Query: select * from alltypes limit 10 ERROR: com.cloudera.impala.common.AnalysisException: Analysis exception (in select * from alltypes limit 10) at com.cloudera.impala.analysis.AnalysisContext.analyze(AnalysisContext.java:177) at com.cloudera.impala.service.Frontend.createExecRequest(Frontend.java:287) at com.cloudera.impala.service.JniFrontend.createExecRequest(JniFrontend.java:106) Caused by: com.cloudera.impala.common.AnalysisException: Unknown table: 'alltypes' at com.cloudera.impala.analysis.Analyzer.registerBaseTableRef(Analyzer.java:178) at com.cloudera.impala.analysis.BaseTableRef.analyze(BaseTableRef.java:51) at com.cloudera.impala.analysis.SelectStmt.analyze(SelectStmt.java:115) at com.cloudera.impala.analysis.AnalysisContext.analyze(AnalysisContext.java:174) ... 2 more The error here is ""Unknown table: 'alltypes'"" but we dump all this stuff that is not user friendly. This might be useful for development but we should have the default not show the stack traces.",-0.026,0.007544654088,neutral
impala,1382,description,We currently preallocate the null tuple indicator bitstring in each tuple stream assuming that very few tuples will be NULL. That can lead to lots of wasted space in the buffer if there are many NULL tuples in the stream. A possible solution is to use a slotted page (buffer) with NULL indicators growing from the end of the page (buffer).,design_debt,non-optimal_design,"Fri, 10 Oct 2014 18:29:51 +0000","Sat, 5 Aug 2017 03:21:22 +0000","Sat, 5 Aug 2017 03:21:22 +0000",88937491,We currently preallocate the null tuple indicator bitstring in each tuple stream assuming that very few tuples will be NULL. That can lead to lots of wasted space in the buffer if there are many NULL tuples in the stream. A possible solution is to use a slotted page (buffer) with NULL indicators growing from the end of the page (buffer).,-0.4,-0.4,neutral
impala,1430,description,"Currently codegen is disabled for the entire aggregation operator if a single aggregate function can't be codegen'd. We should address this by making it so all aggregate functions can be codegen'd, including UDAs. For UDAs in .so's, the codegen'd function will call into the UDA library. This also affects aggregation operator on timestamp. This perf hit can be especially bad for COMPUTE STATS which is heavily CPU bound on the aggregation and because there is no easy way to exclude the TIMESTAMP columns when computing the column stats (i.e., there is no simple workaround). Even if the portions involving TIMESTAMP cannot be codegen'd it would still be worthwhile to come up with a workaround where codegen for the other types is still enabled. *Workaround* If you are experiencing very slow COMPUTE STATS due to this issue, then you may be able to temporarily ALTER the TIMESTAMP columns to STRING or INT type before running COMPUTE STATS. After the command completed, the columns can be altered back to TIMESTAMP. Note the workaround is only apply to text data, not parquet data. parquet require compatibles data type. TIMESTAMP is INT96, it's not compatible with STRING or BIGINT.",design_debt,non-optimal_design,"Tue, 28 Oct 2014 22:43:15 +0000","Thu, 30 Aug 2018 18:46:14 +0000","Wed, 15 Feb 2017 06:06:13 +0000",72602578,"Currently codegen is disabled for the entire aggregation operator if a single aggregate function can't be codegen'd. We should address this by making it so all aggregate functions can be codegen'd, including UDAs. For UDAs in .so's, the codegen'd function will call into the UDA library. This also affects aggregation operator on timestamp. This perf hit can be especially bad for COMPUTE STATS which is heavily CPU bound on the aggregation and because there is no easy way to exclude the TIMESTAMP columns when computing the column stats (i.e., there is no simple workaround). Even if the portions involving TIMESTAMP cannot be codegen'd it would still be worthwhile to come up with a workaround where codegen for the other types is still enabled. Workaround If you are experiencing very slow COMPUTE STATS due to this issue, then you may be able to temporarily ALTER the TIMESTAMP columns to STRING or INT type before running COMPUTE STATS. After the command completed, the columns can be altered back to TIMESTAMP. Note the workaround is only apply to text data, not parquet data. parquet require compatibles data type. TIMESTAMP is INT96, it's not compatible with STRING or BIGINT.",0.02594444444,0.02594444444,negative
impala,1487,description,"It would be useful to have a query option to return more detailed error information to the shell/client. For example, including dlerror strings and stack traces. We don't do this by default because in many cases it makes errors needlessly verbose and confusing.",design_debt,non-optimal_design,"Tue, 18 Nov 2014 23:21:04 +0000","Sat, 3 Nov 2018 00:25:58 +0000","Sat, 3 Nov 2018 00:25:58 +0000",124851894,"It would be useful to have a query option to return more detailed error information to the shell/client. For example, including dlerror strings and stack traces. We don't do this by default because in many cases it makes errors needlessly verbose and confusing.",0.04311111111,0.04311111111,neutral
impala,148,description,"There exists documentation that explains the steps to configure short-circuit reads however, several occurrences of performance issues directly related to this misconfiguration have come up. To help avoid situations where the system is running in sub-optimal mode, a validation check could be put in place that would make the startup of impalad fail if the configuration settings are not correct. This would help prevent people from running unknowingly in a degraded mode.",design_debt,non-optimal_design,"Tue, 19 Mar 2013 00:37:18 +0000","Sun, 20 Dec 2015 00:04:59 +0000","Fri, 29 Mar 2013 05:27:43 +0000",881425,"There exists documentation that explains the steps to configure short-circuit reads https://ccp.cloudera.com/display/IMPALA10BETADOC/Configuring+Impala+for+Performance however, several occurrences of performance issues directly related to this misconfiguration have come up. To help avoid situations where the system is running in sub-optimal mode, a validation check could be put in place that would make the startup of impalad fail if the configuration settings are not correct. This would help prevent people from running unknowingly in a degraded mode.",-0.02291666667,-0.02291666667,neutral
impala,153,description,"All of our logging and the content in the runtime profile uses fragment/query ids. It's currently not very easy to map those to host names. This makes diagnosing issues unnecessarily difficult. e.g. The log says fragment instance foo failed, we don't have an easy way to know which node to go to for the log/more details.",design_debt,non-optimal_design,"Tue, 19 Mar 2013 18:42:31 +0000","Mon, 8 Apr 2013 04:57:20 +0000","Mon, 8 Apr 2013 04:57:20 +0000",1678489,"All of our logging and the content in the runtime profile uses fragment/query ids. It's currently not very easy to map those to host names. This makes diagnosing issues unnecessarily difficult. e.g. The log says fragment instance foo failed, we don't have an easy way to know which node to go to for the log/more details.",0.085,0.085,negative
impala,1598,description,"Some error conditions cause a high volume of output that gets sent to the users, particularly if each backend produces several messages that are aggregated by the coordinator. An example is the Parquet error message when a file spans several blocks: you get one warning per file, all of which are communicating roughly the same thing. Unfortunately we can't just do simple string-matching to de-duplicate the messages, because they're all slightly different (filename changes). We should add an error code to each user-facing message, and use that as the unique key with which to aggregate the messages when there are lots of them.",design_debt,non-optimal_design,"Thu, 11 Dec 2014 00:57:34 +0000","Sun, 1 Mar 2015 05:29:32 +0000","Sun, 1 Mar 2015 05:29:32 +0000",6928318,"Some error conditions cause a high volume of output that gets sent to the users, particularly if each backend produces several messages that are aggregated by the coordinator. An example is the Parquet error message when a file spans several blocks: you get one warning per file, all of which are communicating roughly the same thing. Unfortunately we can't just do simple string-matching to de-duplicate the messages, because they're all slightly different (filename changes). We should add an error code to each user-facing message, and use that as the unique key with which to aggregate the messages when there are lots of them.",-0.375,-0.375,negative
impala,1651,description,"Currently, CREATE TABLE LIKE 'blindly' copies the source table, including the table's and partitions' parameter maps. The parameter maps may contain information about caching directives that do not apply to the new table. I can see two acceptable behaviors (choose one) if the source table is cached: 1. the new table is also cached; we need to issue new caching directives and store them in the params of the new table as appropriate 2. the new table is not cached and must be cached explicitly",design_debt,non-optimal_design,"Fri, 9 Jan 2015 18:05:43 +0000","Wed, 4 Jan 2017 23:58:08 +0000","Thu, 28 Jan 2016 14:04:28 +0000",33163125,"Currently, CREATE TABLE LIKE 'blindly' copies the source table, including the table's and partitions' parameter maps. The parameter maps may contain information about caching directives that do not apply to the new table. I can see two acceptable behaviors (choose one) if the source table is cached: 1. the new table is also cached; we need to issue new caching directives and store them in the params of the new table as appropriate 2. the new table is not cached and must be cached explicitly",0.1812,0.1812,neutral
impala,1691,description,"It used to be the case that the entire Catalogd's memory usage won't go over 1~2GB. However, a single table with 1000 columns and 8000 partitions used up 1.3GB of memory in the catalogd. The attached script created such a table. To see the memory usage, start catalogd without background loading. Observe the initial memory usage. Then run ""describe"" to load the table and observe the memory usage again.",design_debt,non-optimal_design,"Fri, 23 Jan 2015 21:27:05 +0000","Tue, 3 Mar 2015 17:42:45 +0000","Tue, 3 Mar 2015 17:42:45 +0000",3356140,"It used to be the case that the entire Catalogd's memory usage won't go over 1~2GB. However, a single table with 1000 columns and 8000 partitions used up 1.3GB of memory in the catalogd. The attached script created such a table. To see the memory usage, start catalogd without background loading. Observe the initial memory usage. Then run ""describe"" to load the table and observe the memory usage again.",-0.08333333333,-0.08333333333,neutral
impala,1927,description,"I have a CSV table with 14 columns, about 44 million rows. I found that with SELECT * and certain conditions in the WHERE clause, the shell would seem to output all the results (I would see the ++ line at the bottom of the result table) but then would hang and never return to the impala-shell prompt. If I SELECT COUNT(*) with the same WHERE clause, it works. If I SELECT DISTINCT <one of the columnsIf I do a CTAS into a Parquet table with SELECT * from the original table (no WHERE clause), it works. However, if I do the same query via 'impala-shell -q' or with delimited results via 'impala-shell -B -q', it hangs the same way. When I hit Ctrl-C, the resulting message is: ^C Cancelling Query Failed to reconnect and close: ERROR: Cancelled Cancelling Query which is different than what I saw when cancelling other SELECT or INSERT statements that were still in progress. This seems like the query has finished but the shell doesn't close it properly. The equivalent COUNT(*) and DISTINCT queries finish in 1-2 seconds. I let the SELECT * run for 3 minutes or more and it stays stuck. It's possible there is some anomaly somewhere in the CSV files, but like I say I can CTAS the whole contents of the table. It's only outputting in the shell that stalls. Schema, stalled query, actual data, profiles, logs all available on the Cloudera network for diagnosis. (Ping me for the location.)",design_debt,non-optimal_design,"Tue, 31 Mar 2015 06:10:55 +0000","Wed, 12 Oct 2016 05:52:09 +0000","Wed, 12 Oct 2016 05:52:09 +0000",48469274,"I have a CSV table with 14 columns, about 44 million rows. I found that with SELECT * and certain conditions in the WHERE clause, the shell would seem to output all the results (I would see the ------ line at the bottom of the result table) but then would hang and never return to the impala-shell prompt. If I SELECT COUNT with the same WHERE clause, it works. If I SELECT DISTINCT <one of the columns> with the same where clause, it works. If I do a CTAS into a Parquet table with SELECT * from the original table (no WHERE clause), it works. However, if I do the same query via 'impala-shell -q' or with delimited results via 'impala-shell -B -q', it hangs the same way. When I hit Ctrl-C, the resulting message is: ^C Cancelling Query Failed to reconnect and close: ERROR: Cancelled Cancelling Query which is different than what I saw when cancelling other SELECT or INSERT statements that were still in progress. This seems like the query has finished but the shell doesn't close it properly. The equivalent COUNT and DISTINCT queries finish in 1-2 seconds. I let the SELECT * run for 3 minutes or more and it stays stuck. It's possible there is some anomaly somewhere in the CSV files, but like I say I can CTAS the whole contents of the table. It's only outputting in the shell that stalls. Schema, stalled query, actual data, profiles, logs all available on the Cloudera network for diagnosis. (Ping me for the location.)",0.1357282051,0.1709857143,neutral
impala,1934,description,"When LDAP authentication is enabled in Impala, impala-shell always prompts for a password when trying to connect to an Impala daemon, which makes it very hard to use impala-shell in a shell script (where to store the password?) Please make impala-shell support reading password from a command line parameter or from a password file, just like what beeline does.",design_debt,non-optimal_design,"Thu, 2 Apr 2015 00:08:08 +0000","Thu, 7 Apr 2016 17:40:54 +0000","Wed, 20 Jan 2016 01:22:12 +0000",25319644,"When LDAP authentication is enabled in Impala, impala-shell always prompts for a password when trying to connect to an Impala daemon, which makes it very hard to use impala-shell in a shell script (where to store the password?) Please make impala-shell support reading password from a command line parameter or from a password file, just like what beeline does.",-0.025,-0.025,negative
impala,2068,description,"It is currently difficult for clients connected via load balancers to know which coordinator they're connected to. The TOpenSessionResp has a map of configuration settings which are returned, and we can return the connected coordinator in that map. Clients such as Hue can use that information to find the coordinator and thus the debug web UI pages for submitted queries over that session.",design_debt,non-optimal_design,"Mon, 15 Jun 2015 18:06:02 +0000","Fri, 19 Jun 2015 18:44:56 +0000","Fri, 19 Jun 2015 18:44:56 +0000",347934,"It is currently difficult for clients connected via load balancers to know which coordinator they're connected to. The TOpenSessionResp has a map of configuration settings which are returned, and we can return the connected coordinator in that map. Clients such as Hue can use that information to find the coordinator and thus the debug web UI pages for submitted queries over that session.",-0.06666666667,-0.06666666667,neutral
impala,2076,description,"*Problem Statement* The ExecSummary does not report the time spent on the network. The time reported on the EXCHANGE node is misleading and does not count network time. This makes it impossible to identify the performance bottleneck. *Cause* The ExecSummary reports the non-waiting time spent in the EXCHANGE as the network time. Obviously, the active time spent in the EXCHANGE is zero (or close to zero). It should be replaced by the corresponding Data Stream Sender total time instead. *Workaround* Examine the corresponding time in the Data Stream Sender instead.",design_debt,non-optimal_design,"Thu, 18 Jun 2015 02:08:59 +0000","Thu, 28 Apr 2016 23:57:23 +0000","Thu, 21 Apr 2016 18:24:19 +0000",26669720,"Problem Statement The ExecSummary does not report the time spent on the network. The time reported on the EXCHANGE node is misleading and does not count network time. This makes it impossible to identify the performance bottleneck. Cause The ExecSummary reports the non-waiting time spent in the EXCHANGE as the network time. Obviously, the active time spent in the EXCHANGE is zero (or close to zero). It should be replaced by the corresponding Data Stream Sender total time instead. Workaround Examine the corresponding time in the Data Stream Sender instead.",-0.1357142857,-0.1357142857,negative
impala,2128,description,The current partitioned HJ and Agg implementation does not check what's the size of the hash table that it is going to be created. There are cases where the hash table of a partition to be needed to be larger than 1GB. In that case due to IMPALA-1619 we have to fail the query (for more info look the work-around IMPALA-2065). Instead of failing the query we may try to repartition that large partition in an effort to create HTs smaller than 1GB. For example in,design_debt,non-optimal_design,"Tue, 7 Jul 2015 22:25:38 +0000","Wed, 4 Jan 2017 23:58:07 +0000","Fri, 10 Jul 2015 20:09:41 +0000",251043,The current partitioned HJ and Agg implementation does not check what's the size of the hash table that it is going to be created. There are cases where the hash table of a partition to be needed to be larger than 1GB. In that case due to IMPALA-1619 we have to fail the query (for more info look the work-around IMPALA-2065). Instead of failing the query we may try to repartition that large partition in an effort to create HTs smaller than 1GB. For example in PartitionedHashJoinNode::Partition::BuildHashTableInternal,-0.06,0.06,neutral
impala,2174,description,"Whenever we serialize a row batch, even a row batch with 0 materialized slots, we always allocate an array of tuple_offsets per tuple. That means that there is a serialization overhead of 4B per tuple (per row). Currently we do not consider this overhead when we calculate the and consequently the avgRowSize_ which is used for example when we decide which input to We should take into account this overhead. Such a change may affect plans of queries with small avgRowSize_ or multiple tuples (joins).",design_debt,non-optimal_design,"Mon, 3 Aug 2015 20:54:01 +0000","Sat, 6 Feb 2016 20:11:30 +0000","Sat, 6 Feb 2016 20:11:20 +0000",16154239,"Whenever we serialize a row batch, even a row batch with 0 materialized slots, we always allocate an array of tuple_offsets per tuple. That means that there is a serialization overhead of 4B per tuple (per row). Currently we do not consider this overhead when we calculate the TupleDescriptor::avgSerializedSize_ and consequently the avgRowSize_ which is used for example when we decide which input to broadcast/distribute. We should take into account this overhead. Such a change may affect plans of queries with small avgRowSize_ or multiple tuples (joins).",0.0,0.0,neutral
impala,2212,description,"When beeline is accessing the impala, the output is misaligned.",design_debt,non-optimal_design,"Mon, 17 Aug 2015 17:21:51 +0000","Tue, 18 Aug 2015 17:11:21 +0000","Tue, 18 Aug 2015 17:11:21 +0000",85770,"When beeline is accessing the impala, the output is misaligned.",0.0,0.0,neutral
impala,2295,description,doesn't deep copy collections. This leaves the destination Tuple with pointers into potentially invalid memory.,design_debt,non-optimal_design,"Thu, 3 Sep 2015 20:40:00 +0000","Wed, 9 Sep 2015 02:48:07 +0000","Wed, 9 Sep 2015 02:48:07 +0000",454087,BufferedTupleStream::DeepCopyInternal doesn't deep copy collections. This leaves the destination Tuple with pointers into potentially invalid memory.,0.0,0.3,negative
impala,242,description,"After cancelling a query in impala-shell with Control-C, the cancelled query remains in the executing queries section at the top of the debug UI web page and subsequently started queries end up sorted below it, which is contrary to the desired sorting order by start time and inconvenient when webex-ing with limit screen real estate as it makes it harder to see the latest test query status without scrolling to get past these cancelled queries.",design_debt,non-optimal_design,"Fri, 12 Apr 2013 22:58:53 +0000","Sun, 20 Dec 2015 00:05:00 +0000","Tue, 23 Apr 2013 01:54:50 +0000",874557,"After cancelling a query in impala-shell with Control-C, the cancelled query remains in the executing queries section at the top of the impalad:25000/queries debug UI web page and subsequently started queries end up sorted below it, which is contrary to the desired sorting order by start time and inconvenient when webex-ing with limit screen real estate as it makes it harder to see the latest test query status without scrolling to get past these cancelled queries.",-0.025,-0.025,negative
impala,2435,description,"While I was reviewing Dimitris's IMPALA-2369 patch, I realized that in AddBatch() we call Run::Init() without checking the return status of it. That means that Dimitris's patch probably won't do much good :) At Instead of creating a different patch, I will let Dimitris add it to his IMPALA-2369 patch.",design_debt,non-optimal_design,"Sat, 26 Sep 2015 05:44:45 +0000","Sun, 20 Dec 2015 00:05:37 +0000","Tue, 29 Sep 2015 19:09:03 +0000",307458,"While I was reviewing Dimitris's IMPALA-2369 patch, I realized that in AddBatch() we call Run::Init() without checking the return status of it. That means that Dimitris's patch probably won't do much good At https://github.com/cloudera/Impala/blob/cdh5-trunk/be/src/runtime/sorter.cc#L1001 Instead of creating a different patch, I will let Dimitris add it to his IMPALA-2369 patch.",0.2,0.0,negative
impala,2632,description,"Currently impalad needs external LLVM IR files to be compiled separately from the binary and distributed with the binary. This slows down builds, make it easy for the code to get out of sync in dev environments, and complicates distribution. Instead, we could simply link the IR data into the binary file, so impalad is more self-contained. These IR files are also loaded from disk by codegen for each fragment. Linking the IR into the binary would avoid this step, saving some overhead. Could also address IMPALA-799 (.bc file extension instead of .ll extension) at same time.",design_debt,non-optimal_design,"Wed, 4 Nov 2015 17:35:04 +0000","Fri, 13 Nov 2015 21:44:20 +0000","Fri, 13 Nov 2015 21:44:20 +0000",792556,"Currently impalad needs external LLVM IR files to be compiled separately from the binary and distributed with the binary. This slows down builds, make it easy for the code to get out of sync in dev environments, and complicates distribution. Instead, we could simply link the IR data into the binary file, so impalad is more self-contained. These IR files are also loaded from disk by codegen for each fragment. Linking the IR into the binary would avoid this step, saving some overhead. Could also address IMPALA-799 (.bc file extension instead of .ll extension) at same time.",0.03958333333,0.03958333333,neutral
impala,290,description,"A user ran into a problem when they tried to use LZO compressed RC file. Instead of displaying a clear ""Not Yet error, the query failed with an Unknown Codec message. It would be good to improve this.",design_debt,non-optimal_design,"Fri, 19 Apr 2013 20:10:38 +0000","Wed, 24 Apr 2013 23:13:35 +0000","Wed, 24 Apr 2013 23:13:35 +0000",442977,"A user ran into a problem when they tried to use LZO compressed RC file. Instead of displaying a clear ""Not Yet Supported/Implemented"" error, the query failed with an Unknown Codec message. It would be good to improve this.",-0.07066666667,0.01822222222,negative
impala,2911,description,"Query contains functions like ""rand(), if()"" could crash Impala. *Workaround:* turning logging down or off should fix it",design_debt,non-optimal_design,"Fri, 29 Jan 2016 18:46:54 +0000","Fri, 29 Jan 2016 19:01:47 +0000","Fri, 29 Jan 2016 18:49:15 +0000",141,"Query contains functions like ""rand(), if()"" could crash Impala. Workaround: turning logging down or off should fix it",-0.173,-0.346,negative
impala,3008,description,"The filter routing table printed by the coordinator can be slightly improved: # Don't print it if there's no global filtering happening # Use the {{TablePrinter}} class to format it neatly # Print if filters are broadcast, or if they are partition only.",design_debt,non-optimal_design,"Tue, 16 Feb 2016 22:03:29 +0000","Fri, 19 Feb 2016 22:10:48 +0000","Fri, 19 Feb 2016 19:49:16 +0000",251147,"The filter routing table printed by the coordinator can be slightly improved: Don't print it if there's no global filtering happening Use the TablePrinter class to format it neatly Print if filters are broadcast, or if they are partition only.",0.4,0.4,neutral
impala,3077,description,"Because of a quirk in the runtime filters implementation, we currently have to disable them when a join spills. The only reason this is necessary is that the filters are constructed as part of the hash table build. But there is no reason that we need to construct the filters at that point: we could instead construct the filters when doing initial processing of the build input, at which point we see all build-side input rows regardless of whether they are spilled or not. This might actually perform better since it would move some of the CPU work from the CPU-intensive hash table build to the less CPU-intensive construction of the input stream.",design_debt,non-optimal_design,"Wed, 24 Feb 2016 22:16:19 +0000","Wed, 4 Jan 2017 23:58:12 +0000","Wed, 20 Apr 2016 05:51:17 +0000",4779298,"Because of a quirk in the runtime filters implementation, we currently have to disable them when a join spills. The only reason this is necessary is that the filters are constructed as part of the hash table build. But there is no reason that we need to construct the filters at that point: we could instead construct the filters when doing initial processing of the build input, at which point we see all build-side input rows regardless of whether they are spilled or not. This might actually perform better since it would move some of the CPU work from the CPU-intensive hash table build to the less CPU-intensive construction of the input stream.",0.0625,0.0625,neutral
impala,3099,description,"makes one RPC for every fragment instance, serially. There are no dependencies between fragment instances, so we should use to issue the cancellation requests.",design_debt,non-optimal_design,"Sun, 28 Feb 2016 22:35:19 +0000","Tue, 30 Apr 2019 20:01:22 +0000","Thu, 12 Jan 2017 20:33:47 +0000",27554308,"Coordinator::CancelRemoteFragments() makes one RPC for every fragment instance, serially. There are no dependencies between fragment instances, so we should use ExecEnv::rpc_pool() to issue the cancellation requests.",0.0,0.0,neutral
impala,3144,description,"Currently our INSERT logic assumes that the table we're writing to lives only on one filesystem. However, the table could have different partitions on different filesystems. Adjust the logic in so that this becomes possible.",design_debt,non-optimal_design,"Fri, 4 Mar 2016 21:03:09 +0000","Tue, 3 May 2016 16:45:33 +0000","Tue, 3 May 2016 16:45:33 +0000",5168544,"Currently our INSERT logic assumes that the table we're writing to lives only on one filesystem. However, the table could have different partitions on different filesystems. Adjust the logic in Coordinator::FinalizeSuccessfulInsert() so that this becomes possible.",0.0,0.0,neutral
impala,321,description,In the summary node in the profile the info strings for start / end times are specified with second granularity. It would be nice to have them specify milliseconds.,design_debt,non-optimal_design,"Sat, 27 Apr 2013 00:28:49 +0000","Sun, 20 Dec 2015 00:05:02 +0000","Tue, 11 Jun 2013 01:46:59 +0000",3892690,In the summary node in the profile the info strings for start / end times are specified with second granularity. It would be nice to have them specify milliseconds.,0.4125,0.4125,neutral
impala,322,description,"It would be nice to get able to get information about errors the query encountered, like corruptions, missing block metadata, integer overflows, and anything you guys think would be interesting, in the profile so it's all centralized.",design_debt,non-optimal_design,"Sat, 27 Apr 2013 00:35:02 +0000","Sun, 20 Dec 2015 00:05:02 +0000","Tue, 7 May 2013 17:52:04 +0000",926222,"It would be nice to get able to get information about errors the query encountered, like corruptions, missing block metadata, integer overflows, and anything you guys think would be interesting, in the profile so it's all centralized.",0.1521666667,0.1521666667,neutral
impala,324,description,"It's occurred to me that, without a current global query view, situations will likely occur where a runaway query sent to another query planner goes unnoticed in administration but users may complain of poor cluster performance and aren't able to see why unless they somehow know to specifically go to the other query planner node which that runaway query was submitted to. One could suggest the best practice of only having clients connect to one node to submit queries and keep that effectively the only query planner in the cluster but it doesn't prevent the problem if they fail to heed such advice, which some users almost certainly will. Also some more advanced admins may load balance the query planners for HA or load distribution. I propose that there is, if not already, a max query time that a query planner will allow a query to run before self terminating the query across all impalad nodes. This will prevent the runaway query from continuously affecting performance and leaving a hard to trace problem. Given that Impala queries should execute in seconds, or minutes at the worst, maybe 300 or 600 seconds would be a reasonable default max query execution time (wall time). This would need to be configurable by the admin/user both globally for the node's query planner default via --switch/flags file and also per session override eg. in impala-shell some kind of ""set max_query_time="" command. Even if the global query view implementation I've asked for happens down the road, this max query time would also alleviate administrator burden of having to kill runaway user queries manually.",design_debt,non-optimal_design,"Mon, 29 Apr 2013 11:21:20 +0000","Sun, 20 Dec 2015 00:05:02 +0000","Mon, 13 Oct 2014 23:23:24 +0000",46008124,"It's occurred to me that, without a current global query view, situations will likely occur where a runaway query sent to another query planner goes unnoticed in administration but users may complain of poor cluster performance and aren't able to see why unless they somehow know to specifically go to the other query planner node which that runaway query was submitted to. One could suggest the best practice of only having clients connect to one node to submit queries and keep that effectively the only query planner in the cluster but it doesn't prevent the problem if they fail to heed such advice, which some users almost certainly will. Also some more advanced admins may load balance the query planners for HA or load distribution. I propose that there is, if not already, a max query time that a query planner will allow a query to run before self terminating the query across all impalad nodes. This will prevent the runaway query from continuously affecting performance and leaving a hard to trace problem. Given that Impala queries should execute in seconds, or minutes at the worst, maybe 300 or 600 seconds would be a reasonable default max query execution time (wall time). This would need to be configurable by the admin/user both globally for the node's query planner default via --switch/flags file and also per session override eg. in impala-shell some kind of ""set max_query_time="" command. Even if the global query view implementation I've asked for happens down the road, this max query time would also alleviate administrator burden of having to kill runaway user queries manually.",-0.01592916667,-0.01592916667,neutral
impala,3329,description,"I believe the {{impalad}} log rotation policy is causing logs to rotate out that we don't have access to. {{impalad}} defaults to keeping 10 log files in the log directory. This means if {{impalad}} restarts 11 times, the first logs will be rotated out. Many of the custom cluster tests restart {{impalad}}, though. This means {{impalad}} logs in the custom_cluster logs subdirectory are missing. To prove this is happening as a result of the log rotation policy, and it's not our inability to properly collect logs, I ran locally, and to get timestamps to show more about when tests were being run, I used this: (0) This gets us timestamps when there otherwise wouldn't be any. Here's the time range for our custom cluster tests: The earliest timetstamp log files are for 20:58. Contrast this to the end-to-end tests' log directory, which only has a handful of logs and span the 2 hours it took to run the test. The quick fix is to set the custom cluster tests to run with a limitless log rotation policy. The more detailed fix is to probably plumb things through such that we don't ever rotate logs. This is safe in Jenkins since we deleted the logs directory before every run (in code): we won't have a workspace accumulating logs. (0) Yup, you need {{gawk}} for {{strftime}} which {{mawk}}, the default {{awk}} on at least Ubuntu, doesn't have.",design_debt,non-optimal_design,"Mon, 11 Apr 2016 17:13:16 +0000","Wed, 27 Apr 2016 18:56:24 +0000","Wed, 27 Apr 2016 14:22:15 +0000",1372139,"I believe the impalad log rotation policy is causing logs to rotate out that we don't have access to. impalad defaults to keeping 10 log files in the log directory. This means if impalad restarts 11 times, the first logs will be rotated out. Many of the custom cluster tests restart impalad, though. This means impalad logs in the custom_cluster logs subdirectory are missing. To prove this is happening as a result of the log rotation policy, and it's not our inability to properly collect logs, I ran run-all-tests.sh locally, and to get timestamps to show more about when tests were being run, I used this: (0) This gets us timestamps when there otherwise wouldn't be any. Here's the time range for our custom cluster tests: The earliest timetstamp log files are for 20:58. Contrast this to the end-to-end tests' log directory, which only has a handful of logs and span the 2 hours it took to run the test. The quick fix is to set the custom cluster tests to run with a limitless log rotation policy. The more detailed fix is to probably plumb things through run-all-tests.sh such that we don't ever rotate logs. This is safe in Jenkins since we deleted the logs directory before every run (in code): we won't have a workspace accumulating logs. (0) Yup, you need gawk for strftime which mawk, the default awk on at least Ubuntu, doesn't have.",0.03958333333,0.03392857143,neutral
impala,3548,description,"Currently, the FE generates a number of runtime filters and assigns them to plan nodes without taking the value of RUNTIME_FILTER_MODE into account. In the backend, the filters are removed from the exec nodes based on the target node types (local or remote) and the value of the RUNTIME_FILTER_MODE option. This may cause some confusion to users because they may see runtime filters in the output of explain that aren't applied when the query is executed. This operation should be performed in the FE.",design_debt,non-optimal_design,"Mon, 16 May 2016 23:48:01 +0000","Sun, 29 Oct 2017 08:08:27 +0000","Sun, 29 Oct 2017 08:08:27 +0000",45822026,"Currently, the FE generates a number of runtime filters and assigns them to plan nodes without taking the value of RUNTIME_FILTER_MODE into account. In the backend, the filters are removed from the exec nodes based on the target node types (local or remote) and the value of the RUNTIME_FILTER_MODE option. This may cause some confusion to users because they may see runtime filters in the output of explain that aren't applied when the query is executed. This operation should be performed in the FE.",-0.09375,-0.09375,neutral
impala,3652,description,"There is a tricky corner case in our resource transfer model with subplans and limits. The problem is that the limit in the subplan may mean that the exec node is reset before it has returned its full output. The resource transfer logic generally attaches resources to batches at specific points in the output, e.g. end of partition, end of block, so it's possible that batches returned before the Reset() may reference resources that have not yet been transferred. It's unclear if we test this scenario consistently or if it's always handled correctly. One example is this query, reported in IMPALA-5456:",design_debt,non-optimal_design,"Tue, 31 May 2016 20:46:57 +0000","Mon, 19 Nov 2018 10:45:14 +0000","Wed, 7 Nov 2018 23:06:04 +0000",76904347,"There is a tricky corner case in our resource transfer model with subplans and limits. The problem is that the limit in the subplan may mean that the exec node is reset before it has returned its full output. The resource transfer logic generally attaches resources to batches at specific points in the output, e.g. end of partition, end of block, so it's possible that batches returned before the Reset() may reference resources that have not yet been transferred. It's unclear if we test this scenario consistently or if it's always handled correctly. One example is this query, reported in IMPALA-5456:",-0.09833333333,-0.09833333333,neutral
impala,3671,description,"The immediate motivation for this is to enable better testing of graceful failures when spilling is disabled (e.g. IMPALA-3670). Currently we only have control over this via startup options, so we have to implement these as custom cluster tests, but there's no reason in principle we need to start a fresh cluster. The idea would be to add a query option 'scratch_limit' or similar, that limits the amount of scratch directory space that can be used. This would be useful to prevent runaway queries or to prevent queries from spilling when that is not desired.",design_debt,non-optimal_design,"Fri, 3 Jun 2016 21:31:45 +0000","Thu, 15 Dec 2016 00:36:24 +0000","Sat, 24 Sep 2016 04:53:45 +0000",9703320,"The immediate motivation for this is to enable better testing of graceful failures when spilling is disabled (e.g. IMPALA-3670). Currently we only have control over this via startup options, so we have to implement these as custom cluster tests, but there's no reason in principle we need to start a fresh cluster. The idea would be to add a query option 'scratch_limit' or similar, that limits the amount of scratch directory space that can be used. This would be useful to prevent runaway queries or to prevent queries from spilling when that is not desired.",-0.05375,-0.05375,neutral
impala,3828,description,"Today the planner creates left deep trees where joins are ordered based on selectivity. The proposal is to add a rule in the planner that traverses the plan after and flip the build and probe sides if the cardinality suggests so, which produces a bushy plan. This optimization should improve queries against normalized schemas with selective joins and multiple fact tables, more ""efficient"" runtime filters will be created as a result of the plan change The query below generates a left deep plan, where are a bush plan should be created query Plan",design_debt,non-optimal_design,"Wed, 6 Jul 2016 00:36:19 +0000","Fri, 19 Aug 2016 06:10:19 +0000","Fri, 19 Aug 2016 06:10:19 +0000",3821640,"Today the planner creates left deep trees where joins are ordered based on selectivity. The proposal is to add a rule in the planner that traverses the plan after createSingleNodePlan and flip the build and probe sides if the cardinality suggests so, which produces a bushy plan. This optimization should improve queries against normalized schemas with selective joins and multiple fact tables, more ""efficient"" runtime filters will be created as a result of the plan change The query below generates a left deep plan, where are a bush plan should be created query Plan",0.1333333333,0.1333333333,neutral
impala,3859,description,"is designed to log the data around a particular scanner parsing error. However, that log output may include sensitive table data, and should be avoided, particularly for uncompressed text formats.",design_debt,non-optimal_design,"Thu, 14 Jul 2016 00:23:13 +0000","Thu, 28 Sep 2017 15:50:33 +0000","Thu, 25 Aug 2016 16:25:24 +0000",3686531,"HdfsScanNode::LogRowParseError() is designed to log the data around a particular scanner parsing error. However, that log output may include sensitive table data, and should be avoided, particularly for uncompressed text formats.",-0.3,-0.3,neutral
impala,4162,description,"I noticed that during metadata loading or after running ""invalidate metadata"" is that there is an extensive amount of CPU spent and memory allocated When I checked the NameNode log I found thousands of entries with the error message below By default is false. When I enabled the warning and INFO messages stopped and metadata loading after ""invalidate metadata"" was 5-10% faster. This is the call stack for the exception Query timeline with ACLs enabled Query timeline with ACLs disable",design_debt,non-optimal_design,"Mon, 19 Sep 2016 18:07:45 +0000","Tue, 13 Jun 2017 05:28:26 +0000","Tue, 13 Jun 2017 05:27:17 +0000",23023172,"I noticed that during metadata loading or after running ""invalidate metadata"" is that there is an extensive amount of CPU spent and memory allocated When I checked the NameNode log I found thousands of entries with the error message below By default dfs.namenode.acls.enabled is false. When I enabled dfs.namenode.acls.enabled the warning and INFO messages stopped and metadata loading after ""invalidate metadata"" was 5-10% faster. This is the call stack for the exception Query timeline with ACLs enabled Query timeline with ACLs disable",-0.2555555556,-0.1166666667,negative
impala,4182,description,"It would be nice to have a function that returned the name of the current coordinator so that users could locate the webui that has their running queries. Problem statement: As more and more big installations use a load balancer between impala and users/hue, there is a need for users to be able to get to the query profiles for debugging. But there are a few problems with the current approaches: # Many users will not have access to CM # Profile; in shell does not help Hue users, and often the profile output is truncated on screen (via putty) so users have to exit the shell and pipe to file # Users cannot find the coordinator for the webui if they connect through a load balancer and do not have CM access",design_debt,non-optimal_design,"Thu, 22 Sep 2016 15:02:29 +0000","Wed, 28 Sep 2016 16:26:09 +0000","Thu, 22 Sep 2016 16:25:41 +0000",4992,"It would be nice to have a function that returned the name of the current coordinator so that users could locate the webui that has their running queries. Problem statement: As more and more big installations use a load balancer between impala and users/hue, there is a need for users to be able to get to the query profiles for debugging. But there are a few problems with the current approaches: Many users will not have access to CM Profile; in shell does not help Hue users, and often the profile output is truncated on screen (via putty) so users have to exit the shell and pipe to file Users cannot find the coordinator for the webui if they connect through a load balancer and do not have CM access",0.1958888889,0.1958888889,neutral
impala,4320,description,We should switch Impala to build using the gold linker by default to speed up builds and improve the experience of new developers. We already had a discussion on the dev@ mailing list where people seemed to agree with the idea.,design_debt,non-optimal_design,"Wed, 19 Oct 2016 00:18:33 +0000","Thu, 20 Oct 2016 15:08:58 +0000","Thu, 20 Oct 2016 15:08:58 +0000",139825,We should switch Impala to build using the gold linker by default to speed up builds and improve the experience of new developers. We already had a discussion on the dev@ mailing list where people seemed to agree with the idea.,0.15,0.15,positive
impala,4485,description,"Currently table metadata changes are locked at the table level for DML. For tables with partitions, it would help performance if concurrent inserts if locking was based at the partition level based on changes to the partition(s) affected, instead of locking the entire table.",design_debt,non-optimal_design,"Mon, 14 Nov 2016 22:59:37 +0000","Wed, 25 Jan 2017 23:19:00 +0000","Wed, 25 Jan 2017 23:19:00 +0000",6221963,"Currently table metadata changes are locked at the table level for DML. For tables with partitions, it would help performance if concurrent inserts if locking was based at the partition level based on changes to the partition(s) affected, instead of locking the entire table.",0.05,0.05,neutral
impala,4612,description,I'm seeing a lot of error messages in the log when running test_udf.py along the lines of: I haven't seen any adverse effects of these messages but it's adding a lot of noise.,design_debt,non-optimal_design,"Wed, 7 Dec 2016 02:11:31 +0000","Thu, 19 Jan 2017 21:13:25 +0000","Thu, 19 Jan 2017 21:13:25 +0000",3783714,I'm seeing a lot of error messages in the log when running test_udf.py along the lines of: I haven't seen any adverse effects of these messages but it's adding a lot of noise.,0.05,0.05,negative
impala,4639,description,"Some tests are unable to run on remote clusters because of test setup or infrastructure reasons, and not because of product failure. We should be able to selectively skip tests that can't be set up properly to run against a remote cluster.",design_debt,non-optimal_design,"Fri, 9 Dec 2016 23:22:22 +0000","Tue, 29 Aug 2017 16:19:52 +0000","Tue, 29 Aug 2017 16:19:52 +0000",22697850,"Some tests are unable to run on remote clusters because of test setup or infrastructure reasons, and not because of product failure. We should be able to selectively skip tests that can't be set up properly to run against a remote cluster.",0.144,0.144,negative
impala,4671,description,"Kudu's {{ServicePool}} uses Kudu's {{Thread}} class to service RPC requests. While this works well, the threads it creates aren't monitored by Impala's {{ThreadMgr}} subsystem. Eventually we'd like to standardise on one thread class between projects, but for now it would be useful to reimplement {{ServicePool}} in terms of our thread class. The reactor threads and acceptor threads will still be Kudu threads, but those are less likely to do substantial work, so having them missing from monitoring isn't a big problem.",design_debt,non-optimal_design,"Thu, 15 Dec 2016 17:51:19 +0000","Thu, 14 Dec 2017 23:50:20 +0000","Fri, 8 Dec 2017 11:36:12 +0000",30908693,"Kudu's ServicePool uses Kudu's Thread class to service RPC requests. While this works well, the threads it creates aren't monitored by Impala's ThreadMgr subsystem. Eventually we'd like to standardise on one thread class between projects, but for now it would be useful to reimplement ServicePool in terms of our thread class. The reactor threads and acceptor threads will still be Kudu threads, but those are less likely to do substantial work, so having them missing from monitoring isn't a big problem.",0.3155833333,0.3155833333,neutral
impala,4728,description,"Currently Impala uses lazy evaluation for expressions. This can result in a performance overhead when using or reusing expressions in things like a window function order by vs having the expression materialized as a projection from the underlying relation, especially if the expression is used in multiple places.",design_debt,non-optimal_design,"Thu, 5 Jan 2017 01:33:09 +0000","Thu, 27 Apr 2017 18:22:58 +0000","Thu, 27 Apr 2017 18:22:58 +0000",9737389,"Currently Impala uses lazy evaluation for expressions. This can result in a performance overhead when using or reusing expressions in things like a window function order by vs having the expression materialized as a projection from the underlying relation, especially if the expression is used in multiple places.",-0.1,-0.1,negative
impala,4831,description,"If a client unpins some pages, then calls it can leave too many dirty unpinned pages in memory. Similarly, a client can cause mischief by calling AllocateFrom() and ReleaseTo() or by creating child",design_debt,non-optimal_design,"Thu, 26 Jan 2017 22:35:25 +0000","Thu, 16 Mar 2017 03:36:53 +0000","Thu, 16 Mar 2017 03:36:53 +0000",4165288,"If a client unpins some pages, then calls ReservationTracker::DecreaseReservation(), it can leave too many dirty unpinned pages in memory. Similarly, a client can cause mischief by calling AllocateFrom() and ReleaseTo() or by creating child ReservationTrackers.",-0.25,-0.1,negative
impala,4833,description,"Following on from IMPALA-3748, we should consider combining the resource estimates with the schedule to accurate compute the minimum buffer requirement per daemon. Different daemons may have different resource requirements because not every fragment runs on every daemon (e.g. unpartitioned fragments, fragments with fewer scan ranges than daemons).",design_debt,non-optimal_design,"Fri, 27 Jan 2017 17:33:58 +0000","Mon, 14 Aug 2017 16:14:41 +0000","Mon, 14 Aug 2017 16:14:41 +0000",17188843,"Following on from IMPALA-3748, we should consider combining the resource estimates with the schedule to accurate compute the minimum buffer requirement per daemon. Different daemons may have different resource requirements because not every fragment runs on every daemon (e.g. unpartitioned fragments, fragments with fewer scan ranges than daemons).",0.375,0.375,neutral
impala,4862,description,"In the following example the way the peak resource estimate is computed from per-node estimates is wrong. It should be 476.41MB, because the scan node is Open()ed in the backend *while* the concurrent join builds are executing. Another example is this one, where in the backend the aggregations can execute concurrently with the join builds The behaviour for unions also is not accurate - branches of unions within the same fragment are execute serially, but anything below an exchanges is executed concurrently.",design_debt,non-optimal_design,"Wed, 1 Feb 2017 22:28:32 +0000","Wed, 12 Jul 2017 04:20:23 +0000","Wed, 12 Jul 2017 04:20:23 +0000",13845111,"In the following example the way the peak resource estimate is computed from per-node estimates is wrong. It should be 476.41MB, because the scan node is Open()ed in the backend while the concurrent join builds are executing. Another example is this one, where in the backend the aggregations can execute concurrently with the join builds The behaviour for unions also is not accurate - branches of unions within the same fragment are execute serially, but anything below an exchanges is executed concurrently.",-0.2638888889,-0.2638888889,negative
impala,4871,description,"Catalog when fails(or passes) to load block metadata of specific table files, throws following messages in logs. I0128 01:54:33.537742 22702 HdfsTable.java:345] load block md for table-x file 000066_0 I0128 01:54:59.373677 22702 Cancelled while waiting for datanode x.y.z.215:50020: I0128 01:54:59.373868 22702 Cancelled while waiting for datanode x.y.z.148:50020: These logs will be more useful if the entire hierarchy of file location is logged including all levels of partitions. Since just file name like ""000066_0"" can be present for thousands of partitions under same table, it is difficult to isolate the problematic file with a just above log entry.",design_debt,non-optimal_design,"Thu, 2 Feb 2017 18:47:59 +0000","Tue, 20 Jun 2017 04:41:18 +0000","Tue, 20 Jun 2017 04:41:18 +0000",11872399,"Catalog when fails(or passes) to load block metadata of specific table files, throws following messages in logs. I0128 01:54:33.537742 22702 HdfsTable.java:345] load block md for table-x file 000066_0 I0128 01:54:59.373677 22702 BlockStorageLocationUtil.java:167] Cancelled while waiting for datanode x.y.z.215:50020: java.util.concurrent.CancellationException I0128 01:54:59.373868 22702 BlockStorageLocationUtil.java:167] Cancelled while waiting for datanode x.y.z.148:50020: java.util.concurrent.CancellationException These logs will be more useful if the entire hierarchy of file location is logged including all levels of partitions. Since just file name like ""000066_0"" can be present for thousands of partitions under same table, it is difficult to isolate the problematic file with a just above log entry.",0.06325,0.1023333333,neutral
impala,4933,description,"Thrift currently initializes OpenSSL lazily, when the first connection is created. However, the OpenSSL initialization must happen before the Kudu library is used (because Kudu will not be doing the initialization itself, and it will check that the OpenSSL initialization already occurred), so thrift needs to do the initialization on startup.",design_debt,non-optimal_design,"Tue, 14 Feb 2017 23:50:38 +0000","Fri, 26 May 2017 21:18:49 +0000","Tue, 21 Feb 2017 00:01:59 +0000",519081,"Thrift currently initializes OpenSSL lazily, when the first connection is created. However, the OpenSSL initialization must happen before the Kudu library is used (because Kudu will not be doing the initialization itself, and it will check that the OpenSSL initialization already occurred), so thrift needs to do the initialization on startup.",0.25,0.25,neutral
impala,4967,description,"When running REFRESH table partition (x=1) the metadata distribution contains all partitions, not just the single partition that was refreshed. This creates additional load and delays refreshes. To solve metadata updates might have to be serialized (assign unique #, and apply in order), but this behavior would help the timeliness of data and reduce load on the cluster.",design_debt,non-optimal_design,"Wed, 22 Feb 2017 20:41:44 +0000","Fri, 11 Aug 2017 21:59:54 +0000","Fri, 11 Aug 2017 21:59:54 +0000",14692690,"When running REFRESH table partition (x=1) the metadata distribution contains all partitions, not just the single partition that was refreshed. This creates additional load and delays refreshes. To solve metadata updates might have to be serialized (assign unique #, and apply in order), but this behavior would help the timeliness of data and reduce load on the cluster.",0.3,0.3,neutral
impala,4970,description,"Although we retain the histogram of fragment instance startup latencies, we don't record the identity of the most expensive instance, or the host it runs on. This would be helpful in diagnosing slow query start-up times.",design_debt,non-optimal_design,"Wed, 22 Feb 2017 23:52:56 +0000","Tue, 19 Jun 2018 21:22:02 +0000","Tue, 19 Jun 2018 21:22:02 +0000",41635746,"Although we retain the histogram of fragment instance startup latencies, we don't record the identity of the most expensive instance, or the host it runs on. This would be helpful in diagnosing slow query start-up times.",0.3125,0.3125,neutral
impala,5002,description,"As identified in , it is not easy to know how (i.e. build flags) a toolchain build was produced. The build scripts are versioned in the native-toolchain repo, but that is not easily associated with generated toolchain builds. We should have some way to determine this information later. The simplest way to handle this may be to store the native-toolchain githash in the produced toolchain build.",design_debt,non-optimal_design,"Mon, 27 Feb 2017 22:08:12 +0000","Wed, 1 Mar 2017 22:25:34 +0000","Wed, 1 Mar 2017 22:25:34 +0000",173842,"As identified in https://gerrit.cloudera.org/#/c/6165/ , it is not easy to know how (i.e. build flags) a toolchain build was produced. The build scripts are versioned in the native-toolchain repo, but that is not easily associated with generated toolchain builds. We should have some way to determine this information later. The simplest way to handle this may be to store the native-toolchain githash in the produced toolchain build.",-0.073,-0.073,neutral
impala,5150,description,"When doing concurrency testing as part of the competitive benchmarking I noticed that it is very difficult to saturate all CPUs @100% Below is a snapshot from htop during a concurrency run, state below closely mimics the steady state, note that CPUs 41-60 are less busy compared to 1-20. Then I ran the command below which dumps the threads and processor associated with each, reference. for i in $(pgrep impalad); do ps -mo -p $i;done From the man page for ps : The output showed that a large number of threads are running on core 61, not surprisingly the 1K threads are all thrift-server threads, so I am wondering if this is skewing the kernel's ability to evenly distribute the threads across the cores or something. I did a followup experiment using by profiling different core ranges on the system : Run 80 concurrent queries dominated by shuffle exchange Profile cores 01-20 to foo_01-20 Profile cores 41-60 to foo_41-60 Results showed that : Cores 01-20 had 50% more instructions retired Cores 01-20 show significantly more contention on pthread_cond_wait, and __lll_lock_wait Skew is dominated by DataStreamSender ScannerThread(s) also show significant skew",design_debt,non-optimal_design,"Fri, 31 Mar 2017 22:54:47 +0000","Thu, 25 May 2017 15:05:25 +0000","Thu, 25 May 2017 15:05:25 +0000",4723838,"When doing concurrency testing as part of the competitive benchmarking I noticed that it is very difficult to saturate all CPUs @100% Below is a snapshot from htop during a concurrency run, state below closely mimics the steady state, note that CPUs 41-60 are less busy compared to 1-20. Then I ran the command below which dumps the threads and processor associated with each, reference. for i in $(pgrep impalad); do ps -mo pid,tid,fname,user,psr -p $i;done From the man page for ps : The output showed that a large number of threads are running on core 61, not surprisingly the 1K threads are all thrift-server threads, so I am wondering if this is skewing the kernel's ability to evenly distribute the threads across the cores or something. I did a followup experiment using by profiling different core ranges on the system : Run 80 concurrent queries dominated by shuffle exchange Profile cores 01-20 to foo_01-20 Profile cores 41-60 to foo_41-60 Results showed that : Cores 01-20 had 50% more instructions retired Cores 01-20 show significantly more contention on pthread_cond_wait, base::internal::SpinLockDelay and __lll_lock_wait Skew is dominated by DataStreamSender ScannerThread(s) also show significant skew",0.125,0.05,neutral
impala,5612,description,"The degree of inter-node parallelism for a join is determined by its left input, so when inverting a join the planner should be mindful of how the inversion affects parallelism. For example, the left join input may have been reduced by joining with several dimension tables so much that it becomes smaller than the right hand-side (another small dimension table). By inverting such a join the degree of parallelism may be reduced to one or very few nodes, based on how many nodes the right-hand size is executed on.",design_debt,non-optimal_design,"Fri, 30 Jun 2017 23:04:29 +0000","Fri, 18 Sep 2020 23:03:43 +0000","Tue, 22 Aug 2017 20:17:36 +0000",4569187,"The degree of inter-node parallelism for a join is determined by its left input, so when inverting a join the planner should be mindful of how the inversion affects parallelism. For example, the left join input may have been reduced by joining with several dimension tables so much that it becomes smaller than the right hand-side (another small dimension table). By inverting such a join the degree of parallelism may be reduced to one or very few nodes, based on how many nodes the right-hand size is executed on.",0.2408333333,0.2408333333,neutral
impala,563,description,"Catalog will fail to initialize when there's a metastore connection issues (mostly misconfiguration). Impala logs the exception, but no one really reads the log. When a user (or admin) connects to Impala, they see an empty database and wouldn't know what wrong. To improve the experience, Impala should return an exception for all queries submitted (except invalidate metadata) if the catalog wasn't initialized. The exception should also contain the original exception message from the failed metastore connection.",design_debt,non-optimal_design,"Tue, 27 Aug 2013 18:23:13 +0000","Sun, 20 Dec 2015 00:05:07 +0000","Thu, 10 Oct 2013 19:30:13 +0000",3805620,"Catalog will fail to initialize when there's a metastore connection issues (mostly misconfiguration). Impala logs the exception, but no one really reads the log. When a user (or admin) connects to Impala, they see an empty database and wouldn't know what wrong. To improve the experience, Impala should return an exception for all queries submitted (except invalidate metadata) if the catalog wasn't initialized. The exception should also contain the original exception message from the failed metastore connection.",-0.075,-0.075,negative
impala,5963,description,"The Catalog log prints the information below which doesn't clearly show what tables are being loaded, how long is the queue and how far in the queue is a particular table.",design_debt,non-optimal_design,"Wed, 20 Sep 2017 18:27:53 +0000","Fri, 2 Nov 2018 22:36:29 +0000","Fri, 2 Nov 2018 22:36:29 +0000",35266116,"The Catalog log prints the information below which doesn't clearly show what tables are being loaded, how long is the queue and how far in the queue is a particular table.",0.0,0.0,neutral
impala,5997,description,"Currently if we shutdown an impalad, queries running on it will all fail. This is bad in a BI system integrating with Impala as its query engine. If we perform maintenance on the impala cluster, we hope users are not aware of it. For example, to decomission impalads in a rack to retire old machines, we hope these impalads not accept new PlanFragments and shutdown when their existing work is done. provides such a way since 0.128: * Presto workers can be instructed to shutdown by submiting a PUT request to /v1/info/state with the body ""SHUTTING_DOWN"". Once instructed to shutdown, the worker will no longer receive new tasks, and will exit once all existing tasks have completed. Hope we can provide a way to do so.",design_debt,non-optimal_design,"Fri, 29 Sep 2017 13:58:13 +0000","Mon, 9 Oct 2017 23:17:09 +0000","Mon, 9 Oct 2017 23:17:09 +0000",897536,"Currently if we shutdown an impalad, queries running on it will all fail. This is bad in a BI system integrating with Impala as its query engine. If we perform maintenance on the impala cluster, we hope users are not aware of it. For example, to decomission impalads in a rack to retire old machines, we hope these impalads not accept new PlanFragments and shutdown when their existing work is done. Presto provides such a way since 0.128: https://prestodb.io/docs/current/release/release-0.128.html Presto workers can be instructed to shutdown by submiting a PUT request to /v1/info/state with the body ""SHUTTING_DOWN"". Once instructed to shutdown, the worker will no longer receive new tasks, and will exit once all existing tasks have completed. Hope we can provide a way to do so.",-0.1188333333,-0.1188333333,negative
impala,6030,description,Since we introduced the we've forgotten to disable the coordinator specific thread pools on nodes that have only the executor role.,design_debt,non-optimal_design,"Mon, 9 Oct 2017 23:44:59 +0000","Thu, 12 Oct 2017 02:09:10 +0000","Thu, 12 Oct 2017 02:09:10 +0000",181451,"Since we introduced the FLAGS_is_coordinator, we've forgotten to disable the coordinator specific thread pools on nodes that have only the executor role.",-0.2,-0.2,negative
impala,6223,description,"Impala shell can throw a lexer error if it encounters a malformed ""with"" query. This happens because we use shlex to parse the input query to determine if its a DML and it can throw if the input doesn't have balanced quotes. A simple shlex repro of that is as follows, Fix: Either catch the exception and handle it gracefully or have a better way to figure out the query type, using a SQL parser (more involved). This query also repros it:",design_debt,non-optimal_design,"Mon, 20 Nov 2017 21:46:22 +0000","Tue, 26 Mar 2019 01:50:19 +0000","Mon, 9 Jul 2018 20:04:20 +0000",19952278,"Impala shell can throw a lexer error if it encounters a malformed ""with"" query. This happens because we use shlex to parse the input query to determine if its a DML and it can throw if the input doesn't have balanced quotes. A simple shlex repro of that is as follows, Fix: Either catch the exception and handle it gracefully or have a better way to figure out the query type, using a SQL parser (more involved). This query also repros it:",-0.025,-0.025,neutral
impala,6442,description,"has an error message ""File $0 has invalid file metadata at file offset $1."" However, the value reported as ""file offset"" is an offset from the _end_ of the file, not from its _beginning_. This is very misleading, since without explicitly stating that the offset is from the end, it is usually understood to be counted from the beginning. Additionally, although the function name is clearly about a ""footer"", two comments explicitly mention processing the ""header"". This falsely suggests that metadata is at the beginning of the file, when in reality it is at the end.",design_debt,non-optimal_design,"Thu, 25 Jan 2018 14:02:45 +0000","Thu, 13 Sep 2018 20:27:19 +0000","Thu, 13 Sep 2018 20:27:19 +0000",19981474,"HdfsParquetScanner::ProcessFooter has an error message ""File $0 has invalid file metadata at file offset $1."" However, the value reported as ""file offset"" is an offset from theend of the file, not from its beginning. This is very misleading, since without explicitly stating that the offset is from the end, it is usually understood to be counted from the beginning. Additionally, although the function name is clearly about a ""footer"", two comments explicitly mention processing the ""header"". This falsely suggests that metadata is at the beginning of the file, when in reality it is at the end.",-0.31,-0.21,negative
impala,661,description,"Explain plan is an effective tool for tuning query. However, it doesn't give much inside into the (cpu) cost of expression evaluation. For complex predicates, it'll greatly affect the runtime of the query. For example, complex regex is very costly to evaluate. Right now, if a complex join predicate cause the join to slow down, it's not very easy to tell. If explain plan can annotate the cost of predicate evaluation (roughly), then it can guide our user to identify and tune the predicates.",design_debt,non-optimal_design,"Tue, 12 Nov 2013 21:25:53 +0000","Mon, 16 Sep 2019 22:40:22 +0000","Mon, 16 Sep 2019 22:40:22 +0000",184382069,"Explain plan is an effective tool for tuning query. However, it doesn't give much inside into the (cpu) cost of expression evaluation. For complex predicates, it'll greatly affect the runtime of the query. For example, complex regex is very costly to evaluate. Right now, if a complex join predicate cause the join to slow down, it's not very easy to tell. If explain plan can annotate the cost of predicate evaluation (roughly), then it can guide our user to identify and tune the predicates.",0.01143333333,0.01143333333,negative
impala,6709,description,"tests/query_test/ contains several tests that use ""hdfs dfs -copyFromLocal"" to fill tables with files that cannot be created with Impala. The current way of doing this looks like ""copy paste"" and adds unnecessary complexity to the test code. The common pattern of ""create a table, copy 1-2 files to it"" could be covered by a single function, or could be moved to the .test files, which support SHELL sections. for an example of SHELL section, see",design_debt,non-optimal_design,"Tue, 20 Mar 2018 14:39:45 +0000","Mon, 27 Aug 2018 13:53:59 +0000","Mon, 27 Aug 2018 13:53:59 +0000",13821254,"tests/query_test/ contains several tests that use ""hdfs dfs -copyFromLocal"" to fill tables with files that cannot be created with Impala. The current way of doing this looks like ""copy paste"" and adds unnecessary complexity to the test code. The common pattern of ""create a table, copy 1-2 files to it"" could be covered by a single function, or could be moved to the .test files, which support SHELL sections. for an example of SHELL section, see https://github.com/apache/impala/blob/8dde41e802e3566d07e2db7b2bf5cd76030ab3d3/testdata/workloads/functional-query/queries/QueryTest/parquet-resolution-by-name.test#L75",-0.045,-0.045,negative
impala,6806,description,"Take 2 certificate files: cert.pem and truststore.pem cert.pem has 2 certificates in it: A cert for that node (with CN=""hostname"", and signed by And the intermediate CA cert (with and signed by truststore.pem has 1 certificate in it: A cert which is the root CA (with self-signed) This format of certificates don't seem to verify on the OpenSSL command line but works with Thrift. This also doesn't work with KRPC. Workaround for this issue w/ KRPC turned on: If we move the second certificate from cert.pem into truststore.pem, then this seems to work. We'll need to dig into whether this is a PEM file format issue, or a KRPC issue. But the above workaround should unblock us for now.",design_debt,non-optimal_design,"Wed, 4 Apr 2018 21:59:37 +0000","Fri, 6 Apr 2018 16:39:47 +0000","Fri, 6 Apr 2018 05:35:26 +0000",113749,"Take 2 certificate files: cert.pem and truststore.pem cert.pem has 2 certificates in it: A cert for that node (with CN=""hostname"", and signed by CN=CertToolkitIntCA) And the intermediate CA cert (with CN=CertToolkitIntCA, and signed by CN=CertToolkitRootCA) truststore.pem has 1 certificate in it: A cert which is the root CA (with CN=CertToolkitRootCA, self-signed) This format of certificates don't seem to verify on the OpenSSL command line but works with Thrift. This also doesn't work with KRPC. Workaround for this issue w/ KRPC turned on: If we move the second certificate from cert.pem (CN=CertToolkitIntCA) into truststore.pem, then this seems to work. We'll need to dig into whether this is a PEM file format issue, or a KRPC issue. But the above workaround should unblock us for now.",0.1113636364,0.1113636364,neutral
impala,6847,description,"A scenario we've seen play out a couple of times is this. 1. An Impala admin sets up memory-based admission control but sets the default query mem_limit to 0. This means that admission control will use memory estimates instead of mem_limit. Typically admins want some protection from large queries consuming excessive memory but can't or don't want to set a single mem_limit because the workload is unknown or unpredictable. This configuration has caveats and we recommend against it, but this happens and often works well enough as long as the workload is comprised of relatively simple queries. The caveats include: * There is no enforcement that a query stays within the memory estimate. This means that a query can fail or force other queries to fail. * Memory estimates are often inaccurate (this is unavoidable since they depend on cardinality estimates, which are commonly off by 10x even with state-of-the-art query planners). This means that runnable queries may be impossible to admit without setting a mem_limit. 2. Something changes about the workload, e.g. a new query is added, stats are computed, data size changes, we make an otherwise-innocuous planner change. Then we run into the second problem above and one or more queries cannot be executed, e.g. ""Rejected query from pool root.foo: request memory needed 1234.56 GB is greater than pool max mem resources 1000.00 GB. Use the MEM_LIMIT query option to indicate how much memory is required per node. The total memory needed is the per-node MEM_LIMIT times the number of nodes executing the query. See the Admission Control documentation for more information. "" The last problem is the problem this JIRA is intending to work around. The preferred solutions, which work most of the time, are: # Configure a default query memory limit for the pool # Set a mem_limit for the query # Disable memory-based admission control and do admission control based on num_queries, which is simpler and easier to understand. It would however, be useful to have a third fallback in cases where the first two options are difficult or impossible to apply. The basic requirement is to allow an administrator to configure a resource pool such that queries with very high estimates can run. We probably need enough flexibility that they can run concurrently with other smaller queries (i.e. the big query shouldn't take over the whole pool) and ideally we would also have a mem_limit applied to the query so that we're still protected from runaway memory consumption. The long-term solution to this is IMPALA-6460. This is a short-term workaround that can tide users over until we have a comprehensive solution.",design_debt,non-optimal_design,"Fri, 13 Apr 2018 16:43:12 +0000","Thu, 19 Apr 2018 23:51:17 +0000","Wed, 18 Apr 2018 16:25:25 +0000",430933,"A scenario we've seen play out a couple of times is this. 1. An Impala admin sets up memory-based admission control but sets the default query mem_limit to 0. This means that admission control will use memory estimates instead of mem_limit. Typically admins want some protection from large queries consuming excessive memory but can't or don't want to set a single mem_limit because the workload is unknown or unpredictable. This configuration has caveats and we recommend against it, but this happens and often works well enough as long as the workload is comprised of relatively simple queries. The caveats include: There is no enforcement that a query stays within the memory estimate. This means that a query can fail or force other queries to fail. Memory estimates are often inaccurate (this is unavoidable since they depend on cardinality estimates, which are commonly off by 10x even with state-of-the-art query planners). This means that runnable queries may be impossible to admit without setting a mem_limit. 2. Something changes about the workload, e.g. a new query is added, stats are computed, data size changes, we make an otherwise-innocuous planner change. Then we run into the second problem above and one or more queries cannot be executed, e.g. ""Rejected query from pool root.foo: request memory needed 1234.56 GB is greater than pool max mem resources 1000.00 GB. Use the MEM_LIMIT query option to indicate how much memory is required per node. The total memory needed is the per-node MEM_LIMIT times the number of nodes executing the query. See the Admission Control documentation for more information. "" The last problem is the problem this JIRA is intending to work around. The preferred solutions, which work most of the time, are: Configure a default query memory limit for the pool Set a mem_limit for the query Disable memory-based admission control and do admission control based on num_queries, which is simpler and easier to understand. It would however, be useful to have a third fallback in cases where the first two options are difficult or impossible to apply. The basic requirement is to allow an administrator to configure a resource pool such that queries with very high estimates can run. We probably need enough flexibility that they can run concurrently with other smaller queries (i.e. the big query shouldn't take over the whole pool) and ideally we would also have a mem_limit applied to the query so that we're still protected from runaway memory consumption. The long-term solution to this is IMPALA-6460. This is a short-term workaround that can tide users over until we have a comprehensive solution.",0.02963888889,0.02963888889,neutral
impala,6858,description,"If query profiles (or structured plans) had table metadata information like types, testing Impala on a pre-existing query profile would be easier, since we could automate generating the table scaffolding.",design_debt,non-optimal_design,"Mon, 16 Apr 2018 18:13:50 +0000","Mon, 16 Apr 2018 23:14:45 +0000","Mon, 16 Apr 2018 23:14:45 +0000",18055,"If query profiles (or structured plans) had table metadata information like types, testing Impala on a pre-existing query profile would be easier, since we could automate generating the table scaffolding.",0.0,0.0,neutral
impala,6993,description,"We should use Status::Expected() here, just like it's used above. The stack trace isn't interesting and the error is expected once we get down this path. (I'm not sure why we don't just use {{exec_status}} though, but presumably the prefix was added for a reason).",design_debt,non-optimal_design,"Tue, 8 May 2018 19:12:39 +0000","Wed, 9 May 2018 16:33:47 +0000","Wed, 9 May 2018 16:33:47 +0000",76868,"We should use Status::Expected() here, just like it's used above. The stack trace isn't interesting and the error is expected once we get down this path. (I'm not sure why we don't just use exec_status though, but presumably the prefix was added for a reason).",0.03244444444,0.03244444444,negative
impala,7161,description,"installs the Java SDK and sets JAVA_HOME in the current shell. It also adds a command to the to export JAVA_HOME there. This doesn't do the job. tests for JAVA_HOME at the very start of the script, before it has sourced So, the user doesn't have a way of developing over the long term without manually setting up JAVA_HOME. also doesn't detect the system JAVA_HOME. For Ubuntu 16.04, this is fairly simple and if a developer has their system JDK set up appropriately, it would make sense to use it. For example:",design_debt,non-optimal_design,"Mon, 11 Jun 2018 23:50:00 +0000","Fri, 22 Feb 2019 03:42:37 +0000","Fri, 22 Jun 2018 18:46:27 +0000",932187,"bin/bootstrap_system.sh installs the Java SDK and sets JAVA_HOME in the current shell. It also adds a command to the bin/impala-config-local.sh to export JAVA_HOME there. This doesn't do the job. bin/impala-config.sh tests for JAVA_HOME at the very start of the script, before it has sourced bin/impala-config-local.sh. So, the user doesn't have a way of developing over the long term without manually setting up JAVA_HOME. bin/impala-config.sh also doesn't detect the system JAVA_HOME. For Ubuntu 16.04, this is fairly simple and if a developer has their system JDK set up appropriately, it would make sense to use it. For example:",0.05714285714,0.03076923077,neutral
impala,7205,description,"Currently we respond with CANCELLED only when hitting EOS. It seems a bit more robust to always respond with CANCELLED whenever query execution has terminated. That way, if the cancel RPC from the coordinator to a backend fails, the backend will still cancel if it can send status back to the coordinator later on. Without this fix, the query can hang and/or finstances can continue running (until the query is closed, at which point the response to this RPC will be an error).",design_debt,non-optimal_design,"Mon, 25 Jun 2018 16:30:08 +0000","Sat, 13 Apr 2019 02:53:10 +0000","Fri, 29 Jun 2018 00:25:22 +0000",287714,"Currently we respond with CANCELLED only when hitting EOS. It seems a bit more robust to always respond with CANCELLED whenever query execution has terminated. That way, if the cancel RPC from the coordinator to a backend fails, the backend will still cancel if it can send status back to the coordinator later on. Without this fix, the query can hang and/or finstances can continue running (until the query is closed, at which point the response to this RPC will be an error).",-0.0324375,-0.0324375,neutral
impala,7234,description,"The getMajorityFormat method of the FeCatalogUtils currently returns non-deterministic results when its argument is a list of partitions where there is no numerical majority in terms of the number of instances. The result is determined by the order in which the partitions are added to the HashMap. We need more deterministic results which also considers the memory requirement among different types of partitions. Ideally, this function should return the format with higher memory requirements in case of a tie.",design_debt,non-optimal_design,"Mon, 2 Jul 2018 20:22:07 +0000","Wed, 1 Aug 2018 17:30:06 +0000","Wed, 1 Aug 2018 17:30:06 +0000",2581679,"ThegetMajorityFormat method of the FeCatalogUtils currently returns non-deterministic results when its argument is a list of partitions where there is no numerical majority in terms of the number of instances. The result is determined by the order in which the partitions are added to the HashMap. We need more deterministic results which also considers the memory requirement among different types of partitions. Ideally, this function should return the format with higher memory requirements in case of a tie.",0.1,0.1,neutral
impala,7349,description,"We should add admission control support for intelligently choosing how much memory to give a query, based on the memory estimate. Currently we require that you set a single mem_limit per pool. Instead it would be better if we allowed configuring min/max guardrails within which admission control chose an amount of memory that will allow good performance. Initially, I think mem_limit will be the same for all backends. Eventually it could make sense to have different limits per backend depending on the fragments.",design_debt,non-optimal_design,"Wed, 25 Jul 2018 22:37:32 +0000","Mon, 29 Jul 2019 10:47:15 +0000","Mon, 8 Oct 2018 20:28:24 +0000",6472252,"We should add admission control support for intelligently choosing how much memory to give a query, based on the memory estimate. Currently we require that you set a single mem_limit per pool. Instead it would be better if we allowed configuring min/max guardrails within which admission control chose an amount of memory that will allow good performance. Initially, I think mem_limit will be the same for all backends. Eventually it could make sense to have different limits per backend depending on the fragments.",0.1384,0.1384,neutral
impala,7350,description,"For IMPALA-7349, we will be relying more on memory estimates. This is an umbrella JIRA to track improvements to memory estimates where the current estimates are way off and result in over- or under- admission. over-admission is probably the more significant concern.",design_debt,non-optimal_design,"Wed, 25 Jul 2018 22:43:34 +0000","Fri, 4 Dec 2020 16:20:56 +0000","Mon, 23 Sep 2019 21:29:06 +0000",36715532,"For IMPALA-7349, we will be relying more on memory estimates. This is an umbrella JIRA to track improvements to memory estimates where the current estimates are way off and result in over- or under- admission. over-admission is probably the more significant concern.",0.2333333333,0.2333333333,neutral
impala,7388,description,"The THROW_IF_ERROR macros all use a locally scoped variable ""status"". If they're called with a pattern like: then the status variable inside the macro ends up being assigned to itself instead of the outer-scope variable. This makes it not throw or return, which is quite surprising.",design_debt,non-optimal_design,"Thu, 2 Aug 2018 08:16:43 +0000","Tue, 14 Aug 2018 04:39:08 +0000","Tue, 14 Aug 2018 04:15:03 +0000",1022300,"The THROW_IF_ERROR macros all use a locally scoped variable ""status"". If they're called with a pattern like:  then the status variable inside the macro ends up being assigned to itself instead of the outer-scope variable. This makes it not throw or return, which is quite surprising.",0.03333333333,0.03333333333,neutral
impala,7400,description,"""Impala has no DELETE statement."" and ""Impala has no UPDATE statement. "" are not totally true - Impala has those statements but only for Kudu tables. ""For example, Impala does not support natural joins or anti-joins,"" - Impala does support Anti-joins via NOT IN/NOT EXISTS or even explicitly like: ""Within queries, Impala requires query aliases for any subqueries:"" - this is only true for subqueries used as inline views in the FROM clause. E.g. the following works: "" Impala .. requires the CROSS JOIN operator for Cartesian products."" - untrue, this works: ""Have you run the COMPUTE STATS statement on each table involved in join queries"". This isn't specific to queries with joins, although may have more impact. We recommend that users run COMPUTE STATS on all tables. ""A CREATE TABLE statement with no PARTITIONED BY clause stores all the data files in the same physical location,"" - unpartitioned tables with multiple files can have files residing in different locations (and there are already 3 replicas per file by default, so the statement is a little misleading even if there's a single file). I think the latest statement about ""Have you partitioned at the right granularity so that there is enough data in each partition to parallelize the work for each query?"" is also misleading for the same reason. ""The INSERT ... VALUES syntax is suitable for setting up toy tables with a few rows for functional testing, but because each such statement creates a separate tiny file in HDFS"". This advice only applies to HDFS, this should work fine for Kudu tables although the INSERT statements are not particularly fast. ""The number of expressions allowed in an Impala query might be smaller than for some other database systems, causing failures for very complicated queries"" - this doesn't seem right - I don't know why the queries would fail. Also the codegen time isn't really specific to expressions or where clauses. There seems to be a point buried in there, but maybe it's just essentially that ""Complex queries may have high codegen time""",design_debt,non-optimal_design,"Mon, 6 Aug 2018 16:34:24 +0000","Wed, 8 Aug 2018 21:07:52 +0000","Wed, 8 Aug 2018 20:43:20 +0000",187736,"""Impala has no DELETE statement."" and ""Impala has no UPDATE statement. "" are not totally true - Impala has those statements but only for Kudu tables. ""For example, Impala does not support natural joins or anti-joins,"" - Impala does support Anti-joins via NOT IN/NOT EXISTS or even explicitly like: ""Within queries, Impala requires query aliases for any subqueries:"" - this is only true for subqueries used as inline views in the FROM clause. E.g. the following works: "" Impala .. requires the CROSS JOIN operator for Cartesian products."" - untrue, this works: ""Have you run the COMPUTE STATS statement on each table involved in join queries"". This isn't specific to queries with joins, although may have more impact. We recommend that users run COMPUTE STATS on all tables. ""A CREATE TABLE statement with no PARTITIONED BY clause stores all the data files in the same physical location,"" - unpartitioned tables with multiple files can have files residing in different locations (and there are already 3 replicas per file by default, so the statement is a little misleading even if there's a single file). I think the latest statement about ""Have you partitioned at the right granularity so that there is enough data in each partition to parallelize the work for each query?"" is also misleading for the same reason. ""The INSERT ... VALUES syntax is suitable for setting up toy tables with a few rows for functional testing, but because each such statement creates a separate tiny file in HDFS"". This advice only applies to HDFS, this should work fine for Kudu tables although the INSERT statements are not particularly fast. ""The number of expressions allowed in an Impala query might be smaller than for some other database systems, causing failures for very complicated queries"" - this doesn't seem right - I don't know why the queries would fail. Also the codegen time isn't really specific to expressions or where clauses. There seems to be a point buried in there, but maybe it's just essentially that ""Complex queries may have high codegen time""",0.008328125,0.008328125,negative
impala,7406,description,"Currently the file descriptors stored in the catalogd memory for each partition use a FlatBuffer to reduce the number of separate objects on the Java heap. However, the FlatBuffer objects internally each store a ByteBuffer and int position, so each object takes 32 bytes on its own. The ByteBuffer takes 56 bytes since it stores various references, endianness, limit, mark, position, etc. This amounts to about 88 bytes overhead on top of the actual underlying flatbuf byte array which is typically around 100 bytes for a single-block file. So, we're have about a 1:1 ratio of memory overhead and a 2:1 ratio of object count overhead for each partition. If we simply stored the byte[] array and constructed wrappers on demand, we'd save 88 bytes and 2 objects per partition. The downside is that we'd need to do short-lived ByteBuffer allocations at access time, and based on some benchmarking I did, they don't get escape-analyzed out. So, it's not a super clear win, but still worth considering.",design_debt,non-optimal_design,"Tue, 7 Aug 2018 21:14:26 +0000","Fri, 17 Aug 2018 23:19:40 +0000","Fri, 17 Aug 2018 18:42:29 +0000",854883,"Currently the file descriptors stored in the catalogd memory for each partition use a FlatBuffer to reduce the number of separate objects on the Java heap. However, the FlatBuffer objects internally each store a ByteBuffer and int position, so each object takes 32 bytes on its own. The ByteBuffer takes 56 bytes since it stores various references, endianness, limit, mark, position, etc. This amounts to about 88 bytes overhead on top of the actual underlying flatbuf byte array which is typically around 100 bytes for a single-block file. So, we're have about a 1:1 ratio of memory overhead and a 2:1 ratio of object count overhead for each partition. If we simply stored the byte[] array and constructed wrappers on demand, we'd save 88 bytes and 2 objects per partition. The downside is that we'd need to do short-lived ByteBuffer allocations at access time, and based on some benchmarking I did, they don't get escape-analyzed out. So, it's not a super clear win, but still worth considering.",-0.14865625,-0.14865625,neutral
impala,74,description,"Impala hdfs-fs-cache values for namenode address and namenode port should be read from Impala's core-site.xml file rather than input as command line args. If these args are not specified when impalad starts, then it defaults to localhost:20500 which can be confusing. Definition: 20500, ""namenode port""); ""localhost"", ""namenode host"");",design_debt,non-optimal_design,"Wed, 20 Feb 2013 00:16:40 +0000","Sun, 20 Dec 2015 00:04:57 +0000","Wed, 20 Feb 2013 02:18:11 +0000",7291,"Impala hdfs-fs-cache values for namenode address and namenode port should be read from Impala's core-site.xml file rather than input as command line args. If these args are not specified when impalad starts, then it defaults to localhost:20500 which can be confusing. Definition: ./be/src/runtime/hdfs-fs-cache.cc:DEFINE_int32(nn_port, 20500, ""namenode port""); ./be/src/runtime/hdfs-fs-cache.cc:DEFINE_string(nn, ""localhost"", ""namenode host"");",-0.10925,0.095375,neutral
impala,7640,description,"Currently, when I execute ALTER TABLE RENAME on a managed Kudu table it will not rename the underlying Kudu table. Because of IMPALA-5654 it becomes nearly impossible to rename the underlying Kudu table, which is confusing and makes the Kudu tables harder to identify and manage.",design_debt,non-optimal_design,"Thu, 27 Sep 2018 20:56:51 +0000","Sun, 7 Apr 2019 20:21:06 +0000","Sun, 7 Apr 2019 20:21:06 +0000",16586655,"Currently, when I execute ALTER TABLE RENAME on a managed Kudu table it will not rename the underlying Kudu table. Because ofIMPALA-5654 it becomes nearly impossible to rename the underlying Kudu table, which is confusing and makes the Kudu tables harder toidentify and manage.",-0.2185,-0.2185,negative
impala,7902,description,"The {{NumericLiteral}} class is a leaf node in the Impala FE AST. In order to address a number of pending issues, we must adjust this class to improve its semantics and fix a number of bugs. See the linked JIRA tickets for the issues that this roll-up ticket addresses. The issues are combined because they are tightly related; makes more sense to offer them as a single patch than as a series of disjoint patches.",design_debt,non-optimal_design,"Tue, 27 Nov 2018 23:48:34 +0000","Thu, 14 Mar 2019 14:41:33 +0000","Fri, 1 Mar 2019 17:48:02 +0000",8099968,"The NumericLiteral class is a leaf node in the Impala FE AST. In order to address a number of pending issues, we must adjust this class to improve its semantics and fix a number of bugs. See the linked JIRA tickets for the issues that this roll-up ticket addresses. The issues are combined because they are tightly related; makes more sense to offer them as a single patch than as a series of disjoint patches.",0.1166666667,0.1166666667,neutral
impala,8005,description,"Currently, we use the same hash seed for partitioning exchanges at the sender. For a table with skew in distribution in the shuffling keys, multiple queries using the same shuffling keys for exchanges will end up hashing to the same destination fragments running on particular host and potentially overloading that host. We should consider using the query id or other query specific information to seed the hashing function to randomize the destinations for different queries. Thanks to  for pointing this problem out.",design_debt,non-optimal_design,"Wed, 19 Dec 2018 19:46:27 +0000","Thu, 28 May 2020 16:17:52 +0000","Thu, 28 May 2020 16:17:52 +0000",45433885,"Currently, we use the same hash seed for partitioning exchanges at the sender. For a table with skew in distribution in the shuffling keys, multiple queries using the same shuffling keys for exchanges will end up hashing to the same destination fragments running on particular host and potentially overloading that host. We should consider using the query id or other query specific information to seed the hashing function to randomize the destinations for different queries. Thanks to tlipcon for pointing this problem out.",0.0,0.0,neutral
impala,8176,description,"With the initial commit for the unified backend test executable commited (IMPALA-8071), it should be straight-forward to convert another set of tests to use the unified executable. Any test with a simple main() function (such as IMPALA_TEST_MAIN()) should require only modest changes to convert. Command to find such tests: git grep IMPALA_TEST_MAIN It looks like there are dozens of such tests:",design_debt,non-optimal_design,"Fri, 8 Feb 2019 19:12:36 +0000","Fri, 19 Jul 2019 22:51:10 +0000","Fri, 19 Jul 2019 22:51:10 +0000",13923514,"With the initial commit for the unified backend test executable commited (IMPALA-8071), it should be straight-forward to convert another set of tests to use the unified executable. Any test with a simple main() function (such as IMPALA_TEST_MAIN()) should require only modest changes to convert. Command to find such tests: git grep IMPALA_TEST_MAIN It looks like there are dozens of such tests:",0.1138888889,0.1138888889,neutral
impala,8181,description,"A recent change added plan node cardinality to the DESCRIBE output using a ""metric"" format: 123.46M instead of 123456789. This makes the output easier to read. Also, since all numbers are estimate, having seven or eight digits of precision is not very helpful. This ticket requests that the same formatting be used for the other places where the DESCRIBE output shows row counts: table stats, extrapolated row count, partition row counts, and so on.",design_debt,non-optimal_design,"Mon, 11 Feb 2019 17:14:05 +0000","Thu, 14 Mar 2019 23:58:34 +0000","Thu, 14 Mar 2019 23:58:34 +0000",2702669,"A recent change added plan node cardinality to the DESCRIBE output using a ""metric"" format: 123.46M instead of 123456789. This makes the output easier to read. Also, since all numbers are estimate, having seven or eight digits of precision is not very helpful. This ticket requests that the same formatting be used for the other places where the DESCRIBE output shows row counts: table stats, extrapolated row count, partition row counts, and so on.",0.0,0.0,neutral
impala,8187,description,"It's possible for UDF authors to get into a pickle by exporting symbols from UDF/UDA shared objects unintentionally. E.g. if using boost headers, which result in weak symbols in the .so, which then get merged with the (incompatible) symbols in the impalad binary. We should really be keeping symbols hidden by default then exporting the entry points explicitly in the UDF/UDA code",design_debt,non-optimal_design,"Mon, 11 Feb 2019 22:53:06 +0000","Thu, 14 Feb 2019 23:37:41 +0000","Thu, 14 Feb 2019 23:37:41 +0000",261875,"It's possible for UDF authors to get into a pickle by exporting symbols from UDF/UDA shared objects unintentionally. E.g. if using boost headers, which result in weak symbols in the .so, which then get merged with the (incompatible) symbols in the impalad binary. We should really be keeping symbols hidden by default then exporting the entry points explicitly in the UDF/UDA code",-0.1,-0.1,negative
impala,8632,description,"In case of {{INSERT_EVENTS}} if Impala inserts into a table it causes a refresh to the underlying table/partition. This could be unnecessary when there is only one Impala cluster in the system. The existing self-event detection framework cannot identify such events because they are not sending HMS objects like tables and partitions to the HMS. Instead in case of {{INSERT_EVENT}} HMS API only asks for a table name or partition value to fire a insert event on it. We can detect a self-event in such cases if the HMS API to fire a listener event is improved to return the event id. This would be used by EventProcessor to ignore the event when it is fetched later in the next polling cycle. In order to support this, we will need to make a change to Hive as well so that the enhanced API can be used.",design_debt,non-optimal_design,"Thu, 6 Jun 2019 23:59:00 +0000","Thu, 9 Apr 2020 17:50:20 +0000","Thu, 9 Apr 2020 17:50:20 +0000",26589080,"In case of INSERT_EVENTS if Impala inserts into a table it causes a refresh to the underlying table/partition. This could be unnecessary when there is only one Impala cluster in the system. The existing self-event detection framework cannot identify such events because they are not sending HMS objects like tables and partitions to the HMS. Instead in case of INSERT_EVENT HMS API only asks for a table name or partition value to fire a insert event on it. We can detect a self-event in such cases if the HMS API to fire a listener event is improved to return the event id. This would be used by EventProcessor to ignore the event when it is fetched later in the next polling cycle. In order to support this, we will need to make a change to Hive as well so that the enhanced API can be used.",-0.01280952381,-0.01280952381,neutral
impala,8656,description,"Impala's current interaction with clients is pulled-based: it relies on clients to fetch results to trigger the generation of more result row batches until all the result rows have been produced. If a client issues a query without fetching all the results, the query fragments will continue to consume the resources until the query hits is cancelled and unregistered for whatever reasons. This is undesirable as resources are held up by misbehaving clients and other queries may wait for extended period of time in admission control due to this. The high level idea for this JIRA is for Impala to have a mode in which result sets of queries are eagerly fetched and spooled somewhere (preferably some persistent storage). In this way, the cluster's resources are freed up once all result rows have been fetched and stored in the spooling location. Incoming client fetches can be returned from this spooled locations. cc'ing , , ,",design_debt,non-optimal_design,"Wed, 12 Jun 2019 02:22:07 +0000","Wed, 3 Mar 2021 00:38:51 +0000","Tue, 8 Oct 2019 14:57:48 +0000",10240541,"Impala's current interaction with clients is pulled-based: it relies on clients to fetch results to trigger the generation of more result row batches until all the result rows have been produced. If a client issues a query without fetching all the results, the query fragments will continue to consume the resources until the query hits is cancelled and unregistered for whatever reasons. This is undesirable as resources are held up by misbehaving clients and other queries may wait for extended period of time in admission control due to this. The high level idea for this JIRA is for Impala to have a mode in which result sets of queries are eagerly fetched and spooled somewhere (preferably some persistent storage). In this way, the cluster's resources are freed up once all result rows have been fetched and stored in the spooling location. Incoming client fetches can be returned from this spooled locations. cc'ing stakiar, twm378, joemcdonnell, lv",-0.2,-0.2,neutral
impala,8892,description,Our docker images lack a bunch of tools that help to implement health checks and debugging issues.,design_debt,non-optimal_design,"Mon, 26 Aug 2019 23:25:38 +0000","Thu, 29 Aug 2019 19:56:42 +0000","Thu, 29 Aug 2019 19:55:10 +0000",246572,Our docker images lack a bunch of tools that help to implement health checks and debugging issues.,0.0,0.0,negative
impala,8912,description,"For simple queries on HBase tables that has HBaseScanNode as the root of the SingleNodePlan, will be called twice. Stacktrace for the first call: Stacktrace for the second call: Codes of the second call: Logs for a simple query on an old version of Impala: Such kind of queries are usually point queries and are always expected to return fast. is heavy since it requires RPCs to HBase. We should avoid calling it twice.",design_debt,non-optimal_design,"Fri, 30 Aug 2019 12:39:19 +0000","Thu, 5 Sep 2019 00:59:26 +0000","Thu, 5 Sep 2019 00:59:26 +0000",476407,"For simple queries on HBase tables that has HBaseScanNode as the root of the SingleNodePlan, HBaseScanNode#computeStats will be called twice. Stacktrace for the first call: Stacktrace for the second call: Codes of the second call: Logs for a simple query on an old version of Impala: Such kind of queries are usually point queries and are always expected to return fast. HBaseScanNode#computeStats is heavy since it requires RPCs to HBase. We should avoid calling it twice.",-0.05,-0.05,neutral
impala,9146,description,"Since broadcast based hash joins are often chosen, we sometimes see very large tables being broadcast, with sizes that are larger than the destination executor's total memory. This could potentially happen if the cluster membership is not accurately known and the planner's cost computation of the broadcastCost vs partitionCost happens to favor the broadcast distribution. This causes spilling and severely affects performance. Although the DistributedPlanner does a mem_limit check before picking broadcast, the mem_limit is not an accurate reflection since it is assigned during admission control (See Given this scenario, as a safety check it is better to have to an explicit configurable limit for the size of the broadcast input and set it to a reasonable default. The 'reasonable' default can be chosen based on analysis of existing benchmark queries and representative workloads where Impala is currently used.",design_debt,non-optimal_design,"Tue, 12 Nov 2019 00:33:43 +0000","Tue, 10 Dec 2019 01:35:11 +0000","Tue, 10 Dec 2019 01:35:11 +0000",2422888,"Since broadcast based hash joins are often chosen, we sometimes see very large tables being broadcast, with sizes that are larger than the destination executor's total memory. This could potentially happen if the cluster membership is not accurately known and the planner's cost computation of the broadcastCost vs partitionCost happens to favor the broadcast distribution. This causes spilling and severely affects performance. Although the DistributedPlanner does a mem_limit check before picking broadcast, the mem_limit is not an accurate reflection since it is assigned during admission control (See IMPALA-988). Given this scenario, as a safety check it is better to have to an explicit configurable limit for the size of the broadcast input and set it to a reasonable default. The 'reasonable' default can be chosen based on analysis of existing benchmark queries and representative workloads where Impala is currently used.",-0.1444,-0.2176,neutral
impala,9265,description,"If toolchain Kudu provided Java artifacts, then toolchain Kudu could be used standalone without needing a separate Maven repository for Kudu artifacts. This is nice, because it would allow us to use toolchain Kudu for both USE_CDP_HIVE=true and USE_CDP_HIVE=false without any Java artifact version conflicts. Having only a single version of Kudu to build against would simplify Kudu/Impala integration projects. To do this, the native toolchain (which may be a misnomer) would need to push Kudu artifacts to a repository. One option is for it to create a local filesystem repository and then include it in the kudu tarball produced.",design_debt,non-optimal_design,"Thu, 26 Dec 2019 21:14:10 +0000","Mon, 10 Feb 2020 21:07:29 +0000","Tue, 28 Jan 2020 16:20:58 +0000",2833608,"If toolchain Kudu provided Java artifacts, then toolchain Kudu could be used standalone without needing a separate Maven repository for Kudu artifacts. This is nice, because it would allow us to use toolchain Kudu for both USE_CDP_HIVE=true and USE_CDP_HIVE=false without any Java artifact version conflicts. Having only a single version of Kudu to build against would simplify Kudu/Impala integration projects. To do this, the native toolchain (which may be a misnomer) would need to push Kudu artifacts to a repository. One option is for it to create a local filesystem repository and then include it in the kudu tarball produced.",0.07192,0.07192,positive
impala,9530,description,"In some cases pre-aggregations can balloon up and consume lots of memory, forcing the merge aggregation to spill. This is often OK as long as the preaggregation is reducing the input sufficiently, since it reduces the amount of data shuffled over the network. However in some cases it's preferable to be more conservative with memory and just cap the size of the preaggregation to prevent it ballooning too much. It would be useful to add a query option to directly limit the memory consumption of the preaggregations.",design_debt,non-optimal_design,"Tue, 17 Mar 2020 15:42:06 +0000","Wed, 20 May 2020 16:46:41 +0000","Fri, 20 Mar 2020 17:37:31 +0000",266125,"In some cases pre-aggregations can balloon up and consume lots of memory, forcing the merge aggregation to spill. This is often OK as long as the preaggregation is reducing the input sufficiently, since it reduces the amount of data shuffled over the network. However in some cases it's preferable to be more conservative with memory and just cap the size of the preaggregation to prevent it ballooning too much. It would be useful to add a query option to directly limit the memory consumption of the preaggregations.",0.3895833333,0.3895833333,neutral
impala,9560,description,"When working on the Impala 3.4 release, we changed the version on branch-3.4.0 from 3.4.0-SNAPSHOT to 3.4.0-RELEASE. now fails with the following error: The output is expecting a cardinality of 17.91K, but instead the cardinality is 17.90K. The RELEASE version has one character fewer than the SNAPSHOT version. The version gets embedded in parquet files, so the parquet file is slightly smaller than before. The test is estimating cardinality by looking at the size of the parquet file. Apparently, this is right on the edge. This test should tolerate this difference.",design_debt,non-optimal_design,"Fri, 27 Mar 2020 17:19:32 +0000","Sat, 28 Mar 2020 21:37:45 +0000","Fri, 27 Mar 2020 23:31:42 +0000",22330,"When working on the Impala 3.4 release, we changed the version on branch-3.4.0 from 3.4.0-SNAPSHOT to 3.4.0-RELEASE. metadata/test_stats_extrapolation.py::TestStatsExtrapolation::test_stats_extrapolation() now fails with the following error: The output is expecting a cardinality of 17.91K, but instead the cardinality is 17.90K. The RELEASE version has one character fewer than the SNAPSHOT version. The version gets embedded in parquet files, so the parquet file is slightly smaller than before. The test is estimating cardinality by looking at the size of the parquet file. Apparently, this is right on the edge. This test should tolerate this difference.",0.090875,0.08077777778,neutral
impala,979,description,If there is an error starting the statestore subscriber BE (heartbeat server) an Impala service should abort. This helps make it easier to diagnose issues such as port conflicts. This currently doesn't happen because we ignore the Status returned by the heartbeat_server_- From,design_debt,non-optimal_design,"Sun, 4 May 2014 22:05:08 +0000","Sun, 20 Dec 2015 00:05:15 +0000","Wed, 7 May 2014 21:51:35 +0000",258387,If there is an error starting the statestore subscriber BE (heartbeat server) an Impala service should abort. This helps make it easier to diagnose issues such as port conflicts. This currently doesn't happen because we ignore the Status returned by the heartbeat_server_->Start() call: From state-store-subscriber.cc:,-0.2,-0.15,neutral
impala,3403,description,"Currently, most of the verbiage around installation concerns the Cloudera Manager paths for packages or parcels. Probably there will be some new details in the context of an ASF release. Perhaps some of this info can be delegated to build instructions, README types of files, ancillary to the main docs containing the SQL Ref and such. That might be easier for maintenance where there are fast-changing version numbers, package names, etc. (Just an idea.)",documentation_debt,low_quality_documentation,"Fri, 22 Apr 2016 20:11:54 +0000","Fri, 16 Jun 2017 19:01:56 +0000","Tue, 4 Apr 2017 20:49:54 +0000",29983080,"Currently, most of the verbiage around installation concerns the Cloudera Manager paths for packages or parcels. Probably there will be some new details in the context of an ASF release. Perhaps some of this info can be delegated to build instructions, README types of files, ancillary to the main docs containing the SQL Ref and such. That might be easier for maintenance where there are fast-changing version numbers, package names, etc. (Just an idea.)",0.0625,0.0625,neutral
impala,4009,description,"We cannot distribute Oracle binaries with Impala, but it will be useful to explain to someone how to get Oracle set up (server, client) so that he may use it as a reference db for the query generator.",documentation_debt,low_quality_documentation,"Tue, 23 Aug 2016 17:45:31 +0000","Thu, 25 Aug 2016 15:40:08 +0000","Thu, 25 Aug 2016 15:40:08 +0000",165277,"We cannot distribute Oracle binaries with Impala, but it will be useful to explain to someone how to get Oracle set up (server, client) so that he may use it as a reference db for the query generator.",0.5,0.5,neutral
impala,4245,description,"The doc page for Impala UDFs contains multiple links to the sample UDF repo published by Cloudera. However, these links are inconsistent, they point to multiple locations. In the order of occurrence: 1. Just above section (below the code block) there is a link to This is the master head; subject to change. 2. The last paragraph of section points to the UDF sample repo at this is probably the intended location. 3. Typo: The second half of section contains links to the files uda-sample.h and uda-sample.cc; here the links are reversed compared to the file names (possibly a copy-paste mixup). The links point to the UDF sample repo, which I assume is the right location. 4. The last line of the same section contains a link to Cloudera's internal github. This probably wanted to be 5. Section again points to teh sample UDF repo which I assume is the correct location.",documentation_debt,low_quality_documentation,"Tue, 4 Oct 2016 10:10:27 +0000","Fri, 25 May 2018 20:55:56 +0000","Fri, 25 May 2018 20:55:56 +0000",51705929,"The doc page for Impala UDFs (http://www.cloudera.com/documentation/enterprise/latest/topics/impala_udf.html#udf_building) contains multiple links to the sample UDF repo published by Cloudera. However, these links are inconsistent, they point to multiple locations. In the order of occurrence: 1. Just above section http://www.cloudera.com/documentation/enterprise/latest/topics/impala_udf.html#udf_runtime (below the code block) there is a link to https://github.com/cloudera/impala/tree/master/be/src/udf_samples This is the master head; subject to change. 2. The last paragraph of section http://www.cloudera.com/documentation/enterprise/latest/topics/impala_udf.html#udf_demo_env points to the UDF sample repo at https://github.com/cloudera/impala-udf-samples; this is probably the intended location. 3. Typo: The second half of section http://www.cloudera.com/documentation/enterprise/latest/topics/impala_udf.html#udafs contains links to the files uda-sample.h and uda-sample.cc; here the links are reversed compared to the file names (possibly a copy-paste mixup). The links point to the UDF sample repo, which I assume is the right location. 4. The last line of the same section contains a link to Cloudera's internal github. This probably wanted to be https://github.com/cloudera/Impala/blob/cdh5-trunk/be/src/testutil/test-udas.cc 5. Section http://www.cloudera.com/documentation/enterprise/latest/topics/impala_udf.html#udf_tutorial again points to teh sample UDF repo (https://github.com/cloudera/impala-udf-samples), which I assume is the correct location.",0.1034666667,0.1034666667,neutral
impala,4778,description,The issue in IMPALA-1972 doesn't appear to be actively worked on and should be noted in the documentation unless and until it is fixed.,documentation_debt,low_quality_documentation,"Wed, 18 Jan 2017 05:51:21 +0000","Wed, 7 Jun 2017 14:41:30 +0000","Tue, 23 May 2017 22:23:47 +0000",10859546,The issue in IMPALA-1972 doesn't appear to be actively worked on and should be noted in the documentation unless and until it is fixed.,-0.875,-0.875,negative
impala,4956,description,"[This refers to the old mnemonics of EXPLAIN_LEVEL, when {{0}} meant {{normal}} and {{1}} meant {{verbose}}. It should be updated to [the new  - I'm assigning this to you thinking you may know best who can take this on. Please let me know if you want me to find someone else.",documentation_debt,outdated_documentation,"Mon, 20 Feb 2017 21:33:55 +0000","Fri, 23 Jun 2017 23:06:23 +0000","Fri, 23 Jun 2017 23:06:23 +0000",10632748,"This page refers to the old mnemonics of EXPLAIN_LEVEL, when 0 meant normal and 1 meant verbose. It should be updated to the new mnemonics. jrussell - I'm assigning this to you thinking you may know best who can take this on. Please let me know if you want me to find someone else.",0.2958333333,0.221875,neutral
impala,5636,description,"The Parquet writer always adds the BIT_PACKED and RLE encodings even though (I'm pretty sure) we never write out the BIT_PACKED encoding for rep or def levels. The BIT_PACKED encoding is deprecated according to the Parquet specification: and it is not clear that Impala can even read it correctly: IMPALA-3006 One way of seeing that Impala claims to need the BIT_PACKED encoding is to write a parquet file with Impala then inspect it with parquet-tool. BIT_PACKED is never written by Impala's parquet writer - the definition levels are always written using an RleEncoder and reported as Encoding::RLE. Weirdly, the repetition levels are reported as ""BIT_PACKED"", but this encoding has no effect since we don't actually write out repetition levels.",documentation_debt,outdated_documentation,"Sun, 9 Jul 2017 21:08:28 +0000","Thu, 17 Aug 2017 00:23:43 +0000","Mon, 31 Jul 2017 17:37:57 +0000",1888169,"The Parquet writer always adds the BIT_PACKED and RLE encodings even though (I'm pretty sure) we never write out the BIT_PACKED encoding for rep or def levels. The BIT_PACKED encoding is deprecated according to the Parquet specification: https://github.com/Parquet/parquet-format/blob/master/Encodings.md and it is not clear that Impala can even read it correctly: IMPALA-3006 One way of seeing that Impala claims to need the BIT_PACKED encoding is to write a parquet file with Impala then inspect it with parquet-tool. BIT_PACKED is never written by Impala's parquet writer - the definition levels are always written using an RleEncoder and reported as Encoding::RLE. Weirdly, the repetition levels are reported as ""BIT_PACKED"", but this encoding has no effect since we don't actually write out repetition levels.",0.05,0.05,negative
impala,6408,description,"The change in IMPALA-3930 states that if only one partition is written (because all partitioning columns are constant or the target table is not partitioned), then the ""shuffle"" hint leads to a plan where all rows are merged at the coordinator where the table sink is executed. The documentation of the ""shuffle"" hint does not mention this behavior.",documentation_debt,low_quality_documentation,"Wed, 17 Jan 2018 15:48:26 +0000","Wed, 13 Jun 2018 15:48:46 +0000","Tue, 12 Jun 2018 15:39:57 +0000",12613891,"The change in IMPALA-3930 states that if only one partition is written (because all partitioning columns are constant or the target table is not partitioned), then the ""shuffle""hintleads to a plan where all rows are merged at the coordinator where the table sink is executed. The documentation of the ""shuffle"" hint does not mention this behavior.",-0.3125,-0.3125,neutral
impala,7171,description,"On the page: at the end of the section: ""Impala DML Support for Kudu Tables (INSERT, UPDATE, DELETE, UPSERT)"", we should add text like: Starting from Impala 2.9, Impala will automatically add a partition and sort step to INSERTs before sending the rows to Kudu. Since Kudu partitions and sorts rows on write, pre-partitioning and sorting takes some of the load off of Kudu, and helps ensure that large INSERTs complete without timing out, but it may slow down the end-to-end performance of the INSERT. Starting from Impala 2.10, the hints ""/* */"" may be used to turn this pre-partitioning and sorting off. Additionally, since sorting may consume a lot of memory, users should consider setting a ""mem_limit"" for these queries.",documentation_debt,outdated_documentation,"Thu, 14 Jun 2018 18:06:25 +0000","Thu, 21 Feb 2019 05:00:44 +0000","Mon, 18 Jun 2018 23:55:57 +0000",366572,"On the page: http://impala.apache.org/docs/build3x/html/topics/impala_kudu.html, at the end of the section: ""Impala DML Support for Kudu Tables (INSERT, UPDATE, DELETE, UPSERT)"", we should add text like: Starting from Impala 2.9, Impala will automatically add a partition and sort step to INSERTs before sending the rows to Kudu. Since Kudu partitions and sorts rows on write, pre-partitioning and sorting takes some of the load off of Kudu, and helps ensure that large INSERTs complete without timing out, but it may slow down the end-to-end performance of the INSERT. Starting from Impala 2.10, the hints ""/* +noshuffle,noclustered */"" may be used to turn this pre-partitioning and sorting off. Additionally, since sorting may consume a lot of memory, users should consider setting a ""mem_limit"" for these queries.",-0.1243333333,-0.1243333333,neutral
impala,7715,description,"Consider the documentation page [Impala Conditional Multiple functions have ambiguous descriptions. For example: The above is confusing, it essentially means: ""Returns true if a Boolean expression is false or not."" This obviously means the function always returns false, which is not accurate. Reword to say: ""Returns true if the expression is false. Returns false if the expression is true or NULL."" Other ambiguous descriptions: Better: ""Returns true if an expression is true. Returns false if the expression is false or NULL."" Others: Better: ""Returns true if the expression is non-null, tase if the expression is null. Same as {{expression IS NOT NULL}}."" Better: ""Returns true if the expression is NULL, false otherwise. Same as {{expression IS NULL}}""",documentation_debt,low_quality_documentation,"Tue, 16 Oct 2018 20:11:47 +0000","Tue, 23 Oct 2018 18:12:47 +0000","Wed, 17 Oct 2018 19:19:46 +0000",83279,"Consider the documentation page Impala Conditional Functions. Multiple functions have ambiguous descriptions. For example: isfalse(boolean) Purpose: Tests if a Boolean expression is false or not. Returns true if so. The above is confusing, it essentially means: ""Returns true if a Boolean expression is false or not."" This obviously means the function always returns false, which is not accurate. Reword to say: ""Returns true if the expression is false. Returns false if the expression is true or NULL."" Other ambiguous descriptions: istrue(boolean) Purpose: Tests if a Boolean expression is true or not. Returns true if so. Better: ""Returns true if an expression is true. Returns false if the expression is false or NULL."" Others: nonnullvalue(expression) Purpose: Tests if an expression (of any type) is NULL or not. Returns false if so. Better: ""Returns true if the expression is non-null, tase if the expression is null. Same as expression IS NOT NULL."" nullvalue(expression) Purpose: Tests if an expression (of any type) is NULL or not. Returns true if so. Better: ""Returns true if the expression is NULL, false otherwise. Same as expression IS NULL""",0.09571212121,0.1472416667,negative
impala,7861,description,"When it was originally submitted to Hadoop, the ABFS driver disabled TLS when using the URI scheme ""abfs"", and enabled TLS when using the URI scheme ""abfss"". This was reflected in the documentation originally submitted for ABFS: We should update that to reflect that TLS is enabled with either URI scheme unless you disable TLS in configuration by setting in the configuration.",documentation_debt,outdated_documentation,"Fri, 16 Nov 2018 11:20:17 +0000","Sat, 8 Dec 2018 00:49:36 +0000","Sat, 8 Dec 2018 00:49:36 +0000",1862959,"When it was originally submitted to Hadoop, the ABFS driver disabled TLS when using the URI scheme ""abfs"", and enabled TLS when using the URI scheme ""abfss"". This was reflected in the documentation originally submitted for ABFS: https://github.com/apache/impala/commit/5baa289fd039d339c63d5c475645e69fe0c6b8df. We should update that to reflect that TLS is enabled with either URI scheme unless you disable TLS in configuration by setting fs.azure.always.use.https=false in the configuration.",0.0,0.04285714286,neutral
impala,8146,description,These scripts are thin wrappers around make_impala.sh and don't add much value. We should remove them. I think there are a couple of references in the wiki that we can update to show alternative invocations.,documentation_debt,outdated_documentation,"Wed, 30 Jan 2019 10:44:43 +0000","Tue, 5 Feb 2019 10:00:46 +0000","Tue, 5 Feb 2019 10:00:46 +0000",515763,These scripts are thin wrappers around make_impala.sh and don't add much value. We should remove them. I think there are a couple of references in the wiki that we can update to show alternative invocations.,0.0,0.0,negative
impala,8429,description,The Query Option' page needs an update to reflect the changes in IMPALA-5120.,documentation_debt,outdated_documentation,"Wed, 17 Apr 2019 10:44:41 +0000","Mon, 12 Aug 2019 19:35:18 +0000","Mon, 12 Aug 2019 19:33:57 +0000",10140556,The 'DEFAULT_JOIN_DISTRIBUTION_MODE Query Option' page needs an update to reflect the changes in IMPALA-5120.,0.0,-0.15,neutral
impala,8855,description,"The documentation only mentions the values clause in the context of an INSERT statement. It can actually be used anywhere that a SELECT statement could be used, e.g. this is a valid query:",documentation_debt,low_quality_documentation,"Mon, 12 Aug 2019 20:35:28 +0000","Wed, 13 Nov 2019 22:10:11 +0000","Wed, 13 Nov 2019 19:43:16 +0000",8032068,"The documentation only mentions the values clause in the context of an INSERT statement. https://impala.apache.org/docs/build/html/topics/impala_insert.html It can actually be used anywhere that a SELECT statement could be used, e.g. this is a valid query:",0.344,0.344,neutral
impala,8945,description,"Reported by  The Impala docs entry for the IS DISTINCT FROM operator states: The <= But this expression is not equivalent to A <= (A IS NULL AND B IS NULL) OR ((A IS NOT NULL AND B IS NOT NULL) AND (A = B)) This expression should replace the existing incorrect expression. Another expression that is equivalent to A <= if(A IS NULL OR B IS NULL, A IS NULL AND B IS NULL, A = B) This one is a bit easier to follow. If you use this one in the docs, just replace the following line with: The <=> operator can use a hash join, while the if expression cannot.",documentation_debt,low_quality_documentation,"Sat, 14 Sep 2019 01:45:48 +0000","Tue, 17 Sep 2019 17:53:47 +0000","Mon, 16 Sep 2019 21:03:21 +0000",242253,"Reported by icook The Impala docs entry for the IS DISTINCT FROM operator states: The <=> operator, used like an equality operator in a join query, is more efficient than the equivalent clause: A = B OR (A IS NULL AND B IS NULL). The <=> operator can use a hash join, while the OR expression cannot. But this expression is not equivalent to A <=> B. See the attached screenshot demonstrating their non-equivalence. An expression that is equivalent to A <=> B is this: (A IS NULL AND B IS NULL) OR ((A IS NOT NULL AND B IS NOT NULL) AND (A = B)) This expression should replace the existing incorrect expression. Another expression that is equivalent to A <=> B is: if(A IS NULL OR B IS NULL, A IS NULL AND B IS NULL, A = B) This one is a bit easier to follow. If you use this one in the docs, just replace the following line with: The <=> operator can use a hash join, while the if expression cannot.",0.06666666667,0.1,neutral
impala,9013,description,Review Hive implementation to see if anything special needs to be done for DML. The Hive column masking design doc does not reflect the current code.,documentation_debt,outdated_documentation,"Fri, 4 Oct 2019 23:40:08 +0000","Wed, 15 Jan 2020 00:22:05 +0000","Wed, 15 Jan 2020 00:22:05 +0000",8815317,Review Hive implementation to see if anything special needs to be done for DML. The Hive column masking design doc does not reflect the current code.,0.0,0.0,negative
impala,9431,description,"IMPALA-8549 added support for scanning deflate text files. The docs however still have following: ""Deflate Not supported for text files."" Update the docs to reflect that scanning deflate text files are supported.",documentation_debt,outdated_documentation,"Wed, 26 Feb 2020 18:27:16 +0000","Sat, 7 Mar 2020 16:23:44 +0000","Sat, 7 Mar 2020 16:23:44 +0000",856588,"IMPALA-8549 added support for scanning deflate text files. The docs however still have following: ""Deflate Not supported for text files."" Update the docs to reflect that scanning deflate text files are supported.",0.2468333333,0.2468333333,neutral
impala,9443,description,"The {{SHOW TABLE STATS}} output for HDFS tables is outdated on some doc sites or abbreviated with ellipsis. Additionally, some other tables can be broken as well, for example {{SHOW FILES IN}}. I am aware of the following pages: - -",documentation_debt,outdated_documentation,"Mon, 2 Mar 2020 15:49:36 +0000","Fri, 27 Mar 2020 23:39:16 +0000","Fri, 27 Mar 2020 23:39:15 +0000",2188179,"The SHOW TABLE STATS output for HDFS tables is outdated on some doc sites or abbreviated with ellipsis. Additionally, some other tables can be broken as well, for example SHOW FILES IN. I am aware of the following pages: https://impala.apache.org/docs/build/html/topics/impala_show.html https://impala.apache.org/docs/build/html/topics/impala_perf_stats.html",0.2595,0.2595,negative
impala,9467,description,"We enable shell option live_progress in interactive mode by default. As for in the non-interactive mode, live reporting is not supported. Impala-shell will disable live_progress if the mode is detected. Need to update the doc to reflect the changes.",documentation_debt,outdated_documentation,"Thu, 5 Mar 2020 21:31:32 +0000","Tue, 24 Mar 2020 17:57:28 +0000","Tue, 24 Mar 2020 17:57:28 +0000",1628756,"https://gerrit.cloudera.org/#/c/15219/ We enable shell option live_progress in interactive mode by default. As for in the non-interactive mode, live reporting is not supported. Impala-shell will disable live_progress if the mode is detected. Need to update the doc to reflect the changes.",0.03333333333,0.03333333333,neutral
impala,4423,description,"Queries with several AND-ed EXISTS subqueries in the WHERE clause may produce incorrect results if some of the subqueries can be evaluated at query compile time. Repro with wrong plan: Same query as above but flipping the order of subqueries gives the correct plan: The underlying problem is that we substitute out the subqueries with constant literals using an but the Subquery.equals() function is not implemented properly, so the second subquery is replaced with whatever boolean literal corresponds to the first subquery.",requirement_debt,requirement_partially_implemented,"Wed, 2 Nov 2016 22:55:43 +0000","Tue, 15 Nov 2016 13:06:00 +0000","Fri, 4 Nov 2016 01:21:24 +0000",95141,"Queries with several AND-ed EXISTS subqueries in the WHERE clause may produce incorrect results if some of the subqueries can be evaluated at query compile time. Repro with wrong plan: Same query as above but flipping the order of subqueries gives the correct plan: The underlying problem is that we substitute out the subqueries with constant literals using an ExprSubstitutionMap, but the Subquery.equals() function is not implemented properly, so the second subquery is replaced with whatever boolean literal corresponds to the first subquery.",0.025,0.025,negative
impala,5489,description,"In IMPALA-4000 we added basic authorization support for Kudu tables, but it had several limitations: * Only the ALL privilege level can be granted to Kudu tables. (Finer-grained levels such as only SELECT or only INSERT are not supported.) * Column level permissions on Kudu tables are not supported. * Only users with ALL privileges on SERVER may create external Kudu tables. It looks like we could make the following work: * Allow column-level permissions * Allow fine grained privileges SELECT and INSERT for those statement types. However, would require ALL because Sentry doesn't have fine grained privilege actions for those types yet (work is planned though). So Impala can do this work, probably without much effort, but the question is whether or not it makes sense to implement this short-term solution in the context of the mid-to-longer term Kudu, Sentry, and Impala authorization plans. Kudu is currently figuring out what their authorization story will look like. Sentry is also poised for some large upcoming changes.",requirement_debt,requirement_partially_implemented,"Mon, 12 Jun 2017 21:53:42 +0000","Thu, 28 Sep 2017 22:26:54 +0000","Wed, 26 Jul 2017 13:23:42 +0000",3771000,"In IMPALA-4000 we added basic authorization support for Kudu tables, but it had several limitations: Only the ALL privilege level can be granted to Kudu tables. (Finer-grained levels such as only SELECT or only INSERT are not supported.) Column level permissions on Kudu tables are not supported. Only users with ALL privileges on SERVER may create external Kudu tables. It looks like we could make the following work: Allow column-level permissions Allow fine grained privileges SELECT and INSERT for those statement types. However, DELETE/UPDATE/UPSERT would require ALL because Sentry doesn't have fine grained privilege actions for those types yet (work is planned though). So Impala can do this work, probably without much effort, but the question is whether or not it makes sense to implement this short-term solution in the context of the mid-to-longer term Kudu, Sentry, and Impala authorization plans. Kudu is currently figuring out what their authorization story will look like. Sentry is also poised for some large upcoming changes.",0.05128888889,0.05128888889,neutral
impala,8771,description,"We currently don't support column stats for complex typed columns (ingored in `compute stats` statements). However running queries against those columns throws the missing col stats warning which is confusing. We could probably skip the warnings if we detect the missing stats are for complex typed columns, until we support them.",requirement_debt,requirement_partially_implemented,"Thu, 18 Jul 2019 21:38:54 +0000","Wed, 7 Aug 2019 14:23:13 +0000","Wed, 7 Aug 2019 14:23:13 +0000",1701859,"We currently don't support column stats for complex typed columns (ingored in `compute stats` statements). However running queries against those columns throws the missing col stats warning which is confusing.   We could probably skip the warnings if we detect the missing stats are for complex typed columns, until we support them.",-0.3596666667,-0.3596666667,negative
impala,1022,description,"rows_read < rows_in_file) { We should detect the case where doesn't actually equal the number of rows in the file. If abort_on_error is true, this should fail the query, otherwise we should log something via scan_node_-Such handling did not exist before. Need to decide whether we will read at most as many rows as the metadata or continue reading until there are no more rows in the file. We will need to add tests with parquet files whose metadata is not correct.",test_debt,lack_of_tests,"Fri, 30 May 2014 23:53:36 +0000","Sun, 20 Dec 2015 00:05:16 +0000","Tue, 10 Jun 2014 15:01:53 +0000",918497,"be/src/exec/hdfs-parquet-scanner.cc:736: rows_read < rows_in_file) { We should detect the case where file_metadata_.num_row doesn't actually equal the number of rows in the file. If abort_on_error is true, this should fail the query, otherwise we should log something via scan_node_>runtime_state()>LogError(). Such handling did not exist before. Need to decide whether we will read at most as many rows as the metadata or continue reading until there are no more rows in the file. We will need to add tests with parquet files whose metadata is not correct.",-0.2376666667,-0.1358095238,neutral
impala,1147,description,The view compatibility tests are disabled and need to be updated to run against Hive .13.,test_debt,lack_of_tests,"Mon, 11 Aug 2014 17:00:18 +0000","Sun, 20 Dec 2015 00:05:18 +0000","Tue, 2 Sep 2014 22:29:33 +0000",1920555,The view compatibility tests are disabled and need to be updated to run against Hive .13.,0.0,0.0,negative
impala,1290,description,If an AnalyticEvalNode is the root of a plan tree it should have results ready when Open() returns so that clients do not time out once the query is in a ready state and expect to be able to fetch results. This only happens for unpartitioned analytic evaluation. Should test this in,test_debt,lack_of_tests,"Sat, 20 Sep 2014 23:24:53 +0000","Wed, 1 Oct 2014 14:59:30 +0000","Wed, 1 Oct 2014 14:59:30 +0000",920077,If an AnalyticEvalNode is the root of a plan tree it should have results ready when Open() returns so that clients do not time out once the query is in a ready state and expect to be able to fetch results. This only happens for unpartitioned analytic evaluation. Should test this in test_rows_availability.py,-0.07644444444,-0.05733333333,neutral
impala,2386,description,We need to run the Isilon stress test on the RC.,test_debt,lack_of_tests,"Wed, 23 Sep 2015 00:55:37 +0000","Fri, 7 Oct 2016 17:17:06 +0000","Fri, 7 Oct 2016 17:17:06 +0000",32890889,We need to run the Isilon stress test on the RC.,-0.2,-0.2,neutral
impala,3017,description,I saw a private build where an impalad appears to get OOM-killed while executing the below tests. I believe what is probably happening is that a bunch of large allocations happen at the same time.,test_debt,expensive_tests,"Wed, 17 Feb 2016 21:04:11 +0000","Thu, 18 Feb 2016 17:42:59 +0000","Thu, 18 Feb 2016 17:42:59 +0000",74328,I saw a private build where an impalad appears to get OOM-killed while executing the below tests. I believe what is probably happening is that a bunch of large allocations happen at the same time.,-0.275,-0.275,neutral
impala,3718,description,The functional test coverage for Impala on Kudu can be significantly improved by doing the following: * Run TPC-H and TPC-DS against Kudu tables * Add an alltypes test table that covers all the supported Kudu data types and expand existing functional tests to use this table * Add more complex queries in the planner tests that query both Kudu and Hdfs tables * Add more complex expressions (e.g. conditional functions) in the SET portion of UPDATE statements in functional query tests * Improve the test coverage for using and computing stats in Kudu tables,test_debt,low_coverage,"Fri, 10 Jun 2016 17:34:15 +0000","Tue, 11 Oct 2016 03:31:11 +0000","Mon, 10 Oct 2016 17:30:58 +0000",10540603,The functional test coverage for Impala on Kudu can be significantly improved by doing the following: Run TPC-H and TPC-DS against Kudu tables Add an alltypes test table that covers all the supported Kudu data types and expand existing functional tests to use this table Add more complex queries in the planner tests that query both Kudu and Hdfs tables Add more complex expressions (e.g. conditional functions) in the SET portion of UPDATE statements in functional query tests Improve the test coverage for using and computing stats in Kudu tables,0.35,0.35,neutral
impala,4387,description,Haven't seen this before; may be a flaky test.,test_debt,flaky_test,"Thu, 27 Oct 2016 16:20:42 +0000","Tue, 15 Nov 2016 13:05:59 +0000","Sun, 30 Oct 2016 21:11:26 +0000",276644,Haven't seen this before; may be a flaky test.,0.0,0.0,neutral
impala,5084,description,See IMPALA-3208 for the context. Sorter::Run changes: * We can use a similar approach to that used for BufferedTupleStream as described in IMPALA-5085 Testing: Needs end-to-end tests exercising all operators with large operators,test_debt,low_coverage,"Thu, 16 Mar 2017 22:49:31 +0000","Sat, 15 Apr 2017 00:12:16 +0000","Sat, 15 Apr 2017 00:12:16 +0000",2510565,See IMPALA-3208 for the context. Sorter::Run changes: We can use a similar approach to that used for BufferedTupleStream as described in IMPALA-5085 Testing: Needs end-to-end tests exercising all operators with large operators,0.0,0.0,neutral
impala,5341,description,"In our planner tests we want to ignore minor differences in reported file sizes to avoid flaky tests, see IMPALA-2565. However, the introduced filter also matches ""row-size="" which we do not want to filter. Relevant code is in TestUtils.java:",test_debt,flaky_test,"Fri, 19 May 2017 04:36:45 +0000","Thu, 16 Nov 2017 10:27:23 +0000","Thu, 16 Nov 2017 10:27:23 +0000",15659438,"In our planner tests we want to ignore minor differences in reported file sizes to avoid flaky tests, see IMPALA-2565. However, the introduced filter also matches ""row-size="" which we do not want to filter. Relevant code is in TestUtils.java:",-0.06666666667,-0.06666666667,neutral
impala,5499,description,I suspect this is a rare flaky test failure. Filing a JIRA so that we can see if it reoccurs. This test failed and everything else passed. It looked like for some reason the port was occupied and the process then crashed on teardown. Info log: Error log:,test_debt,flaky_test,"Tue, 13 Jun 2017 17:36:12 +0000","Fri, 8 Jun 2018 06:33:00 +0000","Tue, 17 Apr 2018 17:15:30 +0000",26609958,I suspect this is a rare flaky test failure. Filing a JIRA so that we can see if it reoccurs. This test failed and everything else passed. It looked like for some reason the port was occupied and the process then crashed on teardown. Info log: Error log:,-0.22,-0.22,negative
impala,5525,description,"TestScannersFuzzing currently tests compressed parquet but does not test uncompressed parquet. Uncompressed parquet would help with test coverage because there's more potential to corrupt the actual data in interesting ways, not just the headers. To do this we'd probably need to set do a create table as select to write out some parquet data, then do the fuzz testing on that.",test_debt,low_coverage,"Fri, 16 Jun 2017 17:25:34 +0000","Mon, 9 Oct 2017 18:29:06 +0000","Mon, 9 Oct 2017 18:29:06 +0000",9939812,"TestScannersFuzzing currently tests compressed parquet but does not test uncompressed parquet. Uncompressed parquet would help with test coverage because there's more potential to corrupt the actual data in interesting ways, not just the headers. To do this we'd probably need to set COMPRESSION_CODEC=none, do a create table as select to write out some parquet data, then do the fuzz testing on that.",0.2515555556,0.2515555556,neutral
impala,5535,description,"Make sure that we have test coverage for large null-aware anti joins. * The streams start off unpinned, so can spill even when we don't go down the normal spilling path. I think my current iteration of the BufferPool patch doesn't test NAAJ when spilling is enabled",test_debt,low_coverage,"Mon, 19 Jun 2017 22:44:24 +0000","Tue, 8 Aug 2017 23:53:00 +0000","Tue, 8 Aug 2017 23:53:00 +0000",4324116,"Make sure that we have test coverage for large null-aware anti joins. The streams start off unpinned, so can spill even when we don't go down the normal spilling path. I think my current iteration of the BufferPool patch doesn't test NAAJ when spilling is enabled",0.02583333333,0.02583333333,neutral
impala,5640,description,There's a comment in the code saying that it was disabled because of IMPALA-424. This test coverage seems useful - we should add it back,test_debt,low_coverage,"Mon, 10 Jul 2017 21:20:30 +0000","Wed, 12 Jul 2017 00:38:22 +0000","Wed, 12 Jul 2017 00:38:22 +0000",98272,There's a comment in the code saying that it was disabled because of IMPALA-424. This test coverage seems useful - we should add it back,0.25,0.25,neutral
impala,5688,description,"Two tests ({{LongReverse}} and the base64 tests in run their tests over all lengths from 0..{{some length}}. Both take several minutes to complete. This adds a lot of runtime for not much more confidence. If instead we pick a set of 'interesting' (including powers-of-two, prime numbers, edge-cases) lengths, we can get a similar amount of confidence while significantly reducing the runtime of expr-test.",test_debt,expensive_tests,"Thu, 20 Jul 2017 17:54:43 +0000","Sat, 22 Jul 2017 21:12:03 +0000","Sat, 22 Jul 2017 21:12:03 +0000",184640,"Two tests (LongReverse and the base64 tests in StringFunctions) run their tests over all lengths from 0..some length. Both take several minutes to complete. This adds a lot of runtime for not much more confidence. If instead we pick a set of 'interesting' (including powers-of-two, prime numbers, edge-cases) lengths, we can get a similar amount of confidence while significantly reducing the runtime of expr-test.",0.003125,0.003125,neutral
impala,5732,description,"failed on an exhaustive integration jenkins run. I'm not sure if the test is flaky (is it safe to check {{row_regex: .*Rows rejected: 2.43K .*}} - will that be deterministic?) or if there was some other problem, e.g. missing stats and the plan had the wrong join order.",test_debt,flaky_test,"Thu, 27 Jul 2017 15:16:15 +0000","Tue, 22 May 2018 18:28:25 +0000","Thu, 25 Jan 2018 23:23:45 +0000",15754050,"TestRuntimeRowFilters::test_row_filters failed on an exhaustive integration jenkins run. I'm not sure if the test is flaky (is it safe to check row_regex: .Rows rejected: 2.43K . - will that be deterministic?) or if there was some other problem, e.g. missing stats and the plan had the wrong join order.",-0.1225,-0.153125,negative
impala,5779,description,We do not have any end-to-end tests where we attempt to spill buffers larger than the I/O size. I have tested manually but we need some automated testing.,test_debt,lack_of_tests,"Wed, 9 Aug 2017 00:07:13 +0000","Thu, 24 Aug 2017 15:18:48 +0000","Thu, 24 Aug 2017 15:18:48 +0000",1350695,We do not have any end-to-end tests where we attempt to spill buffers larger than the I/O size. I have tested manually but we need some automated testing.,0.2815,0.2815,neutral
impala,5780,description,We do not have any end-to-end test coverage with = true. We should add basic tests for the success and OOM cases where querying a table with no stats with = true.,test_debt,low_coverage,"Wed, 9 Aug 2017 01:04:41 +0000","Thu, 24 Aug 2017 15:18:41 +0000","Thu, 24 Aug 2017 15:18:41 +0000",1347240,We do not have any end-to-end test coverage with disable_unsafe_spills = true. We should add basic tests for the success and OOM cases where querying a table with no stats with disable_unsafe_spills = true.,-0.32975,0.08833333333,neutral
impala,6601,description,"ASAN fails with in during  - Im assigning this to you thinking you might have an idea whats going on here; feel free to find another person or assign back to me if you're swamped. Ive seen this happen in a private Jenkins run. Please ping me if you would like access to the build artifacts. I saw this in builds that also had issues during the e2e tests, so I'm not sure whether this is flaky or reproducibly broken.",test_debt,flaky_test,"Sat, 3 Mar 2018 01:55:02 +0000","Tue, 6 Mar 2018 17:34:44 +0000","Tue, 6 Mar 2018 17:34:44 +0000",315582,"ASAN fails with memcpy-param-overlap in {{impala::RawValue::Write during RowBatchSerializeTest.RowBatchLZ4Success: joemcdonnell - Im assigning this to you thinking you might have an idea whats going on here; feel free to find another person or assign back to me if you're swamped. Ive seen this happen in a private Jenkins run. Please ping me if you would like access to the build artifacts. I saw this in builds that also had issues during the e2e tests, so I'm not sure whether this is flaky or reproducibly broken.",0.125,0.07,negative
impala,8168,description,"Currently we enable Sentry HDFS sync when running on S3 which sometimes can make the authorization tests on S3 flaky. Since HDFS sync is not supported on S3, we should update our build script to only enable Sentry HDFS sync on HDFS and not on S3.",test_debt,flaky_test,"Wed, 6 Feb 2019 17:57:04 +0000","Thu, 14 Mar 2019 14:04:21 +0000","Fri, 22 Feb 2019 00:12:41 +0000",1318537,"Currently we enable Sentry HDFS sync when running on S3 which sometimes can make the authorization tests on S3 flaky. Since HDFS sync is not supported on S3, we should update our build script to only enable Sentry HDFS sync on HDFS and not on S3.",0.3875,0.3875,neutral
impala,8248,description,We have authorization tests that are specific to Sentry and authorization tests that can be applicable to any authorization provider. We need to re-organize the authorization tests to easily differentiate between Sentry-specific tests vs generic authorization tests. This also will improve test coverage for Ranger.,test_debt,low_coverage,"Tue, 26 Feb 2019 06:59:59 +0000","Tue, 28 May 2019 10:26:13 +0000","Fri, 24 May 2019 20:39:06 +0000",7565947,We have authorization tests that are specific to Sentry and authorization tests that can be applicable to any authorization provider. We need to re-organize the authorization tests to easily differentiate between Sentry-specific tests vs generic authorization tests. This also will improve test coverage for Ranger.,0.3,0.3,neutral
impala,8857,description,"I believe this is a flaky tests, since there's no attempt to pass the timestamp from the kudu client that did the insert to the impala client that's doing the reading.",test_debt,flaky_test,"Tue, 13 Aug 2019 00:17:21 +0000","Thu, 9 Apr 2020 16:53:41 +0000","Thu, 9 Apr 2020 16:53:40 +0000",20795779,"I believe this is a flaky tests, since there's no attempt to pass the timestamp from the kudu client that did the insert to the impala client that's doing the reading.",0.0,0.0,negative
thrift,228,description,"Following exception is raised by the read_message_begin method"" Missing version identifier from `receive_message' Comparing the implementation of the method in the Ruby class to its python counterpart TBinaryProtocol shows that the Ruby method is quite outdated. I have changed the method to be: def read_message_begin version = read_i32 if(version <0) if (version & VERSION_MASK != VERSION_1) raise 'Missing version identifier') end type = version & 0000000ff name = read_string seqid = read_i32 else name = type = read_byte seqid = read_i32 end [name, type, seqid] end This does not raise an exception on the strict read condition in the else clause as is raised by the Python version but can be easily added to.",architecture_debt,using_obsolete_technology,"Sun, 7 Dec 2008 20:13:15 +0000","Tue, 1 Nov 2011 02:51:49 +0000","Mon, 16 Mar 2009 04:38:52 +0000",8497537,"Following exception is raised by the read_message_begin method"" /usr/local/lib/site_ruby/1.8/thrift/protocol/binaryprotocol.rb:82:in 'read_message_begin': Missing version identifier (Thrift::ProtocolException) from /usr/local/lib/site_ruby/1.8/thrift/client.rb:26:in `receive_message' Comparing the implementation of the method in the Ruby class Thrift::BinaryProtocol to its python counterpart TBinaryProtocol shows that the Ruby method is quite outdated. I have changed the method to be: def read_message_begin version = read_i32 if(version <0) if (version & VERSION_MASK != VERSION_1) raise ProtocolException.new(ProtocolException::BAD_VERSION, 'Missing version identifier') end type = version & 0000000ff name = read_string seqid = read_i32 else name = trans.readAll(version) type = read_byte seqid = read_i32 end [name, type, seqid] end This does not raise an exception on the strict read condition in the else clause as is raised by the Python version but can be easily added to.",-0.2666666667,-0.05714285714,negative
thrift,4069,description,"Currently our perl package module files contain multiple packages. We should break each package out to an individual file (or at least make sure everything is in the Thrift namespace) and properly version it. Package versioning was introduced in Perl 5.10 so: 1. Update the minimum required perl to 5.10. This is based on indicating that perl version object was added to perl in 5.10. 2. For each package use the {{perl MODULE VERSION}} perlmod syntax, where VERSION is {{v0.11.0}}. This is based on 3. Each module not under the Thrift namespace must be moved there TMessageType, TType). This will be a breaking change, but necessary for proper packaging of the library. Currently if you inspect the Perl PAUSE version metadata for Thrift's sub-modules only the 0.9.0 modules from gslin have version identities. For example if you look at Thrift and in the CPAN list of packages at you will see: There are some anomalies, for example packages defined in Thrift.pm come out at the top level namespace like: So technically if you do 'install I would expect you might get thrift. This is wrong and should be fixed. needs to be inside Thrift, not at the top level. Also we should pull in relevant changes from the patch in THRIFT-4059 around improving packaging. Also we should actually use TProtocolException and TTransportException instead of just TException everywhere.",architecture_debt,violation_of_modularity,"Sun, 5 Feb 2017 16:20:39 +0000","Thu, 14 Dec 2017 13:54:42 +0000","Thu, 30 Mar 2017 21:12:02 +0000",4596683,"Currently our perl package module files contain multiple packages. We should break each package out to an individual file (or at least make sure everything is in the Thrift namespace) and properly version it. Package versioning was introduced in Perl 5.10 so: 1. Update the minimum required perl to 5.10. This is based on http://search.cpan.org/~jpeacock/version-0.9917/lib/version.pod indicating that perl version object was added to perl in 5.10. 2. For each package use the perl MODULE VERSION perlmod syntax, where VERSION is v0.11.0. This is based on http://perldoc.perl.org/functions/package.html. 3. Each module not under the Thrift namespace must be moved there (TApplicationException, TMessageType, TType). This will be a breaking change, but necessary for proper packaging of the library. Currently if you inspect the Perl PAUSE version metadata for Thrift's sub-modules only the 0.9.0 modules from gslin have version identities. For example if you look at Thrift and Thrift::BinaryProtocol in the CPAN list of packages at http://www.cpan.org/modules/02packages.details.txt you will see: There are some anomalies, for example packages defined in Thrift.pm come out at the top level namespace like: So technically if you do 'install TApplicationException' I would expect you might get thrift. This is wrong and should be fixed. TApplicationException needs to be inside Thrift, not at the top level. Also we should pull in relevant changes from the patch in THRIFT-4059 around improving packaging. Also we should actually use TProtocolException and TTransportException instead of just TException everywhere.",-0.01560526316,-0.01560526316,neutral
thrift,4256,description,"Currently, Thrift.cabal has an exact dependency of vector==0.10.12.2, but this version is much, much older than what other packages depend on. This makes it necessary to enable ""allow-newer"", which effectively ignores the dependency, and then breaks when a package is uploaded to hackage, and prevents inclusion of thrift in a stack curated package set. If there's no particular reason for it (and I've been successfully compiling thrift with vector==0.12.0.2), could this dependency be set to a range, .e.g. >=0.12.0? I could then enter a request for thrift to be added to stack's curated package sets.",architecture_debt,using_obsolete_technology,"Wed, 19 Jul 2017 01:05:49 +0000","Thu, 12 Oct 2017 09:39:39 +0000","Wed, 19 Jul 2017 15:21:19 +0000",51330,"Currently, Thrift.cabal has an exact dependency of vector==0.10.12.2, but this version is much, much older than what other packages depend on. This makes it necessary to enable ""allow-newer"", which effectively ignores the dependency, and then breaks when a package is uploaded to hackage, and prevents inclusion of thrift in a stack curated package set. If there's no particular reason for it (and I've been successfully compiling thrift with vector==0.12.0.2), could this dependency be set to a range, .e.g. >=0.12.0? I could then enter a request for thrift to be added to stack's curated package sets.",-0.004777777778,-0.004777777778,neutral
thrift,4321,description,"Current library is still using old project structure (project.json) which is slightly outdated and needs to be migrated to new MSBuild format. In addition to that, I'd like to have separate packages build for different .NET Standard versions starting from 1.4 (UWP) and up to 2.0 with full feature set.",architecture_debt,using_obsolete_technology,"Fri, 8 Sep 2017 09:34:56 +0000","Sat, 2 Feb 2019 13:47:58 +0000","Thu, 19 Oct 2017 13:10:30 +0000",3555334,"Current library is still using old project structure (project.json) which is slightly outdated and needs to be migrated to new MSBuild format. In addition to that, I'd like to have separate packages build for different .NET Standard versions starting from 1.4 (UWP) and up to 2.0 with full feature set.",0.07025,0.07025,neutral
thrift,4369,description,"ws@0.4.32 is very dangerous, please upgrade it.",architecture_debt,using_obsolete_technology,"Tue, 24 Oct 2017 18:17:34 +0000","Tue, 6 Mar 2018 21:04:38 +0000","Tue, 6 Mar 2018 21:04:00 +0000",11501186,"https://snyk.io/vuln/npm:ws:20160104, https://nodesecurity.io/advisories/67, https://snyk.io/vuln/npm:ws:20160624, https://nodesecurity.io/advisories/120, https://snyk.io/vuln/npm:ws:20160920 ws@0.4.32 is very dangerous, please upgrade it.",-0.004166666667,-0.004166666667,negative
thrift,525,description,"The C# Library Solution fails to compile because of problems with the PreBuildEvent for the ThriftTest Project - Thrift.exe is called with the outdated -csharp instead of --gen csharp - All of the existing commands in the PreBuildEvent will fail if there's a space in any directory name in the directory tree (such as ""My Documents"" or ""Visual Studio 2008""). - The recursive forced rmdir command will match other directory trees that share a common root (e.g. c:\test\Work\ will be recursively removed if the project is located in c:\test\Work for Thrift\test\csharp) I have attached a patch that replaces the PreBuildEvent in the ThriftTest project file to fix these problems and work with directory trees that contain spaces. As the thrift.exe compiler does NOT accept paths with spaces when surrounded by quotes, my script generates MSDOS 8.3 pathnames to pass to the thrift compiler and it appears to work just fine. This has only been tested on my machine with VS.net pro 2008 and a current thrift checkout",architecture_debt,using_obsolete_technology,"Tue, 23 Jun 2009 16:18:25 +0000","Tue, 1 Nov 2011 02:51:46 +0000","Thu, 2 Jul 2009 20:28:57 +0000",792632,"The C# Library Solution (thrift\lib\csharp\src\Thrift.sln) fails to compile because of problems with the PreBuildEvent for the ThriftTest Project (thrift\test\csharp\ThriftTest\ThriftTest.csproj). Thrift.exe is called with the outdated -csharp instead of --gen csharp All of the existing commands in the PreBuildEvent will fail if there's a space in any directory name in the directory tree (such as ""My Documents"" or ""Visual Studio 2008""). The recursive forced rmdir command will match other directory trees that share a common root (e.g. c:\test\Work\ will be recursively removed if the project is located in c:\test\Work for Thrift\test\csharp) I have attached a patch that replaces the PreBuildEvent in the ThriftTest project file to fix these problems and work with directory trees that contain spaces. As the thrift.exe compiler does NOT accept paths with spaces when surrounded by quotes, my script generates MSDOS 8.3 pathnames to pass to the thrift compiler and it appears to work just fine. This has only been tested on my machine with VS.net pro 2008 and a current thrift checkout",0.1303777778,0.03385185185,negative
thrift,568,description,"When building thrift python libs on ubuntu, it places the files into site-packages instead of dist-utils",architecture_debt,violation_of_modularity,"Fri, 21 Aug 2009 19:02:02 +0000","Mon, 29 Aug 2011 00:13:10 +0000","Mon, 29 Aug 2011 00:13:10 +0000",63695468,"When building thrift python libs on ubuntu, it places the files into site-packages instead of dist-utils",0.0,0.0,neutral
thrift,1440,description,"A listing of detectable policy problems in the thrift Debian packaging (in contrib/) can be found with a lintian run: I'll note some of them here for posterity. h3. thrift source: libthrift-dev on libthrift0 The libthrift-dev package should have a versioned dependency on libthrift0, i.e., in debian/control: h3. thrift source: build-depends You don't need the ""build-essential"" bit in Build-Depends. h3. thrift-compiler: Syntax is a bit off in debian/control for the Description fields; I'll attach a patch. h3. thrift-compiler: usr/bin/thrift You need a man page for /usr/bin/thrift. h3. python-thrift-dbg: python-thrift-dbg = The python-thrift-dbg package should be in Section: debug. h3. python-thrift-dbg: dir-in-usr-local usr/local/lib/ Debian packages shouldn't be shipping anything in /usr/local; that's supposed to be reserved for the local system admin. There isn't much reason for this anyway; the dirs being shipped by python-thrift-dbg here are empty. h3. libthrift-ruby: libthrift-ruby = The libthrift-ruby package should be in Section: ruby. Also, according to , it looks like Ruby packages are undergoing a name change in the current Debian testing suite. libthrift-ruby probably needs to become ruby-thrift and switch to using gem2deb. h3. libthrift-ruby: This will probably be addressed under THRIFT-1421. h3. libthrift0: libthrift-c-glib0 This is complaining because the package name of a library package should usually reflect the soname of the included library (see [chapter 5 of the Debian Library Packaging for more info). Something is fishy here, though. Did you intend to distribute the c-glib library as ""libthrift0""? If so, where is the cpp library supposed to go? I don't think I see it after a quick search through the packages. h3. php5-thrift: See the lintian explanation for detailed info. Basically, you need some extra Sauce to add a dependency to php5-thrift on a PHP with a compatible API version. h3. libthrift-java: libthrift-java = libthrift-java should be Section: java h3. libthrift-cil: libthrift-cil = libthrift-cil should be Section: cli-mono h3. libthrift-cil: Thrift.dll shouldn't have its executable bit set. h3. libthrift-perl: usr/usr/ Yeah, installing into /usr/usr/local/lib is kinda wacko. Ought to be /usr/lib. - And as a final note, a lot of the packaging here could be pretty greatly simplified and better future-proofed using the Debhelper 7 command sequencer (""{{dh}}"").",build_debt,under-declared_dependencies,"Mon, 28 Nov 2011 22:14:06 +0000","Thu, 20 Jun 2013 16:12:38 +0000","Thu, 20 Jun 2013 16:12:38 +0000",49226312,"A listing of detectable policy problems in the thrift Debian packaging (in contrib/) can be found with a lintian run: I'll note some of them here for posterity. thrift source: weak-library-dev-dependency libthrift-dev on libthrift0 The libthrift-dev package should have a versioned dependency on libthrift0, i.e., in debian/control: thrift source: build-depends-on-build-essential build-depends You don't need the ""build-essential"" bit in Build-Depends. thrift-compiler: description-contains-invalid-control-statement Syntax is a bit off in debian/control for the Description fields; I'll attach a patch. thrift-compiler: binary-without-manpage usr/bin/thrift You need a man page for /usr/bin/thrift. python-thrift-dbg: wrong-section-according-to-package-name python-thrift-dbg => debug The python-thrift-dbg package should be in Section: debug. python-thrift-dbg: dir-in-usr-local usr/local/lib/ Debian packages shouldn't be shipping anything in /usr/local; that's supposed to be reserved for the local system admin. There isn't much reason for this anyway; the dirs being shipped by python-thrift-dbg here are empty. libthrift-ruby: wrong-section-according-to-package-name libthrift-ruby => ruby The libthrift-ruby package should be in Section: ruby. Also, according to http://wiki.debian.org/Teams/Ruby/Packaging , it looks like Ruby packages are undergoing a name change in the current Debian testing suite. libthrift-ruby probably needs to become ruby-thrift and switch to using gem2deb. libthrift-ruby: empty-binary-package This will probably be addressed under THRIFT-1421. libthrift0: package-name-doesnt-match-sonames libthrift-c-glib0 This is complaining because the package name of a library package should usually reflect the soname of the included library (see chapter 5 of the Debian Library Packaging Guide for more info). Something is fishy here, though. Did you intend to distribute the c-glib library as ""libthrift0""? If so, where is the cpp library supposed to go? I don't think I see it after a quick search through the packages. php5-thrift: missing-dependency-on-phpapi See the lintian explanation for detailed info. Basically, you need some extra Sauce to add a dependency to php5-thrift on a PHP with a compatible API version. libthrift-java: wrong-section-according-to-package-name libthrift-java => java libthrift-java should be Section: java libthrift-cil: wrong-section-according-to-package-name libthrift-cil => cli-mono libthrift-cil should be Section: cli-mono libthrift-cil: executable-not-elf-or-script ./usr/lib/cli/thrift/Thrift.dll Thrift.dll shouldn't have its executable bit set. libthrift-perl: non-standard-dir-in-usr usr/usr/ Yeah, installing into /usr/usr/local/lib is kinda wacko. Ought to be /usr/lib. And as a final note, a lot of the packaging here could be pretty greatly simplified and better future-proofed using the Debhelper 7 command sequencer (""dh"").",0.06018518519,0.08583333333,neutral
thrift,1504,description,"The Cocoa generator imports various thrift files as global imports instead of local importss. This ends up requiring that the user alter their header search paths to include the directories the generated thrift code is in, or the user has to edit the generated files to remove this. e.g. #import <TProtocol.h#import <TProcessor.h vs. #import ""TProtocol.h"" #import #import ""TProtocolUtil.h"" #import ""TProcessor.h""",build_debt,build_others,"Wed, 25 Jan 2012 20:41:06 +0000","Fri, 27 Jan 2012 04:13:57 +0000","Fri, 27 Jan 2012 03:08:57 +0000",109671,"The Cocoa generator imports various thrift files as global imports instead of local importss. This ends up requiring that the user alter their header search paths to include the directories the generated thrift code is in, or the user has to edit the generated files to remove this. e.g. #import <TProtocol.h> #import <TApplicationException.h> #import <TProtocolUtil.h> #import <TProcessor.h> vs. #import ""TProtocol.h"" #import ""TApplicationException.h"" #import ""TProtocolUtil.h"" #import ""TProcessor.h""",0.0,0.0,neutral
thrift,1661,description,"In MacPorts we don't want packages to pick up dependencies unless explicitly instructed to, so it's useful to explicitly turn off Qt support otherwise it'll find Qt4 even though the package doesn't list it as a dependency. I chose --with-qt4 since Qt5 will be out soon and I don't know if this code is supported in Qt5.",build_debt,over-declared_dependencies,"Fri, 27 Jul 2012 01:31:50 +0000","Fri, 27 Jul 2012 16:40:07 +0000","Fri, 27 Jul 2012 16:02:54 +0000",52264,"In MacPorts we don't want packages to pick up dependencies unless explicitly instructed to, so it's useful to explicitly turn off Qt support otherwise it'll find Qt4 even though the package doesn't list it as a dependency. I chose --with-qt4 since Qt5 will be out soon and I don't know if this code is supported in Qt5.",-0.08333333333,-0.08333333333,neutral
thrift,1919,description,"I believe this is the same issue as THRIFT-1693, but with different versions. There's a direct dependency on both httpclient and httpcore version 4.1.3 in thriftlib, but httpclient:4.1.3 has a dependency on httpcore:4.1.4. Here are the dependencies Gradle shows me. Here's a copy of the dependency resolution section of my Gradle build in case anyone needs a short term workaround. The force option is what allows the direct dependency to be overridden to 4.1.4 (shown with a * in the above snippet).",build_debt,build_others,"Sun, 7 Apr 2013 03:43:29 +0000","Fri, 21 Jun 2013 20:55:45 +0000","Fri, 21 Jun 2013 19:37:56 +0000",6537267,"I believe this is the same issue as THRIFT-1693, but with different versions. There's a direct dependency on both httpclient and httpcore version 4.1.3 in thriftlib, but httpclient:4.1.3 has a dependency on httpcore:4.1.4. Here are the dependencies Gradle shows me. Here's a copy of the dependency resolution section of my Gradle build in case anyone needs a short term workaround. The force option is what allows the direct dependency to be overridden to 4.1.4 (shown with a * in the above snippet).",0.08,0.08,neutral
thrift,2150,description,"As ""windows/config.h"" includes Winsock2.h, is it not best practise to define before it: This is in case someone includes Windows.h, which automatically includes the incompatible Winsock.h. It also reduces the size of the Win32 header files which helps compilation speeds :)",build_debt,build_others,"Mon, 26 Aug 2013 12:29:13 +0000","Wed, 6 May 2015 13:19:11 +0000","Wed, 6 May 2015 13:19:10 +0000",53398197,"As ""windows/config.h"" includes Winsock2.h, is it not best practise to define before it: This is in case someone includes Windows.h, which automatically includes the incompatible Winsock.h. It also reduces the size of the Win32 header files which helps compilation speeds",-0.095,-0.095,neutral
thrift,2333,description,"As can be seen in this email message to the users list it is possible for some pre-required packages to be missing, but the traditional method for validating their existence in specfile using ""BuildRequires"" won't work for items that are not commonly available as RPMS. I think that in this user's case the missing bit is the bundler gem, which I have a locally built RPM for, but I have to assume that most folks do not and it is not commonly available in most repos (not that I saw as a Centos6.4 user anyway) The patch I am attaching just checks if ruby subpackage was not disabled at the (at the buildrpm level) and if the results of ./configure have determined that ""make install"" won't build the gem (and so the rpm build of the rubygem-thrift subpackage is bound to fail) abort the build with a clear message An alternative solution MAY be to just dynamically skip that package when it is detected that ./configure excluded ruby (I don't know off hand how to do that)",build_debt,build_others,"Sat, 25 Jan 2014 21:13:20 +0000","Wed, 16 Oct 2019 22:27:23 +0000","Thu, 20 Dec 2018 03:13:32 +0000",154591212,"As can be seen in this email message to the users list it is possible for some pre-required packages to be missing, but the traditional method for validating their existence in specfile using ""BuildRequires"" won't work for items that are not commonly available as RPMS. http://mail-archives.apache.org/mod_mbox/thrift-user/201401.mbox/%3CCACeqxwQrLr7ghoBmbabQpu7z_u0FHjh+vbYXQKwP8=ibR4bHKQ@mail.gmail.com%3E I think that in this user's case the missing bit is the bundler gem, which I have a locally built RPM for, but I have to assume that most folks do not and it is not commonly available in most repos (not that I saw as a Centos6.4 user anyway) The patch I am attaching just checks if ruby subpackage was not disabled at the (at the buildrpm level) and if the results of ./configure have determined that ""make install"" won't build the gem (and so the rpm build of the rubygem-thrift subpackage is bound to fail) abort the build with a clear message An alternative solution MAY be to just dynamically skip that package when it is detected that ./configure excluded ruby (I don't know off hand how to do that)",-0.07,-0.07,neutral
thrift,2759,description,The Trusty (Ubuntu 14.04) Vagrantfile in the current trunk does not build clean. Subtasks created for: 1. D not installed 2. Haskell does not compile 3. Go crashes on symlink creation 4. Python Twisted tests fail 5. CPP thrift/test/cpp does not make check due to missing boost packages,build_debt,build_others,"Fri, 3 Oct 2014 01:10:00 +0000","Wed, 5 Nov 2014 04:49:16 +0000","Thu, 9 Oct 2014 03:23:54 +0000",526434,The Trusty (Ubuntu 14.04) Vagrantfile in the current trunk does not build clean. Subtasks created for: 1. D not installed 2. Haskell does not compile 3. Go crashes on symlink creation 4. Python Twisted tests fail 5. CPP thrift/test/cpp does not make check due to missing boost packages,0.05357142857,0.05357142857,negative
thrift,3168,description,"1. THRIFT-2269 was not fixed properly. Sources are supposed to be deployed using a ""jar"" type. cannot resolve them otherwise and IDEs such as IntelliJ IDEA are unable to resolve or even manually attach sources with a ""src"" extension/type. 2. Between 0.9.1 and 0.9.2, the POM for libthrift was modified to remove a dependency on commons-lang3. As a result, the code generated by libthrift no longer builds as it misses this dependency. The dependency needs to be restored, or whatever reason resulted in its removal re-evaluated to take this requirement into account. Personally, I'd prefer if we could just drop this dependency entirely and use native Java language utilities such as `Objects` to solve the problems `commons-lang3` is trying to solve.",build_debt,under-declared_dependencies,"Wed, 20 May 2015 00:23:04 +0000","Tue, 21 Jul 2015 02:21:24 +0000","Tue, 23 Jun 2015 12:40:27 +0000",2981843,"1. THRIFT-2269 was not fixed properly. Sources are supposed to be deployed using a ""jar"" type. maven-dependency-plugin cannot resolve them otherwise and IDEs such as IntelliJ IDEA are unable to resolve or even manually attach sources with a ""src"" extension/type. 2. Between 0.9.1 and 0.9.2, the POM for libthrift was modified to remove a dependency on commons-lang3. As a result, the code generated by libthrift no longer builds as it misses this dependency. The dependency needs to be restored, or whatever reason resulted in its removal re-evaluated to take this requirement into account. Personally, I'd prefer if we could just drop this dependency entirely and use native Java language utilities such as `Objects` to solve the problems `commons-lang3` is trying to solve.",-0.008148148148,-0.008148148148,negative
thrift,3747,description,"As npm download is unstable, node.js build was supposed to be isolated into a separate job on Travis (with haskell which is unstable due to cabal download). In reality, node.js was built in ""Java Lua ..."" job too, so we've been having double chance of download failures.",build_debt,build_others,"Thu, 17 Mar 2016 17:57:18 +0000","Sat, 19 Mar 2016 16:45:38 +0000","Fri, 18 Mar 2016 06:40:40 +0000",45802,"As npm download is unstable, node.js build was supposed to be isolated into a separate job on Travis (with haskell which is unstable due to cabal download). In reality, node.js was built in ""Java Lua ..."" job too, so we've been having double chance of download failures.",-0.1333333333,-0.1333333333,negative
thrift,3983,description,"Hi, libthrift is deployed with a ""pom"" packaging on maven central, see This is very wrong. ""pom"" means ""no jar, just a list of dependencies to be pulled transitively"". As a consequence, some build tools, such as sbt, don't pull the libthrift jar itself. The pom should be be using a ""jar"" packaging, or even no packaging at all as ""jar"" is the default value.",build_debt,build_others,"Wed, 23 Nov 2016 16:58:21 +0000","Thu, 27 Dec 2018 15:25:21 +0000","Wed, 24 Jan 2018 17:31:47 +0000",36894806,"Hi, libthrift is deployed with a ""pom"" packaging on maven central, see https://repo1.maven.org/maven2/org/apache/thrift/libthrift/0.9.3/libthrift-0.9.3.pom <groupId>org.apache.thrift</groupId> <artifactId>libthrift</artifactId> <version>0.9.3</version> <packaging>pom</packaging> <name>Apache Thrift</name> This is very wrong. ""pom"" means ""no jar, just a list of dependencies to be pulled transitively"". As a consequence, some build tools, such as sbt, don't pull the libthrift jar itself. The pom should be be using a ""jar"" packaging, or even no packaging at all as ""jar"" is the default value.",-0.1875,-0.125,negative
thrift,841,description,"The following files are shipped in the binary release artifacts, (despite being removed in the clean target). This creates problems for packaging where it is typical to import to the binary releases into a vcs alongside the packaging artifacts. Also, the following is generated during a build, but not removed on a clean.",build_debt,build_others,"Sat, 7 Aug 2010 00:10:30 +0000","Thu, 10 Jul 2014 13:42:35 +0000","Thu, 10 Jul 2014 13:42:35 +0000",123859925,"The following files are shipped in the binary release artifacts, (despite being removed in the clean target). This creates problems for packaging where it is typical to import to the binary releases into a vcs alongside the packaging artifacts. Also, the following is generated during a build, but not removed on a clean.",-0.325,-0.325,neutral
thrift,1003,description,"attached patch contains following changes: * Added Apache headers to c/h files * Use gtester for running tests. We don't need -wrapper script anymore * Use one-line macros G_DEFINE_TYPE instead of 15-line class definition * Keep formatting closer to glib-like style (one line class definition macroses/remove trailing spaces) Given changes are mostly fixing low hanging fruits. It does not change any logic/api. There are more chages needed, such as * using CLASS_TYPE_new functions instead of * stop using _set_property (aka reflection) in constructors * check more careful about _ref and _unref handling but this requires more careful refactoring so it will be later in a separate patch.",code_debt,low_quality_code,"Mon, 22 Nov 2010 19:56:47 +0000","Tue, 1 Nov 2011 02:51:41 +0000","Fri, 26 Nov 2010 10:18:37 +0000",310910,"attached patch contains following changes: Added Apache headers to c/h files Use gtester for running tests. We don't need -wrapper script anymore Use one-line macros G_DEFINE_TYPE instead of 15-line class definition Keep formatting closer to glib-like style (one line class definition macroses/remove trailing spaces) Given changes are mostly fixing low hanging fruits. It does not change any logic/api. There are more chages needed, such as using CLASS_TYPE_new functions instead of g_object_new(CLASS_TYPE) stop using _set_property (aka reflection) in constructors check more careful about _ref and _unref handling but this requires more careful refactoring so it will be later in a separate patch.",0.05,0.002375,neutral
thrift,1048,description,The only class from commons-lang used in Thrift is It is used in and in It can be replaced with like it is done in,code_debt,low_quality_code,"Wed, 26 Jan 2011 15:18:48 +0000","Thu, 27 Jan 2011 09:44:43 +0000","Wed, 26 Jan 2011 23:41:15 +0000",30147,The only class from commons-lang used in Thrift is org.apache.commons.lang.NotImplementedException. It is used in org.apache.thrift.transport.AutoExpandingBufferReadTransport and in org.apache.thrift.transport.AutoExpandingBufferWriteTransport It can be replaced with java.lang.UnsupportedOperationException like it is done in org.apache.thrift.transport.TMemoryInputTransport,0.0,0.0,neutral
thrift,1100,description,"The python TSSLSocket.py module has TSSLSocket and TSSLServerSocket for outbound and inbound SSL connection wrapping. This ticket is for a patch that makes several improvements: * adds Apache license at top of file * for outbound sockets, SSL certificate validation is now performed by default ** but may be disabled with validate=False in the constructor ** instructs python's ssl library to perform CERT_REQUIRED validation of the certificate ** also checks to make sure the certificate's {{commonName}} matches the hostname we tried to connect to ** raises when the certificate fails validation - tested using google's www.gmail.com (doesnt match) versus mail.google.com (matched cert commonName) ** puts a copy of the peer certificate in self.peercert, regardless of validation status ** sets a public boolean self.is_valid member variable to indicate whether the certificate was validated or not * adds a configurable server certificate file, as a constructor argument {{certfile}} ** allows runtime changing of server cert with setCertfile() on the server, that changes the certfile used in subsequent ssl_wrap() calls ** exposes a class-level variable SSL_PROTOCOL to let the user select ssl.PROTOCOL_TLSv1 or other versions of SSL, instead of hard-coding TLSv1. Defaults to TLSv1 though. * removes unnecessary sys.path modification * adds lots of docstrings In a somewhat unrelated change, this patch changes two lines in TSocket.py where self.handle is compared to None using {{!=}} instead of: {{is not}}.",code_debt,low_quality_code,"Sat, 19 Mar 2011 23:35:33 +0000","Mon, 21 Mar 2011 18:00:02 +0000","Mon, 21 Mar 2011 18:00:02 +0000",152669,"The python TSSLSocket.py module has TSSLSocket and TSSLServerSocket for outbound and inbound SSL connection wrapping. This ticket is for a patch that makes several improvements: adds Apache license at top of file for outbound sockets, SSL certificate validation is now performed by default but may be disabled with validate=False in the constructor instructs python's ssl library to perform CERT_REQUIRED validation of the certificate also checks to make sure the certificate's commonName matches the hostname we tried to connect to raises TTransportExceptions when the certificate fails validation - tested using google's www.gmail.com (doesnt match) versus mail.google.com (matched cert commonName) puts a copy of the peer certificate in self.peercert, regardless of validation status sets a public boolean self.is_valid member variable to indicate whether the certificate was validated or not adds a configurable server certificate file, as a constructor argument certfile allows runtime changing of server cert with setCertfile() on the server, that changes the certfile used in subsequent ssl_wrap() calls exposes a class-level variable SSL_PROTOCOL to let the user select ssl.PROTOCOL_TLSv1 or other versions of SSL, instead of hard-coding TLSv1. Defaults to TLSv1 though. removes unnecessary sys.path modification adds lots of docstrings In a somewhat unrelated change, this patch changes two lines in TSocket.py where self.handle is compared to None using != instead of: is not.",0.03998958333,0.03998958333,neutral
thrift,1121,description,"A user reports a 30% performance regression after upgrading some high-request-rate Java software from Thrift 0.3 to 0.6. After some inspection, it turns out that the changes for THRIFT-959 caused the slowdown. However, even after altering the code to use the TFramedTransport, performance was still only 80% of version 0.3. I believe the problem is that the TFramedTransport must read the length (unbuffered) before reading (only) one message. In one particular workload, sent with oneway streaming, the server is making many more system calls. It wasn't obvious how to compose a Transport that would add back the buffering using existing components. We created our own trivial TServerSocket that adds the socket buffering back. Performance is now back where it was with 0.3.",code_debt,slow_algorithm,"Mon, 28 Mar 2011 18:06:04 +0000","Thu, 3 Jan 2019 04:32:08 +0000","Tue, 27 Sep 2011 17:08:01 +0000",15807717,"A user reports a 30% performance regression after upgrading some high-request-rate Java software from Thrift 0.3 to 0.6. After some inspection, it turns out that the changes for THRIFT-959 caused the slowdown. However, even after altering the code to use the TFramedTransport, performance was still only 80% of version 0.3. I believe the problem is that the TFramedTransport must read the length (unbuffered) before reading (only) one message. In one particular workload, sent with oneway streaming, the server is making many more system calls. It wasn't obvious how to compose a Transport that would add back the buffering using existing components. We created our own trivial TServerSocket that adds the socket buffering back. Performance is now back where it was with 0.3.",0.041375,0.041375,negative
thrift,1176,description,Below I added the var qualifier on the declare field function. This is fairly trivial and fixes the global scope leak.,code_debt,low_quality_code,"Fri, 20 May 2011 23:04:02 +0000","Tue, 1 Nov 2011 02:53:52 +0000","Sun, 22 May 2011 10:03:03 +0000",125941,Below I added the var qualifier on the declare field function. This is fairly trivial and fixes the global scope leak.,-0.1,-0.1,positive
thrift,1199,description,"For example, in the following union it would be nice to be able to do something like {{boolean test = as an alternative to {{boolean test = ==",code_debt,low_quality_code,"Tue, 7 Jun 2011 20:29:48 +0000","Thu, 9 Jun 2011 22:33:47 +0000","Wed, 8 Jun 2011 17:47:12 +0000",76644,"For example, in the following union it would be nice to be able to do something like boolean test = myUnion.is_my_field1; as an alternative to boolean test = (myUnion.getSetField() == _Fields.MY_FIELD1);",0.7565,0.189125,positive
thrift,1231,description,"TAsyncChannel.h includes TTransportUtils.h, but doesn't really need/use it.",code_debt,dead_code,"Tue, 5 Jul 2011 23:39:04 +0000","Fri, 8 Jul 2011 20:05:18 +0000","Fri, 8 Jul 2011 17:33:05 +0000",237241,"TAsyncChannel.h includes TTransportUtils.h, but doesn't really need/use it.",0.0,0.0,negative
thrift,1241,description,Patch is based mainly on but some more improvements: namespace can be specified with dots '.' like for every other language for example: namespace php com.onego.thrift would generate in php file namespace com\onego\thrift to generate php files with namespaces use: thrift --gen php:namespace53 example.thrift,code_debt,low_quality_code,"Wed, 20 Jul 2011 15:24:17 +0000","Wed, 10 Aug 2011 18:27:52 +0000","Thu, 4 Aug 2011 23:00:01 +0000",1323344,"Patch is based mainly on https://issues.apache.org/jira/browse/THRIFT-777, but some more improvements: namespace can be specified with dots '.' like for every other language for example: namespace php com.onego.thrift would generate in php file namespace com\onego\thrift to generate php files with namespaces use: thrift --gen php:namespace53 example.thrift",0.0,0.0,neutral
thrift,1275,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Thu, 4 Mar 2010 00:53:37 +0000 Subject: [PATCH 07/33] thrift: always prefix namespaces with "" ::"" Summary: Thrift always refers to namespaces using their full name. Therefore, it should prefix them with ""::"" to avoid accidentally matching a name defined in one of the current namespaces, rather than at the top-level. For example, if ServiceB is in namespace bar, and inherits from ServiceA in namespace foo, all code emitted for ServiceB now refers to ServiceA as ::foo::ServiceA instead of just foo::ServiceA. This allows the code to compile even if a namespace ::bar::foo also exists. An extra leading whitespace is also emitted, which is needed in cases when the name is used as the first template parameter, so that the emitted code contains ""< ::"" instead of ""<::"". Test Plan: jsong reported a build problem because of this name lookup error, and this change fixed his build. I also tested building [internal fb thing] and [internal fb thing], and they both built successfully. Tags: thrift  | 11 +++++++++-- 1 files changed, 9 insertions(+), 2 deletions(-)",code_debt,low_quality_code,"Thu, 18 Aug 2011 00:47:13 +0000","Fri, 19 Aug 2011 18:43:53 +0000","Fri, 19 Aug 2011 18:23:56 +0000",149803,"From d56203d414d23c7858a269e4aa547ee3164832fd Mon Sep 17 00:00:00 2001 From: Adam Simpkins <simpkins@fb.com> Date: Thu, 4 Mar 2010 00:53:37 +0000 Subject: [PATCH 07/33] thrift: always prefix namespaces with "" ::"" Summary: Thrift always refers to namespaces using their full name. Therefore, it should prefix them with ""::"" to avoid accidentally matching a name defined in one of the current namespaces, rather than at the top-level. For example, if ServiceB is in namespace bar, and inherits from ServiceA in namespace foo, all code emitted for ServiceB now refers to ServiceA as ::foo::ServiceA instead of just foo::ServiceA. This allows the code to compile even if a namespace ::bar::foo also exists. An extra leading whitespace is also emitted, which is needed in cases when the name is used as the first template parameter, so that the emitted code contains ""< ::"" instead of ""<::"". Test Plan: jsong reported a build problem because of this name lookup error, and this change fixed his build. I also tested building [internal fb thing] and [internal fb thing], and they both built successfully. Tags: thrift  compiler/cpp/src/generate/t_cpp_generator.cc | 11 +++++++++-- 1 files changed, 9 insertions, 2 deletions",-0.07966666667,-0.07081481481,neutral
thrift,1290,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Tue, 6 Apr 2010 20:43:23 +0000 Subject: [PATCH 17/33] thrift: TNonblockingServer: clean up state in the destructor Summary: Implement the TNonblockingServer destructor, so that it closes the listen socket, destroys the event_base, and deletes any TConnections left in the connectionStack_. However, TNonblockingServer doesn't keep track of active TConnection objects, so those objects are still leaked. As part of this, I also changed the code to use event_init() rather than event_base_new(). This way we won't set the global event_base inside libevent, and we can be sure that no one else will be using it after the TNonblockingServer is destroyed. I grepped through all of [fb code base] to check for any other direct uses of event_set(), and didn't see any places that weren't also using event_base_set(). Therefore it seems like it should be safe to stop initializing the global event_base pointer. Test Plan: Tested with the test code in [a fb unittest], which creates, stops, and then deletes several Ran it under valgrind, and now it only complains about any active connections being leaked. Revert Plan: OK  | 4 ++-- 1 files changed, 2 insertions(+), 2 deletions(-)",code_debt,low_quality_code,"Wed, 24 Aug 2011 16:52:01 +0000","Thu, 25 Aug 2011 17:47:08 +0000","Thu, 25 Aug 2011 17:29:00 +0000",88619,"From cd9c1a10cb4df058fbdbed1b98a21a7a7470a28c Mon Sep 17 00:00:00 2001 From: Adam Simpkins <simpkins@fb.com> Date: Tue, 6 Apr 2010 20:43:23 +0000 Subject: [PATCH 17/33] thrift: TNonblockingServer: clean up state in the destructor Summary: Implement the TNonblockingServer destructor, so that it closes the listen socket, destroys the event_base, and deletes any TConnections left in the connectionStack_. However, TNonblockingServer doesn't keep track of active TConnection objects, so those objects are still leaked. As part of this, I also changed the code to use event_init() rather than event_base_new(). This way we won't set the global event_base inside libevent, and we can be sure that no one else will be using it after the TNonblockingServer is destroyed. I grepped through all of [fb code base] to check for any other direct uses of event_set(), and didn't see any places that weren't also using event_base_set(). Therefore it seems like it should be safe to stop initializing the global event_base pointer. Test Plan: Tested with the test code in [a fb unittest], which creates, stops, and then deletes several TNonblockingServers. Ran it under valgrind, and now it only complains about any active connections being leaked. Revert Plan: OK  lib/cpp/src/server/TNonblockingServer.cpp | 4 ++-- 1 files changed, 2 insertions, 2 deletions",0.13910625,0.1168416667,neutral
thrift,1314,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Mon, 19 Apr 2010 19:09:16 +0000 Subject: [PATCH 25/33] thrift: add TProcessorFactory Summary: Adds a new TProcessorFactory class, and update all server classes to store a TProcessorFactory instead of a TProcessor. is called once for each newly accepted connection. This will allow a separate processor to be created for each connection, if desired. For now, everything always uses so all connections still end up using the same TProcessor. Some of the servers don't handle it well if getProcessor() throws an exception. TNonblockingServer may leak the fd and a TConnection object. TSimpleServer will exit and stop serving. However, this is no worse than the existing behavior if Test Plan: Ran the test code from [a fb unittest]. Revert Plan: OK Conflicts:  | 39 | 12 ++++++-- | 31 | 9 +++++- | 7 ++++- | 5 +++- 6 files changed, 89 insertions(+), 14 deletions(-)",code_debt,low_quality_code,"Wed, 31 Aug 2011 17:22:11 +0000","Thu, 1 Sep 2011 18:07:29 +0000","Thu, 1 Sep 2011 18:07:29 +0000",89118,"From a01b4ee026a6e0fa269af9f3f16684c4e312cd3c Mon Sep 17 00:00:00 2001 From: Adam Simpkins <simpkins@fb.com> Date: Mon, 19 Apr 2010 19:09:16 +0000 Subject: [PATCH 25/33] thrift: add TProcessorFactory Summary: Adds a new TProcessorFactory class, and update all server classes to store a TProcessorFactory instead of a TProcessor. TProcessorFactory::getProcessor() is called once for each newly accepted connection. This will allow a separate processor to be created for each connection, if desired. For now, everything always uses TSingletonProcessorFactory, so all connections still end up using the same TProcessor. Some of the servers don't handle it well if getProcessor() throws an exception. TNonblockingServer may leak the fd and a TConnection object. TSimpleServer will exit and stop serving. However, this is no worse than the existing behavior if eventHandler_->createContext() throws an exception. Test Plan: Ran the test code from [a fb unittest]. Revert Plan: OK Conflicts: lib/cpp/src/server/TSimpleServer.cpp  lib/cpp/src/TProcessor.h | 39 +++++++++++++++++++++++++++++ lib/cpp/src/server/TNonblockingServer.cpp | 12 ++++++-- lib/cpp/src/server/TServer.h | 31 +++++++++++++++++----- lib/cpp/src/server/TSimpleServer.cpp | 9 +++++- lib/cpp/src/server/TThreadPoolServer.cpp | 7 ++++- lib/cpp/src/server/TThreadedServer.cpp | 5 +++- 6 files changed, 89 insertions, 14 deletions",0.03822222222,0.02023529412,neutral
thrift,1393,description,"This is a very minor issue, but should be addressed nonetheless. The THttpClient class ensures the $uri_ property has a slash prefixed by appending one if needed in the constructor. However in THttpClient::read, there are 2 exceptions thrown where a slash is concatenated between the port and uri. This results in a superfluous slash in the TTransportException message. Example: ""THttpClient: Could not read 184549154 bytes from",code_debt,low_quality_code,"Tue, 18 Oct 2011 15:20:48 +0000","Tue, 18 Oct 2011 15:41:56 +0000","Tue, 18 Oct 2011 15:32:07 +0000",679,"This is a very minor issue, but should be addressed nonetheless. The THttpClient class ensures the $uri_ property has a slash prefixed by appending one if needed in the constructor. However in THttpClient::read, there are 2 exceptions thrown where a slash is concatenated between the port and uri. This results in a superfluous slash in the TTransportException message. Example: ""THttpClient: Could not read 184549154 bytes from xxx.yyy.com:80//randomService""",-0.14,-0.1,neutral
thrift,1480,description,"The python library files have some inconsistencies (different indent levels and docstring placement) and the pep8 linter produces dozens of warnings. There are also several places where tabs are used instead of spaces, which is not good. This patch addresses almost all of the pep8 issues with as little modification of the code as possible. This patch: * converts 3 instances of tabs into the correct number of spaces * removes unnecessary trailing semicolons and backslashes * changes None comparisons to be identity based, 'x != None' becomes 'x is not None' in a handful of places * removes unnecessary '== True' in one if statement * wraps lines at 80 characters and removes trailing whitespace * corrects a handful of grammar problems in docstrings (mostly to help with 80 char line wrap) * converts all the docstrings to use """""" (instead of ''' or "") and makes placement consistent * fixes pep8 warnings about missing spaces around operators, e.g. (a-b) becomes (a - b) * adjusts ordering of stdlib imports to be alphabetical (could be better still) * correct internal indent depths of methods when they switch from 2 to 4 spaces There's a mix of files that use 4-spaces for indentation, versus the majority which use 2-spaces for indentation. This patch doesn't change that. I wanted to get the code as pep8 clean as possible and touch as few lines as possible to get it there. The TType constants defined in Thrift.py have some nice vertical whitespace that isn't pep8-happy, but it looked too clean to touch so I left it unchanged. After this patch, the pep8 utility only reports two warnings: # ""indentation is not a multiple of four"" for most files (no biggie) # ""multiple spaces before operator"" in Thrift.py for the TTypes class constants The unit tests all pass with this patch.",code_debt,low_quality_code,"Sat, 31 Dec 2011 19:26:52 +0000","Tue, 3 Jan 2012 17:56:04 +0000","Tue, 3 Jan 2012 17:33:07 +0000",252375,"The python library files have some inconsistencies (different indent levels and docstring placement) and the pep8 linter produces dozens of warnings. There are also several places where tabs are used instead of spaces, which is not good. This patch addresses almost all of the pep8 issues with as little modification of the code as possible. This patch: converts 3 instances of tabs into the correct number of spaces removes unnecessary trailing semicolons and backslashes changes None comparisons to be identity based, 'x != None' becomes 'x is not None' in a handful of places removes unnecessary '== True' in one if statement wraps lines at 80 characters and removes trailing whitespace corrects a handful of grammar problems in docstrings (mostly to help with 80 char line wrap) converts all the docstrings to use """""" (instead of ''' or "") and makes placement consistent fixes pep8 warnings about missing spaces around operators, e.g. (a-b) becomes (a - b) adjusts ordering of stdlib imports to be alphabetical (could be better still) correct internal indent depths of methods when they switch from 2 to 4 spaces There's a mix of files that use 4-spaces for indentation, versus the majority which use 2-spaces for indentation. This patch doesn't change that. I wanted to get the code as pep8 clean as possible and touch as few lines as possible to get it there. The TType constants defined in Thrift.py have some nice vertical whitespace that isn't pep8-happy, but it looked too clean to touch so I left it unchanged. After this patch, the pep8 utility only reports two warnings: ""indentation is not a multiple of four"" for most files (no biggie) ""multiple spaces before operator"" in Thrift.py for the TTypes class constants The unit tests all pass with this patch.",0.03835227273,0.03835227273,negative
thrift,1583,description,The generated code has memory leaks if you create and destroy an object. I've fixed the main problem on this git repo on branch c_glib_0.8.x but commits can be moved to trunk easily. There are still some errors to control on generated code (there are comments in the code). I'm planning to fix in a short term if you validate the current patches. This git repository has also the patches sent on issues 1582 and 1578 that are also related:,code_debt,low_quality_code,"Fri, 20 Apr 2012 14:56:38 +0000","Fri, 11 May 2012 00:41:39 +0000","Sat, 28 Apr 2012 11:35:10 +0000",679112,The generated code has memory leaks if you create and destroy an object. I've fixed the main problem on this git repo on branch c_glib_0.8.x but commits can be moved to trunk easily. https://github.com/jcaden/thrift There are still some errors to control on generated code (there are comments in the code). I'm planning to fix in a short term if you validate the current patches. This git repository has also the patches sent on issues 1582 and 1578 that are also related: https://issues.apache.org/jira/browse/THRIFT-1582 https://issues.apache.org/jira/browse/THRIFT-1578,-0.10675,-0.10675,neutral
thrift,1688,description,The markup on the IDL page needs to be updated. It's pretty hard to read at the moment.,code_debt,low_quality_code,"Sat, 8 Sep 2012 14:46:41 +0000","Fri, 28 Sep 2012 04:02:30 +0000","Fri, 28 Sep 2012 04:02:30 +0000",1689349,The markup on the IDL page needs to be updated. It's pretty hard to read at the moment.,0.0,0.0,negative
thrift,1813,description,Why? # A lot of static analysis tools understand the annotation and treat those classes differently. # Setting the date field with the date & comments field with the version of the thrift compiler might prove to be valuable for tracing the origins of the generated file. The overheads should be minimal as the retention policy of the annotation is purely at a source level,code_debt,low_quality_code,"Fri, 28 Dec 2012 15:28:08 +0000","Wed, 17 Jun 2015 08:01:22 +0000","Sat, 9 Nov 2013 18:43:06 +0000",27314098,Why? A lot of static analysis tools understand the javax.annotation.Generated annotation and treat those classes differently. Setting the date field with the date & comments field with the version of the thrift compiler might prove to be valuable for tracing the origins of the generated file. The overheads should be minimal as the retention policy of the annotation is purely at a source level,-0.04675,-0.03116666667,neutral
thrift,1873,description,"The binary protocol factory takes strict read/write flags as (optional) arguments, but did in fact never use them.",code_debt,dead_code,"Mon, 4 Mar 2013 22:56:22 +0000","Sat, 8 Jun 2013 03:15:40 +0000","Sat, 8 Jun 2013 03:15:40 +0000",8223558,"The binary protocol factory takes strict read/write flags as (optional) arguments, but did in fact never use them.",0.0,0.0,neutral
thrift,1886,description,The virtual method implementations and *_get/set_property functions are not part of the public API and should be static.,code_debt,low_quality_code,"Thu, 14 Mar 2013 08:19:25 +0000","Sat, 8 Jun 2013 18:35:46 +0000","Sat, 8 Jun 2013 18:35:46 +0000",7467381,The virtual method implementations and *_get/set_property functions are not part of the public API and should be static.,0.875,0.875,negative
thrift,18,description,"gcc 4.2 shows a huge amount of warnings ""warning: deprecated conversion from string constant to 'char*'""",code_debt,low_quality_code,"Wed, 21 May 2008 14:50:42 +0000","Tue, 1 Nov 2011 02:54:27 +0000","Tue, 27 May 2008 02:07:45 +0000",472623,"gcc 4.2 shows a huge amount of warnings ""warning: deprecated conversion from string constant to 'char*'""",-0.2333333333,-0.2333333333,negative
thrift,1932,description,"The Compilation of thrift-0.9.0 ended with the following warnings: * QA Notice: Package triggers severe warnings which indicate that it * may exhibit random runtime failures. * warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing] * warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing] When looking into ... 711 = 712 713 if == 4) { 714 // 0 length event indicates padding 715 if (*((uint32_t == 0) { 716 // T_DEBUG_L(1, ""Got padding""); 717 718 continue; 719 } ... it becomes obvious that the four values casted into a pointer were previously read from the input stream by ::read() in line 661: 661 = ::read(fd_, readBuff_, readBuffSize_); Same issue here: 725 readState_.event_ = new eventInfo(); 726 While it is a two minute job fix this problem, the method looks so fragile that a clean rewrite appears to be more appropriate.",code_debt,low_quality_code,"Thu, 18 Apr 2013 16:10:01 +0000","Mon, 20 May 2013 03:13:13 +0000","Sun, 5 May 2013 21:38:30 +0000",1488509,"The Compilation of thrift-0.9.0 ended with the following warnings: QA Notice: Package triggers severe warnings which indicate that it may exhibit random runtime failures. src/thrift/transport/TFileTransport.cpp:715:56: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing] src/thrift/transport/TFileTransport.cpp:726:84: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing] When looking into src/thrift/transport/TFileTransport.cpp:715 ... 711 readState_.eventSizeBuff_[readState_.eventSizeBuffPos_++] = 712 readBuff_[readState_.bufferPtr_++]; 713 if (readState_.eventSizeBuffPos_ == 4) { 714 // 0 length event indicates padding 715 if (*((uint32_t *)(readState_.eventSizeBuff_)) == 0) { 716 // T_DEBUG_L(1, ""Got padding""); 717 readState_.resetState(readState_.lastDispatchPtr_); 718 continue; 719 } ... it becomes obvious that the four values casted into a pointer were previously read from the input stream by ::read() in line 661: 661 readState_.bufferLen_ = ::read(fd_, readBuff_, readBuffSize_); Same issue here: 725 readState_.event_ = new eventInfo(); 726 readState_.event_->eventSize_ = *((uint32_t*)(readState_.eventSizeBuff_)); 727 While it is a two minute job fix this problem, the method TFileTransport::readEvent() looks so fragile that a clean rewrite appears to be more appropriate.",-0.17,-0.08055555556,negative
thrift,1982,description,"vsnprintf on Windows (visual) return -1 on failure instead of amount of bytes needed. so line like this: int need = STACK_BUF_SIZE, message, ap); won't work. Solution: ugly conditional compilation which will use microsoft specific functions... See patch for details.",code_debt,low_quality_code,"Wed, 29 May 2013 17:29:07 +0000","Wed, 5 Jun 2013 03:11:00 +0000","Tue, 4 Jun 2013 20:27:24 +0000",529097,"vsnprintf on Windows (visual) return -1 on failure instead of amount of bytes needed. so line like this: int need = vsnprintf(stack_buf, STACK_BUF_SIZE, message, ap); won't work. Solution: ugly conditional compilation which will use microsoft specific functions... See patch for details.",-0.4666666667,-0.4666666667,negative
thrift,1999,description,"""unused arguments"" warning. suppressed by (void) x; in patch",code_debt,low_quality_code,"Wed, 5 Jun 2013 15:30:17 +0000","Thu, 10 Jul 2014 13:42:22 +0000","Thu, 10 Jul 2014 13:42:22 +0000",34553525,"""unused arguments"" warning. suppressed by (void) x; in patch",-0.3,-0.3,negative
thrift,2017,description,"In file class t_program : public t_doc { 59 public: 60 path, std::string name) : 61 path_(path), 62 name_(name), 63 out_path_(""./""), 64 { 65 scope_ = new t_scope(); 66 } 67 68 path) : 69 path_(path), 70 out_path_(""./""), 71 { 72 name_ = program_name(path); 73 scope_ = new t_scope(); 74 } In Above code at line number 65 and 73 Resource leaks happens as 1. Allocating memory by calling ""new t_scope"". 2. Assigning: ""this-3. The constructor allocates field ""scope_"" of ""t_program"" but there is no destructor in the code. destructor should deallocate memroy for t_scope. which sometimes causes Resource Leak. Possible patch: There should be destructor to release the resources allocated dynamically. ~t_program() { delete scope_; scope_ = NULL; }",code_debt,low_quality_code,"Thu, 13 Jun 2013 12:45:44 +0000","Mon, 19 Aug 2013 00:31:54 +0000","Sun, 4 Aug 2013 12:21:09 +0000",4491325,"In file compiler/cpp/src/parse/t_program.h class t_program : public t_doc { 59 public: 60 t_program(std::string path, std::string name) : 61 path_(path), 62 name_(name), 63 out_path_(""./""), 64 out_path_is_absolute_(false) { 65 scope_ = new t_scope(); 66 } 67 68 t_program(std::string path) : 69 path_(path), 70 out_path_(""./""), 71 out_path_is_absolute_(false) { 72 name_ = program_name(path); 73 scope_ = new t_scope(); 74 } In Above code at line number 65 and 73 Resource leaks happens as 1. Allocating memory by calling ""new t_scope"". 2. Assigning: ""this->scope_"" = ""new t_scope"". 3. The constructor allocates field ""scope_"" of ""t_program"" but there is no destructor in the code. destructor should deallocate memroy for t_scope. which sometimes causes Resource Leak. Possible patch: There should be destructor to release the resources allocated dynamically. ~t_program() { delete scope_; scope_ = NULL; }",-0.01818181818,-0.04615384615,neutral
thrift,2020,description,"A lot of windows specific files have been ""removed"" recently, but git clone still creates empty versions of those files.",code_debt,low_quality_code,"Thu, 13 Jun 2013 13:55:00 +0000","Fri, 12 Aug 2016 01:29:47 +0000","Fri, 8 Apr 2016 05:36:07 +0000",88962067,"A lot of windows specific files have been ""removed"" recently, but git clone still creates empty versions of those files.",0.0405,0.0405,neutral
thrift,2021,description,"Currently, TBinaryProtocol reads into a temporary buffer, then copies the results into the resulting std::string. For large strings (like a megabyte), the copy can be a performance issue. The attached patch reads directly into the std::string instead.",code_debt,slow_algorithm,"Thu, 13 Jun 2013 13:59:34 +0000","Fri, 12 Aug 2016 01:29:52 +0000","Fri, 8 Apr 2016 05:36:08 +0000",88961794,"Currently, TBinaryProtocol reads into a temporary buffer, then copies the results into the resulting std::string. For large strings (like a megabyte), the copy can be a performance issue. The attached patch reads directly into the std::string instead.",0.0,0.0,neutral
thrift,2028,description,"The current threading implementations have some minor annoyances: All do some amount of internal state tracking with an enum that ends up being overkill. All use weak_ptrs to help manage lifetimes, instead of the base class that was designed for exactly this purpose. All of the specific thread factories implement ""detached"" methods, but the base thread factory doesn't have virtual methods exposing the detached methods. The thread manager has an unused local. Adding a ""UniqueGuard"" class to Mutex.h, to give more flexible RAII management to locks. Currently no clients of this, but I have some patches that will eventually use this.",code_debt,low_quality_code,"Thu, 13 Jun 2013 14:42:57 +0000","Wed, 6 Jan 2016 02:06:04 +0000","Sat, 10 Oct 2015 23:22:11 +0000",73384754,"The current threading implementations have some minor annoyances: All do some amount of internal state tracking with an enum that ends up being overkill. All use weak_ptrs to help manage lifetimes, instead of the enable_shared_from_this base class that was designed for exactly this purpose. All of the specific thread factories implement ""detached"" methods, but the base thread factory doesn't have virtual methods exposing the detached methods. The thread manager has an unused local. Adding a ""UniqueGuard"" class to Mutex.h, to give more flexible RAII management to locks. Currently no clients of this, but I have some patches that will eventually use this.",0.009523809524,0.03452380952,negative
thrift,211,description,"Add a client option that causes clients to monitor their creators and terminate when the creator dies. This makes it possible to prevent client leaks without linking, because the latter causes application code to be killed when a transport error occurs and exits are not trapped.",code_debt,low_quality_code,"Thu, 20 Nov 2008 22:30:30 +0000","Tue, 1 Nov 2011 02:52:21 +0000","Thu, 4 Jun 2009 02:01:37 +0000",16860667,"http://gitweb.thrift-rpc.org/?p=thrift.git;a=log;h=refs/heads/pri/dreiss/erl-tether;hb=HEAD Add a client option that causes clients to monitor their creators and terminate when the creator dies. This makes it possible to prevent client leaks without linking, because the latter causes application code to be killed when a transport error occurs and exits are not trapped.",-0.1,-0.1,neutral
thrift,2246,description,"if enum is not set, ToString() returns a first enum value, but it should indicate that field is empty enum Distance { DISTANCE_1 = 0, DISTANCE_2 = 1, } struct RaceDetails { 1: optional Distance distance, } c#: new result: expected: )""",code_debt,low_quality_code,"Tue, 29 Oct 2013 10:12:32 +0000","Mon, 11 Nov 2013 20:41:33 +0000","Mon, 4 Nov 2013 23:52:38 +0000",567606,"if enum is not set, ToString() returns a first enum value, but it should indicate that field is empty enum Distance { DISTANCE_1 = 0, DISTANCE_2 = 1, } struct RaceDetails { 1: optional Distance distance, } c#: new RaceDetails().ToString() result: ""RaceDetails(Distance:DISTANCE_1)"" expected: ""RaceDetails(Distance: )""",-0.2,0.2,neutral
thrift,241,description,"Having the repr return the repr of __dict__ is confusing, because the object is not a dict and does not act (much) like one. 100% of the 3 python developers I have seen who are new to thrift were confused at first why the repr looked like a dict but obj[attributename] raised KeyError. Additionally, this violates the repr guideline that where possible eval(repr(obj)) == obj. Finally, specifying a __str__ equal to __repr__ is redundant, since str() will use __repr__ if no __str__ is given. Here is a patch: $ diff -u compiler/cpp/sr\  2008-12-24 16:36:54.000000000 +0000 +++ 2008-12-24 16:49:54.000000000 +0000 @@ -614,11 +614,10 @@ // Printing utilities so that on the command line thrift // structs look pretty like dictionaries out << - indent() << ""def __str__(self):"" << endl << - indent() << "" return str(self.__dict__)"" << endl << - endl << indent() << ""def __repr__(self):"" << endl << - indent() << "" return << endl << + indent() << "" L = ['%s=%r' % (key, value)"" << endl << + indent() << "" for key, value in << endl << + indent() << "" return '%s(%s)' % ', '.join(L))"" << endl << endl; // Equality and inequality methods that compare by value",code_debt,low_quality_code,"Wed, 24 Dec 2008 16:53:54 +0000","Thu, 15 Jan 2009 22:15:34 +0000","Mon, 5 Jan 2009 20:19:05 +0000",1049111,"Having the repr return the repr of _dict_ is confusing, because the object is not a dict and does not act (much) like one. 100% of the 3 python developers I have seen who are new to thrift were confused at first why the repr looked like a dict but obj[attributename] raised KeyError. Additionally, this violates the repr guideline that where possible eval(repr(obj)) == obj. Finally, specifying a _str_ equal to _repr_ is redundant, since str() will use _repr_ if no _str_ is given. Here is a patch: $ diff -u compiler/cpp/src/generate/t_py_generator.cc.old compiler/cpp/sr\ c/generate/t_py_generator.cc  compiler/cpp/src/generate/t_py_generator.cc.old 2008-12-24 16:36:54.000000000 +0000 +++ compiler/cpp/src/generate/t_py_generator.cc 2008-12-24 16:49:54.000000000 +0000 @@ -614,11 +614,10 @@ // Printing utilities so that on the command line thrift // structs look pretty like dictionaries out << indent() << ""def _str_(self):"" << endl << indent() << "" return str(self._dict_)"" << endl << endl << indent() << ""def _repr_(self):"" << endl << indent() << "" return repr(self._dict_)"" << endl << + indent() << "" L = ['%s=%r' % (key, value)"" << endl << + indent() << "" for key, value in self._dict_.iteritems()]"" << endl << + indent() << "" return '%s(%s)' % (self._class.name_, ', '.join(L))"" << endl << endl; // Equality and inequality methods that compare by value",-0.1427857143,-0.06108333333,negative
thrift,2561,description,"When an enumerated type is translated to objective-c, the type is replaced with simply ""int"". This takes away half the use of an enumerated type as a type. For instance, it is both clearer and a more precise translation to see ""PersonID"" as a field type in a method than ""int"". However, the objective-c compiler does not typedef the enum and as a result essentially forgets the type that is declared in the thrift IDL.",code_debt,low_quality_code,"Fri, 30 May 2014 16:45:06 +0000","Tue, 5 Dec 2017 17:29:58 +0000","Thu, 19 Jan 2017 21:03:14 +0000",83391488,"When an enumerated type is translated to objective-c, the type is replaced with simply ""int"". This takes away half the use of an enumerated type as a type. For instance, it is both clearer and a more precise translation to see ""PersonID"" as a field type in a method than ""int"". However, the objective-c compiler does not typedef the enum and as a result essentially forgets the type that is declared in the thrift IDL.",0.20325,0.20325,neutral
thrift,275,description,"There are a lot of weirdly named files in the Ruby Thrift library in order for backwards compatibility to work. I think this is of dubious value, and is really confusing. The deprecation code is also very complicated. I think we should just get rid of the deprecated stuff. We're pre-version 1.0, so better to get all the big changes in now.",code_debt,dead_code,"Sat, 17 Jan 2009 00:44:02 +0000","Tue, 1 Nov 2011 02:54:12 +0000","Tue, 24 Mar 2009 05:31:09 +0000",5719627,"There are a lot of weirdly named files in the Ruby Thrift library in order for backwards compatibility to work. I think this is of dubious value, and is really confusing. The deprecation code is also very complicated. I think we should just get rid of the backwards-compatibility deprecated stuff. We're pre-version 1.0, so better to get all the big changes in now.",-0.0037,-0.0037,negative
thrift,2768,description,"Of course I had to make some content changes. The C# patch file is 354 KB, a lot of files are considered completely replaced. The Delphi patch file has ""only"" 282 KB. The ""content"" changes are (1) one indentation alignment in a batch file, and (2) one missing ASF header in a .cs file which I added. Anything else is just TABS, SPACEs and CRs. I'm going to commit that. For the records, here are the patch files.",code_debt,low_quality_code,"Fri, 3 Oct 2014 18:07:55 +0000","Sun, 9 Nov 2014 01:36:40 +0000","Thu, 9 Oct 2014 19:18:26 +0000",522631,"Of course I had to make some content changes. The C# patch file is 354 KB, a lot of files are considered completely replaced. The Delphi patch file has ""only"" 282 KB. The ""content"" changes are (1) one indentation alignment in a batch file, and (2) one missing ASF header in a .cs file which I added. Anything else is just TABS, SPACEs and CRs. I'm going to commit that. For the records, here are the patch files.",0.15625,0.15625,neutral
thrift,277,description,"It's really an abstract method, so not overriding it is an error. We should throw an error when someone calls it inappropriately. The same can probably be said for #write. #open and #open? might also want to change, though that's not quite as certain.",code_debt,low_quality_code,"Sat, 17 Jan 2009 00:48:32 +0000","Tue, 1 Nov 2011 02:53:49 +0000","Wed, 18 Mar 2009 02:41:15 +0000",5190763,"It's really an abstract method, so not overriding it is an error. We should throw an error when someone calls it inappropriately. The same can probably be said for #write. #open and #open? might also want to change, though that's not quite as certain.",-0.0625,-0.0625,negative
thrift,278,description,"If you have an enum field, and its value isn't in the VALID_VALUES of your enum, the exception thrown says that it's not found, but it doesn't say what the value is.",code_debt,low_quality_code,"Tue, 20 Jan 2009 00:52:35 +0000","Tue, 1 Nov 2011 02:54:22 +0000","Wed, 18 Mar 2009 01:51:07 +0000",4928312,"If you have an enum field, and its value isn't in the VALID_VALUES of your enum, the exception thrown says that it's not found, but it doesn't say what the value is.",0.0,0.0,neutral
thrift,2791,description,"There is currently no way in a go server to use buffered sockets. Failing to do so decreases performance significantly in my tests. I added an option on TServerSocket to set the buffer size to use. This will default to 1024 bytes, but can be disabled if desired to get back to the original behavior by setting BufferSize to 0. Github pull request: Patch",code_debt,slow_algorithm,"Fri, 24 Oct 2014 16:50:39 +0000","Mon, 10 Nov 2014 23:14:30 +0000","Wed, 29 Oct 2014 17:58:42 +0000",436083,"There is currently no way in a go server to use buffered sockets. Failing to do so decreases performance significantly in my tests. I added an option on TServerSocket to set the buffer size to use. This will default to 1024 bytes, but can be disabled if desired to get back to the original behavior by setting BufferSize to 0. Github pull request: https://github.com/apache/thrift/pull/249 Patch https://github.com/apache/thrift/pull/249.patch",-0.19,-0.19,negative
thrift,2851,description,"I've been seeing this public Peek() function in the GO library for a while, but still cannot figure out any sense to it. If there are useless, we can remove them right? This PR removes the public Peek() from the implemented transports. All tests still pass. PR:",code_debt,dead_code,"Mon, 24 Nov 2014 18:49:14 +0000","Mon, 8 Dec 2014 20:35:00 +0000","Tue, 25 Nov 2014 20:48:59 +0000",93585,"I've been seeing this public Peek() function in the GO library for a while, but still cannot figure out any sense to it. If there are useless, we can remove them right? This PR removes the public Peek() from the implemented transports. All tests still pass. PR: https://github.com/apache/thrift/pull/283",0.147,0.147,negative
thrift,2868,description,The Go client doesn't do proper error checking. E.g. it doesn't check whether the received method name is correct nor if the message type has the expected value. The following PR enhances the Go client error handling by the following: - Check if method name is correct - - Check if MessageType is thrift.REPLY or EXCEPTION - - Checking the sequence id is done before checking the message type Includes test cases for every error case.,code_debt,low_quality_code,"Mon, 1 Dec 2014 20:32:30 +0000","Mon, 29 Dec 2014 21:36:34 +0000","Mon, 8 Dec 2014 20:52:23 +0000",605993,The Go client doesn't do proper error checking. E.g. it doesn't check whether the received method name is correct nor if the message type has the expected value. The following PR enhances the Go client error handling by the following: Check if method name is correct -> if not return thrift.WRONG_METHOD_NAME Check if MessageType is thrift.REPLY or EXCEPTION -> if not return thrift.INVALID_MESSAGE_TYPE_EXCEPTION Checking the sequence id is done before checking the message type Includes test cases for every error case. https://github.com/apache/thrift/pull/297,-0.20675,-0.1538571429,negative
thrift,2874,description,string_buf_ and string_buf_size_ are never used. Removing these will also resolve THRIFT-2465.,code_debt,dead_code,"Wed, 3 Dec 2014 17:21:17 +0000","Mon, 26 Jan 2015 02:26:32 +0000","Thu, 4 Dec 2014 21:47:24 +0000",102367,string_buf_ and string_buf_size_ are never used. Removing these will also resolve THRIFT-2465.,0.2,0.2,neutral
thrift,2907,description,The {{ntohll}} macro is already defined in on Mac OS X.,code_debt,low_quality_code,"Fri, 19 Dec 2014 23:32:04 +0000","Mon, 26 Jan 2015 02:26:40 +0000","Sat, 20 Dec 2014 12:31:19 +0000",46755,The ntohll macro is already defined in /usr/include/sys/_endian.h on Mac OS X.,0.5,0.25,neutral
thrift,2972,description,It seems that I failed to escape a line ending in THRIFT-2910 fix. For some reason it doesn't break {{make check}} right now but only causes a bootstrap (or configure ?) warning and processor_test executable is missing because of this.,code_debt,low_quality_code,"Sat, 31 Jan 2015 16:21:16 +0000","Tue, 21 Jul 2015 02:21:08 +0000","Mon, 2 Feb 2015 21:05:26 +0000",189850,It seems that I failed to escape a line ending in THRIFT-2910 fix. For some reason it doesn't break make check right now but only causes a bootstrap (or configure ?) warning and processor_test executable is missing because of this.,-0.2506666667,-0.2506666667,negative
thrift,3045,description,Java generated code is adding two suppress warnings as follows: The java_suppressions() seems to be added in 0.9.2 which is right. But there is a default getting added. One of it needs to be turned off.,code_debt,low_quality_code,"Wed, 18 Mar 2015 05:52:07 +0000","Fri, 5 Jun 2015 19:54:44 +0000","Fri, 5 Jun 2015 19:54:44 +0000",6876157,"Java generated code is adding two suppress warnings as follows: The java_suppressions() seems to be added in 0.9.2 which is right. But there is a default SuppressWarnings(""all"") getting added. One of it needs to be turned off.",-0.5121666667,-0.5121666667,negative
thrift,3047,description,"indent_down was called one extra time which gave us an indentation level of -1, then to combat this, we indented the server implementation an extra level. This appeared as if everything was fine unless you generated two services in one thrift file, in which case the indentation would get progressively worse.",code_debt,low_quality_code,"Thu, 19 Mar 2015 05:03:56 +0000","Thu, 30 Apr 2015 18:57:11 +0000","Thu, 16 Apr 2015 20:15:46 +0000",2473910,"indent_down was called one extra time which gave us an indentation level of -1, then to combat this, we indented the server implementation an extra level. This appeared as if everything was fine unless you generated two services in one thrift file, in which case the indentation would get progressively worse.",-0.27925,-0.27925,negative
thrift,3088,description,"Start TThreadPoolServer to server with as transportFactory. While using nc to test the specified port whether reachable, it will leak CLOSE_WAIT socket.That's because nc will close socket at once while successful connect TThreadPoolServer, but the server still try using sasl protocol to build an inputTransport which of course failed at once. However inputTransport is null which makes it can't close socket properly which lead to CLOSE_WAIT socket.",code_debt,low_quality_code,"Thu, 9 Apr 2015 07:18:24 +0000","Wed, 5 Apr 2017 17:17:46 +0000","Sun, 12 Apr 2015 15:47:31 +0000",289747,"Start TThreadPoolServer to server with TSaslServerTransport.Factory as transportFactory. While using nc to test the specified port whether reachable, it will leak CLOSE_WAIT socket.That's because nc will close socket at once while successful connect TThreadPoolServer, but the server still try using sasl protocol to build an inputTransport which of course failed at once. However inputTransport is null which makes it can't close socket properly which lead to CLOSE_WAIT socket.",0.09008333333,0.07206666667,neutral
thrift,3114,description,"Should prefix the field deserialization statements with ""local"" whenever the output is a temporary variable, for example this compiler output: _elem46 = iprot:readI32() should be changed to: local _elem46 = iprot:readI32().",code_debt,low_quality_code,"Sat, 25 Apr 2015 19:10:46 +0000","Sat, 20 Feb 2021 15:26:47 +0000","Sun, 26 Apr 2015 15:49:16 +0000",74310,"Should prefix the field deserialization statements with ""local"" whenever the output is a temporary variable, for example this compiler output: _elem46 = iprot:readI32() should be changed to: local _elem46 = iprot:readI32().",0.0,0.0,neutral
thrift,3140,description,"After running *make check* for JS or *ant test* in {{lib/js/test}}, you can always find several stack traces in I don't know if it's even a bug since it does not cause any test failures. It distracted me from solving real cause of failures for a while through. Cause: Threading issue. Synchronization is needed because all instances of MimeUtil2 seems to share single state.",code_debt,multi-thread_correctness,"Sun, 10 May 2015 15:01:09 +0000","Tue, 21 Jul 2015 02:21:11 +0000","Sun, 10 May 2015 16:31:11 +0000",5402,"After running make check for JS or ant test in lib/js/test, you can always find several ConcurrentModificationException stack traces in lib/js/test/build/log/unittest.log. I don't know if it's even a bug since it does not cause any test failures. It distracted me from solving real cause of failures for a while through. Cause: Threading issue. Synchronization is needed because all instances of MimeUtil2 seems to share single state. http://sourceforge.net/p/mime-util/bugs/33/",0.1,0.05714285714,negative
thrift,3197,description,"While creating ThreadPoolExecutor in TThreadPoolServer, keepAliveTime is hard coded as 60 sec. It should be",code_debt,low_quality_code,"Mon, 22 Jun 2015 11:14:41 +0000","Wed, 24 Jun 2015 14:14:41 +0000","Wed, 24 Jun 2015 13:21:16 +0000",180395,"While creating ThreadPoolExecutor in TThreadPoolServer, keepAliveTime is hard coded as 60 sec. It should be ""args.stopTimeoutVal""",-0.1,-0.06666666667,neutral
thrift,3283,description,"When terminating the C (GLib) tutorial server with Ctrl-C, this message is output to the console: The server should instead exit quietly without reporting an issue.",code_debt,low_quality_code,"Fri, 31 Jul 2015 11:48:46 +0000","Tue, 25 Aug 2015 04:44:51 +0000","Fri, 31 Jul 2015 21:26:09 +0000",34643,"When terminating the C (GLib) tutorial server with Ctrl-C, this message is output to the console: The server should instead exit quietly without reporting an issue.",0.375,0.375,neutral
thrift,3364,description,"Ruby JSON protocol uses pack('m') method to encode Base64 string. It seems that it inserts a ""\n"" character every 60 characters. You can refer to these pages for this behavior. This has been making it impossible to send long binary field data to other languages. I fixed this by using alternative encode method that is added in Ruby 1.9 (which should be OK). After the fix, I had to add Ruby namespace to to avoid name collision of ""Base64"" symbols that is used for new encode method and also as DebugProtoTest message name. I also removed extraneous double quote in encoded binary fields that resulted in invalid JSON.",code_debt,low_quality_code,"Thu, 1 Oct 2015 16:06:11 +0000","Fri, 18 Mar 2016 18:02:06 +0000","Fri, 18 Mar 2016 18:02:06 +0000",14608555,"Ruby JSON protocol uses pack('m') method to encode Base64 string. It seems that it inserts a ""\n"" character every 60 characters. You can refer to these pages for this behavior. http://stackoverflow.com/questions/2620975/strange-n-in-base64-encoded-string-in-ruby http://ruby-doc.org/stdlib-2.2.3/libdoc/base64/rdoc/Base64.html Line feeds are added to every 60 encoded characters. This has been making it impossible to send long binary field data to other languages. I fixed this by using alternative encode method that is added in Ruby 1.9 (which should be OK). After the fix, I had to add Ruby namespace to DebugProtoTest.thrift to avoid name collision of ""Base64"" symbols that is used for new encode method and also as DebugProtoTest message name. I also removed extraneous double quote in encoded binary fields that resulted in invalid JSON.",0.06428571429,0.05,negative
thrift,3391,description,The go test server printf()s bools with wrong formatting: while this is expected:,code_debt,low_quality_code,"Fri, 16 Oct 2015 19:32:55 +0000","Fri, 18 Mar 2016 18:02:04 +0000","Fri, 18 Mar 2016 18:02:04 +0000",13300149,The go test server printf()s bools with wrong formatting: while this is expected:,0.125,0.125,negative
thrift,3409,description,There's at least 3 problems in NodeJS binary field. # API are inconsistent across binary/comact/JSON protocols # compact/JSON wire format is imcompatible with other languages (JSON : THRIFT-3200) # size of compact wire format is 2x of the original binary I propose following changes to fix this # Change JSON protocol return value from string to Buffer (same as binary/compact) # Change JSON protocol wire format to Base64 (same as other languages) # Change compact protocol wire format to plain binary (same as other languages),code_debt,low_quality_code,"Tue, 3 Nov 2015 16:08:13 +0000","Tue, 18 Dec 2018 16:43:00 +0000","Fri, 18 Mar 2016 17:54:32 +0000",11756779,There's at least 3 problems in NodeJS binary field. API are inconsistent across binary/comact/JSON protocols compact/JSON wire format is imcompatible with other languages (JSON : THRIFT-3200) size of compact wire format is 2x of the original binary I propose following changes to fix this Change JSON protocol return value from string to Buffer (same as binary/compact) Change JSON protocol wire format to Base64 (same as other languages) Change compact protocol wire format to plain binary (same as other languages),-0.575,-0.575,negative
thrift,3416,description,"While poking into the IDL syntax, I found that hidden gem: While thinking whether I should do sth about it, I came to the conclusion that the time ist right to ""_Get rid of this_"" since today probably really ""_everyone is using the new hotness_"", as the comments say. Opinions? If there are no objections, I'll provide a patch to convert the warnings into an error and remove the rest of it.",code_debt,low_quality_code,"Wed, 11 Nov 2015 20:42:20 +0000","Fri, 18 Mar 2016 17:54:27 +0000","Fri, 18 Mar 2016 17:54:27 +0000",11049127,"While poking into the IDL syntax, I found that hidden gem: While thinking whether I should do sth about it, I came to the conclusion that the time ist right to ""Get rid of this"" since today probably really ""everyone is using the new hotness"", as the comments say. Opinions? If there are no objections, I'll provide a patch to convert the warnings into an error and remove the rest of it.",0.01411111111,0.01411111111,neutral
thrift,3495,description,"h4. fixes * fix python mapmap test * fix problematic regex compilation in connection retry logic in cross test runner (was compiling already-compiled regex object) h4. enhancement * add some C++ edage-case tests (unicode, double values, empty collections) * flush C++ client stdout so that we have better diagnostics when it crashed/hanged * add go binary test * add ruby unicode string test",code_debt,low_quality_code,"Sun, 20 Dec 2015 12:52:23 +0000","Wed, 6 Jan 2016 02:06:10 +0000","Wed, 23 Dec 2015 17:48:48 +0000",276985,"fixes fix python mapmap test fix problematic regex compilation in connection retry logic in cross test runner (was compiling already-compiled regex object) enhancement add some C++ edage-case tests (unicode, double values, empty collections) flush C++ client stdout so that we have better diagnostics when it crashed/hanged add go binary test add ruby unicode string test",-0.079,0.1476,neutral
thrift,353,description,"service generator needs to capitalize service names, since services are ruby modules and ruby modules are constants.",code_debt,low_quality_code,"Wed, 4 Mar 2009 19:14:17 +0000","Thu, 5 Mar 2009 00:42:52 +0000","Wed, 4 Mar 2009 21:34:23 +0000",8406,"service generator needs to capitalize service names, since services are ruby modules and ruby modules are constants.",-0.5,-0.5,neutral
thrift,3559,description,Changes made in THRIFT-3545 introduce and awkward semi-colons and extra new lines.,code_debt,low_quality_code,"Sun, 17 Jan 2016 18:50:16 +0000","Fri, 22 Apr 2016 19:50:59 +0000","Sat, 16 Apr 2016 21:55:56 +0000",7787140,Changes made in THRIFT-3545 introduce and awkward semi-colons and extra new lines.,-0.271,-0.271,neutral
thrift,3596,description,"py coding_standards.md states it follows PEP8 but currently it does not do so at all. So a typical experience of a first-time (potential) py contributor would be to see red warnings all over the display and then has to either adjust their editor's settings or stop there. On the other hand, a huge downside of global reformat is git-blame experience but will be mostly mitigated by git blame -w in this case.",code_debt,low_quality_code,"Tue, 2 Feb 2016 16:55:43 +0000","Mon, 28 Jan 2019 03:17:53 +0000","Thu, 4 Feb 2016 05:31:43 +0000",131760,"py coding_standards.md states it follows PEP8 but currently it does not do so at all. So a typical experience of a first-time (potential) py contributor would be to see red warnings all over the display and then has to either adjust their editor's settings or stop there. On the other hand, a huge downside of global reformat is git-blame experience but will be mostly mitigated by git blame -w in this case.",-0.2053666667,-0.2053666667,negative
thrift,3634,description,Same as THRIFT-3615. It turned out that the problematic part of TSSLSocket originated from TSocket. I took this opportunity to remove the duplicate code.,code_debt,duplicated_code,"Sun, 14 Feb 2016 13:03:25 +0000","Fri, 19 Feb 2016 02:17:54 +0000","Wed, 17 Feb 2016 15:00:45 +0000",266240,Same as THRIFT-3615. It turned out that the problematic part of TSSLSocket originated from TSocket. I took this opportunity to remove the duplicate code.,0.1333333333,0.1333333333,negative
thrift,3681,description,It was fragile against parallel make because it was erroneously invoking pub twice in a same directory.,code_debt,low_quality_code,"Fri, 26 Feb 2016 18:29:23 +0000","Sat, 27 Feb 2016 09:26:29 +0000","Sat, 27 Feb 2016 08:01:03 +0000",48700,It was fragile against parallel make because it was erroneously invoking pub twice in a same directory.,0.0,0.0,negative
thrift,3744,description,"The precision is lost when converting double to string. E.g: double PI = 3.1415926535897931; string value = format(""%.16g"", PI); The value will be '3.141592653589793' and last 1 is lost after format operation. But expected value should be Solution: string value = format(""%.17g"", PI);",code_debt,low_quality_code,"Tue, 15 Mar 2016 01:51:33 +0000","Thu, 17 Mar 2016 22:04:13 +0000","Wed, 16 Mar 2016 17:20:39 +0000",142146,"The precision is lost when converting double to string. E.g: double PI = 3.1415926535897931; string value = format(""%.16g"", PI); The value will be '3.141592653589793' and last 1 is lost after format operation. But expected value should be '3.1415926535897931'. Solution: string value = format(""%.17g"", PI);",-0.0775,-0.06458333333,neutral
thrift,3839,description,"I have found performance issue when tried to deserialize big thrift binary message with enabled thrift_protocol php extension. Messsage size was 10 mb and it took about 30 seconds to deserialize it. When i have done debug of php extension and php library i have found that issue is because small read buffer is used in TBufferedTransport and i cannot change it from method. So i have added parameter $buffer_size to function from php extension. And also this parameter i have added to method from php library. And i extended class by method putBack, so this class will be used for desearilization without TBufferedTransport warapper. After these changes it takes less than a second to deserizlize message with 10 mb size id read buffer 512 kb. Here is the pull request",code_debt,slow_algorithm,"Mon, 23 May 2016 08:57:37 +0000","Wed, 28 Sep 2016 13:28:46 +0000","Sun, 25 Sep 2016 17:39:30 +0000",10831313,"I have found performance issue when tried to deserialize big thrift binary message with enabled thrift_protocol php extension. Messsage size was 10 mb and it took about 30 seconds to deserialize it. When i have done debug of php extension and php library i have found that issue is because small read buffer is used in TBufferedTransport and i cannot change it from TBinarySerializer::deserialize method. So i have added parameter $buffer_size to function thrift_protocol_read_binary from php extension. And also this parameter i have added to method TBinarySerializer::deserialize from php library. And i extended class Thrift\Transport\TMemoryBuffer by method putBack, so this class will be used for desearilization without TBufferedTransport warapper. After these changes it takes less than a second to deserizlize message with 10 mb size id read buffer 512 kb. Here is the pull request https://github.com/apache/thrift/pull/1014",0.025,0.025,negative
thrift,3907,description,"Previously we were able to reuse Docker layers from prebuilt images pulled from docker hub. This has been reducing total build time by 3 hours out of 7~8 hours total. After Docker 1.10 or so, it is no longer possible and we tend to easily saturate entire Apache's 30 jobs on Travis-CI. Standard solution as of now is to use docker save/load. This typically requires automated file upload on CI to some external storage. Unfortunately it cannot be done with our current Travis-CI account settings. To workaround this, we can put Dockerfile itself to Docker image and see if it's modified after the prebuild time and skip fresh builds if unchanged.",code_debt,slow_algorithm,"Mon, 29 Aug 2016 02:33:58 +0000","Wed, 28 Sep 2016 13:28:51 +0000","Sun, 4 Sep 2016 12:27:01 +0000",553983,"Previously we were able to reuse Docker layers from prebuilt images pulled from docker hub. This has been reducing total build time by 3 hours out of 7~8 hours total. After Docker 1.10 or so, it is no longer possible and we tend to easily saturate entire Apache's 30 jobs on Travis-CI. Standard solution as of now is to use docker save/load. This typically requires automated file upload on CI to some external storage. Unfortunately it cannot be done with our current Travis-CI account settings. To workaround this, we can put Dockerfile itself to Docker image and see if it's modified after the prebuild time and skip fresh builds if unchanged.",0.06257142857,0.06257142857,neutral
thrift,3944,description,"There is a block of code in checkHandshake that attempts to set read/write memory bios to be nonblocking. This code doesn't do anything: Here's what this code looks like, and the problems: - creates a new memory BIO. Not sure why. - BIO_set_nbio() executes BIO_ctrl(..., BIO_C_SET_NBIO, ...). This errors out and return 0 because mem_ctrl does not have a case for BIO_C_SET_NBIO. See: - SSL_set_bio() sets the SSL* to use the memory BIOs. - SSL_set_fd() creates a socket BIO, sets the FD on it, and uses SSL_set_bio() to replace the memory BIOs. As far as I can tell, this block of code does nothing and will not change functionality. If there's a reason that it's there, it needs to be re-implemented.",code_debt,low_quality_code,"Thu, 6 Oct 2016 17:29:23 +0000","Thu, 14 Dec 2017 13:55:25 +0000","Sat, 1 Apr 2017 14:18:47 +0000",15281364,"There is a block of code in checkHandshake that attempts to set read/write memory bios to be nonblocking. This code doesn't do anything: https://github.com/apache/thrift/blob/master/lib/cpp/src/thrift/transport/TSSLSocket.cpp#L441 Here's what this code looks like, and the problems: BIO_new(BIO_s_mem()) creates a new memory BIO. Not sure why. BIO_set_nbio() executes BIO_ctrl(..., BIO_C_SET_NBIO, ...). This errors out and return 0 because mem_ctrl does not have a case for BIO_C_SET_NBIO. See: https://github.com/openssl/openssl/blob/6f0ac0e2f27d9240516edb9a23b7863e7ad02898/crypto/bio/bss_mem.c#L226 SSL_set_bio() sets the SSL* to use the memory BIOs. SSL_set_fd() creates a socket BIO, sets the FD on it, and uses SSL_set_bio() to replace the memory BIOs. As far as I can tell, this block of code does nothing and will not change functionality. If there's a reason that it's there, it needs to be re-implemented.",-0.1714285714,-0.1333333333,negative
thrift,397,description,This is a holdover from the OCaml generator and is unnecessary in Haskell.,code_debt,dead_code,"Tue, 24 Mar 2009 01:14:55 +0000","Tue, 24 Mar 2009 14:47:58 +0000","Tue, 24 Mar 2009 14:47:58 +0000",48783,This is a holdover from the OCaml generator and is unnecessary in Haskell.,0.0,0.0,negative
thrift,39,description,The code that gets generated when you run thrift -javabean has no indentation in it at all. This makes it a little challenging to read through it.,code_debt,low_quality_code,"Wed, 18 Jun 2008 23:50:20 +0000","Tue, 1 Nov 2011 02:54:21 +0000","Fri, 21 Nov 2008 20:25:27 +0000",13466107,The code that gets generated when you run thrift -javabean has no indentation in it at all. This makes it a little challenging to read through it.,0.0,0.0,negative
thrift,4014,description,The meta data in AssemblyInfo.cs are inconsistent and do not reflect the ASF properly in all cases.,code_debt,low_quality_code,"Mon, 26 Dec 2016 10:32:00 +0000","Sat, 14 Jan 2017 09:04:50 +0000","Mon, 26 Dec 2016 10:44:10 +0000",730,The meta data in AssemblyInfo.cs are inconsistent and do not reflect the ASF properly in all cases.,0.0,0.0,negative
thrift,4078,description,"I believe this issue would warrant a 0.10.1 fix. It's quite frustrating. Inside there appears to be a debugging message which was left in. This pollutes the client application's console with ""Received 1"" messages. See Line 84 in the {{receiveBase}} method:",code_debt,dead_code,"Wed, 8 Feb 2017 21:40:51 +0000","Wed, 8 Feb 2017 21:48:58 +0000","Wed, 8 Feb 2017 21:47:16 +0000",385,"I believe this issue would warrant a 0.10.1 fix. It's quite frustrating. Inside lib/java/src/org/apache/thrift/TServiceClient.java, there appears to be a debugging message which was left in. This pollutes the client application's console with ""Received 1"" messages. See Line 84 in the receiveBase method:",-0.0525,-0.04375,negative
thrift,4129,description,"In THRIFT-2789 the error handling for connections where notify fails leaks a file descriptor. This was reported and fixed in a pull request without an Apache Jira entry: When failing to dispatch new connections to other IO threads other than the number 0, we returned these connections for reuse without closing them, so the corresponding fds were leaked forever. We should close these connections instead.",code_debt,low_quality_code,"Wed, 22 Mar 2017 18:52:30 +0000","Thu, 14 Dec 2017 13:56:10 +0000","Wed, 22 Mar 2017 19:11:01 +0000",1111,"In THRIFT-2789 the error handling for connections where notify fails leaks a file descriptor. This was reported and fixed in a pull request without an Apache Jira entry: https://github.com/apache/thrift/pull/1210/files When failing to dispatch new connections to other IO threads other than the number 0, we returned these connections for reuse without closing them, so the corresponding fds were leaked forever. We should close these connections instead.",-0.2333333333,-0.2333333333,negative
thrift,4136,description,Similar to {{is_string()}} the {{is_binary()}} method should be virtual and implemented at {{t_type}}. This simplifies the code and reduces possibilities for making technically wrong casts.,code_debt,complex_code,"Sun, 26 Mar 2017 13:51:41 +0000","Fri, 22 Dec 2017 04:04:29 +0000","Sun, 26 Mar 2017 17:56:30 +0000",14689,Similar to is_string() the is_binary() method should be virtual and implemented at t_type. This simplifies the code and reduces possibilities for making technically wrong casts.,-0.125,-0.125,neutral
thrift,4231,description,"When a required (e.g. string) field is not set, the C# code may throw a non-Thrift nullptr exception, which at this point is a bit unexpected. Happened to me with TJSON but is in fact a problem of required fields not being checked properly in the generated struct.Write() code.",code_debt,low_quality_code,"Fri, 16 Jun 2017 12:22:03 +0000","Thu, 14 Dec 2017 13:54:50 +0000","Sat, 17 Jun 2017 16:06:15 +0000",99852,"When a required (e.g. string) field is not set, the C# code may throw a non-Thrift nullptr exception, which at this point is a bit unexpected. Happened to me with TJSON but is in fact a problem of required fields not being checked properly in the generated struct.Write() code.",-0.1333333333,-0.1333333333,negative
thrift,4245,description,"if p.transport.Write fails, p.buf will not be truncated, which leads to thrift client's memory increasing forever. Is it more reasonable to truncate p.buf when write to transport fails? here are my pull request, i'm new in github&jira, if more details are needed, please tell me, thx.",code_debt,low_quality_code,"Wed, 5 Jul 2017 13:54:36 +0000","Thu, 14 Dec 2017 13:55:48 +0000","Wed, 5 Jul 2017 20:23:18 +0000",23322,"https://github.com/apache/thrift/blob/master/lib/go/thrift/framed_transport.go#L143 if p.transport.Write fails, p.buf will not be truncated, which leads to thrift client's memory increasing forever. Is it more reasonable to truncate p.buf when write to transport fails? here are my pull request, https://github.com/apache/thrift/pull/1303 i'm new in github&jira, if more details are needed, please tell me, thx.",-0.00725,-0.00725,neutral
thrift,4316,description,The TByteBuffer read() method is using the wrong length variable inside the processing loop and may read more than it should.,code_debt,low_quality_code,"Wed, 6 Sep 2017 03:48:40 +0000","Thu, 14 Dec 2017 13:56:12 +0000","Wed, 6 Sep 2017 04:36:28 +0000",2868,The TByteBuffer read() method is using the wrong length variable inside the processing loop and may read more than it should.,-0.25,-0.25,negative
thrift,4468,description,"In Delphi all methods that refer to VCL should do it only from main thread. But class TGUIConsole despite the name does not contain any synchronization methods. My suggestion is to rename this class to TStringsConsole, make method InternalWrite virtual and make new class TGUIConsole inherits from TStringsConsole",code_debt,multi-thread_correctness,"Mon, 22 Jan 2018 14:29:43 +0000","Thu, 27 Dec 2018 15:24:53 +0000","Thu, 25 Jan 2018 23:16:49 +0000",290826,"In Delphi all methodsthat refer to VCLshould do it only from main thread. But classTGUIConsoledespite the namedoes not contain any synchronization methods. My suggestion is to rename this class to TStringsConsole, make methodInternalWrite virtual and make new class TGUIConsoleinherits fromTStringsConsole",-0.25,-0.25,neutral
thrift,447,description,"The Java generator currently uses the generator to create all of the contents of the myService.Client class, including boring stuff like the constructor and instance variables. It seems like we could just factor this common base stuff out into a BaseClient that lives in the library and simplify the generator accordingly.",code_debt,complex_code,"Thu, 9 Apr 2009 17:38:54 +0000","Tue, 8 Feb 2011 17:26:55 +0000","Tue, 8 Feb 2011 17:26:55 +0000",57887281,"The Java generator currently uses the generator to create all of the contents of the myService.Client class, including boring stuff like the constructor and instance variables. It seems like we could just factor this common base stuff out into a BaseClient that lives in the library and simplify the generator accordingly.",-0.2,-0.2,neutral
thrift,4485,description,"If a read or write operation on pipes reaches the set timeout, the read/write operation is not properly cancelled. However, the overlapped struct gets freed when leaving the method, which essentially leaves the pending read or write operation with an undefined pointer. Easily reproducible with buffered transport over pipes, a combination that does not work at all anyways. The workaround for both problems is to not use buffered transport with pipes (use framed instead), and some sane tinemouts (not too short).",code_debt,low_quality_code,"Thu, 1 Feb 2018 22:29:25 +0000","Thu, 27 Dec 2018 15:25:13 +0000","Fri, 2 Feb 2018 16:52:27 +0000",66182,"If a read or write operation on pipes reaches the set timeout, the read/write operation is not properly cancelled. However, the overlapped struct gets freed when leaving the method, which essentially leaves the pending read or write operation with an undefined pointer. Easily reproducible with buffered transport over pipes, a combination that does not work at all anyways. The workaround for both problems is to not use buffered transport with pipes (use framed instead), and some sane tinemouts (not too short).",-0.079625,-0.079625,neutral
thrift,4559,description,"Tested on both 0.11.0 and master. C++ Server, Python Client. SSL sockets. SSL works correctly and communication is successful, however when the client disconnects the server always prints the following message: {{Thrift: Tue Apr 17 15:43:36 2018 TConnectedClient died: SSL_read: error code: 0 (SSL_error_code = 5)}} {{Deeper diving shows that SSL_error_code 5 is SSL_ERROR_SYSCALL. Documentation says to check both errno and the SLL error stack, however upon inspection both return 0 (no error). I believe this message is printed incorrectly.}} Upon inspecting the code for handing SSL_read, it appears that reading is done in a while-loop, which if no error is found is broken out of. At some point a switch-case was added, but the single level of break statements remained, leaving non-errors to break out of the switch instead of the while. A potential fix can be seen here:",code_debt,low_quality_code,"Thu, 19 Apr 2018 11:42:12 +0000","Thu, 27 Dec 2018 15:24:59 +0000","Wed, 2 May 2018 17:40:05 +0000",1144673,"Tested on both 0.11.0 and master.  C++ Server, Python Client. SSL sockets. SSL works correctly and communication is successful, however when the client disconnects the server always prints the following message: Thrift: Tue Apr 17 15:43:36 2018 TConnectedClient died: SSL_read: error code: 0 (SSL_error_code = 5)  Deeper diving shows that SSL_error_code 5 is SSL_ERROR_SYSCALL. Documentation says to check both errno and the SLL error stack, however upon inspection both return 0 (no error). I believe this message is printed incorrectly.  Upon inspecting the code for handing SSL_read, it appears that reading is done in a while-loop, which if no error is found is broken out of. At some point a switch-case was added, but the single level of break statements remained, leaving non-errors to break out of the switch instead of the while.  A potential fix can be seen here: https://github.com/apache/thrift/pull/1549",0.08752777778,0.08752777778,neutral
thrift,4604,description,When using the Node.js libs in the browser (via browser.js) Int64 isn't exposed the same way as it is in a Node environment. This just adds Int64 to the exports in browser.js to make consuming the libs more consistent with how we do in an actual Node environment.,code_debt,low_quality_code,"Sun, 29 Jul 2018 16:25:24 +0000","Thu, 27 Dec 2018 15:25:01 +0000","Wed, 1 Aug 2018 12:59:47 +0000",246863,When using the Node.js libs in the browser (via browser.js) Int64 isn't exposed the same way as it is in a Node environment. This just adds Int64 to the exports in browser.js to make consuming the libs more consistent with how we do in an actual Node environment.,0.0275,0.0275,neutral
thrift,471,description,"When the python generator makes an exception class since THRIFT-241, it does not include a __str__ method. This is problematic since raised exceptions don't include any useful information that might be part of the exception struct. Without __str__: With __str__: Clearly the latter is way more useful. Patch to follow if no one objects.",code_debt,low_quality_code,"Wed, 29 Apr 2009 21:12:16 +0000","Tue, 1 Nov 2011 02:52:11 +0000","Wed, 29 Apr 2009 23:35:22 +0000",8586,"When the python generator makes an exception class since THRIFT-241, it does not include a _str_ method. This is problematic since raised exceptions don't include any useful information that might be part of the exception struct. Without _str_: With _str_: Clearly the latter is way more useful. Patch to follow if no one objects.",-0.25,-0.25,negative
thrift,4830,description,ITNOA I think it useful and have some performance benefits to generate to_string function for enum beside of operator<< overloading.,code_debt,slow_algorithm,"Wed, 20 Mar 2019 14:04:52 +0000","Wed, 16 Oct 2019 22:26:35 +0000","Tue, 2 Jul 2019 00:14:11 +0000",8935759,ITNOA I think it useful and have some performance benefits to generate to_string function for enum beside of operator<< overloading.,0.45,0.45,positive
thrift,4851,description,Remove all calls to and ensure that everything is sent through the logging system so that all logging goes to the same place.,code_debt,low_quality_code,"Wed, 17 Apr 2019 14:59:20 +0000","Wed, 16 Oct 2019 22:26:57 +0000","Fri, 3 May 2019 21:01:14 +0000",1404114,Remove all calls to printStackTrace() and ensure that everything is sent through the logging system so that all logging goes to the same place.,0.2,0.2,neutral
thrift,4857,description,"The {{TField}} hash code implementation is inconsistent with equals, which is a breaking bug. If you know what hash codes are and are familiar with the Java {{Object}} API, then you already know what I'm talking about. Basically Java _requires_ that, if you overriden {{hashCode()}} and {{equals()}}, then for any two objects that are equal they _must_ return the same hash code. The {{TField}} class API contract isn't clear about what is considered equality, but according to the {{TField.equals()}} implementation, fields are equal if and only if: * Both objects are a {{TField}} (I'm generalizing here; there's another subtle bug lurking with class checking, but that's another story). * The fields both have the same {{type}} and {{id}}. In other words, fields are equal _without regard to name_. And this follows the overall Thrift architecture, in which field names are little more than window dressing, and the IDs carry the semantics. Unfortunately includes the name in the hash code calculation! This completely breaks the {{Object}} contract. It makes the hash code inconsistent with equality. To put it another way, two fields {{foo}} and {{bar}} could have the same type and ID, and {{foo.equals(bar)}} would return {{true}}, but they would be given different hash codes!! This is completely forbidden, and means that with this bug you cannot use a {{TField}} as the key in a map, for example, or even reliably keep a {{Set<TField This is simply broken as per the Java {{Object}} API contract.",code_debt,low_quality_code,"Fri, 3 May 2019 16:47:14 +0000","Wed, 16 Oct 2019 22:26:37 +0000","Mon, 13 May 2019 20:54:17 +0000",878823,"The TField hash code implementation is inconsistent with equals, which is a breaking bug. If you know what hash codes are and are familiar with the Java Object API, then you already know what I'm talking about. Basically Java requires that, if you overriden hashCode() and equals(), then for any two objects that are equal they must return the same hash code. The TField class API contract isn't clear about what is considered equality, but according to the TField.equals() implementation, fields are equal if and only if: Both objects are a TField (I'm generalizing here; there's another subtle bug lurking with class checking, but that's another story). The fields both have the same type and id. In other words, fields are equal without regard to name. And this follows the overall Thrift architecture, in which field names are little more than window dressing, and the IDs carry the semantics. Unfortunately TField.hashCode() includes the name in the hash code calculation! This completely breaks the Object contract. It makes the hash code inconsistent with equality. To put it another way, two fields foo and bar could have the same type and ID, and foo.equals(bar) would return true, but they would be given different hash codes!! This is completely forbidden, and means that with this bug you cannot use a TField as the key in a map, for example, or even reliably keep a Set<TField> of fields! If you were to store foo and bar as keys in a map, for example, the different hash codes would put them in different buckets, even though they were considered equal, providing inconsistent and strange lookup results, depending on the name of the field you used to query with. This is simply broken as per the Java Object API contract.",-0.1351714286,-0.1120196078,negative
thrift,4863,description,"A failing call to WinHTTP that leads to an NULL handle should be properly reported with the original error code. The NULL handle is reported, but thats only a symptom, not the real problem.",code_debt,low_quality_code,"Thu, 9 May 2019 22:06:08 +0000","Wed, 16 Oct 2019 22:26:49 +0000","Thu, 9 May 2019 22:42:13 +0000",2165,"A failing call to WinHTTP that leads to an NULL handle should be properly reported with the original error code. The NULL handle is reported, but thats only a symptom, not the real problem.",0.0,0.0,negative
thrift,4886,description,"The WinHTTP transport method {{CreateRequest()}} needs more detailed error information. Otherwise it is pretty hard to guess at which step exactly the problem arises. Additionally, the method should wrap any into Furthermore, the error codes retrieved via GetLastError should be enriched by a textual representation of the error code.",code_debt,low_quality_code,"Wed, 12 Jun 2019 20:07:21 +0000","Wed, 16 Oct 2019 22:27:18 +0000","Thu, 27 Jun 2019 19:46:28 +0000",1294747,"The WinHTTP transport method CreateRequest() needs more detailed error information. Otherwise it is pretty hard to guess at which step exactly the problem arises. Additionally, the method should wrap any non-Thrift-Exception into TTransportException. Furthermore, the error codes retrieved via GetLastError should be enriched by a textual representation of the error code.",-0.1402777778,-0.1052083333,negative
thrift,48,description,"The patch adds the unix_socket parameter for TServerSocket. The patch fixes an error message, because it's confusing to see the message ""Could not connect to localhost:9090"" if you try to connect to /tmp/unix_test.",code_debt,low_quality_code,"Mon, 23 Jun 2008 12:56:07 +0000","Tue, 1 Nov 2011 02:54:21 +0000","Thu, 31 Jul 2008 20:15:25 +0000",3309558,"The patch adds the unix_socket parameter for TServerSocket. The patch fixes an error message, because it's confusing to see the message ""Could not connect to localhost:9090"" if you try to connect to /tmp/unix_test.",-0.20925,-0.20925,neutral
thrift,508,description,"If you define a Thrift service that mentions the same method more than once, the Python compiler generates code for the method more than once. Given that this is (almost?) certainly an error in the Thrift service specification, it should be flagged and, I think, an error should be raised. This will prevent people (like me) from becoming the victim seemingly mysterious Thrift service failures due to silly cut & paste errors when editing specification files.",code_debt,low_quality_code,"Thu, 14 May 2009 20:09:09 +0000","Tue, 1 Nov 2011 02:54:10 +0000","Thu, 7 Apr 2011 15:53:18 +0000",59859849,"If you define a Thrift service that mentions the same method more than once, the Python compiler generates code for the method more than once. Given that this is (almost?) certainly an error in the Thrift service specification, it should be flagged and, I think, an error should be raised. This will prevent people (like me) from becoming the victim seemingly mysterious Thrift service failures due to silly cut & paste errors when editing specification files.",-0.1423928571,-0.1423928571,neutral
thrift,554,description,"There are two more issues affecting the functioning of a Perl service server: 1. Failure to prepend the Perl namespace to the exception name when checking the exception type from a eval'ed method call. 2. writeMessageEnd() should be present after a method call writes its result. I'm attaching a patch which addresses these issues, in addition to the following more minor changes: 1. Tried to make indentation and line breaks more consistent to ensure readability of the generated code. 2. Added a few best practice ideas to improve the code in minor ways. 3. Added a readAll() function to the as the one found in Thrift::Transport uses a while loop to consume the data, which results in a endless loop.",code_debt,low_quality_code,"Wed, 29 Jul 2009 22:42:16 +0000","Tue, 1 Nov 2011 02:53:50 +0000","Fri, 31 Jul 2009 01:32:00 +0000",96584,"There are two more issues affecting the functioning of a Perl service server: 1. Failure to prepend the Perl namespace to the exception name when checking the exception type from a eval'ed method call. 2. writeMessageEnd() should be present after a method call writes its result. I'm attaching a patch which addresses these issues, in addition to the following more minor changes: 1. Tried to make indentation and line breaks more consistent to ensure readability of the generated code. 2. Added a few best practice ideas to improve the code in minor ways. 3. Added a readAll() function to the Thrift::MemoryBuffer, as the one found in Thrift::Transport uses a while loop to consume the data, which results in a endless loop.",0.04375,0.04375,negative
thrift,56,description,"When displaying the caller of a deprecated class/module, any thrift library code should be skipped. This really only affects (as it called each_field, which would trigger the deprecation). The given patch is actually 2 patches (suitable for use with git-am), one for this specific issue, the other fixes the specs to handle deprecation warnings a bit better. I bundled the second with this because the issue with the specs only shows up once the first patch is added.",code_debt,low_quality_code,"Tue, 24 Jun 2008 23:42:51 +0000","Thu, 19 Aug 2010 05:45:17 +0000","Thu, 26 Jun 2008 18:45:40 +0000",154969,"When displaying the caller of a deprecated class/module, any thrift library code should be skipped. This really only affects ThriftStruct#initialize (as it called each_field, which would trigger the deprecation). The given patch is actually 2 patches (suitable for use with git-am), one for this specific issue, the other fixes the specs to handle deprecation warnings a bit better. I bundled the second with this because the issue with the specs only shows up once the first patch is added.",-0.1116875,-0.1116875,negative
thrift,593,description,"Perl client library for Thrift is bit slow.This implementation can make 24.75 QPS. I wrote a I/O buffering patch for this implementation.This makes 147x faster the echo server. The patch is here: regards,",code_debt,slow_algorithm,"Mon, 28 Sep 2009 16:34:33 +0000","Tue, 29 Sep 2009 00:19:08 +0000","Tue, 29 Sep 2009 00:19:08 +0000",27875,"Perl client library for Thrift is bit slow.This implementation can make 24.75 QPS. I wrote a I/O buffering patch for this implementation.This makes 147x faster the echo server. The patch is here: http://gist.github.com/195539 regards,",0.0,0.0,neutral
thrift,597,description,"This class was originally meant for functional testing only, so performance wasn't a concern. But now I'm using it for load testing. :) Two patches here. The first enables buffered I/O. The second allows the http server class to be specified, which allows users to use the ThreadingMixin.",code_debt,slow_algorithm,"Fri, 2 Oct 2009 17:12:03 +0000","Sun, 14 Oct 2012 06:49:13 +0000","Thu, 2 Sep 2010 15:15:44 +0000",28937021,"This class was originally meant for functional testing only, so performance wasn't a concern. But now I'm using it for load testing. Two patches here. The first enables buffered I/O. The second allows the http server class to be specified, which allows users to use the ThreadingMixin.",0.1,0.0,positive
thrift,617,description,One of my thrift services does special handling upon receiving SIGHUP. This causes spurious exceptions about EINTR to be logged. Attached patch adds EINTR retry logic akin to the C++ implementation to handle this.,code_debt,low_quality_code,"Tue, 3 Nov 2009 00:18:50 +0000","Sat, 26 Jan 2019 14:39:42 +0000","Sat, 26 Jan 2019 14:39:42 +0000",291306052,One of my thrift services does special handling upon receiving SIGHUP. This causes spurious exceptions about EINTR to be logged. Attached patch adds EINTR retry logic akin to the C++ implementation to handle this.,-0.2223333333,-0.2223333333,negative
thrift,62,description,"This is the updated patch which adds a C extension to speed up Ruby thrift serialization and deserialization. It's largely been reviewed already, but should still be looked over. The pre-Apache discussions of the bindings, including peer review and modifications happened here: Post that, there have been a few changes prompted by bugs found internally at Powerset, and I fixed the read buffering in This patch also adds automake compatability (make check and such) to the Ruby bindings.",code_debt,slow_algorithm,"Thu, 26 Jun 2008 00:32:20 +0000","Tue, 8 Jul 2008 00:48:12 +0000","Tue, 8 Jul 2008 00:48:12 +0000",1037752,"This is the updated patch which adds a C extension to speed up Ruby thrift serialization and deserialization. It's largely been reviewed already, but should still be looked over. The pre-Apache discussions of the bindings, including peer review and modifications happened here: http://publists.facebook.com/pipermail/thrift/2008-April/000890.html Post that, there have been a few changes prompted by bugs found internally at Powerset, and I fixed the read buffering in Thrift::BufferedTransport. This patch also adds automake compatability (make check and such) to the Ruby bindings.",0.09366666667,0.07025,neutral
thrift,635,description,"thrift --gen java:camel doesn't generate idiomatic names for getters and setters. For instance: struct SlicePredicate { 1: optional list<binary 2: optional SliceRange slice_range, } has generated getters and setters: getSlice_range setSlice_range",code_debt,low_quality_code,"Fri, 20 Nov 2009 06:19:12 +0000","Wed, 27 Oct 2010 18:53:01 +0000","Wed, 27 Oct 2010 18:53:01 +0000",29507629,"thrift --gen java:camel doesn't generate idiomatic names for getters and setters. For instance: struct SlicePredicate { 1: optional list<binary> column_names, 2: optional SliceRange slice_range, } has generated getters and setters: getSlice_range setSlice_range",0.25,0.0,neutral
thrift,639,description,"When a Timeout interrupts a client that is reading a Thrift response, the client may leave unread bytes in the read queue. If this transport instance/queue is reused in a later request, the extra bytes will corrupt that later response. We're currently working around this by having the rescue blocks of our TimeoutExceptions close the transport so that subsequent requests will have to create a new, clean one.",code_debt,low_quality_code,"Wed, 2 Dec 2009 02:43:45 +0000","Fri, 10 Jun 2011 00:31:01 +0000","Fri, 10 Jun 2011 00:31:01 +0000",47944036,"When a Timeout interrupts a client that is reading a Thrift response, the client may leave unread bytes in the read queue. If this transport instance/queue is reused in a later request, the extra bytes will corrupt that later response. We're currently working around this by having the rescue blocks of our TimeoutExceptions close the transport so that subsequent requests will have to create a new, clean one.",-0.15,-0.15,negative
thrift,653,description,"Now that enums are actually enums, their toStrings are acceptable in the overall struct toString. We should remove the special case we built in for this in the past.",code_debt,dead_code,"Wed, 16 Dec 2009 22:50:04 +0000","Fri, 18 Dec 2009 19:34:59 +0000","Fri, 18 Dec 2009 19:34:59 +0000",161095,"Now that enums are actually enums, their toStrings are acceptable in the overall struct toString. We should remove the special case we built in for this in the past.",0.578,0.578,neutral
thrift,673,description,The Python code generator produces code with a number of whitespace issues: - Trailing whitespace at the end of lines. - Multiple blank newlines at the end of files. Both of these issues cause problems if the code is used in a Git repository with `git diff --check' in a hook. The attached patch corrects the code generation to address these issues.,code_debt,low_quality_code,"Wed, 13 Jan 2010 23:59:26 +0000","Thu, 2 Sep 2010 14:21:53 +0000","Thu, 2 Sep 2010 14:21:53 +0000",20010147,The Python code generator produces code with a number of whitespace issues: Trailing whitespace at the end of lines. Multiple blank newlines at the end of files. Both of these issues cause problems if the code is used in a Git repository with `git diff --check' in a hook. The attached patch corrects the code generation to address these issues.,-0.1333333333,-0.1,negative
thrift,716,description,"Try creating a union with the field name ""value"", and the code won't compile. In writeFields for the generated class, you'll have something like the following: <code case VALUE: String value = return; </code ""String value"" conflicts with the parameter ""Object value"".",code_debt,low_quality_code,"Thu, 25 Feb 2010 01:23:17 +0000","Sun, 28 Feb 2010 05:19:49 +0000","Sun, 28 Feb 2010 05:19:49 +0000",273392,"Try creating a union with the field name ""value"", and the code won't compile. In writeFields for the generated class, you'll have something like the following: <code> case VALUE: String value = (String)getFieldValue(); oprot.writeString(value); return; </code> ""String value"" conflicts with the parameter ""Object value"".",-0.2405,-0.1603333333,neutral
thrift,717,description,"The Thrift PHP library makes gratuitous use of the $GLOBALS array to store basic configuration. Globals in PHP are generally bad practice, so I suggest something else: Use constants. Being immutable, constants are more secure than globals (that could be overwritten in scripts susceptible to injection attacks); they also perform much better, since the $GLOBALS variable is a hash-table, lookups are comparatively expensive. I will attach a patch soon unless anyone has any better ideas.",code_debt,low_quality_code,"Thu, 25 Feb 2010 16:40:07 +0000","Tue, 1 Nov 2011 02:54:07 +0000","Mon, 11 Apr 2011 16:13:52 +0000",35422425,"The Thrift PHP library makes gratuitous use of the $GLOBALS array to store basic configuration. Globals in PHP are generally bad practice, so I suggest something else: Use constants. Being immutable, constants are more secure than globals (that could be overwritten in scripts susceptible to injection attacks); they also perform much better, since the $GLOBALS variable is a hash-table, lookups are comparatively expensive. I will attach a patch soon unless anyone has any better ideas.",0.0539375,0.0539375,negative
thrift,728,description,"The patch * Generates Arbitrary instances for Thrift structs, enums and exceptions * Provides Arbitrary instances for GHC's Int64, Data.Map.Map and Data.Set.Set * Makes Thrift enums instances of Bounded and improves the Enum instance declaration Making a type an instance of specifies how to generate random instances of the struct. This is useful for testing. For example, consider the following simple Thrift declaration: With the patch, the following program (import statements elided) is a fuzzer for the log service. In implementing the Arbitrary instances, it was useful to make Thrift enums instances of Bounded and to improve the Enum instance. Specifically, whereas before, would throw an exception, now it behaves as expected without an exception. I consider the patch incomplete. It's more of a starting point for a discussion at this point than a serious candidate for inclusion. If it is of interest, I'd appreciate some direction on testing it as well as style, and I'd welcome any other comments or thoughts.",code_debt,low_quality_code,"Mon, 8 Mar 2010 00:00:50 +0000","Thu, 25 Sep 2014 03:18:28 +0000","Tue, 26 Aug 2014 22:10:17 +0000",141084567,"The patch Generates Arbitrary instances for Thrift structs, enums and exceptions Provides Arbitrary instances for GHC's Int64, Data.Map.Map and Data.Set.Set Makes Thrift enums instances of Bounded and improves the Enum instance declaration Making a type an instance of Test.QuickCheck.Arbitrary specifies how to generate random instances of the struct. This is useful for testing. For example, consider the following simple Thrift declaration: With the patch, the following program (import statements elided) is a fuzzer for the log service. In implementing the Arbitrary instances, it was useful to make Thrift enums instances of Bounded and to improve the Enum instance. Specifically, whereas before, would throw an exception, now it behaves as expected without an exception. I consider the patch incomplete. It's more of a starting point for a discussion at this point than a serious candidate for inclusion. If it is of interest, I'd appreciate some direction on testing it as well as style, and I'd welcome any other comments or thoughts.",0.2218263889,0.2258511905,neutral
thrift,873,description,"All of the tests run in the same JVM, and it seems like is leaking sockets. As a quick fix, dropping that to use only 200 clients instead of 500, and changing each unit test to run in its own JVM instead of sharing them. Also allowing the port used for binding the test servers to be configured from the command line",code_debt,low_quality_code,"Fri, 27 Aug 2010 05:50:41 +0000","Tue, 1 Nov 2011 02:52:01 +0000","Fri, 27 Aug 2010 06:18:02 +0000",1641,"All of the tests run in the same JVM, and it seems like TestAsyncClientManager is leaking sockets. As a quick fix, dropping that to use only 200 clients instead of 500, and changing each unit test to run in its own JVM instead of sharing them. Also allowing the port used for binding the test servers to be configured from the command line",-0.5,-0.5,negative
thrift,992,description,"While updating my FluentCassandra project using the latest Thrift-0.5.0.exe generator against the 0.7 Beta3 release of cassandra.thrift I experienced a generation problem that caused compiler errors when compiling the C# code. It appears the constructor fields are using the old naming convention for the fields, while every other part of the class seems to be using the new naming convention for fields with underscores. You can see a diff of the files I had to manually edit here: The paths starting with should be the ones that you need to worry about for this bug.",code_debt,low_quality_code,"Fri, 5 Nov 2010 12:35:39 +0000","Tue, 1 Nov 2011 02:51:43 +0000","Wed, 10 Nov 2010 21:24:02 +0000",463703,"While updating my FluentCassandra project using the latest Thrift-0.5.0.exe generator against the 0.7 Beta3 release of cassandra.thrift I experienced a generation problem that caused compiler errors when compiling the C# code. It appears the constructor fields are using the old naming convention for the fields, while every other part of the class seems to be using the new naming convention for fields with underscores. You can see a diff of the files I had to manually edit here: https://github.com/managedfusion/fluentcassandra/commit/d2d26f0bfd158cae3c39fd9cd47ec9097bc394f6 The paths starting with ""FluentCassandra/Apache/Cassandra"" should be the ones that you need to worry about for this bug.",-0.1018571429,-0.1018571429,neutral
thrift,1055,description,"resulted in TFramedTransport being improved so it wasn't necessary to disable Nagle. For the case of TServerSocket and TSocket, a simple server case, performance is still very poor, ~30 msg/sec vs. ~11k msg/sec with NoDelay = true. Java and cpp disable nagle in their TSocket and TServerSocket implementations and performance is MUCH improved with nagle disabled for csharp so I'm proposing that csharp should also disable nagle.",design_debt,non-optimal_design,"Tue, 8 Feb 2011 15:35:19 +0000","Tue, 8 Feb 2011 16:39:18 +0000","Tue, 8 Feb 2011 16:39:18 +0000",3839,"https://issues.apache.org/jira/browse/THRIFT-904 resulted in TFramedTransport being improved so it wasn't necessary to disable Nagle. For the case of TServerSocket and TSocket, a simple server case, performance is still very poor, ~30 msg/sec vs. ~11k msg/sec with NoDelay = true. Java and cpp disable nagle in their TSocket and TServerSocket implementations and performance is MUCH improved with nagle disabled for csharp so I'm proposing that csharp should also disable nagle.",0.3337777778,0.3337777778,neutral
thrift,1072,description,"- (id) (id<TProcessor is missing in from the Cocoa headers. Should be added, to let user know that they need to init with this method to delegate stuff to their own processor. Can't see other possibility to make server delegate incomming connections to protocol implementing classes. (no Cocoa server examples available!)",design_debt,non-optimal_design,"Wed, 23 Feb 2011 21:47:42 +0000","Fri, 27 Jan 2012 02:53:24 +0000","Fri, 27 Jan 2012 02:53:23 +0000",29135141,"(id) initWithSharedProcessor: (id<TProcessor>) sharedProcessor; is missing in TSharedProcessorFactory.h from the Cocoa headers. Should be added, to let user know that they need to init with this method to delegate stuff to their own processor. Can't see other possibility to make server delegate incomming connections to protocol implementing classes. (no Cocoa server examples available!)",-0.1,-0.08,negative
thrift,1217,description,"As part of an effort to use the Non-Blocking server (using libevent) on Windows, it was necessary to remove the use of ""pipe"" for the notification mechanism to signal end-of-task. We propose to use evutil_socketpair instead (tested with libevent 2.0.12). Patch included. Please see for more details.",design_debt,non-optimal_design,"Thu, 23 Jun 2011 20:39:21 +0000","Thu, 1 Sep 2011 17:41:04 +0000","Fri, 8 Jul 2011 12:45:13 +0000",1267552,"As part of an effort to use the Non-Blocking server (using libevent) on Windows, it was necessary to remove the use of ""pipe"" for the notification mechanism to signal end-of-task. We propose to use evutil_socketpair instead (tested with libevent 2.0.12). Patch included. Please see https://github.com/aubonbeurre/thrift/blob/alex-0.6.1/README.non.blocking.Windows for more details.",0.19075,0.19075,neutral
thrift,1243,description,"[Context: This patch is part of a larger patch to use thriftnb on Windows/VisualC++. See for more details.] When compiling using Visual C++ 2010, the compiler chokes on casting some callbacks bool(*)() to void(*)(). Although probably valid and supported by gcc, this is further complicated by the fact those casting seem unnecessary: for each of the callbacks returning bool in TAsyncChannel.h: 1. the returned value is never checked 2. they always return true Attached is a trivial patch based on 0.7.0, tested on Ubuntu 11.04 and Visual C++ 2010.",design_debt,non-optimal_design,"Sun, 24 Jul 2011 10:03:04 +0000","Tue, 1 Nov 2011 00:57:57 +0000","Sun, 11 Sep 2011 07:29:12 +0000",4224368,"[Context: This patch is part of a larger patch to use thriftnb on Windows/VisualC++. See https://github.com/aubonbeurre/thrift/blob/alex-0.7.0/README.non.blocking.Windows for more details.] When compiling using Visual C++ 2010, the compiler chokes on casting some callbacks bool() to void(). Although probably valid and supported by gcc, this is further complicated by the fact those casting seem unnecessary: for each of the callbacks returning bool in TAsyncChannel.h: 1. the returned value is never checked 2. they always return true Attached is a trivial patch based on 0.7.0, tested on Ubuntu 11.04 and Visual C++ 2010.",0.1150714286,0.1150714286,neutral
thrift,1248,description,"The ensureCanWrite function in TMemoryBuffer currently ""rebases"" the buffer pointers by subtracting the original buffer pointer from the pointer newly returned by realloc. While this seems to work fine on my linux setup(I couldn't force a reproducer), pointer subtraction between pointers that we allocated separately is an undefined operation. I have run into problems with this on platforms other than linux. I am attaching a patch that removes the undefined operation.",design_debt,non-optimal_design,"Fri, 29 Jul 2011 13:39:40 +0000","Fri, 6 May 2016 22:30:17 +0000","Sun, 10 May 2015 12:45:47 +0000",119315167,"The ensureCanWrite function in TMemoryBuffer currently ""rebases"" the buffer pointers by subtracting the original buffer pointer from the pointer newly returned by realloc. While this seems to work fine on my linux setup(I couldn't force a reproducer), pointer subtraction between pointers that we allocated separately is an undefined operation. I have run into problems with this on platforms other than linux. I am attaching a patch that removes the undefined operation.",0.025,0.025,neutral
thrift,1269,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Wed, 6 Jan 2010 02:10:19 +0000 Subject: [PATCH 01/33] thrift: handle undeclared exceptions in the async error cob Summary: This updates the generated TAsyncProcessor code to correctly handle when an undeclared exception type is passed into an error callback. It now sends back a T_EXCEPTION message to the client, just like the non-async code does. Previously the exception wouldn't be handled, causing the TEventWorker thread to exit. Test Plan: Tested changing the async sort tutorial to pass in a generic TException rather than a SortError, and verified that the client correctly received an exception. Conflicts:  | 38 1 files changed, 32 insertions(+), 6 deletions(-)",design_debt,non-optimal_design,"Wed, 17 Aug 2011 19:44:00 +0000","Tue, 1 Nov 2011 02:54:26 +0000","Mon, 22 Aug 2011 21:41:11 +0000",439031,"From 25366c3648ef4c2d436641a2051c057abde05975 Mon Sep 17 00:00:00 2001 From: Adam Simpkins <simpkins@fb.com> Date: Wed, 6 Jan 2010 02:10:19 +0000 Subject: [PATCH 01/33] thrift: handle undeclared exceptions in the async error cob Summary: This updates the generated TAsyncProcessor code to correctly handle when an undeclared exception type is passed into an error callback. It now sends back a T_EXCEPTION message to the client, just like the non-async code does. Previously the exception wouldn't be handled, causing the TEventWorker thread to exit. Test Plan: Tested changing the async sort tutorial to pass in a generic TException rather than a SortError, and verified that the client correctly received an exception. Conflicts: compiler/cpp/src/generate/t_cpp_generator.cc  compiler/cpp/src/generate/t_cpp_generator.cc | 38 +++++++++++++++++++++---- 1 files changed, 32 insertions, 6 deletions",0.0042,0.003,neutral
thrift,1349,description,There are a couple of spurious calls which can lead to lots of output when talking between slightly different versions of generated code. For instance launching a server which returns a new field to a client will result in the client printing out information about an unknown fid. This leads to lots of logging which you probably don't want. I'd like to remove these unless anyone in dev objects.,design_debt,non-optimal_design,"Mon, 19 Sep 2011 05:07:55 +0000","Thu, 10 Jul 2014 13:42:24 +0000","Thu, 10 Jul 2014 13:42:24 +0000",88590869,There are a couple of spurious error_logger:info_msg/2 calls which can lead to lots of output when talking between slightly different versions of generated code. For instance launching a server which returns a new field to a client will result in the client printing out information about an unknown fid. This leads to lots of logging which you probably don't want. I'd like to remove these unless anyone in dev objects.,-0.21675,-0.183375,negative
thrift,137,description,"As previously discussed on the mailinglist, using Thrift from Google Web Toolkit (GWT) applications (AJAX) would be nice, as it does not only allow you to consume existing Thrift services from GWT applications, but also means that you now can write GWT-consumable RPC services in any language (and say host them on Google Appengine) that are practically source-code compatible with the official GWT RPC framework. Doing this presents two challanges: 1) The GWT compiler only supports a subset of the JRE libraries (luckily, this is rather easy to work around). 2) As the A in AJAX hints, the only way of doing RPC is asynchronously, something not supported by Thrift, by using the XMLHttpRequest object in the browser. Here's what I've done (an excerpt from the mailing-list): This solution works really well for my problem, but it's half-assed in two ways. 1) It only allows for asynchronous client transports (as in the case of the XMLHttpRequest object) and not on the server side (with messages coming back in a non-sequential order). 2) I'm not sure how to solve the client library issues. Right now, I've moved the core classes (those required on the client (GWT) side of things) into while keeping everything else where they are. This allows the GWT compiler to translate while using the same jar both on the client and server. This is not very elegant for people not using GWT (which I suppose is 99.99% of the audience) but short of maintaining two separate Java client libraries, I'm not sure how to solve this issue. The attached patch is only for the compiler, and does not produce compilable client code without the modified client library. Just wanted to get some input before producing a somewhat committable patch. Comments? Ideas?",design_debt,non-optimal_design,"Tue, 16 Sep 2008 09:01:30 +0000","Thu, 12 Apr 2012 04:04:58 +0000","Mon, 9 Apr 2012 18:25:15 +0000",112440225,"As previously discussed on the mailinglist, using Thrift from Google Web Toolkit (GWT) applications (AJAX) would be nice, as it does not only allow you to consume existing Thrift services from GWT applications, but also means that you now can write GWT-consumable RPC services in any language (and say host them on Google Appengine) that are practically source-code compatible with the official GWT RPC framework. Doing this presents two challanges: 1) The GWT compiler only supports a subset of the JRE libraries (luckily, this is rather easy to work around). 2) As the A in AJAX hints, the only way of doing RPC is asynchronously, something not supported by Thrift, by using the XMLHttpRequest object in the browser. Here's what I've done (an excerpt from the mailing-list): -snip- 1) Created a stripped down jar of Thrift, axed most protocol, transport and server implementations, in order to get a JavaScript-translatable version of Thrift. I did not need to change any of the base Thrift classes, nor modify the compiler for GWT to translate the structs, but I might have missed something here (Mathias?). 2) Added an option for the Thrift Java compiler to generate asynchronous service interfaces and client proxies. This is manifested as: public class Repository { public interface Iface { public Document get_document(String uri) throws TException; public int get_count() throws TException; } public interface AsyncIface { public void get_document(String uri, TAsyncCallback<Document> callback) throws TException; public void get_count(TAsyncCallback<Integer> callback) throws TException; } ... This is done in line with GWT's RPC framework and gives the developer the standard synchronous interface to implement on the server side (I use it with embedded Jetty in a daemon) and an asynchonous interface to use in the GWT client. AsyncCallback<T> just has a plain onSuccess(T result) method. 3) Implemented a client transport using GWT's RequestBuilder (the XmlHttpRequest abstraction) that executes the TAsyncCallback asynchronously when the response has been received. 4) Modified the JSONProtocol slightly to be fully JavaScript-translatable. This could probably be more efficiently done by using GWT's JSNI framework, but I really haven't had the time to optimize anything yet. -snip- This solution works really well for my problem, but it's half-assed in two ways. 1) It only allows for asynchronous client transports (as in the case of the XMLHttpRequest object) and not on the server side (with messages coming back in a non-sequential order). 2) I'm not sure how to solve the client library issues. Right now, I've moved the core classes (those required on the client (GWT) side of things) into com.facebook.thrift.gwt, while keeping everything else where they are. This allows the GWT compiler to translate com.facebook.thrift.gwt.* while using the same jar both on the client and server. This is not very elegant for people not using GWT (which I suppose is 99.99% of the audience) but short of maintaining two separate Java client libraries, I'm not sure how to solve this issue. The attached patch is only for the compiler, and does not produce compilable client code without the modified client library. Just wanted to get some input before producing a somewhat committable patch. Comments? Ideas?",0.01553461538,-0.03387037037,positive
thrift,1452,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Mon, 21 Jun 2010 20:24:50 +0000 Subject: [PATCH 3/5] generate a swap() method for all generated structs Summary: Andrei mentioned it would be convenient if thrift generated swap() methods for all C++ thrift types. Apparently the ads team manually writes swap() functions for many of their thrift data types, but have to keep updating them by hand when new fields are added to the thrift interface. This updates the thrift compiler to emit swap() methods for user-defined types. For now, I decided not to emit swap() methods for the internal XXX_args, XXX_pargs, XXX_result, and XXX_presult types. Test Plan: Tested compiling serveral internal projects. I didn't actually test the generated swap() functions, but they look okay. DiffCamp Revision: 124773 Reviewed By: aalexandre Commenters: dreiss, edhall CC: davidrecordon, achao, dreiss, kholst, aalexandre, simpkins, edhall, thrift-team@lists Revert Plan: OK git-svn-id: fb thing]@30392  | 70 1 files changed, 67 insertions(+), 3 deletions(-)",design_debt,non-optimal_design,"Wed, 7 Dec 2011 22:17:56 +0000","Fri, 9 Dec 2011 21:27:30 +0000","Thu, 8 Dec 2011 21:16:16 +0000",82700,"From baa275da65e023af50930a75f9a7ef2a991cdaef Mon Sep 17 00:00:00 2001 From: Adam Simpkins <simpkins@fb.com> Date: Mon, 21 Jun 2010 20:24:50 +0000 Subject: [PATCH 3/5] generate a swap() method for all generated structs Summary: Andrei mentioned it would be convenient if thrift generated swap() methods for all C++ thrift types. Apparently the ads team manually writes swap() functions for many of their thrift data types, but have to keep updating them by hand when new fields are added to the thrift interface. This updates the thrift compiler to emit swap() methods for user-defined types. For now, I decided not to emit swap() methods for the internal XXX_args, XXX_pargs, XXX_result, and XXX_presult types. Test Plan: Tested compiling serveral internal projects. I didn't actually test the generated swap() functions, but they look okay. DiffCamp Revision: 124773 Reviewed By: aalexandre Commenters: dreiss, edhall CC: davidrecordon, achao, dreiss, kholst, aalexandre, simpkins, edhall, thrift-team@lists Revert Plan: OK git-svn-id: svn+ssh://tubbs/svnapps/fbomb/trunk/[internal fb thing]@30392 2248de34-8caa-4a3c-bc55-5e52d9d7b73a  compiler/cpp/src/generate/t_cpp_generator.cc | 70 ++++++++++++++++++++++++- 1 files changed, 67 insertions, 3 deletions",0.4821428571,0.35,neutral
thrift,1533,description,should implement the {{Closable}} interface. Doing so will allow users to perform,design_debt,non-optimal_design,"Sat, 3 Mar 2012 16:37:56 +0000","Sat, 20 Feb 2021 15:27:43 +0000","Fri, 8 Apr 2016 05:45:42 +0000",129301666,org.apache.thrift.transport.TTransport should implement the Closable interface. Doing so will allow users to perform IOUtils.closeQuietly(transport);,0.1,0.02857142857,neutral
thrift,1595,description,The implementation of the java test server should be the same as the languages so our cross platform tests can work.,design_debt,non-optimal_design,"Fri, 4 May 2012 19:34:16 +0000","Sat, 8 Jun 2013 03:15:47 +0000","Sat, 8 Jun 2013 03:15:47 +0000",34501291,The implementation of the java test server should be the same as the languages so our cross platform tests can work.,0.6,0.6,neutral
thrift,1672,description,"I'm trying to use Thrift C# library under MonoTouch. Seems like everything is working from the box. The only issue is Class THttpHandler using System.Web namespace to reach IHttpHandler interface and HttpContext class. Unfortunately, that namespace is not implemented for MonoTouch framework. Is it possible to implement THttpHandler in other way? Note that issue in Xamarin's Bugzilla is here:",design_debt,non-optimal_design,"Wed, 8 Aug 2012 06:37:47 +0000","Sat, 18 Aug 2012 18:53:18 +0000","Sat, 18 Aug 2012 18:53:18 +0000",908131,"I'm trying to use Thrift C# library under MonoTouch. Seems like everything is working from the box. The only issue is /src/Transport/THttpHandler.cs. Class THttpHandler using System.Web namespace to reach IHttpHandler interface and HttpContext class. Unfortunately, that namespace is not implemented for MonoTouch framework. Is it possible to implement THttpHandler in other way? Note that issue in Xamarin's Bugzilla is here: https://bugzilla.xamarin.com/show_bug.cgi?id=6420",-0.06428571429,-0.05,neutral
thrift,1799,description,"Improvements to HTML Generator * Removed HTML page name at <a href=""..."" * Option added to embed CSS inline instead of style.css. Both improvements are quite helpful in some situations, e.g. when the HTML is accessed in a way other than via the original file name, or when the separate CSS file cannot be provided along with the HTML for some reason.",design_debt,non-optimal_design,"Fri, 21 Dec 2012 20:16:32 +0000","Sat, 8 Jun 2013 03:15:51 +0000","Sat, 8 Jun 2013 03:15:50 +0000",14540358,"Improvements to HTML Generator Removed HTML page name at <a href=""...""> links for targets located in the same file Option added to embed CSS inline instead of style.css. Both improvements are quite helpful in some situations, e.g. when the HTML is accessed in a way other than via the original file name, or when the separate CSS file cannot be provided along with the HTML for some reason.",0.2083333333,0.2083333333,positive
thrift,1829,description,"If you attempt to build the cpp unit tests using 'make -j x' the build will fail as there's a race between the unit test code targets depending on the thrift-generated code targets. I think the real fix would be to use one of the approaches described in the 'Multiple Outputs' section of the automake manual (see I experimented with one of them and it seemed to help but never got it quite working. However an easier workaround is to simply disable parallel builds by using the "".NOTPARALLEL"" special target which forces make to run serially. from .NOTPARALLEL If .NOTPARALLEL is mentioned as a target, then this invocation of make will be run serially, even if the -j option is given. Any recursively invoked make command will still run recipes in parallel (unless its makefile also contains this target). Any prerequisites on this target are ignored. That's the approach I ended up taking as I'm short on time.",design_debt,non-optimal_design,"Tue, 15 Jan 2013 14:51:47 +0000","Sun, 17 Feb 2013 19:49:22 +0000","Tue, 15 Jan 2013 22:22:24 +0000",27037,"If you attempt to build the cpp unit tests using 'make -j x' the build will fail as there's a race between the unit test code targets depending on the thrift-generated code targets. I think the real fix would be to use one of the approaches described in the 'Multiple Outputs' section of the automake manual (see http://www.gnu.org/software/automake/manual/html_node/Multiple-Outputs.html). I experimented with one of them and it seemed to help but never got it quite working. However an easier workaround is to simply disable parallel builds by using the "".NOTPARALLEL"" special target which forces make to run serially. from http://www.gnu.org/software/make/manual/html_node/Special-Targets.html .NOTPARALLEL If .NOTPARALLEL is mentioned as a target, then this invocation of make will be run serially, even if the -j option is given. Any recursively invoked make command will still run recipes in parallel (unless its makefile also contains this target). Any prerequisites on this target are ignored. That's the approach I ended up taking as I'm short on time.",0.0381,0.03463636364,negative
thrift,2032,description,"The C# client code does not correctly clean up the transport used, so the programmer has to take care on his own about this. This may even lead to a program hang in certain scenarios. Furthermore, the generated client should support IDisposable. Note that in contrast, the server side handles this automatically without any explicit manual coding. TODO: * modify generated code to add IDisposable support * modify TProtocol to add IDisposable support * update the tutorial code accordingly",design_debt,non-optimal_design,"Sat, 15 Jun 2013 01:16:56 +0000","Thu, 25 Jul 2013 03:40:04 +0000","Tue, 25 Jun 2013 20:23:29 +0000",932793,"The C# client code does not correctly clean up the transport used, so the programmer has to take care on his own about this. This may even lead to a program hang in certain scenarios. Furthermore, the generated client should support IDisposable. Note that in contrast, the server side handles this automatically without any explicit manual coding. TODO: modify generated code to add IDisposable support modify TProtocol to add IDisposable support update the tutorial code accordingly",0.06666666667,0.06666666667,negative
thrift,2116,description,Cocoa generator improvements relating to * *Server-side exception handling* in the actual TProcessor implementation for the service * Better handling of *service inheritance* ** Service clients can now use the client subclass directly to invoke calls to the base service ** Servers must only adopt the child protocol and will automatically adopt the base protocol over inheritance ** The improvement should not break any existing service implementations See the revisions for the implemented cocoa tutorial for instance * For a better overview of the resulting gen-files after the improvements see the attached [diff An *implementation* for the *Thrift tutorial* * Server and client implementation based on the corresponding Python version * Update relating to the improved handling of service inheritance,design_debt,non-optimal_design,"Sun, 11 Aug 2013 14:49:54 +0000","Thu, 10 Oct 2019 23:00:05 +0000","Mon, 14 Jan 2019 15:03:33 +0000",171245619,Cocoa generator improvements relating to Server-side exception handling in the actual TProcessor implementation for the service Better handling of service inheritance Service clients can now use the client subclass directly to invoke calls to the base service Servers must only adopt the child protocol and will automatically adopt the base protocol over inheritance The improvement should not break any existing service implementations See the revisions for the implemented cocoa tutorial for instance For a better overview of the resulting gen-files after the improvements see the attached diff file An implementation for the Thrift tutorial Server and client implementation based on the corresponding Python version Update relating to the improved handling of service inheritance,0.1488571429,0.1488571429,neutral
thrift,221,description,"The way that the current Java library and test builds are set up, they rely on an external jar that's assumed to be at some specific path. This isn't where the jar resides on my machine, so every time I want to do some building, I have to hand edit the file, and then when I want to generate a diff for posting on jira, I have to revert the changes. We should make it so the class path is configurable in a way that doesn't require checkins. There are a couple different ways to do this, but a properties file with the classpath is probably one of the easiest.",design_debt,non-optimal_design,"Wed, 3 Dec 2008 18:31:15 +0000","Tue, 1 Nov 2011 02:54:09 +0000","Thu, 29 Jan 2009 01:46:28 +0000",4864513,"The way that the current Java library and test builds are set up, they rely on an external jar that's assumed to be at some specific path. This isn't where the jar resides on my machine, so every time I want to do some building, I have to hand edit the file, and then when I want to generate a diff for posting on jira, I have to revert the changes. We should make it so the class path is configurable in a way that doesn't require checkins. There are a couple different ways to do this, but a properties file with the classpath is probably one of the easiest.",-0.1375,-0.1375,neutral
thrift,2225,description,destroy the SSLContext before cleanup the openSSL to avoid recreating some openssl internal resource.,design_debt,non-optimal_design,"Wed, 9 Oct 2013 19:59:14 +0000","Mon, 2 Apr 2018 22:53:08 +0000","Sun, 2 Feb 2014 22:57:23 +0000",10033089,destroy the SSLContext before cleanup the openSSL to avoid recreating some openssl internal resource.,-0.4,-0.4,neutral
thrift,2255,description,"Cpp generated Class of struct dosn't have a parent Class, this will cause the Program hold the Object must be using The instance class direct. The parent class may be helper full in for program to process manay type Struct batch. I hope the New version of Thrift implement it.",design_debt,non-optimal_design,"Thu, 7 Nov 2013 02:08:12 +0000","Thu, 10 Jul 2014 13:42:32 +0000","Thu, 10 Jul 2014 13:42:32 +0000",21209660,"Cpp generated Class of struct dosn't have a parent Class, this will cause the Program hold the Object must be using The instance class direct. The parent class may be helper full in for program to process manay type Struct batch. I hope the New version of Thrift implement it.",0.073,0.073,neutral
thrift,2263,description,Previously Thrift only generated high-quality hashCode implementations when configured due to the dependency on commons-lang3. After THRIFT-2260 we no long have this dependency and can unconditionally give the better implementation.,design_debt,non-optimal_design,"Sat, 16 Nov 2013 20:52:15 +0000","Sun, 14 Feb 2016 01:56:13 +0000","Sun, 8 Dec 2013 21:13:44 +0000",1902089,Previously Thrift only generated high-quality hashCode implementations when configured due to the dependency on commons-lang3. After THRIFT-2260 we no long have this dependency and can unconditionally give the better implementation.,0.0,0.0,neutral
thrift,2279,description,"TSerializer copies from the transport into a 1024 length byte array and returns the array, which will effectively clips off everything after the first 1k. The attached patch instead returns a copy of the underlying memory buffer used by the transport.",design_debt,non-optimal_design,"Mon, 2 Dec 2013 18:59:41 +0000","Fri, 13 Dec 2013 16:41:36 +0000","Tue, 3 Dec 2013 22:00:11 +0000",97230,"TSerializer copies from the transport into a 1024 length byte array and returns the array, which will effectively clips off everything after the first 1k. The attached patch instead returns a copy of the underlying memory buffer used by the transport.",0.4,0.4,neutral
thrift,240,description,"Now that we have a deep copy constructor, TBase should implement cloneable.",design_debt,non-optimal_design,"Sat, 20 Dec 2008 00:00:30 +0000","Tue, 1 Nov 2011 02:53:55 +0000","Thu, 22 Jan 2009 16:30:27 +0000",2910597,"Now that we have a deep copy constructor, TBase should implement cloneable.",0.0,0.0,neutral
thrift,2415,description,"The performance of the named pipes Delphi server is sub-optimal. Furthermore, BYTE message modes should be used (instead of MESSAGE)",design_debt,non-optimal_design,"Wed, 19 Mar 2014 22:35:18 +0000","Tue, 1 Apr 2014 20:02:22 +0000","Thu, 20 Mar 2014 20:52:07 +0000",80209,"The performance of the named pipes Delphi server is sub-optimal. Furthermore, BYTE message modes should be used (instead of MESSAGE)",0.0,0.0,negative
thrift,2555,description,produces which starts to become annoying the larger the gaps between the numbers are.,design_debt,non-optimal_design,"Wed, 28 May 2014 22:46:01 +0000","Fri, 30 May 2014 16:27:52 +0000","Fri, 30 May 2014 15:55:32 +0000",148171,produces which starts to become annoying the larger the gaps between the numbers are.,-1.0,-1.0,negative
thrift,255,description,is a thin wrapper around TFDTransport that simplifies the process of opening a file.,design_debt,non-optimal_design,"Sat, 10 Jan 2009 01:07:06 +0000","Tue, 1 Nov 2011 02:52:03 +0000","Thu, 26 Mar 2009 06:23:50 +0000",6499004,TSimpleFileTransport is a thin wrapper around TFDTransport that simplifies the process of opening a file.,0.0,0.0,neutral
thrift,2568,description,"Enhance the C# TLS-Transport with a paramater to give it a specific certificate handler function. This enables to easely accept untrusted certificates or accept a specific certificate, which can be useful while debuging.",design_debt,non-optimal_design,"Thu, 5 Jun 2014 08:51:11 +0000","Thu, 5 Jun 2014 20:34:50 +0000","Thu, 5 Jun 2014 20:04:49 +0000",40418,"Enhance the C# TLS-Transport with a paramater to give it a specific certificate handler function. This enables to easely accept untrusted certificates or accept a specific certificate, which can be useful while debuging.",0.15,0.15,neutral
thrift,2636,description,"The class has ""type"" and ""message"" fields, but these are not exposed as GObject properties. Instead clients are expected to modify the object's member variables directly&mdash;a bad practice. The attached patch exposes the two fields as GObject properties (and adds relevant test cases), allowing clients to set and access these fields in a conventional manner.",design_debt,non-optimal_design,"Thu, 24 Jul 2014 20:58:30 +0000","Wed, 5 Nov 2014 04:48:25 +0000","Thu, 24 Jul 2014 21:59:41 +0000",3671,"The ThriftApplicationException class has ""type"" and ""message"" fields, but these are not exposed as GObject properties. Instead clients are expected to modify the object's member variables directlya bad practice. The attached patch exposes the two fields as GObject properties (and adds relevant test cases), allowing clients to set and access these fields in a conventional manner.",-0.1936666667,-0.1936666667,negative
thrift,2666,description,"The hash function introduced with THRIFT-2621 breaks with Python older than 3.2, because PYTHONHASHSEED has been introduced with 3.2. The proposed solution is to define a constant PYTHONHASHSEED as a workaround, but only if PYTHONHASHSEED is not available.",design_debt,non-optimal_design,"Thu, 14 Aug 2014 20:14:46 +0000","Wed, 5 Nov 2014 04:48:58 +0000","Mon, 1 Sep 2014 21:08:54 +0000",1558448,"The hash function introduced with THRIFT-2621 breaks with Python older than 3.2, because PYTHONHASHSEED has been introduced with 3.2. The proposed solution is to define a constant PYTHONHASHSEED as a workaround, but only if PYTHONHASHSEED is not available.",0.1,0.1,neutral
thrift,2932,description,"I've been investigating using the Node.js client in a project however it seems like there are instances which don't follow Node.js best practices. In particular http_connection.js and connection.js throw errors during callbacks. This is considered an anti-pattern in Node because it both removes the Exception from the context of the callback making it hard to associate with a request as well as throwing it in the context of the EventEmitter code which can cause inconsistencies in the Node process. This means under some error conditions an uncaught exception would be thrown or at least an 'error' event on the singleton client (again removing it from the request context). Both transport receivers share the same copy-pasta code which contains: I'm working on a patch, but I'm curious about some of the history of the code. In particular the exception based loop flow control and the using the seqid to track the callback which makes it hard to properly associate it with exception handling.",design_debt,non-optimal_design,"Wed, 7 Jan 2015 05:51:21 +0000","Tue, 21 Jul 2015 02:21:10 +0000","Sat, 14 Feb 2015 23:02:36 +0000",3345075,"I've been investigating using the Node.js client in a project however it seems like there are instances which don't follow Node.js best practices. In particular http_connection.js and connection.js throw errors during callbacks. This is considered an anti-pattern in Node because it both removes the Exception from the context of the callback making it hard to associate with a request as well as throwing it in the context of the EventEmitter code which can cause inconsistencies in the Node process. This means under some error conditions an uncaught exception would be thrown or at least an 'error' event on the singleton client (again removing it from the request context). Both transport receivers share the same copy-pasta code which contains: I'm working on a patch, but I'm curious about some of the history of the code. In particular the exception based loop flow control and the using the seqid to track the callback which makes it hard to properly associate it with exception handling.",0.05186666667,0.05186666667,negative
thrift,3027,description,"In Go, as per words in names that are initialisms or acronyms should have a consistent case. For example, if you have a struct like: One would expect it to compile to: Rather than:- It would be pretty difficult to handle all cases of initialisms in the Go compiler of course, but there is a set of common initialisms that have been identified by the authors of Golint and could be handled relatively easily:-",design_debt,non-optimal_design,"Fri, 6 Mar 2015 14:23:55 +0000","Fri, 22 May 2015 17:24:08 +0000","Sat, 14 Mar 2015 14:39:43 +0000",692148,"In Go, as per https://github.com/golang/go/wiki/CodeReviewComments#initialisms, words in names that are initialisms or acronyms should have a consistent case. For example, if you have a struct like: One would expect it to compile to: Rather than:- It would be pretty difficult to handle all cases of initialisms in the Go compiler of course, but there is a set of common initialisms that have been identified by the authors of Golint and could be handled relatively easily:- https://github.com/golang/lint/blob/master/lint.go#L692",0.08333333333,0.08333333333,neutral
thrift,3157,description,Is there any reason this isn't the case? The wildcarding doesn't appear to give us anything extra. I've been told that it may help with uses of the Comparable. Can anyone think of a use?,design_debt,non-optimal_design,"Sat, 16 May 2015 04:49:32 +0000","Fri, 18 Mar 2016 18:02:05 +0000","Fri, 18 Mar 2016 18:02:05 +0000",26572353,Is there any reason this isn't the case? The wildcarding doesn't appear to give us anything extra. I've been told that it may help with uses of the Comparable. Can anyone think of a use?,0.1,0.1,neutral
thrift,3280,description,"Currently retry variables are only initialized after a connection has been successfully established. When the initial connection fails the retry logic is broken since the state has not been properly initialized. To solve this, we need to initialize the retry state before the initial connect() request.",design_debt,non-optimal_design,"Thu, 30 Jul 2015 07:05:39 +0000","Tue, 25 Aug 2015 04:44:50 +0000","Thu, 30 Jul 2015 21:54:16 +0000",53317,"Currently retry variables are only initialized after a connection has been successfully established. When the initial connection fails the retry logic is broken since the state has not been properly initialized. To solve this, we need to initialize the retry state before the initial connect() request.",0.175,0.175,negative
thrift,3535,description,The default Dart compiler behavior is to generate a new library for each thrift file. Add an argument to generate files and imports that are usable in an existing library. This produces a file structure like:,design_debt,non-optimal_design,"Sat, 9 Jan 2016 20:29:29 +0000","Fri, 18 Mar 2016 17:54:34 +0000","Fri, 18 Mar 2016 17:54:34 +0000",5952305,The default Dart compiler behavior is to generate a new library for each thrift file. Add an argument to generate files and imports that are usable in an existing library. This produces a file structure like:,-0.1666666667,-0.1666666667,neutral
thrift,3733,description,The socket timeout handling has room for improvements,design_debt,non-optimal_design,"Thu, 10 Mar 2016 19:03:44 +0000","Thu, 17 Mar 2016 19:08:54 +0000","Thu, 10 Mar 2016 19:14:10 +0000",626,The socket timeout handling has room for improvements,0.0,0.0,neutral
thrift,3760,description,"* Perl: /usr/usr/local -* Python: /usr/local - perl fix for ""local"" part is frankly bad but at least it works. The patch also removes .pyo files from python package that should be generated by install process instead. This resolves most of lintian warnings.",design_debt,non-optimal_design,"Fri, 25 Mar 2016 00:00:32 +0000","Thu, 14 Apr 2016 10:27:47 +0000","Tue, 29 Mar 2016 17:51:44 +0000",409872,"Perl: /usr/usr/local -> /usr Python: /usr/local -> /usr perl fix for ""local"" part is frankly bad but at least it works. The patch also removes .pyo files from python package that should be generated by install process instead. This resolves most of lintian warnings.",-0.025,-0.025,neutral
thrift,3864,description,"I have a service (Golang) with a handler that returns a large slice. A new one is allocated every time, resulting in large heap allocations per client call, while it could be reused through a sync.Pool and improve the overall performance through fewer GC calls. (.thrift example) struct Slice { 1: required list<Element} service MyService { Slice GetSlice() throws (...) } I have experimented with modifying the auto-generated code and got this functionality through adding a (.go) pool.Put(*Slice) // adding the newly generated return value to a pool) call in but doing so creates a nasty dependency between the handler and the processor. Modifying the signature of the handler should also work (.go) GetSlice(*Slice) (*Slice, error) but does breaks all compatibility with previous compilers... Has some solution to this problem been explored? If nothing else some optional Release(retval) after oprot.Flush() in would be very helpful",design_debt,non-optimal_design,"Mon, 27 Jun 2016 14:23:22 +0000","Thu, 3 Jan 2019 14:33:44 +0000","Thu, 3 Jan 2019 14:33:44 +0000",79488622,"I have a service (Golang) with a handler that returns a large slice. A new one is allocated every time, resulting in large heap allocations per client call, while it could be reused through a sync.Pool and improve the overall performance through fewer GC calls. (.thrift example) struct Slice { 1: required list<Element> element } service MyService { Slice GetSlice() throws (...) } I have experimented with modifying the auto-generated code and got this functionality through adding a (.go) pool.Put(*Slice) // adding the newly generated return value to a pool) call in https://github.com/apache/thrift/blob/master/compiler/cpp/src/generate/t_go_generator.cc#L2798 but doing so creates a nasty dependency between the handler and the processor. Modifying the signature of the handler should also work (.go) GetSlice(*Slice) (*Slice, error) but does breaks all compatibility with previous compilers... Has some solution to this problem been explored? If nothing else some optional Release(retval) after oprot.Flush() in https://github.com/apache/thrift/blob/master/compiler/cpp/src/generate/t_go_generator.cc#L2798 would be very helpful",0.1818181818,0.1818181818,neutral
thrift,3868,description,"The identity check is cheap and should be done before comparing fields of a struct. Idiomatic equals methods always include this check especially if the field by field comparison can be expensive. Check to add: if(that == this) return true; 1864 out << indent() << ""public boolean equals("" << tstruct-1865 indent_up(); 1866 out << indent() << ""if (that == null)"" << endl << indent() << "" return false;"" << endl; INSERT IDENTITY CHECK HERE 1867 1868 const vector<t_field*1869 vector<t_field*1870 for (m_iter = members.begin(); m_iter != members.end(); ++m_iter) { 1871 out << endl; 1872",design_debt,non-optimal_design,"Wed, 29 Jun 2016 14:11:27 +0000","Tue, 11 Oct 2016 01:12:35 +0000","Fri, 7 Oct 2016 17:08:23 +0000",8650616,"The identity check is cheap and should be done before comparing fields of a struct. Idiomatic equals methods always include this check especially if the field by field comparison can be expensive. Check to add: if(that == this) return true; 1864 out << indent() << ""public boolean equals("" << tstruct->get_name() << "" that) {"" << endl; 1865 indent_up(); 1866 out << indent() << ""if (that == null)"" << endl << indent() << "" return false;"" << endl; INSERT IDENTITY CHECK HERE 1867 1868 const vector<t_field*>& members = tstruct->get_members(); 1869 vector<t_field*>::const_iterator m_iter; 1870 for (m_iter = members.begin(); m_iter != members.end(); ++m_iter) { 1871 out << endl; 1872",0.03108333333,0.03108333333,neutral
thrift,3905,description,"In Dart it is desirable to initialize bool (false), int (0), and double (0.0) required properties (those that are not marked optional), instead of leaving them with the value of null.",design_debt,non-optimal_design,"Sun, 28 Aug 2016 00:33:13 +0000","Wed, 28 Sep 2016 13:28:39 +0000","Wed, 31 Aug 2016 20:38:41 +0000",331528,"In Dart it is desirable to initialize bool (false), int (0), and double (0.0) required properties (those that are not marked optional), instead of leaving them with the value of null.",0.3375,0.3375,neutral
thrift,3941,description,"thrift_poll() for WINVER <= 0x0502 in shadows the 'time_out' variable, and it ends up passing the destructed copy to select(): timeval time_out; timeval* time_out_ptr = NULL; if (timeout timeval time_out = {timeout / 1000, (timeout % 1000) * 1000}; time_out_ptr = &time_out; } else { // to avoid compiler warnings (void)time_out; (void)timeout; } int sktready = select(1, read_fds_ptr, write_fds_ptr, NULL, time_out_ptr); Stepping through this code in the debugger, it looks like MSVC reserves a large enough stack frame to avoid overwriting the variable when calling select(), which may be why this hasn't been caught yet.",design_debt,non-optimal_design,"Tue, 4 Oct 2016 15:56:29 +0000","Tue, 11 Oct 2016 01:12:30 +0000","Wed, 5 Oct 2016 10:33:06 +0000",66997,"thrift_poll() for WINVER <= 0x0502 in thrift/windows/WinFnctl.cpp shadows the 'time_out' variable, and it ends up passing the destructed copy to select(): timeval time_out; timeval* time_out_ptr = NULL; if (timeout >= 0) { timeval time_out = {timeout / 1000, (timeout % 1000) * 1000} ; time_out_ptr = &time_out; } else { // to avoid compiler warnings (void)time_out; (void)timeout; } int sktready = select(1, read_fds_ptr, write_fds_ptr, NULL, time_out_ptr); Stepping through this code in the debugger, it looks like MSVC reserves a large enough stack frame to avoid overwriting the variable when calling select(), which may be why this hasn't been caught yet.",0.1455,0.07275,neutral
thrift,4030,description,"The main motivation for me to introduce this script was to avoid docker build which was failing too often due to occasional apt download failures. But for some unknown reason apt is not failing any longer. Although reusing reduces build time by ~10min for each job, we may well remove the script and always build images from scratch.",design_debt,non-optimal_design,"Sat, 14 Jan 2017 07:06:27 +0000","Fri, 1 Feb 2019 05:29:49 +0000","Fri, 1 Feb 2019 05:29:49 +0000",64621402,"The main motivation for me to introduce this script was to avoid docker build which was failing too often due to occasional apt download failures. But for some unknown reason apt is not failing any longer. Although reusing reduces build time by ~10min for each job, we may well remove the script and always build images from scratch.",0.277,0.277,neutral
thrift,4130,description,"There is a connection leak in the THttpClient when using the Apache HttpClient with the Without calling releaseConnection on the HttpPost object, the connections are never returned to the pool. Under heavy load, this can lead to both failures for subsequent calls to be able to get a connection from the pool and connections being held by the underlying OS, eventually resulting in the inability to grab another client port for outgoing connections. Per the Apache HttpClient ""In order to ensure correct deallocation of system resources the user MUST either fully consume the response content or abort request execution by calling This might have not been an issue when using the 3.x version of the HttpClient, but it's definitely an issue in the 4.x line. See for more details.",design_debt,non-optimal_design,"Wed, 22 Mar 2017 22:53:52 +0000","Thu, 14 Dec 2017 13:55:26 +0000","Thu, 23 Mar 2017 00:34:25 +0000",6033,"There is a connection leak in the THttpClient when using the Apache HttpClient with the PoolingClientConnectionManager. Without calling releaseConnection on the HttpPost object, the connections are never returned to the pool. Under heavy load, this can lead to both failures for subsequent calls to be able to get a connection from the pool and connections being held by the underlying OS, eventually resulting in the inability to grab another client port for outgoing connections. Per the Apache HttpClient examples/documentation: ""In order to ensure correct deallocation of system resources the user MUST either fully consume the response content or abort request execution by calling HttpGet#releaseConnection()."" This might have not been an issue when using the 3.x version of the HttpClient, but it's definitely an issue in the 4.x line. See https://hc.apache.org/httpcomponents-client-4.2.x/quickstart.html for more details.",0.1042222222,0.1007916667,negative
thrift,4164,description,"In a project where thrift is used, i was investigating a core in an assertion in (pthread variety). The mutex in question was one of the locking mutexes that thrift gives to openssl. The core occurred in where the mutexes are destroyed (on the last line). I suspect that we might be changing the locking callbacks too early in the cleanup process; perhaps one of the other cleanup calls that follows it would have released a mutex in some situations? In any case, this needs to be investigated and I am assigning it to myself.",design_debt,non-optimal_design,"Mon, 3 Apr 2017 14:24:40 +0000","Thu, 14 Dec 2017 13:55:39 +0000","Tue, 4 Apr 2017 13:37:13 +0000",83553,"In a project where thrift is used, i was investigating a core in an assertion in apache::thrift::concurrency::~Mutex (pthread variety). The mutex in question was one of the locking mutexes that thrift gives to openssl. The core occurred in TSSLSocket::cleanupOpenSSL() where the mutexes are destroyed (on the last line). I suspect that we might be changing the locking callbacks too early in the cleanup process; perhaps one of the other cleanup calls that follows it would have released a mutex in some situations? In any case, this needs to be investigated and I am assigning it to myself.",-0.16,-0.16,neutral
thrift,418,description,"Currently the ruby struct code sorts the field ids of each struct every time it is serialized, which is an unnecessary drag on performance.",design_debt,non-optimal_design,"Wed, 1 Apr 2009 20:39:58 +0000","Wed, 10 Aug 2011 18:27:52 +0000","Thu, 16 Jun 2011 03:15:03 +0000",69575705,"Currently the ruby struct code sorts the field ids of each struct every time it is serialized, which is an unnecessary drag on performance.",-0.2,-0.2,negative
thrift,4230,description,After a lot of tests with HBASE Thrift server we found a problem. If the connection is dropped on the client side (using route or iptables) it may be still opened on the Thrift server side. Such situation will occur in case of unstable connection. After several iterations the Thrift server application will have a lot of opened connections and *will not accept *any new one. The only WA found is to restart the Thrift server. I believe Thrift server should have something like socket timeouts and heartbeats.,design_debt,non-optimal_design,"Fri, 16 Jun 2017 07:05:05 +0000","Fri, 1 Feb 2019 03:58:09 +0000","Fri, 1 Feb 2019 03:58:09 +0000",51396784,After a lot of tests with HBASE Thrift server we found a problem. If the connection is dropped on the client side (using route or iptables) it may be still opened on the Thrift server side. Such situation will occur in case of unstable connection. After several iterations the Thrift server application will have a lot of opened connections and *will not accept *any new one. The only WA found is to restart the Thrift server. I believe Thrift server should have something like socket timeouts and heartbeats.,-0.1599166667,-0.1599166667,negative
thrift,4362,description,"In some cases the method size)}} gets called with a ""size"" parameter that has not been validated by the existing method size)}}. This is true if the method is called by of the same class. The method {{readString()}} checks the size correctly before calling size)}}. Since the methods size)}} and are public, there may be other callers who don't check the size correctly. We encountered this issue in production several times. Because of this we are currently using our own patched version of libthrift-0.9.3. The patch is attached, but it is surely not the best solution, because with this patch the size may be checked twice, depending on the caller.",design_debt,non-optimal_design,"Tue, 10 Oct 2017 18:05:00 +0000","Thu, 14 Dec 2017 13:55:02 +0000","Wed, 25 Oct 2017 12:42:05 +0000",1276625,"In some cases the method org.apache.thrift.protocol.TBinaryProtocol.readStringBody(int size) gets called with a ""size"" parameter that has not been validated by the existing method checkStringReadLength(int size). This is true if the method is called by readMessageBegin() of the same class. The method readString() checks the size correctly before calling readStringBody(int size). Since the methods readStringBody(int size) and readMessageBegin() are public, there may be other callers who don't check the size correctly. We encountered this issue in production several times. Because of this we are currently using our own patched version of libthrift-0.9.3. The patch is attached, but it is surely not the best solution, because with this patch the size may be checked twice, depending on the caller.",-0.05161111111,-0.03317857143,negative
thrift,4416,description,"The perl library has a few files and a shell script designed to automate the production of a perl package for CPAN. It mostly works, however: 1. CPAN cannot parse the package version references we use when we point to lib/Thrift.pm from other packages. This is fixed by adding a provides{} stanza to META.json. 2. Users of CPAN don't care about anything but obtaining the ""Thrift"" package that contains all the others, so identifying the others (like is not necessary. This is done by adding a provides{} stanza to META.json. 3. The CPAN package no longer needs the META.yml file, as META.json is sufficient. 4. These changes were made by hand to the 0.11.0 package before uploading to CPAN. This needs to be automated.",design_debt,non-optimal_design,"Sat, 9 Dec 2017 13:29:01 +0000","Thu, 27 Dec 2018 15:25:06 +0000","Sat, 9 Dec 2017 22:02:41 +0000",30820,"The perl library has a few files and a shell script designed to automate the production of a perl package for CPAN. It mostly works, however: 1. CPAN cannot parse the package version references we use when we point to lib/Thrift.pm from other packages. This is fixed by adding a provides{} stanza to META.json. 2. Users of CPAN don't care about anything but obtaining the ""Thrift"" package that contains all the others, so identifying the others (like Thrift::BinaryProtocol) is not necessary. This is done by adding a provides{} stanza to META.json. 3. The CPAN package no longer needs the META.yml file, as META.json is sufficient. 4. These changes were made by hand to the 0.11.0 package before uploading to CPAN. This needs to be automated.",0.075,0.075,neutral
thrift,4437,description,"When using a WebSocket Transport and doing two service calls immediately, without waiting for the first to return, e.g. like this: The callback to the first invocation is called twice, and the second never, i.e. console shows: instead of the expected I suspect this bug was introduced with the patch for where for some reason the callback registered twice when set:",design_debt,non-optimal_design,"Wed, 27 Dec 2017 11:08:19 +0000","Thu, 27 Dec 2018 15:25:16 +0000","Thu, 28 Dec 2017 13:03:35 +0000",93316,"When using a WebSocket Transport and doing two service calls immediately, without waiting for the first to return, e.g. like this: The callback to the first invocation is called twice, and the second never, i.e. console shows: instead of the expected I suspect this bug was introduced with the patch for https://issues.apache.org/jira/browse/THRIFT-4131 where for some reason the callback registered twice when set: https://github.com/apache/thrift/pull/1372/files",-0.1083333333,-0.1083333333,neutral
thrift,4446,description,"In the C# and .NET Core libraries, the JSONProtocol's Binary Encoding to Base64 trims padding from the user provided byte arrays before encoding into Base64. This behavior is incorrect, as the user provided data should be encoded exactly as provided. Otherwise, data may be lost. Fixed by no longer trimming padding on encode. Padding must still be trimmed on decode, in accordance with the Base64 specification. For example: * Before this patch, encoding the byte array [0x01, 0x3d, 0x3d] yields [0x01] upon decode. This is incorrect, as I should decode the exact data that I encoded. * After this patch, it yields [0x01, 0x3d, 0x3d], as expected. I have submitted a pull request",design_debt,non-optimal_design,"Tue, 9 Jan 2018 17:44:26 +0000","Sat, 2 Feb 2019 13:47:56 +0000","Thu, 11 Jan 2018 02:14:22 +0000",116996,"In the C# and .NET Core libraries, the JSONProtocol's Binary Encoding to Base64 trims padding from the user provided byte arrays before encoding into Base64. This behavior is incorrect, as the user provided data should be encoded exactly as provided. Otherwise, data may be lost. Fixed by no longer trimming padding on encode. Padding must still be trimmed on decode, in accordance with the Base64 specification. For example: Before this patch, encoding the byte array [0x01, 0x3d, 0x3d] yields [0x01] upon decode. This is incorrect, as I should decode the exact data that I encoded. After this patch, it yields [0x01, 0x3d, 0x3d], as expected. I have submitted a pull request here",0.03535,0.03535,negative
thrift,4715,description,"`union` option for netcore generator was fixed in 0.12, but what it generates doesn't seem very user-friendly. Following thrift: Generates: Usage: Is there a reason for the `public abstract object Data`? If we get rid of that and instead generate a strongly-type getter we don't need to cast `Data`: I could have sworn it worked like that with the ""csharp"" generator in 0.11.0 but it generates the same now. Is the intended usage different from what I'm doing?",design_debt,non-optimal_design,"Fri, 4 Jan 2019 05:54:29 +0000","Wed, 16 Oct 2019 22:27:24 +0000","Thu, 24 Jan 2019 17:29:12 +0000",1769683,"`union` option for netcore generator was fixed in 0.12, but what it generates doesn't seem very user-friendly. Following thrift: ```thrift struct PlayMsg { 1: string url, } union RequestMsg { 1: PlayMsg Play, } ``` Generates: Usage: Is there a reason for the `public abstract object Data`? If we get rid of that and instead generate a strongly-type getter we don't need to cast `Data`:   I could have sworn it worked like that with the ""csharp"" generator in 0.11.0 but it generates the same now. Is the intended usage different from what I'm doing?",-0.3125,-0.3125,negative
thrift,4822,description,"Certain CTORs accept two boolean flags {{public SomeTransport( arg1, arg2, ..., bool useBufferedSockets = false, bool useFramedTransport = false)}} The only valid combinations here are in fact (false,false), (true,false), (false,true) - the forth combination does not make sense because framed by design already acts as a buffer. Not to mention, that multiple boolean arguments are usually less coder-friendly. Therefore, the parameterlist should be shortened to the more readable, maintainable and concise style like so (proposal):",design_debt,non-optimal_design,"Thu, 14 Mar 2019 08:08:30 +0000","Wed, 16 Oct 2019 22:27:30 +0000","Fri, 15 Mar 2019 00:35:52 +0000",59242,"Certain CTORs accept two boolean flags public SomeTransport( arg1, arg2, ..., bool useBufferedSockets = false, bool useFramedTransport = false) The only valid combinations here are in fact (false,false), (true,false), (false,true) - the forth combination does not make sense because framed by design already acts as a buffer. Not to mention, that multiple boolean arguments are usually less coder-friendly. Therefore, the parameterlist should be shortened to the more readable, maintainable and concise style like so (proposal):",0.1262,0.1262,neutral
thrift,487,description,"The method and its associated class have errors and race conditions that cause frequent failures in the test. Some failure modes return to the caller, but others just hang the test forever. The main problem is that the test function relies on indirect indications that its tasks have reached certain known conditions. The indications that it sees are actually caused by the ThreadManager's Workers, and this results in a number of race conditions between the test function and the BlockTasks. There are 2 patch files attached. One patches the file to fix the test. The other patches Tests.cpp so that it will run the blockTest in a loop. In my experience, an unpatched version of generally fails within 100 iterations. After being patched, the test will still fail unless a separate patch is applied to the ThreadManager.cpp file. That patch is part of another issue that I haven't entered yet (because it needs to refer to this one). When I know its number I will add a comment. Anyway, if the patch is applied to but not to ThreadManager.cpp, then the test will still fail, generally with an assert, but sometimes with a Bus Error. Test Procedure: 1) Apply the Tests.cpp patch. This makes the thread-manager portion of the test run ONLY the blockTest in an effectively infinite loop. 2) Run a make in lib/cpp in order to rebuild concurrency-test 3) Run concurrency test with the command line argument ""thread-manager"". This will start the blockTest loop. It should fail in a fairly short time. Repeated runs may fail different ways, including infinite hangs. 4) Apply the patch to 5) Run make in lib/cpp to rebuild concurrency-test 6) Run concurrency_test as before. It should probably run for a longer period of time. I have seen it run for an hour or more after beng patched. Eventually it should fail either with an assert in Monitor.cpp while trying to destroy a pthread_mutex_t, or it will get a Bus Error because it tried to execute invalid memory. I have also seen it hang forever. In order to resolve the remaining issues, a patch needs to be applied to ThreadManager.cpp. I will add a comment about that as soon as the issue is filed and the patch is available.",design_debt,non-optimal_design,"Tue, 5 May 2009 23:10:30 +0000","Thu, 11 Jan 2018 12:43:09 +0000","Thu, 11 Jan 2018 12:43:01 +0000",274109551,"The ThreadManagerTests::blockTest() method and its associated ThreadManagerTests::BlockTask class have errors and race conditions that cause frequent failures in the test. Some failure modes return to the caller, but others just hang the test forever. The main problem is that the test function relies on indirect indications that its tasks have reached certain known conditions. The indications that it sees are actually caused by the ThreadManager's Workers, and this results in a number of race conditions between the test function and the BlockTasks. There are 2 patch files attached. One patches the ThreadManagerTests.h file to fix the test. The other patches Tests.cpp so that it will run the blockTest in a loop. In my experience, an unpatched version of ThreadManagerTests.h generally fails within 100 iterations. After being patched, the test will still fail unless a separate patch is applied to the ThreadManager.cpp file. That patch is part of another issue that I haven't entered yet (because it needs to refer to this one). When I know its number I will add a comment. Anyway, if the patch is applied to ThreadManagerTests.h, but not to ThreadManager.cpp, then the test will still fail, generally with an assert, but sometimes with a Bus Error. Test Procedure: 1) Apply the Tests.cpp patch. This makes the thread-manager portion of the test run ONLY the blockTest in an effectively infinite loop. 2) Run a make in lib/cpp in order to rebuild concurrency-test 3) Run concurrency test with the command line argument ""thread-manager"". This will start the blockTest loop. It should fail in a fairly short time. Repeated runs may fail different ways, including infinite hangs. 4) Apply the patch to ThreadManagerTests.h. 5) Run make in lib/cpp to rebuild concurrency-test 6) Run concurrency_test as before. It should probably run for a longer period of time. I have seen it run for an hour or more after beng patched. Eventually it should fail either with an assert in Monitor.cpp while trying to destroy a pthread_mutex_t, or it will get a Bus Error because it tried to execute invalid memory. I have also seen it hang forever. In order to resolve the remaining issues, a patch needs to be applied to ThreadManager.cpp. I will add a comment about that as soon as the issue is filed and the patch is available.",-0.0575,-0.05092857143,negative
thrift,4949,description,"I am currently improving the HTTP/1 test case. I found that the servlet 2.5 dependency package is relatively old, and users need to create the http environment themselves, so I added the embedded package of tomcat, Second, i improved the test case of the http application layer, so the client and server can communicate with each other and print the string.",design_debt,non-optimal_design,"Thu, 5 Sep 2019 14:00:41 +0000","Thu, 11 Feb 2021 22:27:45 +0000","Mon, 21 Oct 2019 14:22:33 +0000",3975712,"I am currently improving the HTTP/1 test case. I found that the servlet 2.5 dependency package is relatively old, and users need to create the http environment themselves, so I added the embedded package of tomcat, Second, i improved the test case of the http application layer, so the client and server can communicate with each other and print the string.",0.45,0.45,neutral
thrift,544,description,"The current generator produces multiple -define statements with the same name, which isn't valid erlang code (and also isn't valid semantically if we want two different values). produces: In the patched version, it produces this:",design_debt,non-optimal_design,"Tue, 21 Jul 2009 15:00:39 +0000","Fri, 10 Sep 2010 17:04:55 +0000","Thu, 5 Aug 2010 23:24:18 +0000",32862219,"The current generator produces multiple -define statements with the same name, which isn't valid erlang code (and also isn't valid semantically if we want two different values). produces: In the patched version, it produces this:",-0.1,-0.1,negative
thrift,628,description,"It turns out that Enums in Java don't have good hashcode behavior. It uses object ID, which can be inconsistent between different invocations of the same application, which breaks things like Hadoop partitioning. We should use the hash of the actual thrift field id instead of the hash of the enum version of the field.",design_debt,non-optimal_design,"Tue, 17 Nov 2009 21:27:06 +0000","Tue, 17 Nov 2009 21:56:51 +0000","Tue, 17 Nov 2009 21:56:51 +0000",1785,"It turns out that Enums in Java don't have good hashcode behavior. It uses object ID, which can be inconsistent between different invocations of the same application, which breaks things like Hadoop partitioning. We should use the hash of the actual thrift field id instead of the hash of the enum version of the field.",-0.446,-0.446,negative
thrift,636,description,"The intention of this idea is to minimize the amount of typing necessary to navigate thrift structures. The idea is to be able to ""include"" a field within a struct, like so (I don't know the syntax of Thrift annotations but this is the idea): struct B { 1: required i32 f1; 2: required i32 f2; } struct A { 1: @include required B b; 2: required i32 field2; } If we have an instance of A named ""a"", we can access the inner B's fields by saying ""a.get_f1()"". There's the obvious problem of name conflicts, but I think it's fine to leave it to the programmer to make sure the code is safe.",design_debt,non-optimal_design,"Fri, 20 Nov 2009 21:50:05 +0000","Thu, 2 May 2013 02:29:23 +0000","Fri, 26 Mar 2010 23:16:22 +0000",10891577,"The intention of this idea is to minimize the amount of typing necessary to navigate thrift structures. The idea is to be able to ""include"" a field within a struct, like so (I don't know the syntax of Thrift annotations but this is the idea): struct B { 1: required i32 f1; 2: required i32 f2; } struct A { 1: @include required B b; 2: required i32 field2; } If we have an instance of A named ""a"", we can access the inner B's fields by saying ""a.get_f1()"". There's the obvious problem of name conflicts, but I think it's fine to leave it to the programmer to make sure the code is safe.",0.03933333333,0.03933333333,neutral
thrift,685,description,"After poking around a bit and comparing how Thrift performs versus Protocol Buffers, I think we should change our transports and protocols to support optional direct buffer access behavior. Basically, the way this works is that if the transport is backed by a buffer, then it can give access to that buffer to the protocol. The protocol can then do things like read a byte without instantiating a new one-byte array or decode a string without an intermediate byte[] copy. In my initial testing, we can reduce the amount of time it takes to deserialize a struct by at least 25%. There are probably further gains to be had as well.",design_debt,non-optimal_design,"Thu, 21 Jan 2010 23:07:10 +0000","Thu, 18 Feb 2010 18:28:23 +0000","Thu, 18 Feb 2010 18:28:23 +0000",2402473,"After poking around a bit and comparing how Thrift performs versus Protocol Buffers, I think we should change our transports and protocols to support optional direct buffer access behavior. Basically, the way this works is that if the transport is backed by a buffer, then it can give access to that buffer to the protocol. The protocol can then do things like read a byte without instantiating a new one-byte array or decode a string without an intermediate byte[] copy. In my initial testing, we can reduce the amount of time it takes to deserialize a struct by at least 25%. There are probably further gains to be had as well.",0.2870666667,0.2870666667,neutral
thrift,701,description,"In glancing at one of our jars of generated Thrift class files, I noticed that it was pretty huge. Further digging showed that while certainly most of the bulk was attributable to the number of class files we have, part of it was due to lots of unnecessary internal classes. This is because we use a specific syntax for defining some maps inline in structs and enums. Since this is only syntactic sugar, I think it would make sense to write a tiny bit of helper code and avoid the need for a dynamic internal class.",design_debt,non-optimal_design,"Wed, 10 Feb 2010 23:42:07 +0000","Tue, 23 Mar 2010 05:39:48 +0000","Tue, 23 Mar 2010 05:39:48 +0000",3477461,"In glancing at one of our jars of generated Thrift class files, I noticed that it was pretty huge. Further digging showed that while certainly most of the bulk was attributable to the number of class files we have, part of it was due to lots of unnecessary internal classes. This is because we use a specific syntax for defining some maps inline in structs and enums. Since this is only syntactic sugar, I think it would make sense to write a tiny bit of helper code and avoid the need for a dynamic internal class.",0.25625,0.25625,neutral
thrift,710,description,TBinaryProtocol can probably take even more advantage of direct buffer access than TCompactProtocol. Identify the relevant spots and clean it up.,design_debt,non-optimal_design,"Thu, 18 Feb 2010 18:29:55 +0000","Tue, 2 Mar 2010 18:49:10 +0000","Tue, 2 Mar 2010 18:49:10 +0000",1037955,TBinaryProtocol can probably take even more advantage of direct buffer access than TCompactProtocol. Identify the relevant spots and clean it up.,0.523,0.523,neutral
thrift,714,description,"THsHaServer instantiates its ThreadPoolExecutor with a That behavior is documented in java as: There are three general strategies for queuing: ... 2. Unbounded queues. Using an unbounded queue (for example a LinkedBlockingQueue without a predefined capacity) will cause new tasks to wait in the queue when all corePoolSize threads are busy. Thus, no more than corePoolSize threads will ever be created. (And the value of the maximumPoolSize therefore doesn't have any effect.) This may be appropriate when each task is completely independent of others, so tasks cannot affect each others execution; for example, in a web page server. While this style of queuing can be useful in smoothing out transient bursts of requests, it admits the possibility of unbounded work queue growth when commands continue to arrive on average faster than they can be processed. therefore changing maxWorkerThreads (passed as maximumPoolSize) has no effect. The parameter should probably just be removed and minWorkerThreads renamed to numThreads, since setting minWorkerThreads does have an effect and is a workaround.",design_debt,non-optimal_design,"Wed, 24 Feb 2010 16:20:58 +0000","Wed, 28 Jul 2010 21:32:18 +0000","Wed, 28 Jul 2010 21:32:18 +0000",13324280,"THsHaServer instantiates its ThreadPoolExecutor with a LinkedBlockingQueue. That behavior is documented in java as: There are three general strategies for queuing: ... 2. Unbounded queues. Using an unbounded queue (for example a LinkedBlockingQueue without a predefined capacity) will cause new tasks to wait in the queue when all corePoolSize threads are busy. Thus, no more than corePoolSize threads will ever be created. (And the value of the maximumPoolSize therefore doesn't have any effect.) This may be appropriate when each task is completely independent of others, so tasks cannot affect each others execution; for example, in a web page server. While this style of queuing can be useful in smoothing out transient bursts of requests, it admits the possibility of unbounded work queue growth when commands continue to arrive on average faster than they can be processed. therefore changing maxWorkerThreads (passed as maximumPoolSize) has no effect. The parameter should probably just be removed and minWorkerThreads renamed to numThreads, since setting minWorkerThreads does have an effect and is a workaround.",0.1194444444,0.1075,neutral
thrift,730,description,"The configure script that's generated fails the ""checking for boost"" part if g++ is not installed. This confuses people into thinking something's wrong with the boost installation, whereas in fact missing g++ is the issue. A g++ check should be moved in front of the boost check.",design_debt,non-optimal_design,"Thu, 11 Mar 2010 21:08:42 +0000","Mon, 20 Sep 2010 22:57:01 +0000","Mon, 20 Sep 2010 22:57:01 +0000",16681699,"The configure script that's generated fails the ""checking for boost"" part if g++ is not installed. This confuses people into thinking something's wrong with the boost installation, whereas in fact missing g++ is the issue. A g++ check should be moved in front of the boost check.",0.0375,0.0375,negative
thrift,750,description,"The C++ Compiler currelty emits most functions in the *Client class as non-virtual. This makes it difficult to subclass the generated *Client class and override its functions. In particular, if a subclass overrides the *_send and *_recv functions, it must also override the function itself. Otherwise, the *Client version of the function calls the *Client versions of *_send and *_recv. A workaround is to inherit from the interface class *If, which has virtual functions, and use them to call *Client class member functions. But this can be cumbersome in some situations, and still requires additional functions to be overridden. I propose to add a virtual option to the C++ compiler that emits function declarations as virtual. I have attached a patched version of t_cpp_generator.cc from Thrift 0.2.0 - I can work out how to turn it into a patch file if needed. Is this worth merging into the trunk?",design_debt,non-optimal_design,"Fri, 2 Apr 2010 04:31:57 +0000","Thu, 14 Dec 2017 13:54:39 +0000","Thu, 26 Jan 2017 01:55:31 +0000",215213014,"The C++ Compiler currelty emits most functions in the *Client class as non-virtual. This makes it difficult to subclass the generated *Client class and override its functions. In particular, if a subclass overrides the *_send and *_recv functions, it must also override the function itself. Otherwise, the *Client version of the function calls the *Client versions of *_send and *_recv. A workaround is to inherit from the interface class *If, which has virtual functions, and use them to call *Client class member functions. But this can be cumbersome in some situations, and still requires additional functions to be overridden. I propose to add a virtual option to the C++ compiler that emits function declarations as virtual. I have attached a patched version of t_cpp_generator.cc from Thrift 0.2.0 - I can work out how to turn it into a patch file if needed. Is this worth merging into the trunk?",0.12405,0.12405,neutral
thrift,840,description,"The Perl protocol implementation can loop arbitrarily on corrupt data because Protocol::skip() skips nothing if it doesn't recognise a type. It might be nice if it threw an exception instead. For example, the loop to skip a map will keep looking at the same invalid bytes for every iteration if it hits an unrecognised type. If corrupt data has resulted in a bogus huge map size, then the loop goes on for a long while, rather than dying quickly after running out of available data. I recognise that input validation probably isn't a priority for Thrift (usually communications are well-formed!), but wonder if you'd consider the attached patch which dies if skip or skipBinary encounter types that they don't know what to do with. It probably needs more thought than I've given it, as I'm not sure what the correct behaviour should be for valid types not handled by the existing code: in the patch I'm throwing an exception for those saying that they cannot be skipped. An additional TType constant of MAX_TYPE might be helpful in writing a better solution. I know it's a bit weird to be suggesting this; I'm using Thrift for serialisation among other things, and with an data store. This ""infinite"" loop was the only instance in which your existing error handling didn't adequately flag up bad data. Clearly I should also be doing checksums on the data beforehand, but I just thought I'd suggest this to you. Thanks, Conrad",design_debt,non-optimal_design,"Fri, 6 Aug 2010 20:46:27 +0000","Thu, 2 Sep 2010 00:26:34 +0000","Thu, 2 Sep 2010 00:26:34 +0000",2259607,"The Perl protocol implementation can loop arbitrarily on corrupt data because Protocol::skip() skips nothing if it doesn't recognise a type. It might be nice if it threw an exception instead. For example, the loop to skip a map (lib/perl/lib/Thrift/Protocol.pm:374) will keep looking at the same invalid bytes for every iteration if it hits an unrecognised type. If corrupt data has resulted in a bogus huge map size, then the loop goes on for a long while, rather than dying quickly after running out of available data. I recognise that input validation probably isn't a priority for Thrift (usually communications are well-formed!), but wonder if you'd consider the attached patch which dies if skip or skipBinary encounter types that they don't know what to do with. It probably needs more thought than I've given it, as I'm not sure what the correct behaviour should be for valid types not handled by the existing code: in the patch I'm throwing an exception for those saying that they cannot be skipped. An additional TType constant of MAX_TYPE might be helpful in writing a better solution. I know it's a bit weird to be suggesting this; I'm using Thrift for serialisation among other things, and with an occasionally-imperfect data store. This ""infinite"" loop was the only instance in which your existing error handling didn't adequately flag up bad data. Clearly I should also be doing checksums on the data beforehand, but I just thought I'd suggest this to you. Thanks, Conrad",-0.04775,-0.04407692308,negative
thrift,897,description,"Through looking at THRIFT-544 and THRIFT-895, it's come to my attention that we currently register each of every enum's values as a global (and scoped) constant. This allows you to do things like: This is handy, insofar as you might want to use the values of an enum in constant or default circumstances. However, this behavior is unstable - if you have two enums with values that have the same name, all constant references will point at the last occurrence of the name. Further, in order to allow this to go on, we must not check if any constant has been declared twice, which means you can get stupid, detectable errors in your IDL very easily. I propose that we stop allowing this method of access, and instead require the enum values referenced in constant context to be prefixed with the enum type's name. For instance:",design_debt,non-optimal_design,"Fri, 10 Sep 2010 18:59:08 +0000","Sun, 12 Sep 2010 14:39:15 +0000","Sun, 12 Sep 2010 14:39:15 +0000",157207,"Through looking at THRIFT-544 and THRIFT-895, it's come to my attention that we currently register each of every enum's values as a global (and scoped) constant. This allows you to do things like: This is handy, insofar as you might want to use the values of an enum in constant or default circumstances. However, this behavior is unstable - if you have two enums with values that have the same name, all constant references will point at the last occurrence of the name. Further, in order to allow this to go on, we must not check if any constant has been declared twice, which means you can get stupid, detectable errors in your IDL very easily. I propose that we stop allowing this method of access, and instead require the enum values referenced in constant context to be prefixed with the enum type's name. For instance:",-0.03411904762,-0.03411904762,neutral
thrift,906,description,"The current haskell type mappings are awkward and error prone to work with: binary -string -byte -i16 -i32 -i64 - This patch updates the mappings to the canonical types of the correct length in Haskell: binary -string -byte -i16 -i32 -i64 - THIS BREAKS EXISTING CODE. It is, however, very arguably broken already. For convenience of patching, this patch is a superset of THRIFT-743. Thoughts?",design_debt,non-optimal_design,"Sun, 19 Sep 2010 02:20:29 +0000","Wed, 22 Sep 2010 00:49:12 +0000","Wed, 22 Sep 2010 00:49:12 +0000",253723,"The current haskell type mappings are awkward and error prone to work with: binary -> String string -> String byte -> Int i16 -> Int i32 -> Int i64 -> Int This patch updates the mappings to the canonical types of the correct length in Haskell: binary -> Data.ByteString.Lazy.ByteString string -> String byte -> Data.Word.Word8 i16 -> Data.Int.Int16 i32 -> Data.Int.Int32 i64 -> Data.Int.Int64 THIS BREAKS EXISTING CODE. It is, however, very arguably broken already. For convenience of patching, this patch is a superset of THRIFT-743. Thoughts?",0.00025,-0.01326666667,negative
thrift,931,description,"Right now, slf4j-simple is used when the tests are run. Unfortunately, the ""simple"" logger doesn't support debug level messages at all, which is somewhat of a pain. It would be good to switch to slf4j-log4j for the tests so we have debug logs available.",design_debt,non-optimal_design,"Mon, 27 Sep 2010 21:35:01 +0000","Tue, 1 Nov 2011 02:54:12 +0000","Mon, 27 Sep 2010 23:51:32 +0000",8191,"Right now, slf4j-simple is used when the tests are run. Unfortunately, the ""simple"" logger doesn't support debug level messages at all, which is somewhat of a pain. It would be good to switch to slf4j-log4j for the tests so we have debug logs available.",0.2232222222,0.2232222222,negative
thrift,959,description,"I was looking through TSocket today while reviewing THRIFT-106 and I noticed that in TSocket, when we open the socket/stream, we wrap the input/output streams with objects and use those for reading and writing. Two things stand out about this. Firstly, for some reason we're setting the buffer size specifically to 1KB, which is 1/8 the default. I think that number should be *at least* 8KB and more likely something like 32KB would be better. Anyone have any idea why we chose this size? Secondly, though, is the fact that we probably shouldn't be doing buffering here at all. The general pattern is to open a TSocket and wrap it in a TFramedTransport, which means that today, even though we're fully buffering in the framed transport, we're wastefully buffering again in the TSocket. This means we're wasting time and memory, and I wouldn't be surprised if this is artificially slowing down throughput, specifically for multi-KB requests and responses. If we remove the buffering from TSocket, I think we will probably need to add a TBufferedTransport to support users who are talking to non-Framed servers but still need buffering for performance.",design_debt,non-optimal_design,"Fri, 15 Oct 2010 21:13:09 +0000","Thu, 3 Jan 2019 04:32:08 +0000","Thu, 3 Jan 2019 04:29:35 +0000",259312586,"I was looking through TSocket today while reviewing THRIFT-106 and I noticed that in TSocket, when we open the socket/stream, we wrap the input/output streams with Buffered(Input|Output)Stream objects and use those for reading and writing. Two things stand out about this. Firstly, for some reason we're setting the buffer size specifically to 1KB, which is 1/8 the default. I think that number should be at least 8KB and more likely something like 32KB would be better. Anyone have any idea why we chose this size? Secondly, though, is the fact that we probably shouldn't be doing buffering here at all. The general pattern is to open a TSocket and wrap it in a TFramedTransport, which means that today, even though we're fully buffering in the framed transport, we're wastefully buffering again in the TSocket. This means we're wasting time and memory, and I wouldn't be surprised if this is artificially slowing down throughput, specifically for multi-KB requests and responses. If we remove the buffering from TSocket, I think we will probably need to add a TBufferedTransport to support users who are talking to non-Framed servers but still need buffering for performance.",0.04124074074,0.04124074074,neutral
thrift,1063,description,"A user was having issues with the erlang tutorial as it was out of date. I fixed and commited the change, but figured people would like a record of it being done.",documentation_debt,outdated_documentation,"Wed, 16 Feb 2011 18:08:52 +0000","Wed, 9 Mar 2011 18:17:14 +0000","Wed, 16 Feb 2011 18:10:25 +0000",93,"A user was having issues with the erlang tutorial as it was out of date. I fixed and commited the change, but figured people would like a record of it being done.",0.0,0.0,negative
thrift,1135,description,It would be great to have a tutorial for the Node.js implementation located at tutorial/nodejs/,documentation_debt,low_quality_documentation,"Fri, 8 Apr 2011 21:20:38 +0000","Wed, 12 Mar 2014 02:36:59 +0000","Wed, 12 Mar 2014 02:36:59 +0000",92294181,It would be great to have a tutorial for the Node.js implementation located at tutorial/nodejs/,0.3,0.3,positive
thrift,1294,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Tue, 6 Apr 2010 20:59:32 +0000 Subject: [PATCH 20/33] thrift: fix log message typos in TSimpleServer Summary: TSimpleSimple -- Also cleaned up some references that could be const. Test Plan: It compiles. Revert Plan: OK  | 15 +++++++++ 1 files changed, 9 insertions(+), 6 deletions(-)",documentation_debt,low_quality_documentation,"Thu, 25 Aug 2011 16:50:47 +0000","Thu, 25 Aug 2011 18:12:56 +0000","Thu, 25 Aug 2011 17:44:41 +0000",3234,"From 1cedaf9061760446e5c70de0797b93c3837e9841 Mon Sep 17 00:00:00 2001 From: Adam Simpkins <simpkins@fb.com> Date: Tue, 6 Apr 2010 20:59:32 +0000 Subject: [PATCH 20/33] thrift: fix log message typos in TSimpleServer Summary: TSimpleSimple --> TSimpleServer Also cleaned up some references that could be const. Test Plan: It compiles. Revert Plan: OK  lib/cpp/src/server/TSimpleServer.cpp | 15 +++++++++------ 1 files changed, 9 insertions, 6 deletions",0.25,0.1875,neutral
thrift,1426,description,"dist package missing javame, cpp concurrency headers, nodejs, erl tutorial, all windows additions, and other random files",documentation_debt,low_quality_documentation,"Tue, 15 Nov 2011 13:18:27 +0000","Wed, 16 Nov 2011 13:25:27 +0000","Wed, 16 Nov 2011 12:59:10 +0000",85243,"dist package missing javame, cpp concurrency headers, nodejs, erl tutorial, all windows additions, and other random files",-0.4,-0.4,negative
thrift,1466,description,"When I try to generate the cglib code for the Cassandra Interface, I do not get a skeleton file. Also, I do not see any example files onto how to use the generated code, and I can't seem to find any examples around. So can anybody please offer some proper examples on how to use the Thrift generated glibc code for Cassandra ? Thanks",documentation_debt,low_quality_documentation,"Thu, 15 Dec 2011 15:35:32 +0000","Fri, 27 Jan 2012 02:34:29 +0000","Fri, 27 Jan 2012 02:34:28 +0000",3668336,"When I try to generate the cglib code for the Cassandra Interface, I do not get a skeleton file. Also, I do not see any example files onto how to use the generated code, and I can't seem to find any examples around. So can anybody please offer some proper examples on how to use the Thrift generated glibc code for Cassandra ? Thanks",0.191375,0.191375,negative
thrift,147,description,"Generated code should include the provided docstrings (from the IDL) as RDoc on the classes. If the fields weren't generated at runtime, they should have RDoc too.",documentation_debt,low_quality_documentation,"Mon, 22 Sep 2008 20:52:23 +0000","Thu, 2 May 2013 02:29:17 +0000","Fri, 31 Oct 2008 00:33:45 +0000",3296482,"Generated code should include the provided docstrings (from the IDL) as RDoc on the classes. If the fields weren't generated at runtime, they should have RDoc too.",0.0,0.0,neutral
thrift,1702,description,"Attached is a manual for the Thrift compiler in mdoc format, and a patch to eliminate the brutal 80-line error message that was usage(). The man page may be previewed at The tail end is incomplete because the information didn't happen to be included in the usage() text. I hope you agree the manual is easier to read and more convenient to use.",documentation_debt,low_quality_documentation,"Sat, 22 Sep 2012 03:20:42 +0000","Sat, 29 Sep 2012 00:31:31 +0000","Sat, 29 Sep 2012 00:31:31 +0000",594649,"Attached is a manual for the Thrift compiler in mdoc format, and a patch to eliminate the brutal 80-line error message that was usage(). The man page may be previewed at http://www.schemamania.org/thrift/thrift.pdf. The tail end is incomplete because the information didn't happen to be included in the usage() text. I hope you agree the manual is easier to read and more convenient to use.",0.175,0.13125,neutral
thrift,1734,description,"The front webpage of is still displaying the current release version as v0.8, instead of the newly released v0.9 as displayed on the downloads page. I presume this is because the instructions at do not specify that this page needs updating as well.",documentation_debt,outdated_documentation,"Thu, 18 Oct 2012 08:31:21 +0000","Sat, 8 Jun 2013 03:15:40 +0000","Sat, 8 Jun 2013 03:15:39 +0000",20112258,"The front webpage of http://thrift.apache.org is still displaying the current release version as v0.8, instead of the newly released v0.9 as displayed on the downloads page. I presume this is because the instructions at http://thrift.apache.org/docs/committers/HowToPublish do not specify that this page needs updating as well.",0.228,0.228,negative
thrift,1800,description,"Comments containing non-ascii characters (like for example the swedish ""Vi mter temperaturer i C"") show up incorrectly in the generated HTML file. Only a handful of characters is encoded at all, and only at a few rare occasions.",documentation_debt,low_quality_documentation,"Fri, 21 Dec 2012 21:45:26 +0000","Sat, 8 Jun 2013 03:15:43 +0000","Sat, 8 Jun 2013 03:15:43 +0000",14535017,"Comments containing non-ascii characters (like for example the swedish ""Vi mter temperaturer i C"") show up incorrectly in the generated HTML file. Only a handful of characters is encoded at all, and only at a few rare occasions.",0.0,0.0,negative
thrift,1879,description,"It would be nice if thrift_c_glib bindings supported GObject Introspection (see This would make Vala bindings (THRIFT-1741) easy. Actually adding G-I is pretty easy (I already have a patch to do it), but would require migrating the documentation to the gtk-doc format, so before I invest the time I'd like to make sure this is something you're interested in.",documentation_debt,outdated_documentation,"Mon, 11 Mar 2013 07:14:27 +0000","Sat, 8 Jun 2013 18:34:31 +0000","Sat, 8 Jun 2013 18:34:31 +0000",7730404,"It would be nice if thrift_c_glib bindings supported GObject Introspection (see https://live.gnome.org/GObjectIntrospection). This would make Vala bindings (THRIFT-1741) easy. Actually adding G-I is pretty easy (I already have a patch to do it), but would require migrating the documentation to the gtk-doc format, so before I invest the time I'd like to make sure this is something you're interested in.",0.4015,0.4006111111,positive
thrift,1883,description,"Right now thrift_c_glib contains a few sporadic doxygen-style comments which are, AFAICT, not actually used to generate any documentation. In order to generate a GObject Introspection repository g-ir-scanner scans the source code and looks for GTK-Doc comments which contain annotations (see necessary for the GIR to work properly. I've fully documented thrift_c_glib (well, the API is fully covered, although I'm the first to admit the documentation could be better) using GTK-Doc comments and added support to the build system to go ahead and actually build a manual.",documentation_debt,low_quality_documentation,"Thu, 14 Mar 2013 07:41:50 +0000","Sat, 8 Jun 2013 18:35:53 +0000","Sat, 8 Jun 2013 18:35:53 +0000",7469643,"Right now thrift_c_glib contains a few sporadic doxygen-style comments which are, AFAICT, not actually used to generate any documentation. In order to generate a GObject Introspection repository g-ir-scanner scans the source code and looks for GTK-Doc comments which contain annotations (see https://live.gnome.org/GObjectIntrospection/Annotations) necessary for the GIR to work properly. I've fully documented thrift_c_glib (well, the API is fully covered, although I'm the first to admit the documentation could be better) using GTK-Doc comments and added support to the build system to go ahead and actually build a manual.",0.1932333333,0.1932333333,neutral
thrift,2088,description,"There are a few typos in the delphi Compiler options text. Additionally, the text is not really well written.",documentation_debt,low_quality_documentation,"Sun, 14 Jul 2013 11:49:07 +0000","Thu, 25 Jul 2013 03:40:06 +0000","Sun, 14 Jul 2013 11:57:14 +0000",487,"There are a few typos in the delphi Compiler options text. Additionally, the text is not really well written.",-0.3155,-0.3155,negative
thrift,2290,description,"-When THRIFT-2232 has been accepted, the Go tutorial (both code and web site) will need an appropriate update.- No changes necessary, only another minor bug has been uncovered while building the Go tutorial:",documentation_debt,outdated_documentation,"Sat, 14 Dec 2013 12:23:57 +0000","Thu, 2 Jan 2014 22:30:12 +0000","Tue, 24 Dec 2013 16:20:52 +0000",878215,"When THRIFT-2232 has been accepted, the Go tutorial (both code and web site) will need an appropriate update. No changes necessary, only another minor bug has been uncovered while building the Go tutorial:",0.49,0.4916666667,neutral
thrift,2375,description,"This was working ""right"" (at least how I thought it should work) back in 0.9.0 going back at least to 0.7.0 and introduced in 0.9.1 The issue is that there is a '<br I will attach complete sample thrift files and html output, but here is a summary: This is what the output USED to look like And this is what it looks like today",documentation_debt,low_quality_documentation,"Sun, 23 Feb 2014 02:53:28 +0000","Tue, 1 Apr 2014 20:01:32 +0000","Tue, 11 Mar 2014 20:33:57 +0000",1446029,"This was working ""right"" (at least how I thought it should work) back in 0.9.0 going back at least to 0.7.0 and introduced in 0.9.1 https://git-wip-us.apache.org/repos/asf?p=thrift.git;a=commit;h=63e3c63 The issue is that there is a '<br>' in HTML after every newline in the source comment (thrift). I assume that this was not the intention of the THRIFT-1800 change. I will attach complete sample thrift files and html output, but here is a summary: This is what the output USED to look like <tr><td>1</td><td>theThing</td><td><code>i32</code></td><td>Some comments can go for quite a while and may span multiple lines. What looks like a good spot to break a line in the thrift file may turn out to be not so great in HTML. In fact in HTML we should let the browser decide when to start a new lines. Users can still insert a break<br> when they really need/want it </td><td>required</td><td></td></tr> </table><br/></div></div></body></html> And this is what it looks like today <tr><td>1</td><td>theThing</td><td><code>i32</code></td><td>Some comments can go for quite a while and may span multiple<br/>lines. What looks like a good spot to break a li ne in the<br/>thrift file may turn out to be not so great in HTML. In<br/>fact in HTML we should let the browser decide when to start<br/>a new lines. Users can still<br/> insert a break<br> when they really need/want it<br/></td><td>required</td><td></td></tr> </table><br/></div></div></body></html>",0.5756666667,0.335462963,neutral
thrift,2487,description,"Several people had problems following the tutorial, especially under Windows. The Thrift web site offers an EXE download and the {{tutorial.thrift}} file on the [tutorial However, the file {{shared.thrift}}, which is also needed as it is included in {{tutorial.thrift}} is not available there. The workaround is to get the file from teh source tree, e.g. from",documentation_debt,low_quality_documentation,"Sat, 19 Apr 2014 21:13:04 +0000","Thu, 10 Jul 2014 13:42:30 +0000","Thu, 10 Jul 2014 13:42:30 +0000",7057766,"Several people had problems following the tutorial, especially under Windows. The Thrift web site offers an EXE download and the tutorial.thrift file on the tutorial pages. However, the file shared.thrift, which is also needed as it is included in tutorial.thrift is not available there. The workaround is to get the file from teh source tree, e.g. from https://git-wip-us.apache.org/repos/asf?p=thrift.git;a=blob_plain;f=tutorial/shared.thrift",-0.03333333333,-0.02857142857,negative
thrift,2677,description,"Some recent changes in the {{network}} library for Haskell require conditional compilation, both the haskell tutorial and library need to be updated to take this into account. Description of the solution is on {{network}}'s hackage page: Travis fails with: Travis run with this patch applied: Tutorial successfully compiles: cc:",documentation_debt,outdated_documentation,"Tue, 26 Aug 2014 05:21:45 +0000","Wed, 5 Nov 2014 04:49:05 +0000","Mon, 1 Sep 2014 19:56:18 +0000",570873,"Some recent changes in the network library for Haskell require conditional compilation, both the haskell tutorial and library need to be updated to take this into account. Description of the solution is on network's hackage page: http://hackage.haskell.org/package/network-2.6.0.1 Travis fails with: Travis run with this patch applied: https://travis-ci.org/cheecheeo/thrift/builds/33556425 Tutorial successfully compiles: cc: roger.meier",0.07083333333,0.04722222222,neutral
thrift,2833,description,"It is written in the download page that Thrift 0.9.2 is available on Maven Central. Sadly, this is not the case.",documentation_debt,low_quality_documentation,"Mon, 17 Nov 2014 13:56:36 +0000","Mon, 26 Jan 2015 01:56:37 +0000","Tue, 18 Nov 2014 18:35:48 +0000",103152,"It is written in the download page (https://thrift.apache.org/download) that Thrift 0.9.2 is available on Maven Central. Sadly, this is not the case.",-0.1875,-0.1875,negative
thrift,2853,description,THRIFT-2852 changed behavior of which made some comments invalid. This PR removes the remaining comments. PR:,documentation_debt,outdated_documentation,"Mon, 24 Nov 2014 22:11:49 +0000","Tue, 25 Nov 2014 00:18:22 +0000","Mon, 24 Nov 2014 23:40:16 +0000",5307,THRIFT-2852 changed behavior of iostream_transport.go which made some comments invalid. This PR removes the remaining comments. PR: https://github.com/apache/thrift/pull/286,0.0,0.125,negative
thrift,3297,description,"The tutorial says: However, when I run I get: There is no definition in other files either: What am I missing?",documentation_debt,low_quality_documentation,"Mon, 17 Aug 2015 17:51:41 +0000","Tue, 25 Aug 2015 04:44:51 +0000","Wed, 19 Aug 2015 17:39:42 +0000",172081,"The tutorial says: For each service the Thrift compiler generates an abstract base class from which handler implementations should inherit. In our case TutorialCalculatorHandler inherits from CalculatorHandler, defined in gen-c_glib/calculator.h. However, when I run I get: There is no definition in other files either: What am I missing?",0.4,0.3,negative
thrift,3311,description,The readme formatting is incorrect - see the attached screenshot.,documentation_debt,low_quality_documentation,"Sun, 30 Aug 2015 17:14:51 +0000","Fri, 25 Sep 2015 02:43:16 +0000","Sun, 30 Aug 2015 18:01:51 +0000",2820,The readme formatting is incorrect - see the attached screenshot.,0.0,0.0,negative
thrift,4015,description,"Some people prefer to spell Thrift as ""Thirft"", which is not really correct.",documentation_debt,low_quality_documentation,"Mon, 26 Dec 2016 10:34:49 +0000","Sat, 14 Jan 2017 09:04:50 +0000","Mon, 26 Dec 2016 10:43:55 +0000",546,"Some people prefer to spell Thrift as ""Thirft"", which is not really correct.",-0.875,-0.875,negative
thrift,962,description,"It's just a weak skeleton. I mean, really, really weak. At the very least, we should point to TRUNK/tutorial/ for where to look at code examples.",documentation_debt,low_quality_documentation,"Mon, 18 Oct 2010 23:53:50 +0000","Thu, 10 Jul 2014 13:42:34 +0000","Thu, 10 Jul 2014 13:42:34 +0000",117553724,"It's just a weak skeleton. I mean, really, really weak. At the very least, we should point to TRUNK/tutorial/ for where to look at code examples.",-0.2666666667,-0.2666666667,negative
thrift,2031,description,"It's a socket option that we haven't made configurable yet, and sometimes you need it.",requirement_debt,requirement_partially_implemented,"Fri, 14 Jun 2013 20:57:16 +0000","Fri, 12 Aug 2016 01:29:46 +0000","Fri, 8 Apr 2016 05:36:10 +0000",88850334,"It's a socket option that we haven't made configurable yet, and sometimes you need it.",0.0,0.0,neutral
thrift,21,description,"TThreadPoolServer currently accepts incoming connections in threads. This means that at any time, as long as the thread pool is not completely full of running connections, there are multiple threads currently blocking on the #accept call. This is dangerous because the accept syscall is not documented as being thread-safe. The only reason this actually works in ruby is because of the cooperative threading, but if this library is used in any ruby interpreter that supports native threads (e.g. MacRuby, jruby, etc) I would expect it to start manifesting strange bugs.",requirement_debt,non-functional_requirements_not_fully_satisfied,"Mon, 26 May 2008 23:45:00 +0000","Thu, 26 Jun 2008 17:33:40 +0000","Thu, 26 Jun 2008 17:33:40 +0000",2656120,"TThreadPoolServer currently accepts incoming connections in threads. This means that at any time, as long as the thread pool is not completely full of running connections, there are multiple threads currently blocking on the #accept call. This is dangerous because the accept syscall is not documented as being thread-safe. The only reason this actually works in ruby is because of the cooperative threading, but if this library is used in any ruby interpreter that supports native threads (e.g. MacRuby, jruby, etc) I would expect it to start manifesting strange bugs.",0.04166666667,0.04166666667,negative
thrift,2292,description,Importing of Android Library Project into Android build from plugin dont implemented. So i cant add some sdks into native plugin that use dependencies as android library project.,requirement_debt,requirement_partially_implemented,"Fri, 20 Dec 2013 09:53:21 +0000","Mon, 23 Dec 2013 16:12:40 +0000","Mon, 23 Dec 2013 16:12:40 +0000",281959,Importing of Android Library Project into Android build from plugin dont implemented. So i cant add some sdks into native plugin that use dependencies as android library project.,0.0,0.0,negative
thrift,2404,description,"As long as there is no special treatment of {{list<byte Thus, the compiler should emit an adequate warning when {{list<byte",requirement_debt,requirement_partially_implemented,"Fri, 14 Mar 2014 21:42:30 +0000","Tue, 1 Apr 2014 20:01:29 +0000","Sun, 16 Mar 2014 14:49:06 +0000",147996,"As long as there is no special treatment of list<byte> implemented, it is a common beginner's trap to choose list<byte> where really binary would be the better choice. Thus, the compiler should emit an adequate warning when list<byte> is found in the IDL.",0.054,0.102,neutral
thrift,2622,description,"When I deploy my application in production Environment it always generate error ""Expecting Source code is // framed transport while (data.length) { if (frameLeft === 0) { // TODO assumes we have all 4 bytes if (data.length < 4) { residual = data; break; //throw Error(""Expecting } frameLeft = 0); frame = new Buffer(frameLeft); framePos = 0; data = data.slice(4, data.length); } I don't know why Can anybody help me",requirement_debt,requirement_partially_implemented,"Fri, 11 Jul 2014 06:17:23 +0000","Sun, 27 Jul 2014 02:52:19 +0000","Tue, 22 Jul 2014 09:24:42 +0000",961639,"When I deploy my application in production Environment it always generate error ""Expecting > 4 bytes, found only 2"" every one or two minutes Source code is // framed transport while (data.length) { if (frameLeft === 0) { // TODO assumes we have all 4 bytes if (data.length < 4) { console.log(""Expecting > 4 bytes, found only "" + data.length); residual = data; break; //throw Error(""Expecting > 4 bytes, found only "" + data.length); } frameLeft = binary.readI32(data, 0); frame = new Buffer(frameLeft); framePos = 0; data = data.slice(4, data.length); } I don't know why Can anybody help me",-0.24,-0.09166666667,negative
thrift,3592,description,"Unlike test server, client can be partially implemented. The patch just converts the existing manual test client to cross test client by adding argument support.",requirement_debt,requirement_partially_implemented,"Mon, 1 Feb 2016 12:57:29 +0000","Sat, 27 Feb 2016 09:26:30 +0000","Sat, 27 Feb 2016 08:01:02 +0000",2228613,"Unlike test server, client can be partially implemented. The patch just converts the existing manual test client to cross test client by adding argument support.",0.2,0.2,neutral
thrift,4443,description,"The skip function is unsupported in the node.js implementation of the json_protocol. Interestingly, the compact_protocol implements the skip function, as does the JavaScript version.",requirement_debt,requirement_partially_implemented,"Fri, 5 Jan 2018 22:39:16 +0000","Thu, 27 Dec 2018 15:25:21 +0000","Thu, 11 Jan 2018 02:21:47 +0000",445351,"The skip function is unsupported in the node.js implementation of the json_protocol. Interestingly, the compact_protocol implements the skip function, as does the JavaScript version.",0.03333333333,0.03333333333,neutral
thrift,4829,description,"The current implementation of the has no possibility to use layered protocols like framed, because the class does not support it. Patch follows.",requirement_debt,requirement_partially_implemented,"Wed, 20 Mar 2019 13:48:34 +0000","Wed, 16 Oct 2019 22:27:40 +0000","Thu, 21 Mar 2019 08:07:32 +0000",65938,"The current implementation of the THttpServerTransport has no possibility to use layered protocols like framed, because the class does not support it. Patch follows.",-0.2,-0.2,negative
thrift,4882,description,"WinHTTP comes with limited AutoProxy support. In order to actually use it, there is some extra work required at the application level. See",requirement_debt,requirement_partially_implemented,"Thu, 6 Jun 2019 22:34:43 +0000","Wed, 16 Oct 2019 22:27:40 +0000","Fri, 7 Jun 2019 20:27:25 +0000",78762,"WinHTTP comes with limited AutoProxy support. In order to actually use it, there is some extra work required at the application level. See https://docs.microsoft.com/en-us/windows/desktop/winhttp/winhttp-autoproxy-api",0.2166666667,0.2166666667,negative
thrift,5152,description,"Currently go TSocket only contains *single timeout* for connection timeout and read/write timeout. Meanwhile in java TSocket contains *connectTimeout_* and *socketTimeout_* for connection timeout and read/write timeout. In the real production environment, we do need different timeout for connection timeout and read/write timeout, so I created this Jira to improve this.",requirement_debt,requirement_partially_implemented,"Thu, 26 Mar 2020 09:39:04 +0000","Thu, 11 Feb 2021 22:26:33 +0000","Thu, 4 Feb 2021 10:19:14 +0000",27218410,"Currently go TSocket only contains single timeout for connection timeout and read/write timeout. Meanwhile in java TSocket containsconnectTimeout_ and socketTimeout_forconnection timeout and read/write timeout. In the real production environment, we do need different timeout forconnection timeout and read/write timeout, so I created this Jira to improve this.",0.3,0.3,neutral
thrift,1103,description,"New implementation of zlib compressed transport for python. The attached patch provides a zlib compressed transport wrapper for python. It is similar to the TFramedTransport, in that it wraps another transport, implementing the data compression as a transformation layer on top of the underlying transport that it wraps. The compression level is configurable in the constructor, from 0 (none) to 9 (best) and defaults to 9 for best compression. The way this works is that every write() to the transport appends more data to the internal cStringIO write buffer. When the transport's flush() method is called, the buffered bytes are then passed to a zlib Compressor object and flush()ed with zlib.Z_SYNC_FLUSH. Because the thrift API calls the transport's flush() after writeMessageEnd(), this means very small thrift RPC calls don't get compressed well. This transport works best on thrift protocols where the payload contains strings longer than 10 characters. As with all data compression, the more redundancy in the uncompressed input, the greater the resulting compression. The TZlibTransport class also implements some basic statistics that track the number of raw bytes written and read, versus the decompressed equivalent. The getCompRatio() method returns a tuple of where ratio is computed using: (So 10 compression is 0.10, meaning smaller numbers are better.) The getCompSavings() method returns the actual number of which might be negative when the compression of non-compressible data ends up expanding the data. So hopefully, anyone who uses this transport will be able to tell whether the compression is saving bandwidth or not. I will add the patch in a few minutes. I haven't tested this against the C++ TZlibTransport, only against itself.",test_debt,lack_of_tests,"Mon, 21 Mar 2011 01:48:29 +0000","Tue, 22 Mar 2011 18:06:16 +0000","Tue, 22 Mar 2011 18:06:16 +0000",145067,"New implementation of zlib compressed transport for python. The attached patch provides a zlib compressed transport wrapper for python. It is similar to the TFramedTransport, in that it wraps another transport, implementing the data compression as a transformation layer on top of the underlying transport that it wraps. The compression level is configurable in the constructor, from 0 (none) to 9 (best) and defaults to 9 for best compression. The way this works is that every write() to the transport appends more data to the internal cStringIO write buffer. When the transport's flush() method is called, the buffered bytes are then passed to a zlib Compressor object and flush()ed with zlib.Z_SYNC_FLUSH. Because the thrift API calls the transport's flush() after writeMessageEnd(), this means very small thrift RPC calls don't get compressed well. This transport works best on thrift protocols where the payload contains strings longer than 10 characters. As with all data compression, the more redundancy in the uncompressed input, the greater the resulting compression. The TZlibTransport class also implements some basic statistics that track the number of raw bytes written and read, versus the decompressed equivalent. The getCompRatio() method returns a tuple of (readCompressionRatio,writeCompressionRatio) where ratio is computed using: compressed_bytes/uncompressed_bytes. (So 10 compression is 0.10, meaning smaller numbers are better.) The getCompSavings() method returns the actual number of (saved_read_bytes,saved_write_bytes) which might be negative when the compression of non-compressible data ends up expanding the data. So hopefully, anyone who uses this transport will be able to tell whether the compression is saving bandwidth or not. I will add the patch in a few minutes. I haven't tested this against the C++ TZlibTransport, only against itself.",0.1592708333,0.1812745098,neutral
thrift,1311,description,"I see that some people have been working very hard on the Java library and that's really appreciated, however, some of the changes seem to have interfered with the java server JSON/HTTP implementation which is used to test the JS client. Please see below: thrift/lib/js/test$ ant testserver [...] [java] New connection thread [java] Incoming content: [java] Outgoing content: Error:Unexpected character:[ [java] New connection thread [java] Incoming content: [java] Outgoing content: Error:Unexpected character:[ ps.: I hope one day we can automatize these tests with node.js, or something like that, so we don't need to run this manually",test_debt,low_coverage,"Tue, 30 Aug 2011 13:36:47 +0000","Fri, 2 Sep 2011 09:09:34 +0000","Fri, 2 Sep 2011 09:09:34 +0000",243167,"I see that some people have been working very hard on the Java library and that's really appreciated, however, some of the changes seem to have interfered with the java server JSON/HTTP implementation which is used to test the JS client. Please see below: thrift/lib/js/test$ ant testserver [...] [java] New connection thread [java] Incoming content: [1,""testSet"",1,0,{""1"":{""set"":[""i32"",3,1,2,3]}}] [java] Outgoing content: Error:Unexpected character:[ [java] New connection thread [java] Incoming content: [1,""testList"",1,0,{""1"":{""lst"":[""i32"",3,1,2,3]}}] [java] Outgoing content: Error:Unexpected character:[ ps.: I hope one day we can automatize these tests with node.js, or something like that, so we don't need to run this manually",0.1458333333,0.1099358974,positive
thrift,1810,description,"we need ruby as part of the cross language test suite, currently just a shell script (test/test.sh) running on Linux. references THRIFT-1766",test_debt,lack_of_tests,"Thu, 27 Dec 2012 00:19:05 +0000","Sun, 23 Feb 2014 20:56:22 +0000","Sun, 23 Feb 2014 20:56:14 +0000",36621429,"we need ruby as part of the cross language test suite, currently just a shell script (test/test.sh) running on Linux. references THRIFT-1766",0.0,0.0,neutral
thrift,2171,description,"There is hardly any test coverage for the NodeJS implementation. Please comment if you have ideas for how to architect tests. Patches welcome, too.",test_debt,low_coverage,"Fri, 6 Sep 2013 05:04:24 +0000","Sun, 6 Jul 2014 04:01:01 +0000","Thu, 3 Apr 2014 23:36:43 +0000",18124339,"There is hardly any test coverage for the NodeJS implementation. Please comment if you have ideas for how to architect tests. Patches welcome, too.",0.3166666667,0.3166666667,neutral
thrift,2210,description,"TSimpleJSONProtocol can emit JSON with maps whose keys are not string (which is not allowed is the JSON spec). This happens if the key in a map is anything other than a String (int, enum, etc) For example, it can emit JSON like this: which should be: I have a path that fixes this, I'll upload it shortly (still trying to get my dev environment to run the tests). Also AFAICT there is no unit test for TSimpleJSONProtocol -- I'll try and add one to the patch. Thanks! Alex",test_debt,lack_of_tests,"Thu, 26 Sep 2013 06:37:02 +0000","Sun, 6 Mar 2016 17:25:17 +0000","Fri, 27 Sep 2013 14:13:50 +0000",113808,"TSimpleJSONProtocol can emit JSON with maps whose keys are not string (which is not allowed is the JSON spec). This happens if the key in a map is anything other than a String (int, enum, etc) For example, it can emit JSON like this: which should be: I have a path that fixes this, I'll upload it shortly (still trying to get my dev environment to run the tests). Also AFAICT there is no unit test for TSimpleJSONProtocol  I'll try and add one to the patch. Thanks! Alex",0.09873333333,0.09873333333,neutral
thrift,2351,description,"There are two typos in that prevent correct decoding of any message. They are on consecutive lines: First $seqId does not match case of argument $seqid so this is not set PHP is not case sensitive for class names but IS for variable names. Second and more importantly, on the following line $name has the decoded length assigned to it instead of $result. This means the method name ""hello"" ends up being decoded as ""6"" (the length of the string plus length prefix) and hence any processor trying to dispatch the event will fail. I can submit a patch/pull request but its only 6 chars to change and clearly there are no automated tests for this protocol implementation as that should have been caught before. I guess not many other people are using PHP as a thrift service processor with this protocol.",test_debt,low_coverage,"Thu, 6 Feb 2014 18:00:17 +0000","Mon, 29 Jun 2015 21:32:53 +0000","Thu, 10 Jul 2014 13:42:25 +0000",13290128,"There are two typos in TCompactProtocol::readMessageBegin that prevent correct decoding of any message. They are on consecutive lines: https://github.com/apache/thrift/blob/master/lib/php/lib/Thrift/Protocol/TCompactProtocol.php#L390-L391 First $seqId does not match case of argument $seqid so this is not set PHP is not case sensitive for class names but IS for variable names. Second and more importantly, on the following line $name has the decoded length assigned to it instead of $result. This means the method name ""hello"" ends up being decoded as ""6"" (the length of the string plus length prefix) and hence any processor trying to dispatch the event will fail. I can submit a patch/pull request but its only 6 chars to change and clearly there are no automated tests for this protocol implementation as that should have been caught before. I guess not many other people are using PHP as a thrift service processor with this protocol.",-0.1013611111,-0.1013611111,negative
thrift,2431,description,"This test is flaky, I can easily make it fail: Seems like this has hit a in Jenkins already.",test_debt,flaky_test,"Sun, 30 Mar 2014 05:34:11 +0000","Wed, 6 Jan 2016 02:06:07 +0000","Sun, 29 Nov 2015 16:28:18 +0000",52656847,"This test is flaky, I can easily make it fail: Seems like this has hit a couple times in Jenkins already.",-0.4,-0.4,negative
thrift,2589,description,Compiling a thrift file containing const definitions for basetype variables results in static properties {{public static whatsits}} being generated. Should generate const properties {{public const whatsits}}. Current version generates this from The code should instead look like this. A patch for this change is supplied. As i don't really know yet how the testing for these kind of changes is done i haven't supplied one. But generating all the .thrift files in test with a before and after version of the compiler and comparing the output of both looked good to me.,test_debt,lack_of_tests,"Wed, 25 Jun 2014 09:06:27 +0000","Wed, 5 Nov 2014 04:48:22 +0000","Thu, 7 Aug 2014 20:44:10 +0000",3757063,Compiling a thrift file containing const definitions for basetype variables results in static properties public static whatsits being generated. Should generate const properties public const whatsits. Current version generates this from test/ConstantsDemo.thrift The code should instead look like this. A patch for this change is supplied. As i don't really know yet how the testing for these kind of changes is done i haven't supplied one. But generating all the .thrift files in test with a before and after version of the compiler and comparing the output of both looked good to me.,-0.01414285714,-0.012375,neutral
thrift,3440,description,"It runs every combination of I want to reduce this keeping the same code coverage, so that we can run it more often.",test_debt,expensive_tests,"Mon, 23 Nov 2015 10:00:28 +0000","Wed, 6 Jan 2016 02:06:12 +0000","Mon, 23 Nov 2015 14:54:38 +0000",17650,"It runs every combination of compiler-option/server/transport/protocol/zlib/ssl. I want to reduce this keeping the same code coverage, so that we can run it more often.",0.2,0.1,neutral
thrift,4419,description,"Related to THRIFT-4390 Description copied form there: While working on improving test coverage and fixing busted cross tests I reworked the cpp test client to send binary in at size 0, 1, 2, 4, 6, 16, ..., 131072 and after 4096 the rust server gave up.",test_debt,low_coverage,"Mon, 11 Dec 2017 16:12:36 +0000","Thu, 27 Dec 2018 15:25:18 +0000","Sat, 17 Mar 2018 08:40:11 +0000",8267255,"Related to THRIFT-4390 Description copied form there: While working on improving test coverage and fixing busted cross tests I reworked the cpp test client to send binary in at size 0, 1, 2, 4, 6, 16, ..., 131072 and after 4096 the rust server gave up.",0.55,0.55,neutral
