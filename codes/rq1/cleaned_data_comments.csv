project,issue_number,issue_type,text,classification,indicator,zz_created,zz_updated,zz_resolved,zz_duration,zz_text,zz_wink_a,zz_wink_b,roberta
camel,201,comment_1,Johathan. The patch looks great. Only two issues with the ident of the code. One @override was not idented properly. And one method parameter was on a new line instead of singleline. Just nitpicking. Would love to have it documented on the wiki that we got this new transform DSL now. And we should remember to add it to the release notes that setOutBody() is depreacted and replaced with transform() And since setOutBody() is to be replaced with transform. Could we have a unit test that verifies a setOutBody() test that is done by transform render the same OUT body?,documentation_debt,outdated_documentation,"Fri, 2 Nov 2007 17:37:12 +0000","Fri, 11 Jul 2008 04:25:15 +0000","Tue, 6 May 2008 16:22:20 +0000",16065908,Johathan. The patch looks great. Only two issues with the ident of the code. One @override was not idented properly. And one method parameter was on a new line instead of singleline. Just nitpicking. Would love to have it documented on the wiki that we got this new transform DSL now. And we should remember to add it to the release notes that setOutBody() is depreacted and replaced with transform() And since setOutBody() is to be replaced with transform. Could we have a unit test that verifies a setOutBody() test that is done by transform render the same OUT body?,0.1077777778,0.1077777778,positive
camel,201,comment_2,"Hey Claus, Nitpick away :) I fixed up the indentations and added a test that verifies the deprecated setOutBody method still behaves. Yeah, the wiki will need to be updated once this feature gets in.",documentation_debt,outdated_documentation,"Fri, 2 Nov 2007 17:37:12 +0000","Fri, 11 Jul 2008 04:25:15 +0000","Tue, 6 May 2008 16:22:20 +0000",16065908,"Hey Claus, Nitpick away I fixed up the indentations and added a test that verifies the deprecated setOutBody method still behaves. Yeah, the wiki will need to be updated once this feature gets in.",0.27025,0.2405,positive
camel,201,comment_3,"Patch applied. Excellent contribution, thanks Jon. I will leave this issue open until we update the documentation.",documentation_debt,outdated_documentation,"Fri, 2 Nov 2007 17:37:12 +0000","Fri, 11 Jul 2008 04:25:15 +0000","Tue, 6 May 2008 16:22:20 +0000",16065908,"Patch applied. Excellent contribution, thanks Jon. I will leave this issue open until we update the documentation.",0.125,0.125,positive
camel,201,comment_4,"Updated the docs for this at: Unfortunately, it doesn't look as nice as it should because confluence is barfing all over the place with the following message: ""An error occurred: Connection refused. The system administrator has been notified.""",documentation_debt,low_quality_documentation,"Fri, 2 Nov 2007 17:37:12 +0000","Fri, 11 Jul 2008 04:25:15 +0000","Tue, 6 May 2008 16:22:20 +0000",16065908,"Updated the docs for this at: http://cwiki.apache.org/confluence/display/CAMEL/Message+Translator Unfortunately, it doesn't look as nice as it should because confluence is barfing all over the place with the following message: ""An error occurred: Connection refused. The system administrator has been notified.""",-0.090625,-0.090625,positive
camel,201,comment_0,"I finally got around to doing this one up. There are no XQuery specific tests (mainly because of not knowing where to put them ;)), but it is generic enough to work for any expression language. Let me know if you have any questions!",test_debt,lack_of_tests,"Fri, 2 Nov 2007 17:37:12 +0000","Fri, 11 Jul 2008 04:25:15 +0000","Tue, 6 May 2008 16:22:20 +0000",16065908,"I finally got around to doing this one up. There are no XQuery specific tests (mainly because of not knowing where to put them ), but it is generic enough to work for any expression language. Let me know if you have any questions!",0.09741666667,0.06322222222,positive
camel,201,comment_6,"IIRC one of my issues for putting a spring transform test in camel-saxon was that the public camel xsd did not contain the transform element *yet* and so it failed to resolve. The camel-spring module has some magic in there to use a locally built xsd, so the transform element could be tested there fine. Since putting an XQuery test in camel-spring is a bad idea (circular dependency!), I opted for a test case using the ""simple"" expression language. Make sense?",test_debt,low_coverage,"Fri, 2 Nov 2007 17:37:12 +0000","Fri, 11 Jul 2008 04:25:15 +0000","Tue, 6 May 2008 16:22:20 +0000",16065908,"IIRC one of my issues for putting a spring transform test in camel-saxon was that the public camel xsd did not contain the transform element yet and so it failed to resolve. The camel-spring module has some magic in there to use a locally built xsd, so the transform element could be tested there fine. Since putting an XQuery test in camel-spring is a bad idea (circular dependency!), I opted for a test case using the ""simple"" expression language. Make sense?",-0.1024666667,-0.1024666667,positive
camel,302,comment_1,"Could you look at the solution I made in CAMEL-254 and say if it is OK for you? We could have also your solution implemented (as by default in CAMEL-254 I skip all headers that start with 'org.apache.camel') but I think it could be too much ;) If it works for you, then lets close the issue.",code_debt,low_quality_code,"Mon, 21 Jan 2008 14:45:50 +0000","Mon, 16 Feb 2009 05:53:17 +0000","Thu, 11 Sep 2008 16:12:35 +0000",20222805,"Could you look at the solution I made in CAMEL-254 and say if it is OK for you? We could have also your solution implemented (as by default in CAMEL-254 I skip all headers that start with 'org.apache.camel') but I think it could be too much If it works for you, then lets close the issue.",0.1229166667,0.09375,neutral
camel,302,comment_2,In CAMEL-254 you add all headers except in the HTTP Header. In this way it is more difficulty to exclude some headers. It is better perhaps to use a special map for these headings. Does thing think of it?,code_debt,low_quality_code,"Mon, 21 Jan 2008 14:45:50 +0000","Mon, 16 Feb 2009 05:53:17 +0000","Thu, 11 Sep 2008 16:12:35 +0000",20222805,"In CAMEL-254 you add all headers except ""org.apache.camel.component.http.query"" in the HTTP Header. In this way it is more difficulty to exclude some headers. It is better perhaps to use a special map for these headings. Does thing think of it?",0.010125,0.0045,neutral
camel,302,comment_4,Roman we might need a for end users to easier adding their own headers to ignore instead of clearing the already default ignored ones.,design_debt,non-optimal_design,"Mon, 21 Jan 2008 14:45:50 +0000","Mon, 16 Feb 2009 05:53:17 +0000","Thu, 11 Sep 2008 16:12:35 +0000",20222805,Roman we might need a addIgnoreHeader(key) for end users to easier adding their own headers to ignore instead of clearing the already default ignored ones.,-0.3666666667,-0.15,neutral
camel,383,comment_1,A patch with the following changes: - copyright headers for missing files - removed an unused import - new feature to transfer exchange objects using TCP protocol see how to use it The new feature only works for the TCP protocol and for the default codec (= object). So do not use textline=true as option.,code_debt,low_quality_code,"Wed, 12 Mar 2008 16:18:36 +0000","Mon, 12 May 2008 12:45:33 +0000","Mon, 24 Mar 2008 11:47:55 +0000",1020559,A patch with the following changes: copyright headers for missing files removed an unused import new feature to transfer exchange objects using TCP protocol (transferExchange=true) see MinaTransferExchangeOptionTest how to use it The new feature only works for the TCP protocol and for the default codec (= object). So do not use textline=true as option.,-0.39425,-0.3154,neutral
camel,383,comment_2,I think this ticket should be changed from bug to new feature. Also anyone got a better name for the option (transferExchange)? I considered the option bodyOnly also but then I needed to make sure the defaults would be bodyOnly=true even when the parameter is not specified.,code_debt,low_quality_code,"Wed, 12 Mar 2008 16:18:36 +0000","Mon, 12 May 2008 12:45:33 +0000","Mon, 24 Mar 2008 11:47:55 +0000",1020559,I think this ticket should be changed from bug to new feature. Also anyone got a better name for the option (transferExchange)? I considered the option bodyOnly also but then I needed to make sure the defaults would be bodyOnly=true even when the parameter is not specified.,0.3455,0.3455,neutral
camel,383,comment_3,TODO: Remember to update wiki component documentation with the new option if patch is accepted.,documentation_debt,outdated_documentation,"Wed, 12 Mar 2008 16:18:36 +0000","Mon, 12 May 2008 12:45:33 +0000","Mon, 24 Mar 2008 11:47:55 +0000",1020559,TODO: Remember to update wiki component documentation with the new option if patch is accepted.,0.2,0.2,neutral
camel,383,comment_5,"Hi Willem I am on vacation at my parents at the time being and there I have not access to the net. So its a bit challenge to work on Camel using maven in offline mode. I decided in last minute to remove the transfer of exchangeid, but I forgot to remove it in the unit tests that still could pass if the generated ids are the same. Today I have access to the net for a few hours so I am trying to catch up on the mail and the camel commits to merge with a few changes in the mina component I have improved during the last 3 days. I will update the wiki documents on sunday when I am back home. I am also working on providing a camel-mina-example sample for the distribution.",documentation_debt,outdated_documentation,"Wed, 12 Mar 2008 16:18:36 +0000","Mon, 12 May 2008 12:45:33 +0000","Mon, 24 Mar 2008 11:47:55 +0000",1020559,"Hi Willem I am on vacation at my parents at the time being and there I have not access to the net. So its a bit challenge to work on Camel using maven in offline mode. I decided in last minute to remove the transfer of exchangeid, but I forgot to remove it in the unit tests that still could pass if the generated ids are the same. Today I have access to the net for a few hours so I am trying to catch up on the mail and the camel commits to merge with a few changes in the mina component I have improved during the last 3 days. I will update the wiki documents on sunday when I am back home. I am also working on providing a camel-mina-example sample for the distribution.",0.1822777778,0.1822777778,neutral
camel,383,comment_4,"Hi Claus, I just committed your patch. Here is one issue for the since we do not mashal or unmashal the exchange's exchangeId, so I comment out the process assertion for the exchangeId to pass the test . BTW, the document for the transferExchange properties is highly demanded, and I like to use the transferExchange as the parameter name since it is more clear for the user :) Regards, Willem.",test_debt,lack_of_tests,"Wed, 12 Mar 2008 16:18:36 +0000","Mon, 12 May 2008 12:45:33 +0000","Mon, 24 Mar 2008 11:47:55 +0000",1020559,"Hi Claus, I just committed your patch. Here is one issue for the MinaTransferExchangeOptionTest, since we do not mashal or unmashal the exchange's exchangeId, so I comment out the process assertion for the exchangeId to pass the test . BTW, the document for the transferExchange properties is highly demanded, and I like to use the transferExchange as the parameter name since it is more clear for the user Regards, Willem.",0.2333333333,0.2,neutral
camel,478,comment_0,"The patch is in the trunk now, so we need to add some document to show how to use the CamelSourceAdpater and CamelTargetAdpater.",documentation_debt,low_quality_documentation,"Wed, 23 Apr 2008 09:24:18 +0000","Wed, 18 Jun 2008 05:04:27 +0000","Fri, 16 May 2008 13:11:54 +0000",2000856,"The patch is in the trunk now, so we need to add some document to show how to use the CamelSourceAdpater and CamelTargetAdpater.",0.0,0.0,neutral
camel,612,comment_2,-1 [edit: retracting my +1] In other words if there is no explicit otherwise() to go the errorHandler (no redelivery of course). Thinking more about this I think the behavior of choice should be the way it already is. If no clause matches and there is no otherwise it should silently succeed with a noop. Users are familiar with this from switch() statements and is i think a reasonable expectation. I think it's perfectly reasonable to add the otherwise (as one would add a default: ) to make it explicit that the exchange should fail if there is no match: What I would do though is adding a log (below INFO) in the if otherwise == null that would say that exchange is not processed.,code_debt,low_quality_code,"Mon, 16 Jun 2008 06:38:46 +0000","Fri, 11 Jul 2008 04:21:45 +0000","Mon, 23 Jun 2008 12:41:58 +0000",626592,-1 [edit: retracting my +1] In other words if there is no explicit otherwise() to go the errorHandler (no redelivery of course). Thinking more about this I think the behavior of choice should be the way it already is. If no clause matches and there is no otherwise it should silently succeed with a noop. Users are familiar with this from switch() statements and is i think a reasonable expectation. I think it's perfectly reasonable to add the otherwise (as one would add a default: ) to make it explicit that the exchange should fail if there is no match: What I would do though is adding a log (below INFO) in the ChoiceProcessor.process() if otherwise == null that would say that exchange is not processed.,0.11408,0.09506666667,neutral
camel,612,comment_0,Gert I do think that the choice() should *always* have an otherwise() so there always is a match. If the otherwise() is missing on the choice() then Camel should thrown an exception. Maybe checked during the route creation stuff.,design_debt,non-optimal_design,"Mon, 16 Jun 2008 06:38:46 +0000","Fri, 11 Jul 2008 04:21:45 +0000","Mon, 23 Jun 2008 12:41:58 +0000",626592,Gert I do think that the choice() should always have an otherwise() so there always is a match. If the otherwise() is missing on the choice() then Camel should thrown an exception. Maybe checked during the route creation stuff.,-0.1333333333,-0.1333333333,neutral
camel,612,comment_3,"Hadrian good comments. How can we archive this, the DeadLetterChannel is probably already ""chaining"" the routing? Would be good to have it documented somehow. I do think the wiki needs a section where each DSL types is documented one-by-one. That is a however a extensive work to do.",documentation_debt,outdated_documentation,"Mon, 16 Jun 2008 06:38:46 +0000","Fri, 11 Jul 2008 04:21:45 +0000","Mon, 23 Jun 2008 12:41:58 +0000",626592,"Hadrian good comments. >In other words if there is no explicit otherwise() to go the errorHandler (no redelivery of course). How can we archive this, the DeadLetterChannel is probably already ""chaining"" the routing? Would be good to have it documented somehow. I do think the wiki needs a section where each DSL types is documented one-by-one. That is a however a extensive work to do.",0.4304,0.3586666667,neutral
camel,612,comment_4,"OK, I wasn't aware of the {{throwFault()}} method that allows a user to easily make the message exchange fail instead of having to use the {{Processor}} trick. Reducing the priority of the issue, because I guess I only need to document the default, messsage filtering behavior of the choice() on the site to make people more aware of it.",documentation_debt,outdated_documentation,"Mon, 16 Jun 2008 06:38:46 +0000","Fri, 11 Jul 2008 04:21:45 +0000","Mon, 23 Jun 2008 12:41:58 +0000",626592,"OK, I wasn't aware of the throwFault() method that allows a user to easily make the message exchange fail instead of having to use the Processor trick. Reducing the priority of the issue, because I guess I only need to document the default, messsage filtering behavior of the choice() on the site to make people more aware of it.",-0.089,-0.089,neutral
camel,706,comment_3,"code committed, need to update wiki",documentation_debt,outdated_documentation,"Sat, 12 Jul 2008 05:45:52 +0000","Thu, 23 Oct 2008 04:37:12 +0000","Wed, 10 Sep 2008 19:52:34 +0000",5234802,"code committed, need to update wiki",0.5,0.5,neutral
camel,789,comment_2,"This patch pushes down... well... all of the repositories that were in the root pom lower in the build tree. This should speed up builds quite a bit! :) Note that if you build from a clean repo, the build will fail trying to find the mina ftpserver. This patch does not introduce this failure however. See for more details about this failure.",build_debt,build_others,"Thu, 7 Aug 2008 04:15:15 +0000","Thu, 23 Oct 2008 04:37:12 +0000","Thu, 7 Aug 2008 18:24:53 +0000",50978,"This patch pushes down... well... all of the repositories that were in the root pom lower in the build tree. This should speed up builds quite a bit! Note that if you build from a clean repo, the build will fail trying to find the mina ftpserver. This patch does not introduce this failure however. See http://www.nabble.com/-HEADS-UP--Missing-ftpserver-dependency-td18870393s22882.html for more details about this failure.",-0.01573333333,-0.04406666667,positive
camel,872,comment_0,"This is the third time I attempt this and the change set becomes pretty large, making it quite hard to deal with failures. I will make the fixes in a few stages and this will impact other developers with potential need for merge. It would be great if you could avoid commits for the next 24-48 hours or so (if it's not too much to ask).",code_debt,low_quality_code,"Mon, 1 Sep 2008 09:23:29 +0000","Fri, 31 Jul 2009 06:33:35 +0000","Tue, 11 Nov 2008 22:37:48 +0000",6182059,"This is the third time I attempt this and the change set becomes pretty large, making it quite hard to deal with failures. I will make the fixes in a few stages and this will impact other developers with potential need for merge. It would be great if you could avoid commits for the next 24-48 hours or so (if it's not too much to ask).",0.04722222222,0.04722222222,negative
camel,903,comment_3,TODO: Need to document it in wiki,documentation_debt,outdated_documentation,"Mon, 15 Sep 2008 09:26:57 +0000","Thu, 23 Oct 2008 04:37:13 +0000","Tue, 16 Sep 2008 06:18:43 +0000",75106,TODO: Need to document it in wiki,0.0,0.0,neutral
camel,910,comment_0,#9 its not easily understood how the client can send using the template. Maybe divide the server and client into each section. The client just need much less code than the server. #10 webservice example. It is not clear that its the *true* parameter that turns the multicast into parallel mode. This is not documented to well in the code.,code_debt,low_quality_code,"Tue, 16 Sep 2008 17:47:21 +0000","Thu, 23 Oct 2008 04:39:20 +0000","Mon, 22 Sep 2008 11:55:24 +0000",497283,#9 its not easily understood how the client can send using the template. Maybe divide the server and client into each section. The client just need much less code than the server. #10 webservice example. It is not clear that its the true parameter that turns the multicast into parallel mode. This is not documented to well in the code.,-0.2006666667,-0.2301666667,negative
camel,946,comment_0,There is somekind of routebuilder ref you can add to ref to a single class. However we could consider restrucutre the DSL a bit so the package / routebuilderef or what we call it is in a configuration element. We could move all the jmx and other stuff in that one too. so you have a <configuration> element to start with where we can add all the weird camel stuff. And then have a more clean root element with fewer high level tags to select among.,architecture_debt,violation_of_modularity,"Mon, 29 Sep 2008 12:17:19 +0000","Sat, 21 Nov 2009 11:57:54 +0000","Tue, 14 Apr 2009 09:40:38 +0000",17011399,There is somekind of routebuilder ref you can add to ref to a single class. However we could consider restrucutre the DSL a bit so the package / routebuilderef or what we call it is in a configuration element. We could move all the jmx and other stuff in that one too. so you have a <configuration> element to start with where we can add all the weird camel stuff. And then have a more clean root element with fewer high level tags to select among.,0.0288,0.0288,neutral
camel,948,comment_0,1) Added cookbook and tutorials into the book page. 2) Fixed the missing images error 3) Added the Languages Appendix page,documentation_debt,low_quality_documentation,"Tue, 30 Sep 2008 06:16:24 +0000","Mon, 16 Feb 2009 05:51:54 +0000","Mon, 20 Oct 2008 13:47:22 +0000",1755058,1) Added cookbook and tutorials into the book page. 2) Fixed the missing images error 3) Added the Languages Appendix page,-0.2,-0.2,neutral
camel,990,comment_3,BTW - its not really a big deal to use annotations from any particular framework in your code as annotations are a soft dependency. e.g. adding Camel annotations to your code doesn't add any direct dependency on camel. You can then use your code totally fine without any camel jars on the classpath. So there's not a huge need to 'hide' dependencies on Camel annotations - as you only need them to compile your source code. But using indirect annotations can be useful; e.g. you can have a foo.jar which defines a @Foo annotation which builds itself with camel - then folks can just add foo.jar to their classpath to get camel goodness without adding camel jars to the build. So you could introduce a kinda macro annotation that includes various framework annotations on it (say spring and camel and guice annotations :),design_debt,non-optimal_design,"Thu, 16 Oct 2008 11:35:25 +0000","Mon, 16 Feb 2009 05:51:55 +0000","Thu, 23 Oct 2008 06:06:58 +0000",585093,BTW - its not really a big deal to use annotations from any particular framework in your code as annotations are a soft dependency. e.g. adding Camel annotations to your code doesn't add any direct dependency on camel. You can then use your code totally fine without any camel jars on the classpath. So there's not a huge need to 'hide' dependencies on Camel annotations - as you only need them to compile your source code. But using indirect annotations can be useful; e.g. you can have a foo.jar which defines a @Foo annotation which builds itself with camel - then folks can just add foo.jar to their classpath to get camel goodness without adding camel jars to the build. So you could introduce a kinda macro annotation that includes various framework annotations on it (say spring and camel and guice annotations,0.24225,0.2485,neutral
camel,990,comment_4,"The original need is the following: Let's say I build a Routing System based on Camel. Of course my System will depend on Camel. But I don't want the end user of my System (who will be responsible for adding routes, converters...) to depend on any camel jar. Your solution is better (and also more time consuming than my simple metaannotation patch :)), will think about it.",design_debt,non-optimal_design,"Thu, 16 Oct 2008 11:35:25 +0000","Mon, 16 Feb 2009 05:51:55 +0000","Thu, 23 Oct 2008 06:06:58 +0000",585093,"The original need is the following: Let's say I build a Routing System based on Camel. Of course my System will depend on Camel. But I don't want the end user of my System (who will be responsible for adding routes, converters...) to depend on any camel jar. Your solution is better (and also more time consuming than my simple metaannotation patch ), will think about it.",0.04875,0.045,neutral
camel,990,comment_1,We must remember to update the wiki with this new feature (if patch is committed) Do you mind writing a snippet what such documentation should be? As accepting a patch is only really useable if we also updates and improves the related documentation as well. However I feel your idea is great. Maybe there should be a that just contains the few annotations needed and with your strategy end-users can use their own annotations and thus imports.,documentation_debt,outdated_documentation,"Thu, 16 Oct 2008 11:35:25 +0000","Mon, 16 Feb 2009 05:51:55 +0000","Thu, 23 Oct 2008 06:06:58 +0000",585093,We must remember to update the wiki with this new feature (if patch is committed) http://activemq.apache.org/camel/type-converter.html Do you mind writing a snippet what such documentation should be? As accepting a patch is only really useable if we also updates and improves the related documentation as well. However I feel your idea is great. Maybe there should be a camel-annotation.jar that just contains the few annotations needed and with your strategy end-users can use their own annotations and thus imports.,0.3775833333,0.3020666667,neutral
camel,1112,comment_1,"Wiki documentation needed for file component: - idempotent options *DONE* - headers with current index and total *DONE* - important camel 2.0 changes *DONE* - file filter options *DONE* - file sorter options *DONE* - out of box file sorters for: by name, by timestamp, etc. *DONE* - idempotent sample *DONE* - file filter sample *DONE* - file sorter sample *DONE* - file sortBy sample *DONE* - sort by: ignore case option *DONE*",documentation_debt,outdated_documentation,"Mon, 24 Nov 2008 15:48:05 +0000","Fri, 31 Jul 2009 06:33:43 +0000","Tue, 2 Dec 2008 07:41:32 +0000",662007,"Wiki documentation needed for file component: idempotent options DONE headers with current index and total DONE important camel 2.0 changes DONE file filter options DONE file sorter options DONE out of box file sorters for: by name, by timestamp, etc. DONE idempotent sample DONE file filter sample DONE file sorter sample DONE file sortBy sample DONE sort by: ignore case option DONE",0.1,0.1,neutral
camel,1165,comment_0,I've put up a manual with simple cover page at let me know what you think. I still need to put in something that will dynamically insert the version number. That is for tomorrow though :),documentation_debt,low_quality_documentation,"Tue, 9 Dec 2008 09:46:04 +0000","Fri, 31 Jul 2009 06:33:50 +0000","Wed, 10 Dec 2008 14:54:13 +0000",104889,"I've put up a manual with simple cover page at http://people.apache.org/~janstey/temp/camel-manual-2.0-SNAPSHOT.pdf, let me know what you think. I still need to put in something that will dynamically insert the version number. That is for tomorrow though",0.227,0.09366666667,neutral
camel,1173,comment_1,Put in an improved scatter-gather example in revision 740056. The problem with the first example was that it was not dynamic enough. Also added wiki docs here,code_debt,low_quality_code,"Tue, 9 Dec 2008 19:01:43 +0000","Mon, 23 Mar 2009 08:40:31 +0000","Mon, 2 Feb 2009 19:39:35 +0000",4754272,Put in an improved scatter-gather example in revision 740056. The problem with the first example was that it was not dynamic enough. Also added wiki docs here http://cwiki.apache.org/confluence/display/CAMEL/Scatter-Gather,-0.01666666667,-0.01666666667,negative
camel,1173,comment_0,Adding Transmitting file data . Committed revision 724833. This is my first stab at the pattern. Not completely happy with it at the moment... maybe we need to put better support into Camel somewhere for this kinda thing.,documentation_debt,low_quality_documentation,"Tue, 9 Dec 2008 19:01:43 +0000","Mon, 23 Mar 2009 08:40:31 +0000","Mon, 2 Feb 2009 19:39:35 +0000",4754272,Adding camel-core/src/test/java/org/apache/camel/processor/ScatterGatherTest.java Transmitting file data . Committed revision 724833. This is my first stab at the pattern. Not completely happy with it at the moment... maybe we need to put better support into Camel somewhere for this kinda thing.,-0.1125,-0.09,negative
camel,1256,comment_0,"A fix has been submitted. Here is some highlights. * The fix is for 2.0 only since it involves some APIs changes that is an overkill to keep it backward compatible. Will update wiki. * It depends on CXF 2.2-SNAPSHOT now. Will upgrade CXF once the next release of CXF is available. * CxfBinding, Bus [CAMEL-1239], can be looked up from registry by the ""#"" notation. * Decouple CXF Message from Camel Message. That is, users are no longer required to cast Camel's message body to CXF Message in order to access SOAP headers and body in PAYLOAD. In PAYLOAD mode, Camel message body now returns a new type CxfPayload which contains SOAP headers and body. With CxfPayload being the body in PAYLOAD mode, It makes using Camel converter to convert to other types pretty easy and transparent. * Cleaning up the old CxfBinding and ""invoking context"". Both tried to do Camel/CXF message binding. CxfBinding is now an interface that an custom impl can be set on each endpoint. impl can also be set on each endpoint. * [CAMEL-1254] support serviceClass=#bean * some major refactoring to make code cleaner.",code_debt,low_quality_code,"Wed, 14 Jan 2009 15:13:12 +0000","Fri, 31 Jul 2009 06:33:58 +0000","Wed, 28 Jan 2009 03:48:19 +0000",1168507,"A fix has been submitted. Here is some highlights. The fix is for 2.0 only since it involves some APIs changes that is an overkill to keep it backward compatible. Will update wiki. It depends on CXF 2.2-SNAPSHOT now. Will upgrade CXF once the next release of CXF is available. CxfBinding, Bus CAMEL-1239, HeaderFilterStrategy can be looked up from registry by the ""#"" notation. Decouple CXF Message from Camel Message. That is, users are no longer required to cast Camel's message body to CXF Message in order to access SOAP headers and body in PAYLOAD. In PAYLOAD mode, Camel message body now returns a new type CxfPayload which contains SOAP headers and body. With CxfPayload being the body in PAYLOAD mode, It makes using Camel converter to convert to other types pretty easy and transparent. Cleaning up the old CxfBinding and ""invoking context"". Both tried to do Camel/CXF message binding. CxfBinding is now an interface that an custom impl can be set on each endpoint. HeaderFilterStrategy impl can also be set on each endpoint. CAMEL-1254 support serviceClass=#bean some major refactoring to make code cleaner.",0.081,0.081,neutral
camel,1304,comment_8,"Hi, Actually I toyed with this a while ago and it's not hard to implement. It would help in a few ways, one being distributing our own tests on multiple nodes and reduce the ridiculously long building time (but hey, we have some 4000 unit tests, something to brag about). The issue is that gridgain repackaged and distribute their api under ASL2.0 at our own's jstrachan request quite a while ago. The issue is that the nodes are still GPL so I have no idea how we could test such a component unless they would make available a mock or something as ASL2.0 as well. I am willing to bet that they won't relicense the whole thing. I spoke to Nikita some 6 months ago about this and it looks like the GG crowd is not quite happy about the prospect. One alternative would be to implement and host it at the fuse forge and camel extra and depending on the community interest see if we could work something out with GG and move the component later at apache.",code_debt,slow_algorithm,"Thu, 29 Jan 2009 08:49:55 +0000","Sun, 24 Apr 2011 09:58:24 +0000","Sun, 26 Sep 2010 16:28:51 +0000",52299536,"Hi, Actually I toyed with this a while ago and it's not hard to implement. It would help in a few ways, one being distributing our own tests on multiple nodes and reduce the ridiculously long building time (but hey, we have some 4000 unit tests, something to brag about). The issue is that gridgain repackaged and distribute their api under ASL2.0 at our own's jstrachan request quite a while ago. The issue is that the nodes are still GPL so I have no idea how we could test such a component unless they would make available a mock or something as ASL2.0 as well. I am willing to bet that they won't relicense the whole thing. I spoke to Nikita some 6 months ago about this and it looks like the GG crowd is not quite happy about the prospect. One alternative would be to implement and host it at the fuse forge and camel extra and depending on the community interest see if we could work something out with GG and move the component later at apache.",0.1482222222,0.1482222222,positive
camel,1320,comment_1,"It is very similar to zip data format - it even uses the same deflate algorithm. The point is, that zip data format uses deflate algorithm directly, while gzip would use gzip file format, that is (I believe more popular). The problem with zip data format is that is is not a zip really - zip is a format that compresses set of files, while our zip data format doesn't - it just compresses data using compression algorithm of zip. On the other hand gzip is a format that compresses data (not files), uses the same algorithm, but has its own headers, that has to be interpreted. I even believe that our zip data format should be renamed to 'deflate' data format as it is the name that describes what it really is.",design_debt,non-optimal_design,"Fri, 6 Feb 2009 14:55:23 +0000","Sat, 21 Nov 2009 11:57:55 +0000","Thu, 19 Mar 2009 15:18:16 +0000",3543773,"It is very similar to zip data format - it even uses the same deflate algorithm. The point is, that zip data format uses deflate algorithm directly, while gzip would use gzip file format, that is (I believe more popular). http://java.sun.com/j2se/1.5.0/docs/api/java/util/zip/GZIPInputStream.html http://java.sun.com/j2se/1.5.0/docs/api/java/util/zip/InflaterInputStream.html The problem with zip data format is that is is not a zip really - zip is a format that compresses set of files, while our zip data format doesn't - it just compresses data using compression algorithm of zip. On the other hand gzip is a format that compresses data (not files), uses the same algorithm, but has its own headers, that has to be interpreted. I even believe that our zip data format should be renamed to 'deflate' data format as it is the name that describes what it really is.",0.04,0.04,neutral
camel,1320,comment_4,"And we need to add it to the wiki page as well, under data formats.",documentation_debt,outdated_documentation,"Fri, 6 Feb 2009 14:55:23 +0000","Sat, 21 Nov 2009 11:57:55 +0000","Thu, 19 Mar 2009 15:18:16 +0000",3543773,"And we need to add it to the wiki page as well, under data formats.",0.631,0.631,neutral
camel,1507,comment_0,might need some cleanup - but I have tested it and it works for me,code_debt,low_quality_code,"Wed, 1 Apr 2009 00:50:08 +0000","Sat, 27 Nov 2010 06:42:58 +0000","Thu, 2 Apr 2009 07:42:27 +0000",111139,might need some cleanup - but I have tested it and it works for me,0.5815,0.5815,positive
camel,1507,comment_4,I renamed the constant so its according to the Camel 2.0 syntax: rev 761182 on trunk,code_debt,low_quality_code,"Wed, 1 Apr 2009 00:50:08 +0000","Sat, 27 Nov 2010 06:42:58 +0000","Thu, 2 Apr 2009 07:42:27 +0000",111139,I renamed the constant so its according to the Camel 2.0 syntax: rev 761182 on trunk,0.5,0.5,positive
camel,1507,comment_5,"Ryan/William We also need to update the mail wiki page with this feature. At least document its possible and give some hints how to do it. And it would be great with a sample how to do it, and you can use the SNIPPETS in the unit test to put it on the wiki pages.",documentation_debt,outdated_documentation,"Wed, 1 Apr 2009 00:50:08 +0000","Sat, 27 Nov 2010 06:42:58 +0000","Thu, 2 Apr 2009 07:42:27 +0000",111139,"Ryan/William We also need to update the mail wiki page with this feature. At least document its possible and give some hints how to do it. And it would be great with a sample how to do it, and you can use the SNIPPETS in the unit test to put it on the wiki pages.",0.2,0.2,positive
camel,1608,comment_0,"Hello Hadrian In Maven, when you put the: <project declaration on multiple lines, Maven will flatten it into one line and throw out the preceding comments -in this case the copyright. I guess it also messes up all the spacing. On the OW2 projects, we solved the issue by putting the declaration on one line. It doesn't look very good that way, but it works :) Cheers S. Ali Tokmen",design_debt,non-optimal_design,"Wed, 13 May 2009 03:03:57 +0000","Sat, 21 Nov 2009 11:58:00 +0000","Tue, 16 Jun 2009 02:16:40 +0000",2934763,"Hello Hadrian In Maven, when you put the: <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd""> declaration on multiple lines, Maven will flatten it into one line and throw out the preceding comments -in this case the copyright. I guess it also messes up all the spacing. On the OW2 projects, we solved the issue by putting the declaration on one line. It doesn't look very good that way, but it works Cheers S. Ali Tokmen http://ali.tokmen.com/",0.089,0.06866666667,neutral
camel,1641,comment_0,trunk: 778354. When/if CAMEL-1644 is implemented then this bug is already fixed. However remember to add the unit test in java. There is a TODO.,test_debt,lack_of_tests,"Sat, 23 May 2009 08:14:55 +0000","Sat, 21 Nov 2009 11:58:01 +0000","Mon, 25 May 2009 13:26:34 +0000",191499,trunk: 778354. When/if CAMEL-1644 is implemented then this bug is already fixed. However remember to add the unit test in FtpProducerConcurrentTest java. There is a TODO.,0.0,0.0,neutral
camel,1734,comment_0,"Hi Fernando, Thanks for the contribution. I just have quick review of your code, there is no unit test, can you add them ? Since Progress OpenEdge is a commercial product , I don't know if we could download the jar from public mvn repository and use it freely, can you clarify it ? If not , we need to find a way to accept your code. BTW, the codes have no Apache License declaration, you can take the other camel component module as an example. Willem",test_debt,lack_of_tests,"Sat, 20 Jun 2009 20:41:11 +0000","Sun, 7 Feb 2010 09:58:26 +0000","Thu, 2 Jul 2009 11:53:59 +0000",1005168,"Hi Fernando, Thanks for the contribution. I just have quick review of your code, there is no unit test, can you add them ? Since Progress OpenEdge is a commercial product , I don't know if we could download the jar from public mvn repository and use it freely, can you clarify it ? If not , we need to find a way to accept your code. BTW, the codes have no Apache License declaration, you can take the other camel component module as an example. Willem",0.1666666667,0.1666666667,neutral
camel,1734,comment_1,"Regarding the dependency on Progress OpenEdge, it is only available to their customers, which will need instructions for wrapping and deploying it in SMX. I will work on the unit tests and Apache License issues right away.",test_debt,lack_of_tests,"Sat, 20 Jun 2009 20:41:11 +0000","Sun, 7 Feb 2010 09:58:26 +0000","Thu, 2 Jul 2009 11:53:59 +0000",1005168,"Regarding the dependency on Progress OpenEdge, it is only available to their customers, which will need instructions for wrapping and deploying it in SMX. I will work on the unit tests and Apache License issues right away.",0.48175,0.48175,neutral
camel,1734,comment_3,"I don't have any problem with that, let me know how to proceed. Updated the submission with the Apache License (are we going to stick to it?), no unit tests yet.",test_debt,lack_of_tests,"Sat, 20 Jun 2009 20:41:11 +0000","Sun, 7 Feb 2010 09:58:26 +0000","Thu, 2 Jul 2009 11:53:59 +0000",1005168,"I don't have any problem with that, let me know how to proceed. Updated the submission with the Apache License (are we going to stick to it?), no unit tests yet.",0.1333333333,0.1333333333,neutral
camel,1846,comment_2,"ugh, I didn't notice those links all went to the same place. Can those be fixed to point at the pdf, or at least removed to avoid confusing anyone else?",documentation_debt,low_quality_documentation,"Wed, 22 Jul 2009 20:15:26 +0000","Sun, 7 Feb 2010 09:58:27 +0000","Sun, 13 Sep 2009 18:55:42 +0000",4574416,"ugh, I didn't notice those links all went to the same place. Can those be fixed to point at the pdf, or at least removed to avoid confusing anyone else?",-0.35925,-0.35925,negative
camel,1846,comment_3,"Well, the pdf manual doesn't have the details about the xml elements, so I guess the only option at the moment is to try to read the xsd directly, or figure out how to generate the xsddoc pages. :(",documentation_debt,outdated_documentation,"Wed, 22 Jul 2009 20:15:26 +0000","Sun, 7 Feb 2010 09:58:27 +0000","Sun, 13 Sep 2009 18:55:42 +0000",4574416,"Well, the pdf manual doesn't have the details about the xml elements, so I guess the only option at the moment is to try to read the xsd directly, or figure out how to generate the xsddoc pages.",0.114,0.628,negative
camel,1846,comment_4,Fixed the links. Sadly we cannot maintain easily a v1.x and 2.0 branch of the online docs as part of build process etc. The xsddoc do not bring much value so I am looking into removing it as well.,documentation_debt,outdated_documentation,"Wed, 22 Jul 2009 20:15:26 +0000","Sun, 7 Feb 2010 09:58:27 +0000","Sun, 13 Sep 2009 18:55:42 +0000",4574416,Fixed the links. Sadly we cannot maintain easily a v1.x and 2.0 branch of the online docs as part of build process etc. The xsddoc do not bring much value so I am looking into removing it as well.,0.064,0.064,negative
camel,1846,comment_6,The maven reports is just getting to old and intermixed with 1.x and trunk releases. We should just remove them as they dont bring much value anyway. The xsddoc has been removed from the build files. So its now a matter of removing from the apache http site (people with credentials is needed),documentation_debt,outdated_documentation,"Wed, 22 Jul 2009 20:15:26 +0000","Sun, 7 Feb 2010 09:58:27 +0000","Sun, 13 Sep 2009 18:55:42 +0000",4574416,The maven reports is just getting to old and intermixed with 1.x and trunk releases. We should just remove them as they dont bring much value anyway. The xsddoc has been removed from the build files. So its now a matter of removing from the apache http site (people with credentials is needed),0.04,0.04,negative
camel,1863,comment_1,trunk: 798902. TODO: HL7 segment that fails validation and add it to the validate unit test,test_debt,low_coverage,"Wed, 29 Jul 2009 12:36:06 +0000","Sun, 7 Feb 2010 09:56:20 +0000","Thu, 30 Jul 2009 06:59:36 +0000",66210,trunk: 798902. TODO: HL7 segment that fails validation and add it to the validate unit test,-0.05,-0.05,negative
camel,1881,comment_1,"@Stan, maybe you want to add a unit test? Many thanks for the patch!",test_debt,lack_of_tests,"Wed, 5 Aug 2009 17:53:32 +0000","Sun, 7 Feb 2010 09:56:22 +0000","Sat, 8 Aug 2009 13:53:15 +0000",244783,"@Stan, maybe you want to add a unit test? Many thanks for the patch!",0.3,0.3,positive
camel,2446,comment_0,Good work Chris. Is it possible to remove the as its not commonly use and the XML files should help new users to understand what happens. So its best to be a bit more verbose than playing tricks with auto wiring.,design_debt,non-optimal_design,"Thu, 4 Feb 2010 02:20:01 +0000","Sun, 24 Apr 2011 09:57:31 +0000","Thu, 26 Aug 2010 06:56:20 +0000",17555779,Good work Chris. Is it possible to remove the default-autowire as its not commonly use and the XML files should help new users to understand what happens. So its best to be a bit more verbose than playing tricks with auto wiring.,0.3876666667,0.371,positive
camel,2481,comment_2,@Gert Yes we have removed that as a default aggregator as its just too confusing for new users. We want people to force to provide their own aggregation strategy so they learn how to use them and merge the data themselves.,design_debt,non-optimal_design,"Wed, 17 Feb 2010 15:18:34 +0000","Sun, 24 Apr 2011 10:01:32 +0000","Wed, 17 Feb 2010 18:05:58 +0000",10044,@Gert Yes we have removed that UseLastAggregationStrategy as a default aggregator as its just too confusing for new users. We want people to force to provide their own aggregation strategy so they learn how to use them and merge the data themselves.,-0.02283333333,-0.02283333333,neutral
camel,2559,comment_3,"I think I got it covered now, will do more testing. But I have reworked camel-http and camel-jetty a little big to - use endpoint configured options over component configured - not mess with the component configured - using a single shared as that is whats meant - properly registering http/https with correct port number on SchemeRegistry on",test_debt,low_coverage,"Thu, 18 Mar 2010 15:04:22 +0000","Sun, 24 Apr 2011 10:01:31 +0000","Sun, 21 Mar 2010 12:29:22 +0000",249900,"I think I got it covered now, will do more testing. But I have reworked camel-http and camel-jetty a little big to use endpoint configured options over component configured not mess with the component configured using a single shared clientConnectionManager as that is whats meant properly registering http/https with correct port number on SchemeRegistry on clientConnectionManager",0.159375,0.159375,neutral
camel,2670,comment_1,"@ Charles I just checked out the example, I think you can put all the camel routes configuration files into a single module and use Profile to start the services one by one.",architecture_debt,violation_of_modularity,"Fri, 23 Apr 2010 08:17:35 +0000","Mon, 26 Apr 2010 07:54:46 +0000","Fri, 23 Apr 2010 08:20:09 +0000",154,"@ Charles I just checked out the example, I think you can put all the camel routes configuration files into a single module and use Profile to start the services one by one.",0.0,0.0,neutral
camel,2756,comment_0,Willem I think the xxxConverter name is misleading as you would think its a Camel TypeConverter. I wonder if there is a better naming for this? or the likes.,code_debt,low_quality_code,"Thu, 27 May 2010 02:09:50 +0000","Sun, 24 Apr 2011 10:00:59 +0000","Thu, 27 May 2010 03:10:03 +0000",3613,Willem I think the xxxConverter name is misleading as you would think its a Camel TypeConverter. I wonder if there is a better naming for this? AuthenticationAdapter AuthenticationStrategy AuthenticationService or the likes.,0.1055,0.1055,negative
camel,2979,comment_3,"Hi I've tested this and it works. When logging in with incorrect user/pass it only polls once and skips, which is nice. However could it be possible for the component to throw an exception as I would like to catch this exception and make sure that no further polls are done to the ftp (ie shut down the route/application).",design_debt,non-optimal_design,"Wed, 21 Jul 2010 09:07:59 +0000","Sun, 24 Apr 2011 09:57:22 +0000","Thu, 22 Jul 2010 15:00:59 +0000",107580,"Hi I've tested this and it works. When logging in with incorrect user/pass it only polls once and skips, which is nice. However could it be possible for the component to throw an exception as I would like to catch this exception and make sure that no further polls are done to the ftp (ie shut down the route/application).",0.566,0.566,positive
camel,3469,comment_0,I will document this procedure by working on,documentation_debt,low_quality_documentation,"Tue, 28 Dec 2010 18:19:17 +0000","Sun, 24 Apr 2011 09:58:05 +0000","Tue, 28 Dec 2010 23:11:56 +0000",17559,I will document this procedure by working on CAMEL-3471,0.6,0.6,neutral
camel,3576,comment_1,"trunk: 1067682. Don't use thread pool for single threaded reply manager as Spring DMLC is a bit pants as it will keep using new tasks every second when idle, and that just confuses people, as task count will grow very large.",design_debt,non-optimal_design,"Sat, 22 Jan 2011 08:23:42 +0000","Tue, 25 Oct 2011 11:36:12 +0000","Sun, 6 Feb 2011 13:25:12 +0000",1314090,"trunk: 1067682. Don't use thread pool for single threaded reply manager as Spring DMLC is a bit pants as it will keep using new tasks every second when idle, and that just confuses people, as task count will grow very large.",-0.25,-0.25,negative
camel,3709,comment_0,You found the easter egg. There is a todo in the source code to support endpoint via ref's // TODO: Support lookup endpoint by ref (requires a bit more work),requirement_debt,requirement_partially_implemented,"Wed, 23 Feb 2011 12:12:35 +0000","Tue, 25 Oct 2011 11:35:47 +0000","Thu, 24 Feb 2011 05:21:03 +0000",61708,You found the easter egg. There is a todo in the source code to support endpoint via ref's // TODO: Support lookup endpoint by ref (requires a bit more work),0.2333333333,0.2333333333,positive
camel,3769,comment_0,I suggest to change this section of the code to always use getInstance instead of getDefaultInstance.,code_debt,low_quality_code,"Tue, 8 Mar 2011 16:08:49 +0000","Tue, 25 Oct 2011 11:35:59 +0000","Wed, 9 Mar 2011 09:22:07 +0000",61998,I suggest to change this section of the code to always use getInstance instead of getDefaultInstance.,0.0,0.0,neutral
camel,3777,comment_0,"We should possible have a that uses pax exam as its base line. Willem have upgraded to pax exam 2, so it may be easier to do now.",design_debt,non-optimal_design,"Thu, 10 Mar 2011 17:42:37 +0000","Sat, 11 Jul 2015 22:19:13 +0000","Sat, 11 Jul 2015 22:19:13 +0000",136874196,"We should possible have a camel-test-pax-exam, that uses pax exam as its base line. Willem have upgraded to pax exam 2, so it may be easier to do now.",0.0,0.0,neutral
camel,3777,comment_1,There is some ticket about an osgi test component. However as there is pax-exam this is less needed as ppl can use plain pax-exam,design_debt,non-optimal_design,"Thu, 10 Mar 2011 17:42:37 +0000","Sat, 11 Jul 2015 22:19:13 +0000","Sat, 11 Jul 2015 22:19:13 +0000",136874196,There is some ticket about an osgi test component. However as there is pax-exam this is less needed as ppl can use plain pax-exam,-0.375,-0.375,neutral
camel,3888,comment_1,"Dan as said before just because a method is deprecated do *not* mean we should not unit test it. And hence why you would see unit test that uses the deprecated methods. The handled(true|false) on try .. catch, error handler, was a mistake and there are *no* alternative on those. Instead you should use for example onException (exception clause). Or in case of a doCatch you can rethrow the exception if you want that.",code_debt,low_quality_code,"Thu, 21 Apr 2011 12:12:23 +0000","Mon, 27 Jun 2011 08:05:27 +0000","Mon, 27 Jun 2011 08:05:27 +0000",5773984,"Dan as said before just because a method is deprecated do not mean we should not unit test it. And hence why you would see unit test that uses the deprecated methods. The handled(true|false) on try .. catch, error handler, was a mistake and there are no alternative on those. Instead you should use for example onException (exception clause). Or in case of a doCatch you can rethrow the exception if you want that.",0.01865,0.01865,neutral
camel,3888,comment_0,"This is definitely not resolved. There are a lot of tests that are using which is marked deprecated. So far, I haven't found any documentation about an alternative to use. THere are a bunch (22) of tests that use it and I haven't been able to figure out any type of alternative for it. Thus, IMO, if there isn't an alternative and is important enough to be required by a bunch of tests, I have to wonder why it's deprecated. But no details on that either. Similar situation for Deprecated, no alternative. For TryDefinition, I could see a ""rethrow()"" or similar method added to define that behavior (which would match the Java users expectation), but that certainly isn't there right now.",documentation_debt,low_quality_documentation,"Thu, 21 Apr 2011 12:12:23 +0000","Mon, 27 Jun 2011 08:05:27 +0000","Mon, 27 Jun 2011 08:05:27 +0000",5773984,"This is definitely not resolved. There are a lot of tests that are using DefaultErrorHandlerBuilder.handled which is marked deprecated. So far, I haven't found any documentation about an alternative to use. THere are a bunch (22) of tests that use it and I haven't been able to figure out any type of alternative for it. Thus, IMO, if there isn't an alternative and is important enough to be required by a bunch of tests, I have to wonder why it's deprecated. But no details on that either. Similar situation for TryDefinition.handled. Deprecated, no alternative. For TryDefinition, I could see a ""rethrow()"" or similar method added to define that behavior (which would match the Java users expectation), but that certainly isn't there right now.",-0.2046666667,-0.1488484848,neutral
camel,3888,comment_2,"@Claus, if you read more carefully, the reason for reopening was that we need to document what to use instead of the deprecated feature. Has this been done? How would a user know what to do? Why did you mark the issue as resolved?",documentation_debt,outdated_documentation,"Thu, 21 Apr 2011 12:12:23 +0000","Mon, 27 Jun 2011 08:05:27 +0000","Mon, 27 Jun 2011 08:05:27 +0000",5773984,"@Claus, if you read more carefully, the reason for reopening was that we need to document what to use instead of the deprecated feature. Has this been done? How would a user know what to do? Why did you mark the issue as resolved?",0.2,0.2,neutral
camel,3888,comment_5,"Dan/Hadrian you are welcome to look at the added documentation to @deprecated javadoc. The easiest is probably to see this commit rev: I spotted a minor mistake and corrected it in this: I checked all the i could find in (eg in main, and *not* test). You are of course welcome to improve the documentation if you find anything needed to be added.",documentation_debt,low_quality_documentation,"Thu, 21 Apr 2011 12:12:23 +0000","Mon, 27 Jun 2011 08:05:27 +0000","Mon, 27 Jun 2011 08:05:27 +0000",5773984,"Dan/Hadrian you are welcome to look at the added documentation to @deprecated javadoc. The easiest is probably to see this commit rev: http://svn.apache.org/viewvc?rev=1095673&view=rev I spotted a minor mistake and corrected it in this: http://svn.apache.org/viewvc?rev=1095735&view=rev I checked all the @Deprecated/@deprecated i could find in camel-core/src/main/java (eg in main, and not test). You are of course welcome to improve the documentation if you find anything needed to be added.",0.4083333333,0.4083333333,neutral
camel,3888,comment_6,I believe most of the users are looking at the wiki/manual we need to document there. I'll take care of it.,documentation_debt,outdated_documentation,"Thu, 21 Apr 2011 12:12:23 +0000","Mon, 27 Jun 2011 08:05:27 +0000","Mon, 27 Jun 2011 08:05:27 +0000",5773984,I believe most of the users are looking at the wiki/manual we need to document there. I'll take care of it.,0.2,0.2,neutral
camel,3888,comment_7,Information about @deprecation in the API is standard documented in the java doc. And this is where users would expect the information and go look. We have never had any special wiki page or the likes where @deprecated API is being further detailed. IMHO this ticket can be resolved.,documentation_debt,outdated_documentation,"Thu, 21 Apr 2011 12:12:23 +0000","Mon, 27 Jun 2011 08:05:27 +0000","Mon, 27 Jun 2011 08:05:27 +0000",5773984,Information about @deprecation in the API is standard documented in the java doc. And this is where users would expect the information and go look. We have never had any special wiki page or the likes where @deprecated API is being further detailed. IMHO this ticket can be resolved.,0.06666666667,0.06666666667,neutral
camel,4202,comment_0,"Okay I think there is a tradeoff if we lower the spring receiveTimeout to lower than 1 sec. For example if we use 1 millis, then it will overload and send pull packets to the AMQ broker to receive messages. The reason why temporary queues is faster is because they dont have any JMSMessageListener as the persistent does. So with temporary queues it can pull every 1sec and pickup messages as fast as possible, as the received message is always for the reply manager.",code_debt,slow_algorithm,"Sat, 9 Jul 2011 14:05:41 +0000","Mon, 19 Sep 2011 20:32:51 +0000","Sun, 14 Aug 2011 10:07:44 +0000",3096123,"Okay I think there is a tradeoff if we lower the spring receiveTimeout to lower than 1 sec. For example if we use 1 millis, then it will overload and send pull packets to the AMQ broker to receive messages. The reason why temporary queues is faster is because they dont have any JMSMessageListener as the persistent does. So with temporary queues it can pull every 1sec and pickup messages as fast as possible, as the received message is always for the reply manager.",-0.06566666667,-0.06566666667,neutral
camel,4202,comment_2,"If there is an use case for using persistent queues that are *exclusive* for the Camel consumer, then we can avoid using JMSMessageSelector and thus be as fast as temporary queues.",code_debt,slow_algorithm,"Sat, 9 Jul 2011 14:05:41 +0000","Mon, 19 Sep 2011 20:32:51 +0000","Sun, 14 Aug 2011 10:07:44 +0000",3096123,"If there is an use case for using persistent queues that are exclusive for the Camel consumer, then we can avoid using JMSMessageSelector and thus be as fast as temporary queues.",0.1,0.1,neutral
camel,4202,comment_1,A workaround is for the end user to use to eg pull messages 4 times per sec and thus be 4x faster.,design_debt,non-optimal_design,"Sat, 9 Jul 2011 14:05:41 +0000","Mon, 19 Sep 2011 20:32:51 +0000","Sun, 14 Aug 2011 10:07:44 +0000",3096123,"A workaround is for the end user to use ?receiveTimeout=250, to eg pull messages 4 times per sec and thus be 4x faster.",0.0,0.0,neutral
camel,4202,comment_3,"The reason why the persistent reply to example at the nabble discussion, is because Camel will have to use a Dummy value for JMSMessageSelector when there is no expected replies to receive. And it thus takes 1 sec. for it to timeout, before it can update the JMSMessageSelector with the CorrelationsIDs to receive. We may try to suspend/resume the message listener container, but we could potential end up with some synchronized issue where we would suspend the listener, where as in the mean time on another thread a new message is being send. And thus we may miss resume the listener. Therefore we end up not pulling the message from the AMQ broker. We should add some documentation / FAQ as the drawbacks of using persistent queues.",documentation_debt,low_quality_documentation,"Sat, 9 Jul 2011 14:05:41 +0000","Mon, 19 Sep 2011 20:32:51 +0000","Sun, 14 Aug 2011 10:07:44 +0000",3096123,"The reason why the persistent reply to example at the nabble discussion, is because Camel will have to use a Dummy value for JMSMessageSelector when there is no expected replies to receive. And it thus takes 1 sec. for it to timeout, before it can update the JMSMessageSelector with the CorrelationsIDs to receive. We may try to suspend/resume the message listener container, but we could potential end up with some synchronized issue where we would suspend the listener, where as in the mean time on another thread a new message is being send. And thus we may miss resume the listener. Therefore we end up not pulling the message from the AMQ broker. We should add some documentation / FAQ as the drawbacks of using persistent queues.",-0.08571428571,-0.08571428571,neutral
camel,4202,comment_4,"A new option is to be introduced: replyToType and its an enum with the following options: Temporary, Shared, Exclusive So if you set the then Camel assumes the reply queue is exclusive for Camel and will not use any JMS message selectors, as it pickup all the messages arriving on that queue. This means its as fast as temporary queues. In fact this will also help highlight the fact the current fixed replyTo queues are consider Shared by default. I will add better documentation at the jms page to help highlight this.",documentation_debt,low_quality_documentation,"Sat, 9 Jul 2011 14:05:41 +0000","Mon, 19 Sep 2011 20:32:51 +0000","Sun, 14 Aug 2011 10:07:44 +0000",3096123,"A new option is to be introduced: replyToType and its an enum with the following options: Temporary, Shared, Exclusive So if you set the replyToType=Exclusive then Camel assumes the reply queue is exclusive for Camel and will not use any JMS message selectors, as it pickup all the messages arriving on that queue. This means its as fast as temporary queues. In fact this will also help highlight the fact the current fixed replyTo queues are consider Shared by default. I will add better documentation at the jms page to help highlight this.",0.2229166667,0.2270833333,neutral
camel,4226,comment_0,"Frankly sometimes its best to post on @dev before opening a ticket. Anyway end users should use the EIPs supplied. And the recipient list is the dynamic EIP. There is a FAQ as well We should also be careful to not overload the DSL with new options etc. as it just confuses end users as they get a huge list in the IDE when accessing code completion. Likewise we should keep the DSL in sync between Java, XML and Scala (later is a bit more hard). But the Java and XML is in sync due the JAXB models. And you cannot with JAXB define that an expression should be *optional* so you can keep doing <to uri=""xxx""/ This is not possible and you get a validation exception as it would expect an expression, so you would have to do <to <constant</to Which is ugly/verbose, and breaks every Camel applications that are using XML DSL. I dont think its a good idea to add this only to Java DSL, as we have end users who migrate between Java and XML. Likewise its best they are 1:1.",design_debt,non-optimal_design,"Wed, 13 Jul 2011 18:13:16 +0000","Sun, 31 Jul 2011 10:54:19 +0000","Thu, 14 Jul 2011 17:18:13 +0000",83097,"Frankly sometimes its best to post on @dev before opening a ticket. Anyway end users should use the EIPs supplied. And the recipient list is the dynamic EIP. There is a FAQ as well http://camel.apache.org/how-do-i-use-dynamic-uri-in-to.html We should also be careful to not overload the DSL with new options etc. as it just confuses end users as they get a huge list in the IDE when accessing code completion. Likewise we should keep the DSL in sync between Java, XML and Scala (later is a bit more hard). But the Java and XML is in sync due the JAXB models. And you cannot with JAXB define that an expression should be optional so you can keep doing <to uri=""xxx""/> in XML. This is not possible and you get a validation exception as it would expect an expression, so you would have to do <to> <constant>xxx</constant> </to> Which is ugly/verbose, and breaks every Camel applications that are using XML DSL. I dont think its a good idea to add this only to Java DSL, as we have end users who migrate between Java and XML. Likewise its best they are 1:1.",0.1174166667,0.105675,neutral
camel,4357,comment_2,"There is a CS error as the license header is missing in one file. When adding a new package in camel-core, a package.html file should be included, which briefly summarizes the package. See some of the other for examples to copy. And in the camel-core/pom.xml file there is some javadoc grouping of packages. I guess the new main package is to be added as well.",code_debt,low_quality_code,"Fri, 19 Aug 2011 13:38:41 +0000","Mon, 22 Aug 2011 17:08:15 +0000","Mon, 22 Aug 2011 13:34:55 +0000",258974,"There is a CS error as the license header is missing in one file. When adding a new package in camel-core, a package.html file should be included, which briefly summarizes the package. See some of the other for examples to copy. And in the camel-core/pom.xml file there is some javadoc grouping of packages. I guess the new main package is to be added as well.",0.033,0.033,neutral
camel,4398,comment_0,"Sorry the issue is the tools JAR is added as dep on the SNAPSHOTs, but not on the releases etc. So its a bit weird. Track down to be possible in parent/pom.xml with some antrun tasks that does some osgi versioning regex replacement. And since this dependency is added to camel-core it gets inheirted by all the other artifcats. But only for SNAPSHOTs, and not releases. So I guess the release profile makes something different. The tools JAR is thus not needed at all as a dependency to use Camel, so what we want is to get rid of tools JAR in the camel-core/pom.xml file.",design_debt,non-optimal_design,"Wed, 31 Aug 2011 09:19:39 +0000","Mon, 19 Sep 2011 13:47:19 +0000","Wed, 31 Aug 2011 15:05:52 +0000",20773,"Sorry the issue is the tools JAR is added as dep on the SNAPSHOTs, but not on the releases etc. So its a bit weird. Track down to be possible in parent/pom.xml with some antrun tasks that does some osgi versioning regex replacement. And since this dependency is added to camel-core it gets inheirted by all the other artifcats. But only for SNAPSHOTs, and not releases. So I guess the release profile makes something different. The tools JAR is thus not needed at all as a dependency to use Camel, so what we want is to get rid of tools JAR in the camel-core/pom.xml file.",-0.1102222222,-0.1102222222,neutral
camel,4417,comment_3,"@Christian, all the changes you make in 2.9 should be backwards compatible. So if you make any changes, please make sure leave existing classes in place (even as extensions of refactored classes) and change as few tests as possible, ideally none. That ensures two things: one that we didn't break anything and existing code still works, second that users have a migration path that could take at any time. We can remove the old classes later in 3.0. Changes that break backward compatibility I'd leave for later.",code_debt,low_quality_code,"Mon, 5 Sep 2011 13:46:59 +0000","Tue, 22 Oct 2013 07:34:27 +0000","Tue, 22 Oct 2013 07:34:27 +0000",67196848,"@Christian, all the changes you make in 2.9 should be backwards compatible. So if you make any changes, please make sure leave existing classes in place (even as extensions of refactored classes) and change as few tests as possible, ideally none. That ensures two things: one that we didn't break anything and existing code still works, second that users have a migration path that could take at any time. We can remove the old classes later in 3.0. Changes that break backward compatibility I'd leave for later.",0.1731,0.1731,neutral
camel,4417,comment_2,"Patch moving all support classes from impl to support. I moved the following classes and placed compatibility classes in their impl location: DefaultComponent, DefaultConsumer, DefaultEndpoint, DefaultExchange, DefaultMessage, DefaultProducer, DefaultUnitOfWork, ExpressionSupport, ProcessorEndpoint, ProducerCache, I moved the following classes without compat stubs as they were not needed outside camel-core: DefaultRouteNode, ExpressionAdapter, MDCUnitOfWork, MessageSupport, SimpleUuidGenerator I moved from processor to util as it is needed from support. The only problematic class I moved was DefaultConsumer as it needed So the above including the inner class had to move to util. The problem here was that the Bridge had to extend DelegateProcessor which of course is in processor. As util should not depend on processor I had to introduce an interface DelegateProcessor in camel that could be used to abstract from DelegateProcessor and This is a good thing anyway and I will open a jira to do this first. I also had to move PipelineHelper and to util as they were used from support classes. This is a fairly large patch. So I am not sure if it is good for 2.9. On the other hand if we wait with this till 3.0 we are either really incompatible or we can not remove the deprecated classes",design_debt,non-optimal_design,"Mon, 5 Sep 2011 13:46:59 +0000","Tue, 22 Oct 2013 07:34:27 +0000","Tue, 22 Oct 2013 07:34:27 +0000",67196848,"Patch moving all support classes from impl to support. I moved the following classes and placed compatibility classes in their impl location: DefaultAsyncProducer, DefaultComponent, DefaultConsumer, DefaultEndpoint, DefaultExchange, DefaultMessage, DefaultPollingEndpoint, DefaultProducer, DefaultUnitOfWork, ExpressionSupport, HeaderFilterStrategyComponent, InterceptSendToMockEndpointStrategy, LoggingExceptionHandler, PollingConsumerSupport, ProcessorEndpoint, ProducerCache, ScheduledPollConsumer, ScheduledPollEndpoint, I moved the following classes without compat stubs as they were not needed outside camel-core: DefaultPollingConsumerPollStrategy, DefaultRouteNode, DefaultScheduledPollConsumer, DefaultSubUnitOfWork, DefaultTracedRouteNodes, EventDrivenPollingConsumer, ExpressionAdapter, InterceptSendToEndpoint, MDCUnitOfWork, MessageSupport, ProcessorPollingConsumer, SimpleUuidGenerator I moved AsyncProcessorConverterHelper from processor to util as it is needed from support. The only problematic class I moved was DefaultConsumer as it needed AsyncProcessorConverterHelper. So the above including the inner class ProcessorToAsyncProcessorBridge had to move to util. The problem here was that the Bridge had to extend DelegateProcessor which of course is in processor. As util should not depend on processor I had to introduce an interface DelegateProcessor in camel that could be used to abstract from ProcessorToAsyncProcessorBridge, DelegateProcessor and DelegateAsyncProcessor. This is a good thing anyway and I will open a jira to do this first. I also had to move PipelineHelper and SimpleUuidGeneratopr to util as they were used from support classes. This is a fairly large patch. So I am not sure if it is good for 2.9. On the other hand if we wait with this till 3.0 we are either really incompatible or we can not remove the deprecated classes",-0.05022222222,0.1,neutral
camel,4417,comment_5,This change is probably too destructive even for 3.0,design_debt,non-optimal_design,"Mon, 5 Sep 2011 13:46:59 +0000","Tue, 22 Oct 2013 07:34:27 +0000","Tue, 22 Oct 2013 07:34:27 +0000",67196848,This change is probably too destructive even for 3.0,-0.75,-0.75,neutral
camel,4543,comment_1,"Patch that creates a new module called and moves all aries specific stuff into that. camel-blueprint no longer depends on any Aries classes. Tested camel-blueprint on JBoss 7.0.2 and works wonderfully. Haven't been able to test the new jar yet, as I haven't setup ServiceMix or the like..",test_debt,lack_of_tests,"Thu, 13 Oct 2011 16:19:13 +0000","Sun, 1 May 2016 10:43:46 +0000","Sun, 1 May 2016 10:43:46 +0000",143576673,"Patch that creates a new module called camel-aries-blueprint-support and moves all aries specific stuff into that. camel-blueprint no longer depends on any Aries classes. Tested camel-blueprint on JBoss 7.0.2 and works wonderfully. Haven't been able to test the new camel-aries-blueprint-support jar yet, as I haven't setup ServiceMix or the like..",-0.008416666667,0.1275833333,positive
camel,4793,comment_1,"Hi Christian, if you need any help with this let me know. BTW is there any agreed naming convention for components URI options and message headers? Looking at the existing components seems like URI options can have component specific names whereas the headers use format? But in this case, if you want to override an option from the URI using the message header, you have to specify a different name, which may be lead to confusion. Any thoughts?",design_debt,non-optimal_design,"Sun, 18 Dec 2011 13:09:51 +0000","Tue, 27 Dec 2011 22:43:26 +0000","Mon, 19 Dec 2011 14:26:25 +0000",90994,"Hi Christian, if you need any help with this let me know. BTW is there any agreed naming convention for components URI options and message headers? Looking at the existing components seems like URI options can have component specific names whereas the headers use CamelComponnetNameXXX format? But in this case, if you want to override an option from the URI using the message header, you have to specify a different name, which may be lead to confusion. Any thoughts?",0.0825,0.0825,neutral
camel,4793,comment_2,Will make sure the documentation is updated in the next few hours,documentation_debt,outdated_documentation,"Sun, 18 Dec 2011 13:09:51 +0000","Tue, 27 Dec 2011 22:43:26 +0000","Mon, 19 Dec 2011 14:26:25 +0000",90994,Will make sure the documentation is updated in the next few hours,0.0,0.0,neutral
camel,4793,comment_3,"There is still no documentation for sdb component, I will add updated docs today",documentation_debt,outdated_documentation,"Sun, 18 Dec 2011 13:09:51 +0000","Tue, 27 Dec 2011 22:43:26 +0000","Mon, 19 Dec 2011 14:26:25 +0000",90994,"There is still no documentation for sdb component, I will add updated docs today",0.281,0.281,neutral
camel,4958,comment_0,"Is now less verbose, and Spring no longer complains about no error handler configured. Added options to tweak logging levels and verbosity.",code_debt,low_quality_code,"Tue, 31 Jan 2012 06:28:34 +0000","Tue, 31 Jan 2012 08:22:28 +0000","Tue, 31 Jan 2012 08:22:28 +0000",6834,"Is now less verbose, and Spring no longer complains about no error handler configured. Added options to tweak logging levels and verbosity.",0.2,0.2,neutral
camel,4959,comment_5,"Yeah I have corrected the coverters to deal with NaN. And yes we need to lookup fast in type converter registry as its used a lot in Camel, and it should be as fast as possible, as it can become a bottleneck.",code_debt,slow_algorithm,"Tue, 31 Jan 2012 07:06:19 +0000","Tue, 28 Feb 2012 11:43:59 +0000","Thu, 23 Feb 2012 12:47:32 +0000",2007673,"Yeah I have corrected the coverters to deal with NaN. And yes we need to lookup fast in type converter registry as its used a lot in Camel, and it should be as fast as possible, as it can become a bottleneck.",0.2,0.2,neutral
camel,4959,comment_4,"IMHO the changes introduced by of this ticket causes regression failure. To avoid null values being misinterpreted as cache misses we do now convert e.g. Float.NaN = which was not the case before. Why not just simply *not* mark the conversion as miss *if* the conversion result is (Float.NaN / Double.NaN == And I simply don't get why the misses cache is a ConcurrentMap: To my understanding would be just fine! Why do we need a Map to memorize the cache misses? Where we then do things like On it which is not really intuitive while reading the code, is this maybe because ideally we want to do lookup by the misses cache in O(1) instead of O(N)? See also:",design_debt,non-optimal_design,"Tue, 31 Jan 2012 07:06:19 +0000","Tue, 28 Feb 2012 11:43:59 +0000","Thu, 23 Feb 2012 12:47:32 +0000",2007673,"IMHO the changes introduced by of this ticket causes regression failure. To avoid null values being misinterpreted as cache misses we do now convert e.g. Float.NaN => (Byte) 0 which was not the case before. Why not just simply not mark the conversion as miss if the conversion result is (Float.NaN / Double.NaN ==> null). That's do not add any entry for such conversion results into the misses cache and revert back ObjectConverter to what it was before. Does this make sense to you? And I simply don't get why the misses cache is a ConcurrentMap: To my understanding would be just fine! Why do we need a Map to memorize the cache misses? Where we then do things like On it which is not really intuitive while reading the code, is this maybe because ideally we want to do lookup by the misses cache (misses.containsKey()) in O(1) instead of O(N)? See also: http://camel.465427.n5.nabble.com/ObjectConverter-problem-td5517376.html",-0.1444444444,-0.1,neutral
camel,4959,comment_0,That is correct. Andrey can you provide a patch with unit test that test this fix?,test_debt,low_coverage,"Tue, 31 Jan 2012 07:06:19 +0000","Tue, 28 Feb 2012 11:43:59 +0000","Thu, 23 Feb 2012 12:47:32 +0000",2007673,That is correct. Andrey can you provide a patch with unit test that test this fix?,0.4375,0.4375,neutral
camel,4959,comment_2,"Yeah lets return 0 for the primitive types. Patches is welcome, with unit tests.",test_debt,low_coverage,"Tue, 31 Jan 2012 07:06:19 +0000","Tue, 28 Feb 2012 11:43:59 +0000","Thu, 23 Feb 2012 12:47:32 +0000",2007673,"Yeah lets return 0 for the primitive types. Patches is welcome, with unit tests.",0.475,0.475,neutral
camel,5008,comment_0,Works if streaming gets out commented in test. Sorry if the test is a bit incomplete.,test_debt,low_coverage,"Wed, 15 Feb 2012 11:51:24 +0000","Thu, 16 Feb 2012 08:02:43 +0000","Thu, 16 Feb 2012 05:42:50 +0000",64286,Works if streaming gets out commented in test. Sorry if the test is a bit incomplete.,0.125,0.125,neutral
camel,5012,comment_0,Now Camel is less verbose by default.,code_debt,low_quality_code,"Thu, 16 Feb 2012 17:07:49 +0000","Thu, 16 Feb 2012 17:18:42 +0000","Thu, 16 Feb 2012 17:18:42 +0000",653,Now Camel is less verbose by default.,-0.5,-0.5,neutral
camel,5527,comment_1,I removed the surefire plugin as its no longer needed,code_debt,dead_code,"Tue, 21 Aug 2012 10:13:47 +0000","Tue, 21 Aug 2012 10:34:23 +0000","Tue, 21 Aug 2012 10:34:05 +0000",1218,I removed the surefire plugin as its no longer needed,0.0,0.0,neutral
camel,6043,comment_3,"This further improved performance for bean and simple language when using OGNL like expressions, that is evaluated at runtime. Now the cache ensures we do not introspect the same class types over and over again.",code_debt,slow_algorithm,"Wed, 6 Feb 2013 21:53:19 +0000","Thu, 28 Feb 2013 05:30:55 +0000","Thu, 28 Feb 2013 05:30:55 +0000",1841856,"This further improved performance for bean and simple language when using OGNL like expressions, that is evaluated at runtime. Now the cache ensures we do not introspect the same class types over and over again.",0.31875,0.31875,positive
camel,6178,comment_5,I added this to the Camel 2.11 release notes and updated the WIKI pages with the correct version which with this option is available.,documentation_debt,low_quality_documentation,"Mon, 18 Mar 2013 21:54:13 +0000","Wed, 27 Mar 2013 15:59:07 +0000","Wed, 27 Mar 2013 10:13:41 +0000",735568,I added this to the Camel 2.11 release notes and updated the WIKI pages with the correct version which with this option is available.,0.875,0.875,neutral
camel,6291,comment_0,"says: ""Reusable routes The routes defined in <routeContext/So, this is a bug in documented feature. If you think, it's an enhancement, the page should be corrected.",documentation_debt,low_quality_documentation,"Tue, 16 Apr 2013 14:45:35 +0000","Mon, 26 Aug 2013 09:50:28 +0000","Mon, 26 Aug 2013 09:50:28 +0000",11387093,"http://camel.apache.org/configuring-camel.html says: ""Reusable routes The routes defined in <routeContext/> can be reused by multiple <camelContext/>."" So, this is a bug in documented feature. If you think, it's an enhancement, the page should be corrected.",0.53125,0.3541666667,negative
camel,6320,comment_1,I think your test file data.ics is missing. Could you please add it? Adding a bit of logging would help too. Thanks for your contribution.,code_debt,low_quality_code,"Fri, 26 Apr 2013 17:23:36 +0000","Fri, 5 Jul 2013 21:44:07 +0000","Fri, 5 Jul 2013 21:44:07 +0000",6063631,I think your test file data.ics is missing. Could you please add it? Adding a bit of logging would help too. Thanks for your contribution.,0.12,0.12,neutral
camel,6320,comment_2,"Lukasz, thanks for the contribution. I did commit it to the trunk for now. Will create a placeholder for doc in the wiki, we'd appreciate some documentation. Since it's a new component and 2.12 will not be out anytime soon, I will port it back to the maintenance branch and then close this issue.",documentation_debt,outdated_documentation,"Fri, 26 Apr 2013 17:23:36 +0000","Fri, 5 Jul 2013 21:44:07 +0000","Fri, 5 Jul 2013 21:44:07 +0000",6063631,"Lukasz, thanks for the contribution. I did commit it to the trunk for now. Will create a placeholder for doc in the wiki, we'd appreciate some documentation. Since it's a new component and 2.12 will not be out anytime soon, I will port it back to the maintenance branch and then close this issue.",0.25,0.25,neutral
camel,6320,comment_3,Can we get this new component documented?,documentation_debt,outdated_documentation,"Fri, 26 Apr 2013 17:23:36 +0000","Fri, 5 Jul 2013 21:44:07 +0000","Fri, 5 Jul 2013 21:44:07 +0000",6063631,Can we get this new component documented?,0.0,0.0,neutral
camel,6403,comment_1,"Ideally the consumer should write its response in the async callback done method, then there UoW is not done yet. Though some of these http related components is a bit different and needed this support. Well spotted Gert.",requirement_debt,requirement_partially_implemented,"Tue, 28 May 2013 07:25:26 +0000","Mon, 12 Aug 2013 17:45:21 +0000","Fri, 9 Aug 2013 07:14:44 +0000",6306558,"Ideally the consumer should write its response in the async callback done method, then there UoW is not done yet. Though some of these http related components is a bit different and needed this support. Well spotted Gert.",0.3436666667,0.3436666667,neutral
camel,6446,comment_0,Thanks for the patch. Feel free to help with adding some docs to:,documentation_debt,outdated_documentation,"Mon, 10 Jun 2013 16:32:56 +0000","Tue, 25 Jun 2013 09:10:24 +0000","Tue, 25 Jun 2013 09:10:24 +0000",1269448,Thanks for the patch. Feel free to help with adding some docs to: http://camel.apache.org/json,0.35,0.35,positive
camel,6578,comment_1,Don't forget to update the Camel 2.12.0 release page at,documentation_debt,outdated_documentation,"Fri, 26 Jul 2013 11:28:11 +0000","Wed, 31 Jul 2013 07:24:53 +0000","Mon, 29 Jul 2013 08:02:40 +0000",246869,Don't forget to update the Camel 2.12.0 release page at https://cwiki.apache.org/confluence/display/CAMEL/Camel+2.12.0+Release,0.2,0.2,neutral
camel,6620,comment_0,"I just checked the code AuthMethod is just used in camel-http component, not the camel-http4 component. So it is not strange that there is no mention of AuthMethod in the http4 document. I also checked the camel-http wiki page, there is entry of the options about the AuthMethod.",documentation_debt,outdated_documentation,"Thu, 8 Aug 2013 18:52:34 +0000","Fri, 9 Aug 2013 01:03:08 +0000","Fri, 9 Aug 2013 01:03:08 +0000",22234,"I just checked the code AuthMethod is just used in camel-http component, not the camel-http4 component. So it is not strange that there is no mention of AuthMethod in the http4 document. I also checked the camel-http wiki page, there is entry of the options about the AuthMethod.",0.06666666667,0.06666666667,neutral
camel,6735,comment_1,This works in 2.12.x onwards. Hunting this down on 2.11.x is low priority. End users is encourage to upgrade if they really need this.,defect_debt,uncorrected_known_defects,"Wed, 11 Sep 2013 20:25:23 +0000","Fri, 14 Feb 2014 16:02:02 +0000","Fri, 14 Feb 2014 16:02:02 +0000",13462599,This works in 2.12.x onwards. Hunting this down on 2.11.x is low priority. End users is encourage to upgrade if they really need this.,0.1599,0.1599,neutral
camel,6735,comment_0,attached a polished test from Preben which shows the issue.,test_debt,lack_of_tests,"Wed, 11 Sep 2013 20:25:23 +0000","Fri, 14 Feb 2014 16:02:02 +0000","Fri, 14 Feb 2014 16:02:02 +0000",13462599,attached a polished test from Preben which shows the issue.,0.4,0.4,neutral
camel,6826,comment_0,"I have been able to successfully speed up the producer tests. However, the consumer tests are a bit tricky. They use a callback API to do their work. I don't know if it's worth mocking all of that out or not, but I'll play with it. I'm committing what I have, though.",test_debt,expensive_tests,"Fri, 4 Oct 2013 13:51:25 +0000","Sat, 11 Jul 2015 18:27:31 +0000","Sat, 11 Jul 2015 18:27:21 +0000",55744556,"I have been able to successfully speed up the producer tests. However, the consumer tests are a bit tricky. They use a callback API to do their work. I don't know if it's worth mocking all of that out or not, but I'll play with it. I'm committing what I have, though.",0.2913,0.2913,neutral
camel,6826,comment_1,"I have implemented mocks for the consumer tests. The spring-based tests still need to be mocked out as well as the seda tests. It is much faster now, though.",test_debt,expensive_tests,"Fri, 4 Oct 2013 13:51:25 +0000","Sat, 11 Jul 2015 18:27:31 +0000","Sat, 11 Jul 2015 18:27:21 +0000",55744556,"I have implemented mocks for the consumer tests. The spring-based tests still need to be mocked out as well as the seda tests. It is much faster now, though.",-0.07644444444,-0.07644444444,neutral
camel,7015,comment_1,What is the idea with the Will it not confuse end users that there is 2 timeouts now?,design_debt,non-optimal_design,"Tue, 26 Nov 2013 22:24:15 +0000","Fri, 5 Sep 2014 07:48:10 +0000","Fri, 5 Sep 2014 07:48:10 +0000",24398635,What is the idea with the maxCompletionTimeout? Will it not confuse end users that there is 2 timeouts now?,0.4,0.2,neutral
camel,7015,comment_2,"Willem hold on any merges, as at first glance I think its confusing with that max timeout option.",design_debt,non-optimal_design,"Tue, 26 Nov 2013 22:24:15 +0000","Fri, 5 Sep 2014 07:48:10 +0000","Fri, 5 Sep 2014 07:48:10 +0000",24398635,"Willem hold on any merges, as at first glance I think its confusing with that max timeout option.",-0.437,-0.437,neutral
camel,7015,comment_3,"At first I though of adding an option telling whether or not the timeout is for the first message received in this correlation group or for the last message. But then I though, why not provide both timeouts ? You might want to 5 seconds after the last exchange received but not more than 60 seconds after the first message. That way, if you have an exchange every 4 seconds, you'll aggregate after one minute an not any longer. Thus I named this parameter because I didn't wanted to rename the parameter. I'm not saying that it's the right way, or maybe it's not properly named. Don't hesitate to ask if you want more insights about this patch.",design_debt,non-optimal_design,"Tue, 26 Nov 2013 22:24:15 +0000","Fri, 5 Sep 2014 07:48:10 +0000","Fri, 5 Sep 2014 07:48:10 +0000",24398635,"At first I though of adding an option telling whether or not the timeout is for the first message received in this correlation group or for the last message. But then I though, why not provide both timeouts ? You might want to 5 seconds after the last exchange received but not more than 60 seconds after the first message. That way, if you have an exchange every 4 seconds, you'll aggregate after one minute an not any longer. Thus I named this parameter maxCompletionTimeout because I didn't wanted to rename the completionTimeout parameter. I'm not saying that it's the right way, or maybe it's not properly named. Don't hesitate to ask if you want more insights about this patch.",-0.03242857143,-0.03242857143,neutral
camel,7071,comment_1,This is strange way to solve that. Should not we (application developers) use exception wrapping? Please note this method is used By,design_debt,non-optimal_design,"Mon, 16 Dec 2013 14:03:19 +0000","Wed, 18 Dec 2013 12:35:13 +0000","Tue, 17 Dec 2013 03:02:23 +0000",46744,This is strange way to solve that. Should not we (application developers) use exception wrapping? Please note this method is used By ExchangeExceptionannotation.,0.06666666667,0.06666666667,negative
camel,7133,comment_3,"Can you remove the <cxf:rsServer id=""rsServer"" I don't think you can put the url options into rsServer address attribut.",code_debt,low_quality_code,"Tue, 14 Jan 2014 14:54:55 +0000","Thu, 16 Jan 2014 07:44:18 +0000","Thu, 16 Jan 2014 07:44:18 +0000",146963,"Can you remove the <cxf:rsServer id=""rsServer"" > part ? I don't think you can put the url options into rsServer address attribut.",0.0,0.0,negative
camel,7300,comment_0,Pushed to master. Not this patch makes use of a deprecated API call. In time we should move the entire camel-hl7 component to the new HAPI API: CAMEL-7301,code_debt,low_quality_code,"Mon, 17 Mar 2014 09:01:31 +0000","Tue, 25 Mar 2014 08:59:16 +0000","Mon, 17 Mar 2014 09:32:58 +0000",1887,Pushed to master. Not this patch makes use of a deprecated API call. In time we should move the entire camel-hl7 component to the new HAPI API: CAMEL-7301,0.1666666667,0.1666666667,negative
camel,7342,comment_1,"Having looked at this issue afresh I wonder if my description of implementing the flag is a cause of confusion and reluctance to add this feature. On re-reading, I realise that this description of what is required is misleading - of course you always want the consumer to attempt to connect upon start of the route, however if the bind to the SMSC fails for whatever reason, I need it to enter into a reconnection attempt cycle until the SMSC is available rather than stopping the route (and any others in the same camel context). Anyway, this is a requirement for me, and so I have implemented the functionality change for myself as a branch from 2.15.1. It is configured using a new option If this is set to true, then a reconnect thread is started instead of trying once only to create the session. The major change here is the removal of the wait for the thread completion in the reconnect method (to allow doStart to complete), and movement of the reconnectLock mutex.",documentation_debt,low_quality_documentation,"Thu, 3 Apr 2014 09:15:34 +0000","Fri, 10 Apr 2015 15:35:20 +0000","Mon, 6 Apr 2015 13:27:27 +0000",31810313,"Having looked at this issue afresh I wonder if my description of implementing the ""lazySessionCreation"" flag is a cause of confusion and reluctance to add this feature. On re-reading, I realise that this description of what is required is misleading - of course you always want the consumer to attempt to connect upon start of the route, however if the bind to the SMSC fails for whatever reason, I need it to enter into a reconnection attempt cycle until the SMSC is available rather than stopping the route (and any others in the same camel context). Anyway, this is a requirement for me, and so I have implemented the functionality change for myself as a branch from 2.15.1. It is configured using a new option ""startConsumerIfDown"". If this is set to true, then a reconnect thread is started instead of trying once only to create the session. The major change here is the removal of the wait for the thread completion in the reconnect method (to allow doStart to complete), and movement of the reconnectLock mutex. https://github.com/RaySlater/camel/tree/CAMEL-7342",0.1564,0.1117142857,neutral
camel,7413,comment_0,"I fixed this by making all of the parameters configurable, and updating the documentation. Here are the new docs. ## Usage ## The plugin configuration has the following properties. * - Salesforce url to use to generate the dtos. Defaults to but is useful for development environments. * - Maximum time to hold the connection open to Salesforce when generating the dtos. Defaults to 60 seconds, which is usually fine unless you have a big Salesforce schema or slow connection. * - Salesforce client Id for Remote API access * - Salesforce client secret for Remote API access * - Salesforce account user name * - Salesforce account password (including secret token) * - Salesforce Rest API version, defaults to 27.0 NOTE: Currently no version higher than 27.0 will work. * - Directory where to place generated DTOs, defaults to * - List of SObject types to include * - List of SObject types to exclude * - Java RegEx for SObject types to include * - Java RegEx for SObject types to exclude * - Java package name for generated DTOs, defaults to For obvious security reasons it is recommended that the clientId, clientSecret, userName and password fields be not set in the pom.xml. The plugin should be configured for the rest of the properties, and can be executed using the following command: mvn The generated DTOs use Jackson and XStream annotations. All Salesforce field types are supported. Date and time fields are mapped to Joda DateTime, and picklist fields are mapped to generated Java Enumerations.",documentation_debt,outdated_documentation,"Mon, 5 May 2014 15:12:16 +0000","Sun, 18 Mar 2018 20:30:17 +0000","Sun, 18 Mar 2018 20:30:01 +0000",122102265,"I fixed this by making all of the parameters configurable, and updating the documentation. Here are the new docs. Usage ## The plugin configuration has the following properties. camelSalesforce.loginUrl - Salesforce url to use to generate the dtos. Defaults to https://login.salesforce.com, but https://test.salesforce.com is useful for development environments. camelSalesforce.timeOutInMilliseconds - Maximum time to hold the connection open to Salesforce when generating the dtos. Defaults to 60 seconds, which is usually fine unless you have a big Salesforce schema or slow connection. camelSalesforce.clientId - Salesforce client Id for Remote API access camelSalesforce.clientSecret - Salesforce client secret for Remote API access camelSalesforce.userName - Salesforce account user name camelSalesforce.password - Salesforce account password (including secret token) camelSalesforce.version - Salesforce Rest API version, defaults to 27.0 NOTE: Currently no version higher than 27.0 will work. camelSalesforce.outputDirectory - Directory where to place generated DTOs, defaults to ${project.build.directory}/generated-sources/camel-salesforce camelSalesforce.includes - List of SObject types to include camelSalesforce.excludes - List of SObject types to exclude camelSalesforce.includePattern - Java RegEx for SObject types to include camelSalesforce.excludePattern - Java RegEx for SObject types to exclude camelSalesforce.packageName - Java package name for generated DTOs, defaults to org.apache.camel.salesforce.dto. For obvious security reasons it is recommended that the clientId, clientSecret, userName and password fields be not set in the pom.xml. The plugin should be configured for the rest of the properties, and can be executed using the following command: mvn camel-salesforce:generate -DcamelSalesforce.clientId=<clientid> -DcamelSalesforce.clientSecret=<clientsecret> -DcamelSalesforce.userName=<username> -DcamelSalesforce.password=<password> The generated DTOs use Jackson and XStream annotations. All Salesforce field types are supported. Date and time fields are mapped to Joda DateTime, and picklist fields are mapped to generated Java Enumerations.",0.05,0.002702702703,neutral
camel,7681,comment_2,I was going to ask you the best way to update the documentation but you were faster. Thanks !,documentation_debt,low_quality_documentation,"Mon, 11 Aug 2014 20:43:28 +0000","Wed, 13 Aug 2014 08:44:23 +0000","Wed, 13 Aug 2014 08:38:45 +0000",129317,I was going to ask you the best way to update the documentation but you were faster. Thanks !,0.6375,0.6375,positive
camel,7690,comment_0,A plain JAR example. As a WAB is not simple to create.,documentation_debt,low_quality_documentation,"Wed, 13 Aug 2014 07:51:39 +0000","Wed, 13 Aug 2014 09:57:47 +0000","Wed, 13 Aug 2014 09:57:47 +0000",7568,A plain JAR example. As a WAB is not simple to create.,-0.375,-0.375,neutral
camel,7923,comment_1,"It doesn't fail each time. It's only failing a few times. But by having too many ""fragile"" test, it's annoying to do a build because of the many many attempts you need.",test_debt,flaky_test,"Fri, 17 Oct 2014 05:30:28 +0000","Sat, 18 Oct 2014 17:30:55 +0000","Sat, 18 Oct 2014 17:30:55 +0000",129627,"It doesn't fail each time. It's only failing a few times. But by having too many ""fragile"" test, it's annoying to do a build because of the many many attempts you need.",-0.3333333333,-0.3333333333,negative
camel,7954,comment_0,"Component updated, need to update documentation",documentation_debt,outdated_documentation,"Fri, 24 Oct 2014 20:18:18 +0000","Tue, 6 Aug 2019 05:14:42 +0000","Tue, 6 Aug 2019 05:14:42 +0000",150886584,"Component updated, need to update documentation",0.0,0.0,neutral
camel,7956,comment_0,"Component updated, need to update documentation",documentation_debt,outdated_documentation,"Fri, 24 Oct 2014 20:19:09 +0000","Tue, 6 Aug 2019 05:14:50 +0000","Tue, 6 Aug 2019 05:14:50 +0000",150886541,"Component updated, need to update documentation",0.0,0.0,neutral
camel,8029,comment_0,I suggest to add some defails as <description Then you can tell the user how to install that bundle needed.,documentation_debt,low_quality_documentation,"Tue, 11 Nov 2014 00:24:02 +0000","Sun, 30 Nov 2014 20:58:10 +0000","Sun, 30 Nov 2014 15:24:44 +0000",1695642,"I suggest to add some defails as <description> or <note> or what that xml tag is in the features.xml file you can do. Then people can see it using features:info camel-xxx. We have this for a few other features, you can find as examples. Then you can tell the user how to install that bundle needed.",0.0,0.1,neutral
camel,8101,comment_3,We need to update the documentation with the new command,documentation_debt,outdated_documentation,"Mon, 1 Dec 2014 20:11:20 +0000","Mon, 9 Feb 2015 05:33:20 +0000","Mon, 9 Feb 2015 05:33:20 +0000",5995320,We need to update the documentation with the new command http://camel.apache.org/mongodb,0.0,0.0,neutral
camel,8101,comment_4,Yes Totaly agree. I also want to add a aggregate example. I just sign the comiter agreement yesterday. Will update wiki soon. Best regards,documentation_debt,outdated_documentation,"Mon, 1 Dec 2014 20:11:20 +0000","Mon, 9 Feb 2015 05:33:20 +0000","Mon, 9 Feb 2015 05:33:20 +0000",5995320,Yes Totaly agree. I also want to add a aggregate example. I just sign the comiter agreement yesterday. Will update wiki soon. Best regards,0.295,0.295,neutral
camel,8101,comment_5,Hi Pierre Did you have any chance to update the documentation?,documentation_debt,outdated_documentation,"Mon, 1 Dec 2014 20:11:20 +0000","Mon, 9 Feb 2015 05:33:20 +0000","Mon, 9 Feb 2015 05:33:20 +0000",5995320,Hi Pierre Did you have any chance to update the documentation?,0.4,0.4,neutral
camel,8312,comment_2,"Hi Claus, are you sure that you want to delay this till 2.15.0? I think this issue is serious. Best regards Stephan",defect_debt,uncorrected_known_defects,"Wed, 4 Feb 2015 07:25:25 +0000","Tue, 3 Mar 2015 21:12:58 +0000","Mon, 2 Mar 2015 12:01:31 +0000",2262966,"Hi Claus, are you sure that you want to delay this till 2.15.0? I think this issue is serious. Best regards Stephan",0.2916666667,0.2916666667,negative
camel,8312,comment_0,"One thing I overlooked: the javadoc of the method says: ... For example you can set it to InputSource to use SAX streams. By default Camel uses Document as the type. ... Using InputSource as documentType is a particularily bad idea. If the XPath implementation supports it (Saxon does, the JDK implementation doesn't), SAXSource can be a more memory efficient choice for the docuementType. We should probably also change the Javadoc here...",design_debt,non-optimal_design,"Wed, 4 Feb 2015 07:25:25 +0000","Tue, 3 Mar 2015 21:12:58 +0000","Mon, 2 Mar 2015 12:01:31 +0000",2262966,"One thing I overlooked: the javadoc of the XPathBuilder.documentType() method says: ... For example you can set it to InputSource to use SAX streams. By default Camel uses Document as the type. ... Using InputSource as documentType is a particularily bad idea. If the XPath implementation supports it (Saxon does, the JDK implementation doesn't), SAXSource can be a more memory efficient choice for the docuementType. We should probably also change the Javadoc here...",-0.18,-0.15,negative
camel,8321,comment_3,"Why are Box headers being used/fed in other These headers are only supposed to be fed to the Box component, then should be cleaned up n the next step. I wasn't aware of the Camel 'no dots in header' convention. I'll have to take a look at the API framework to see if that's affected. At the very minimum code generation maven plugin is affected since it uses that convention for all generated components, and also would make the new components backward incompatible.",design_debt,non-optimal_design,"Fri, 6 Feb 2015 11:28:52 +0000","Wed, 18 Feb 2015 01:45:56 +0000","Wed, 18 Feb 2015 01:45:56 +0000",1001824,"Why are Box headers being used/fed in other components/processors? These headers are only supposed to be fed to the Box component, then should be cleaned up n the next step. I wasn't aware of the Camel 'no dots in header' convention. I'll have to take a look at the API framework to see if that's affected. At the very minimum code generation maven plugin is affected since it uses that convention for all generated components, and also would make the new components backward incompatible.",-0.24075,-0.1926,neutral
camel,8328,comment_3,I've changed the name of the test package as OSX seems to confuse test package name with class Can you give it a shot now?,code_debt,low_quality_code,"Tue, 10 Feb 2015 05:58:48 +0000","Mon, 16 Feb 2015 14:18:45 +0000","Sun, 15 Feb 2015 21:01:35 +0000",486167,I've changed the name of the test package as OSX seems to confuse test package name org.apache.camel.spring.boot.fatjarrouter with class org.apache.camel.spring.boot.FatJarRouter. Can you give it a shot now?,-0.4,-0.03333333333,neutral
camel,8734,comment_0,"Hi, Please consider that URLs should be case sensitive according to W3. [quote]URLs in general are case-sensitive (with the exception of machine names). There may be URLs, or parts of URLs, where case doesn't matter, but identifying these may not be easy. Users should always consider that URLs are The header CamelHttpPath should provide me what the user actually entered.",code_debt,low_quality_code,"Sun, 3 May 2015 09:47:45 +0000","Fri, 11 Sep 2015 14:19:08 +0000","Fri, 17 Jul 2015 09:07:17 +0000",6477572,"Hi, Please consider that URLs should be case sensitive according to W3. [quote]URLs in general are case-sensitive (with the exception of machine names). There may be URLs, or parts of URLs, where case doesn't matter, but identifying these may not be easy. Users should always consider that URLs are case-sensitive.[/quote] http://www.w3.org/TR/WD-html40-970708/htmlweb.html The header CamelHttpPath should provide me what the user actually entered.",-0.026,-0.0208,neutral
camel,8879,comment_1,Would you be able to attach an unit test also?,test_debt,lack_of_tests,"Wed, 17 Jun 2015 12:56:35 +0000","Sun, 21 Feb 2016 10:19:02 +0000","Sun, 21 Feb 2016 10:19:02 +0000",21504147,Would you be able to attach an unit test also?,0.688,0.688,neutral
camel,9226,comment_2,Ah good to hear. You are welcome to submit your unit test as a PR / .patch file then we can add it to camel-metrics so we have it there too.,test_debt,lack_of_tests,"Thu, 15 Oct 2015 08:14:35 +0000","Fri, 16 Oct 2015 12:57:40 +0000","Fri, 16 Oct 2015 12:57:40 +0000",103385,Ah good to hear. You are welcome to submit your unit test as a PR / .patch file then we can add it to camel-metrics so we have it there too.,0.5086666667,0.5086666667,positive
camel,9403,comment_0,"We also only had some of the examples in the bom, not all of them.",documentation_debt,outdated_documentation,"Wed, 9 Dec 2015 16:05:59 +0000","Fri, 11 Dec 2015 07:59:44 +0000","Fri, 11 Dec 2015 07:59:29 +0000",143610,"We also only had some of the examples in the bom, not all of them.",0.0,0.0,neutral
camel,9594,comment_0,We can now use /swagger.json or /swagger.yaml with camel-swagger-java so we can update the examples to use the url.,documentation_debt,low_quality_documentation,"Fri, 12 Feb 2016 09:52:20 +0000","Mon, 21 Mar 2016 13:48:34 +0000","Mon, 21 Mar 2016 13:48:34 +0000",3297374,We can now use /swagger.json or /swagger.yaml with camel-swagger-java so we can update the examples to use the url.,0.0,0.0,neutral
camel,9643,comment_4,"Hi Claus, This issue seems to have been fixed by 2.16.3 (I will test it more thoroughly later). But I still cannot test it under 2.17.0 or 2.17.1 because I have to use Spring for DSL, transactions and jdbc templates but I have trouble to even load my bundle in Karaf with camel-spring (or spring-dm). camel-sql also cannot be loaded into Karaf (installing it would cause Karaf to hang). If the issue is indeed fully fixed in 2.16.3, then I can start to move my application off 2.15.x. I hope more tutorials are provided and/or updated for 2.17.x, especially in the area of deploying camel/cxf with spring in karaf. Thank you for your help!",documentation_debt,outdated_documentation,"Thu, 25 Feb 2016 14:51:24 +0000","Sun, 22 May 2016 06:36:30 +0000","Sun, 22 May 2016 06:36:30 +0000",7487106,"Hi Claus, This issue seems to have been fixed by 2.16.3 (I will test it more thoroughly later). But I still cannot test it under 2.17.0 or 2.17.1 because I have to use Spring for DSL, transactions and jdbc templates but I have trouble to even load my bundle in Karaf with camel-spring (or spring-dm). camel-sql also cannot be loaded into Karaf (installing it would cause Karaf to hang). If the issue is indeed fully fixed in 2.16.3, then I can start to move my application off 2.15.x. I hope more tutorials are provided and/or updated for 2.17.x, especially in the area of deploying camel/cxf with spring in karaf. Thank you for your help!",0.07278571429,0.07278571429,neutral
camel,9721,comment_1,"Hi Claus I've read your email, it sounds like the new version would not present the issue, anyway, if camel-spring-batch depends only on org.springbatch, that should be the dependency and not camel-spring. I think the pom.xml and features.xml should be aligned. As this project has many, many dependencies, the shorter the dependency chain the better. Does it make sense?",build_debt,under-declared_dependencies,"Thu, 17 Mar 2016 12:19:12 +0000","Thu, 17 Mar 2016 13:21:25 +0000","Thu, 17 Mar 2016 12:46:03 +0000",1611,"Hi Claus I've read your email, it sounds like the new version would not present the issue, anyway, if camel-spring-batch depends only on org.springbatch, that should be the dependency and not camel-spring. I think the pom.xml and features.xml should be aligned. As this project has many, many dependencies, the shorter the dependency chain the better. Does it make sense?",0.07142857143,0.07142857143,neutral
camel,9789,comment_5,"That's good to know, i was a bit miss lead by the documentation. I think a sample for ""good pratice"" with spring boot, java config and camel would help.... Much to do at the moment, but i will try to zip a basic sample project and upload it to this JIRA",documentation_debt,low_quality_documentation,"Fri, 1 Apr 2016 09:20:05 +0000","Thu, 7 Apr 2016 08:17:53 +0000","Thu, 7 Apr 2016 08:17:53 +0000",514668,"That's good to know, i was a bit miss lead by the documentation. I think a sample for ""good pratice"" with spring boot, java config and camel would help.... Much to do at the moment, but i will try to zip a basic sample project and upload it to this JIRA",0.388,0.388,positive
camel,9812,comment_0,"Thanks for reporting. Sound like the consumer need to call some stop/shutdown on kafka somewhere if its not already doing that, or missing something.",design_debt,non-optimal_design,"Mon, 4 Apr 2016 13:00:56 +0000","Tue, 5 Apr 2016 11:19:48 +0000","Tue, 5 Apr 2016 09:24:07 +0000",73391,"Thanks for reporting. Sound like the consumer need to call some stop/shutdown on kafka somewhere if its not already doing that, or missing something.",0.05,0.05,neutral
camel,10476,comment_1,Let me know if I can provide any more information or if something isn't clear. I also was able to successfully test the workaround and I can run locally(in an IDE) using java:exec with the same properties file that the unit tests load by overriding and reading that config file. The only thing I am not confident in with the workaround is the difference between running in a DefaultCamelContext vs a in the IDE.,design_debt,non-optimal_design,"Mon, 14 Nov 2016 16:09:36 +0000","Fri, 16 Dec 2016 08:10:55 +0000","Fri, 18 Nov 2016 17:09:14 +0000",349178,Let me know if I can provide any more information or if something isn't clear. I also was able to successfully test the workaround and I can run locally(in an IDE) using java:exec with the same properties file that the unit tests load by overriding loadConfigAdminConfigurationFile and reading that config file. The only thing I am not confident in with the workaround is the difference between running in a DefaultCamelContext vs a BlueprintCamelContext in the IDE.,0.04866666667,0.04866666667,neutral
camel,10563,comment_0,Need to add better handling for hz instance cleanup,code_debt,low_quality_code,"Tue, 6 Dec 2016 16:23:30 +0000","Tue, 13 Dec 2016 13:20:24 +0000","Tue, 13 Dec 2016 13:20:22 +0000",593812,Need to add better handling for hz instance cleanup,0.5,0.5,neutral
camel,11196,comment_0,"We should also make it easy to reuse an existing connector and have separate configuration, eg so the same user can use a twitter-connector and then configure it for user A and user B in the same application without configuration clashes. Today the component level would reuse the same component / configuration. So we may want to find a way to clone a component or whatever we can think of. A little bit of problem is the generated spring boot auto configuration which is hard-coded to one prefix.",design_debt,non-optimal_design,"Mon, 24 Apr 2017 14:22:57 +0000","Mon, 15 May 2017 15:18:48 +0000","Mon, 15 May 2017 15:18:48 +0000",1817751,"We should also make it easy to reuse an existing connector and have separate configuration, eg so the same user can use a twitter-connector and then configure it for user A and user B in the same application without configuration clashes. Today the component level would reuse the same component / configuration. So we may want to find a way to clone a component or whatever we can think of. A little bit of problem is the generated spring boot auto configuration which is hard-coded to one prefix.",0.048,0.048,neutral
camel,11408,comment_0,"The component docs are in adoc files with the source code - the wiki is dead so don't update there. Make sure to fix/update in adoc, and if you want you can do wiki too. But wiki only changes will be lost in the future when wiki is discarded completely",documentation_debt,outdated_documentation,"Wed, 14 Jun 2017 07:16:37 +0000","Thu, 1 Feb 2018 15:34:14 +0000","Thu, 1 Feb 2018 15:34:14 +0000",20074657,"The component docs are in adoc files with the source code - the wiki is dead so don't update there. Make sure to fix/update in adoc, and if you want you can do wiki too. But wiki only changes will be lost in the future when wiki is discarded completely",-0.2166666667,-0.2166666667,negative
camel,11504,comment_3,That could be a good start and an easy way to fix broken links we have currently. I was hoping that we might have a build-time tool/integration with a tool to check for links. I think that would be useful for PR jobs (when those get fixed).,test_debt,lack_of_tests,"Mon, 3 Jul 2017 08:41:20 +0000","Mon, 22 Apr 2019 16:21:52 +0000","Mon, 22 Apr 2019 16:21:44 +0000",56878824,That could be a good start and an easy way to fix broken links we have currently. I was hoping that we might have a build-time tool/integration with a tool to check for links. I think that would be useful for PR jobs (when those get fixed).,0.3964444444,0.3964444444,positive
camel,11655,comment_0,"IMHO this's a breaking change for the upcoming {{2.20.0}} version as the query parameter has been simply removed and replaced with a new {{encryption}} query parameter, which _would_ break user's code making use of this option. AFAIK a minor release should not include any breaking changes, right? Shouldn't we better somehow mark as deprecated and encourage users to make use of the new {{encryption}} parameter instead? Other than that some feedback on your made code changes: What would be wrong to do: Instead of the following if / else if: As because: Also maybe mark the enum itself as deprecated so we don't forget to remove it in Camel 3.",design_debt,non-optimal_design,"Wed, 9 Aug 2017 12:27:03 +0000","Tue, 22 Aug 2017 15:38:03 +0000","Tue, 22 Aug 2017 06:30:11 +0000",1101788,"ancosen IMHO this's a breaking change for the upcoming 2.20.0 version as the encryptionMethod query parameter has been simply removed and replaced with a new encryption query parameter, which would break user's code making use of this option. AFAIK a minor release should not include any breaking changes, right? Shouldn't we better somehow mark encryptionMethod as deprecated and encourage users to make use of the new encryption parameter instead? Other than that some feedback on your made code changes: What would be wrong to do: Instead of the following if / else if: As because: Also maybe mark the NagiosEncryptionMethod enum itself as deprecated so we don't forget to remove it in Camel 3.",0.12025,0.12025,neutral
camel,11655,comment_4,"LOL it's not critical for me but for the community :-) I am not convineced by your answers and kindly ask  &  to share their thoughts about this ticket as well as it's corresponding code changes. To summurize: - The made changes are breaking for the upcoming 2.20.0 _minor_ release if a Camel based application already makes use of the query parameter today in production. - IMHO Camel should be transparent and not restrict an underlying library API just because it seems to be buggy or non-working. Bugs _should_ be resolved by the underlying library itself and not _artificially_ through Camel by hiding/restricting a given API. See my comments above regarding this point. - In general, shouldn't we mark a Camel API as deprecated and encourage users to make use of the right/new API. E.g. as it's done when some {{StringHelper}} new utiliy methods were extracted out of {{ObjectHelper}} and the corresponding {{ObjectHelper}} methods were marked as deprecated, see CAMEL-10389. This would make it much much easier both for Camel code base itself as well as user's code to avoid using the legacy API. I guess that's exactly what the {{@Deprecated}} annotation good for. I don't seem to understand what it would be wrong with such an approach, that's, marking query parameter as deprecaterd and encourage useres to make use of the new {{encryption}} query parameter and then support both of them. Supporting both would not break a pre-existing application on the production and then we could find and remove the deprecated API by Camel 3 much easier. WDYT?",design_debt,non-optimal_design,"Wed, 9 Aug 2017 12:27:03 +0000","Tue, 22 Aug 2017 15:38:03 +0000","Tue, 22 Aug 2017 06:30:11 +0000",1101788,"If this is something critical for you, you can revert commit and do what you think is best for this component. It was just an attempt to avoid the death of camel-nagios. ancosen LOL it's not critical for me but for the community I am not convineced by your answers and kindly ask davsclaus & lb to share their thoughts about this ticket as well as it's corresponding code changes. To summurize: The made changes are breaking for the upcoming 2.20.0 minor release if a Camel based application already makes use of the encryptionMethod query parameter today in production. IMHO Camel should be transparent and not restrict an underlying library API just because it seems to be buggy or non-working. Bugs should be resolved by the underlying library itself and not artificially through Camel by hiding/restricting a given API. See my comments above regarding this point. In general, shouldn't we mark a not-recommended-to-be-used Camel API as deprecated and encourage users to make use of the right/new API. E.g. as it's done when some StringHelper new utiliy methods were extracted out of ObjectHelper and the corresponding ObjectHelper methods were marked as deprecated, see CAMEL-10389. This would make it much much easier both for Camel code base itself as well as user's code to avoid using the legacy API. I guess that's exactly what the @Deprecated annotation good for. I don't seem to understand what it would be wrong with such an approach, that's, marking encryptionMethod query parameter as deprecaterd and encourage useres to make use of the new encryption query parameter and then support both of them. Supporting both would not break a pre-existing application on the production and then we could find and remove the deprecated API by Camel 3 much easier. WDYT?",0.2647866667,0.1805833333,neutral
camel,11655,comment_1,"That code portion is needed because currently jsendnsca works only with Encryption NONE, XOR and Triple des, the others aren't working. I guess that we can accept a change like this one is just a minor change to align with the new library. So IMO it's not a problem for the end users.",requirement_debt,requirement_partially_implemented,"Wed, 9 Aug 2017 12:27:03 +0000","Tue, 22 Aug 2017 15:38:03 +0000","Tue, 22 Aug 2017 06:30:11 +0000",1101788,"That code portion is needed because currently jsendnsca works only with Encryption NONE, XOR and Triple des, the others aren't working. I guess that we can accept a change like this one is just a minor change to align with the new library. So IMO it's not a problem for the end users.",0.2,0.2,neutral
camel,11655,comment_3,"It's stated here in this thread: for Blowfish, yes, in case it will be supported we can add another else if there. It doesn't make sense to allow all the encryption if most of them don't work in nsca. Marking the as deprecated in my opinion it's not so important, it's just an option. If we start to deprecate all the options when we upgrade something we will end with a lot of dead code until Camel 3.0 (that actually doesn't have an ETA by the way). If this is something critical for you, you can revert commit and do what you think is best for this component. It was just an attempt to avoid the death of camel-nagios.",requirement_debt,requirement_partially_implemented,"Wed, 9 Aug 2017 12:27:03 +0000","Tue, 22 Aug 2017 15:38:03 +0000","Tue, 22 Aug 2017 06:30:11 +0000",1101788,"It's stated here in this thread: https://groups.google.com/forum/#!msg/jsend-nsca/QI4OUTqzv6w/jNhOC-8z4AwJ for Blowfish, yes, in case it will be supported we can add another else if there. It doesn't make sense to allow all the encryption if most of them don't work in nsca. Marking the NagiosEncryptionMethod as deprecated in my opinion it's not so important, it's just an option. If we start to deprecate all the options when we upgrade something we will end with a lot of dead code until Camel 3.0 (that actually doesn't have an ETA by the way). If this is something critical for you, you can revert commit and do what you think is best for this component. It was just an attempt to avoid the death of camel-nagios.",0.05763888889,0.05763888889,neutral
camel,11734,comment_2,great :-) Maybe we can document very well how everything works and then setup an example in OSGi (we currently have a spring-boot example and a kubernetes one).,documentation_debt,low_quality_documentation,"Fri, 1 Sep 2017 06:17:00 +0000","Fri, 8 Sep 2017 08:49:52 +0000","Fri, 8 Sep 2017 08:49:52 +0000",613972,dmvolod great Maybe we can document very well how everything works and then setup an example in OSGi (we currently have a spring-boot example and a kubernetes one).,0.55775,0.6103333333,positive
camel,11868,comment_1,"It might be a good idea, since not all features is supported in the high level rest client yet I already got it to work with the rest client, but then I would suggest we create a super class for and a and In the we decide which one we should instantiate based on the type client on the class path. This of course mean the client dependency is not included in the the component. Regarding the it's not possible to use the Hight level REST client since it only support ELK from 5.6.0 and forward. One think to be aware of with the REST API that it is slightly different, which mean various request and response object is not called the same with the REST client. So either we have to document this in the manual or we have to come up with a common request / response object for both clients",design_debt,non-optimal_design,"Sat, 30 Sep 2017 05:29:19 +0000","Wed, 18 Oct 2017 06:54:13 +0000","Wed, 18 Oct 2017 06:54:13 +0000",1560294,"It might be a good idea, since not all features is supported in the high level rest client yet I already got it to work with the rest client, but then I would suggest we create a super class for ElasticsearchProducer and a ElasticsearchTransportProducer and ElasticsearchRestProducer. In the ElasticsearchEndpoint we decide which one we should instantiate based on the type client on the class path. This of course mean the client dependency is not included in the the camel-elasticsearch5 component. Regarding the camel-elasticsearch, it's not possible to use the Hight level REST client since it only support ELK from 5.6.0 and forward. One think to be aware of with the REST API that it is slightly different, which mean various request and response object is not called the same with the REST client. So either we have to document this in the manual or we have to come up with a common request / response object for both clients",-0.1164,-0.097,neutral
camel,11868,comment_2,"Can we not create a new component that use this new rest client only. And then the two older components can stay for older users, and eventually we can start marking them as deprecated because the java client is EOL from Elastic and the rest client is the future/way to go.",design_debt,non-optimal_design,"Sat, 30 Sep 2017 05:29:19 +0000","Wed, 18 Oct 2017 06:54:13 +0000","Wed, 18 Oct 2017 06:54:13 +0000",1560294,"Can we not create a new camel-elasticsearch5-rest component that use this new rest client only. And then the two older components can stay for older users, and eventually we can start marking them as deprecated because the java client is EOL from Elastic and the rest client is the future/way to go.",0.3125,0.3125,neutral
camel,12042,comment_0,"The old wiki system is deprecated, a new website and documentation is in the works. You can help with the new docs which are the adoc files in the source code you can find in the src/main/docs folder of the various Camel components.",documentation_debt,outdated_documentation,"Mon, 27 Nov 2017 11:45:17 +0000","Mon, 27 Nov 2017 12:21:12 +0000","Mon, 27 Nov 2017 12:09:36 +0000",1459,"The old wiki system is deprecated, a new website and documentation is in the works. You can help with the new docs which are the adoc files in the source code you can find in the src/main/docs folder of the various Camel components.",0.5,0.5,neutral
camel,12104,comment_5,I think we can extend cxf Continuation interface a bit to add an isTimeout method so that we know the timeout happen and can handle this situation accordingly outside CXF,design_debt,non-optimal_design,"Wed, 27 Dec 2017 18:31:14 +0000","Mon, 26 Mar 2018 15:56:24 +0000","Tue, 20 Mar 2018 01:27:00 +0000",7109746,I think we can extend cxf Continuation interface a bit to add an isTimeout method so that we know the timeout happen and can handle this situation accordingly outside CXF,0.2,0.2,neutral
camel,12104,comment_0,Would you be able to build an unit test of this sample code so we can take that and add to the tests of camel-cxf and work on a fix.,test_debt,lack_of_tests,"Wed, 27 Dec 2017 18:31:14 +0000","Mon, 26 Mar 2018 15:56:24 +0000","Tue, 20 Mar 2018 01:27:00 +0000",7109746,Would you be able to build an unit test of this sample code so we can take that and add to the tests of camel-cxf and work on a fix.,0.644,0.644,neutral
camel,12166,comment_3,"It end up calling some rollback code, that performs the disconnect. Its a bit more complicated as ftp extends file component. But as said you use an old version of Camel we dont support anymore.",code_debt,complex_code,"Fri, 19 Jan 2018 13:41:03 +0000","Mon, 29 Jan 2018 18:09:43 +0000","Fri, 19 Jan 2018 13:46:32 +0000",329,"It end up calling some rollback code, that performs the disconnect. Its a bit more complicated as ftp extends file component. But as said you use an old version of Camel we dont support anymore.",-0.06666666667,-0.06666666667,negative
camel,12414,comment_2,"Okay it works fine with an unit test that does not use pax-exam. Its some weird osgi stuff, it may not start/run properly via pax-exam inside the osgi/karaf container with the unit test.",code_debt,low_quality_code,"Wed, 28 Mar 2018 18:36:34 +0000","Wed, 22 Aug 2018 12:04:17 +0000","Wed, 22 Aug 2018 11:52:14 +0000",12676540,"Okay it works fine with an unit test that does not use pax-exam. Its some weird osgi stuff, it may not start/run properly via pax-exam inside the osgi/karaf container with the unit test.",0.102,0.102,neutral
camel,13111,comment_5,": No sir. I think I had a misspelling on the PR, so the bot didn't pick anything up. This ticket is done. Sorry for the confusion.",documentation_debt,low_quality_documentation,"Wed, 23 Jan 2019 10:18:52 +0000","Fri, 1 Mar 2019 13:25:05 +0000","Thu, 7 Feb 2019 17:45:36 +0000",1322804,"ancosen : No sir. I think I had a misspelling on the PR, so the bot didn't pick anything up. This ticket is done. Sorry for the confusion.",-0.25,-0.25,negative
camel,13214,comment_0,"It should not be moved, but be there both places. Then endpoint override component level, this is how its done in other components.",design_debt,non-optimal_design,"Mon, 18 Feb 2019 09:17:11 +0000","Thu, 7 Mar 2019 03:49:48 +0000","Thu, 7 Mar 2019 03:49:39 +0000",1449148,"It should not be moved, but be there both places. Then endpoint override component level, this is how its done in other components.",0.0,0.0,neutral
camel,13533,comment_0,Xml related tests are now ignored for composite batch api,test_debt,low_coverage,"Thu, 16 May 2019 07:16:23 +0000","Thu, 16 May 2019 09:19:05 +0000","Thu, 16 May 2019 09:19:05 +0000",7362,Xml related tests are now ignored for composite batch api,-0.4,-0.4,negative
hadoop,93,comment_0,"With such big input files the default logic should split things into dfs block-sized splits. Smaller splits should only be used if this would result in fewer than mapred.map.tasks splits. What value do you have for mapred.map.tasks in your mapred-default.xml? Let's make sure that is working before we add a new min.split.size feature. I don't oppose the feature, but it should be generating 356*30G/32M splits, not 356*30G/2K splits as you claim. That's still a lot of splits. If it is too many then we should add the feature you're adding. Note that, as a workaround, it is also easy to implement this w/o patching by defining an InputFormat that subclasses InputFormatBase and specifies a different minSplitSize. But making that a long is a good idea. So, in summary, can you please confirm that the actual number of splits that you object to is 356*30G/32M splits, not 356*30G/2K? Thanks.",design_debt,non-optimal_design,"Sat, 18 Mar 2006 03:59:07 +0000","Wed, 8 Jul 2009 16:51:41 +0000","Wed, 22 Mar 2006 01:38:19 +0000",337152,"With such big input files the default logic should split things into dfs block-sized splits. Smaller splits should only be used if this would result in fewer than mapred.map.tasks splits. What value do you have for mapred.map.tasks in your mapred-default.xml? Let's make sure that is working before we add a new min.split.size feature. I don't oppose the feature, but it should be generating 356*30G/32M splits, not 356*30G/2K splits as you claim. That's still a lot of splits. If it is too many then we should add the feature you're adding. Note that, as a workaround, it is also easy to implement this w/o patching by defining an InputFormat that subclasses InputFormatBase and specifies a different minSplitSize. But making that a long is a good idea. So, in summary, can you please confirm that the actual number of splits that you object to is 356*30G/32M splits, not 356*30G/2K? Thanks.",0.08340740741,0.08340740741,neutral
hadoop,93,comment_1,"From what I've seen, it is always 32M fragments, but that is still 300k input splits/maps, which is a lot. We'd like to be able to drop that by an order of magnitude. (I think in this case that the input splitter never finished, so we don't know.)",design_debt,non-optimal_design,"Sat, 18 Mar 2006 03:59:07 +0000","Wed, 8 Jul 2009 16:51:41 +0000","Wed, 22 Mar 2006 01:38:19 +0000",337152,"From what I've seen, it is always 32M fragments, but that is still 300k input splits/maps, which is a lot. We'd like to be able to drop that by an order of magnitude. (I think in this case that the input splitter never finished, so we don't know.)",0.175,0.175,neutral
hadoop,93,comment_2,"Doug, you are right. The number of splits we got was 356*30G/32M, but still too many.",design_debt,non-optimal_design,"Sat, 18 Mar 2006 03:59:07 +0000","Wed, 8 Jul 2009 16:51:41 +0000","Wed, 22 Mar 2006 01:38:19 +0000",337152,"Doug, you are right. The number of splits we got was 356*30G/32M, but still too many.",0.404,0.404,neutral
hadoop,95,comment_4,"Posting here a similar performance test, which is related to HADOOP-72 Just because these two tests have too much in common. TestDFSIO measures performance of the cluster for reads and writes.",code_debt,duplicated_code,"Sat, 18 Mar 2006 09:31:35 +0000","Wed, 8 Jul 2009 16:41:49 +0000","Wed, 18 Oct 2006 18:31:16 +0000",18521981,"Posting here a similar performance test, which is related to HADOOP-72 Just because these two tests have too much in common. TestDFSIO measures performance of the cluster for reads and writes.",-0.25,-0.25,neutral
hadoop,95,comment_0,"I posted a similar complaint in HADOOP-35, including a patch to make it possible to report on individual file health from the dfs -ls command. I agree that something needs to be added, if only to facilitate debugging of the various block-loss problems I've been seeing.",design_debt,non-optimal_design,"Sat, 18 Mar 2006 09:31:35 +0000","Wed, 8 Jul 2009 16:41:49 +0000","Wed, 18 Oct 2006 18:31:16 +0000",18521981,"I posted a similar complaint in HADOOP-35, including a patch to make it possible to report on individual file health from the dfs -ls command. I agree that something needs to be added, if only to facilitate debugging of the various block-loss problems I've been seeing.",-0.325,-0.325,neutral
hadoop,98,comment_0,"This patch consolidate all of the places where status was put in to or out of the taskTrackers status map, so that the counts of maps and reduces are maintained consistently. I also added the cluster status to the webapp so that you can get a quick view of how busy the cluster is.",design_debt,non-optimal_design,"Thu, 23 Mar 2006 16:13:39 +0000","Wed, 8 Jul 2009 16:51:42 +0000","Fri, 24 Mar 2006 04:08:21 +0000",42882,"This patch consolidate all of the places where status was put in to or out of the taskTrackers status map, so that the counts of maps and reduces are maintained consistently. I also added the cluster status to the webapp so that you can get a quick view of how busy the cluster is.",0.297,0.297,neutral
hadoop,289,comment_3,Please replace the getLocalizedMessage and implicit toString with calls to which includes both the message and the call stack. The call stack helps a lot in finding and debugging the problem.,code_debt,low_quality_code,"Thu, 8 Jun 2006 09:34:01 +0000","Wed, 8 Jul 2009 16:41:54 +0000","Sat, 10 Jun 2006 00:16:56 +0000",139375,"Please replace the getLocalizedMessage and implicit toString with calls to StringUtils.stringifyException, which includes both the message and the call stack. The call stack helps a lot in finding and debugging the problem.",0.1,0.06666666667,neutral
hadoop,481,comment_2,Instead of passing a long[] you should pass a struct that implements Writable. Probably TaskMetrics would be a good name for this.,code_debt,low_quality_code,"Thu, 24 Aug 2006 22:48:37 +0000","Wed, 8 Jul 2009 16:51:53 +0000","Tue, 18 Sep 2007 19:38:45 +0000",33684608,Instead of passing a long[] you should pass a struct that implements Writable. Probably TaskMetrics would be a good name for this.,0.388,0.388,neutral
hadoop,537,comment_0,"We shouldn't need a clean-X target for every compile-X target, since compile-X targets should place things in the build directory and the build directory should be removed by the single clean target. We should also try not to clutter the top-level build.xml. If a separate clean-X target is required for productive development in some subtree, then it should be placed in a separate build.xml or Makefile in that source subtree. It can then remove appropriate items from the top-level build directory. But, ideally, the top-level 'clean' target should be able to remove all generated items by simply removing the top-level build directory.",code_debt,low_quality_code,"Fri, 15 Sep 2006 00:43:20 +0000","Wed, 8 Jul 2009 17:05:59 +0000","Mon, 25 Sep 2006 23:08:59 +0000",944739,"We shouldn't need a clean-X target for every compile-X target, since compile-X targets should place things in the build directory and the build directory should be removed by the single clean target. We should also try not to clutter the top-level build.xml. If a separate clean-X target is required for productive development in some subtree, then it should be placed in a separate build.xml or Makefile in that source subtree. It can then remove appropriate items from the top-level build directory. But, ideally, the top-level 'clean' target should be able to remove all generated items by simply removing the top-level build directory.",0.1589285714,0.1589285714,neutral
hadoop,537,comment_1,"This is an orthogonal approach but I like it too. In this case we should not have any clean-X targets at all. Some more details on this particular issue. The real problem is that ""compile-libhdfs"" creates temporary files in the source directory. Sure they need to be cleaned, but they should not be created there in the first place. I think this have already been discussed, don't remember where.",code_debt,low_quality_code,"Fri, 15 Sep 2006 00:43:20 +0000","Wed, 8 Jul 2009 17:05:59 +0000","Mon, 25 Sep 2006 23:08:59 +0000",944739,"This is an orthogonal approach but I like it too. In this case we should not have any clean-X targets at all. Some more details on this particular issue. The real problem is that ""compile-libhdfs"" creates temporary files in the source directory. Sure they need to be cleaned, but they should not be created there in the first place. I think this have already been discussed, don't remember where.",-0.06666666667,-0.06666666667,neutral
hadoop,537,comment_2,Here's a patch which ensures that no temporary files are created in the src/c++/libhdfs; including .o/.so etc. They are all put in build/libhdfs. In fact I've ensured that even the doxygen generated docs go to build/libhdfs/docs and can be packaged aptly. Konstantin - can you please confirm that it works for you? thanks. I've kept the 'clean-libhdfs' target around in build.xml so that devs working on libhdfs can use it without needing to invoke the top-level 'clean' which nukes the 'build' directory.,code_debt,low_quality_code,"Fri, 15 Sep 2006 00:43:20 +0000","Wed, 8 Jul 2009 17:05:59 +0000","Mon, 25 Sep 2006 23:08:59 +0000",944739,Here's a patch which ensures that no temporary files are created in the src/c++/libhdfs; including .o/.so etc. They are all put in build/libhdfs. In fact I've ensured that even the doxygen generated docs go to build/libhdfs/docs and can be packaged aptly. Konstantin - can you please confirm that it works for you? thanks. I've kept the 'clean-libhdfs' target around in build.xml so that devs working on libhdfs can use it without needing to invoke the top-level 'clean' which nukes the 'build' directory.,0.1814814815,0.1814814815,neutral
hadoop,659,comment_3,"Name-node needs some form of the global map of blocks in order to be able to remove blocks from neededReplications e.g. during block reports. We should also use a parallel list view of blocks that lists them in the priority order. Since we plan to have only 2 priority levels, we can add first-priority items to the beginning of the list and low-priority to the end. That way when we iterate the list we will first choose higher priority blocks automatically. Add and remove to/from the list are both O(1), so the performance remains the same as we have now. Iterating through the list is linear, which is better than what we have now O(n log n). I agree the main purpose of the issue is to replicate blocks that are about to become extinct. But if we can improve map/reduce performance by treating replication of config and jar files similarly, why not do that?",code_debt,slow_algorithm,"Tue, 31 Oct 2006 20:29:31 +0000","Wed, 8 Jul 2009 16:42:06 +0000","Thu, 25 Jan 2007 22:59:13 +0000",7439382,"Name-node needs some form of the global map of blocks in order to be able to remove blocks from neededReplications e.g. during block reports. We should also use a parallel list view of blocks that lists them in the priority order. Since we plan to have only 2 priority levels, we can add first-priority items to the beginning of the list and low-priority to the end. That way when we iterate the list we will first choose higher priority blocks automatically. Add and remove to/from the list are both O(1), so the performance remains the same as we have now. Iterating through the list is linear, which is better than what we have now O(n log n). I agree the main purpose of the issue is to replicate blocks that are about to become extinct. But if we can improve map/reduce performance by treating replication of config and jar files similarly, why not do that?",0.0772,0.0772,neutral
hadoop,659,comment_4,"I am OK to priotize blocks that have less than 1/3 of their replicas in place. Then I would introduce 3 priority levels. The blocks that have only one replica have the highest priority, followed by blocks having less than 1/3 replicas, and then followed by the rest of the blocks. The data structures that suggested by Konstantin has the performance advantage. But I went through some code design and felt that the data structures and manipulations were quite complicated. It's not a simple solution. I am convinced that it is the right way to go. Besides, adding blocks to the begining or the end of the priority list is not an extensible solution when we have more than 2 priority levels.",design_debt,non-optimal_design,"Tue, 31 Oct 2006 20:29:31 +0000","Wed, 8 Jul 2009 16:42:06 +0000","Thu, 25 Jan 2007 22:59:13 +0000",7439382,"I am OK to priotize blocks that have less than 1/3 of their replicas in place. Then I would introduce 3 priority levels. The blocks that have only one replica have the highest priority, followed by blocks having less than 1/3 replicas, and then followed by the rest of the blocks. The data structures that suggested by Konstantin has the performance advantage. But I went through some code design and felt that the data structures and manipulations were quite complicated. It's not a simple solution. I am convinced that it is the right way to go. Besides, adding blocks to the begining or the end of the priority list is not an extensible solution when we have more than 2 priority levels.",0.09333333333,0.09333333333,neutral
hadoop,715,comment_0,Here is a simple fix... I've also renamed the 'hadoop.log.dir' variable in ant to 'test.log.dir' since it is used only in the 'test' target for consistency and to ensure people don't get confused.,code_debt,low_quality_code,"Tue, 14 Nov 2006 03:44:53 +0000","Fri, 1 Dec 2006 22:43:36 +0000","Tue, 14 Nov 2006 21:53:35 +0000",65322,Here is a simple fix... I've also renamed the 'hadoop.log.dir' variable in ant to 'test.log.dir' since it is used only in the 'test' target for consistency and to ensure people don't get confused.,0.06,0.06,neutral
hadoop,758,comment_3,"The exception in the bug is that last exception that that occurred. It masks the first exception that would be a better indicator of the problem. ReduceTask.java (around line 313)Looks like try { /* run reducer */ } finally { /* close some streams */ } The above trace and the one in HADOOP-757 both are in finally {} and mask the exception in try {}. I will submit a patch that prints the exception thrown in try {} if finally block throws one. While trying reproduce the above trace I managed to produce ""Bad File Descriptor"" exception in HADOOP-757. In summary, it looks like these failures are possible with low tmp spaces but we don't log the exceptions that were triggered initially.",code_debt,low_quality_code,"Wed, 29 Nov 2006 05:41:43 +0000","Wed, 8 Jul 2009 16:42:11 +0000","Thu, 25 Jan 2007 21:31:03 +0000",4981760,"The exception in the bug is that last exception that that occurred. It masks the first exception that would be a better indicator of the problem. ReduceTask.java (around line 313)Looks like try { /* run reducer */ } finally { /* close some streams */ } The above trace and the one in HADOOP-757 both are in finally {} and mask the exception in try {}. I will submit a patch that prints the exception thrown in try {} if finally block throws one. While trying reproduce the above trace I managed to produce ""Bad File Descriptor"" exception in HADOOP-757. In summary, it looks like these failures are possible with low tmp spaces but we don't log the exceptions that were triggered initially.",-0.1571428571,-0.1571428571,neutral
hadoop,758,comment_7,"... This is exactly what I had in my devel code. The method ('double code path') in the patch was Owen's preferred approach. If we don't need to do all the cleanups (may be because this will actually close the Java process and open a new one for the new task), then we don't need finally at all.",code_debt,low_quality_code,"Wed, 29 Nov 2006 05:41:43 +0000","Wed, 8 Jul 2009 16:42:11 +0000","Thu, 25 Jan 2007 21:31:03 +0000",4981760,"> Wouldn't something like the following work? > IOException ioe = null; > try { > ... body ... ... This is exactly what I had in my devel code. The method ('double code path') in the patch was Owen's preferred approach. If we don't need to do all the cleanups (may be because this will actually close the Java process and open a new one for the new task), then we don't need finally at all.",0.1666666667,-0.025,neutral
hadoop,758,comment_5,"Rather than ignore all those exceptions, wouldn't it be better to at least log them? Also, I'm not sure we need to proceed with all cleanups if any fail. And we shouldn't replicate the cleanup code. The problem is that exceptions in cleanups are masking the exception thrown in the body. Wouldn't something like the following work? IOException ioe = null; try { ... body ... } catch (IOException e) { ioe = e; throw e; } finally { try { ... cleanups... } catch (IOException e) { if (ioe != null) LOG.warn(e) else throw e; } if (ioe != null) throw ioe; }",design_debt,non-optimal_design,"Wed, 29 Nov 2006 05:41:43 +0000","Wed, 8 Jul 2009 16:42:11 +0000","Thu, 25 Jan 2007 21:31:03 +0000",4981760,"Rather than ignore all those exceptions, wouldn't it be better to at least log them? Also, I'm not sure we need to proceed with all cleanups if any fail. And we shouldn't replicate the cleanup code. The problem is that exceptions in cleanups are masking the exception thrown in the body. Wouldn't something like the following work? IOException ioe = null; try { ... body ... } catch (IOException e) { ioe = e; throw e; } finally { try { ... cleanups... } catch (IOException e) { if (ioe != null) LOG.warn(e) else throw e; } if (ioe != null) throw ioe; }",-0.15,-0.15,neutral
hadoop,758,comment_8,"I think we should make a good-faith effort to cleanup: user-errors can be common, and this also runs under LocalRunner, not always as a separate process. If there are errors in the cleanups we should log these as warnings, since they should not occur. Which raises the related question: why did the cleanup fail? Closing an open file shouldn't throw an exception. That looks like a bug in DFSClient, no?",design_debt,non-optimal_design,"Wed, 29 Nov 2006 05:41:43 +0000","Wed, 8 Jul 2009 16:42:11 +0000","Thu, 25 Jan 2007 21:31:03 +0000",4981760,"I think we should make a good-faith effort to cleanup: user-errors can be common, and this also runs under LocalRunner, not always as a separate process. If there are errors in the cleanups we should log these as warnings, since they should not occur. Which raises the related question: why did the cleanup fail? Closing an open file shouldn't throw an exception. That looks like a bug in DFSClient, no?",-0.1416,-0.1416,neutral
hadoop,930,comment_1,Second patch with the following changes: * Send Content-MD5 header to perform message integrity checks for writes. * Fix warnings from Jets3t to do with not closing streams. * Change property names to be independent of existing S3FileSystem: and * Findbugs and formatting fixes.,code_debt,low_quality_code,"Thu, 25 Jan 2007 14:48:52 +0000","Thu, 2 May 2013 02:29:14 +0000","Fri, 6 Jun 2008 21:07:05 +0000",43049893,Second patch with the following changes: Send Content-MD5 header to perform message integrity checks for writes. Fix warnings from Jets3t to do with not closing streams. Change property names to be independent of existing S3FileSystem: fs.s3n.awsAccessKeyId and fs.s3n.awsSecretAccessKey. Findbugs and formatting fixes.,-0.2333333333,-0.0875,neutral
hadoop,930,comment_3,Fixed release audit warnings and a few formatting warnings from checkstyle.,code_debt,low_quality_code,"Thu, 25 Jan 2007 14:48:52 +0000","Thu, 2 May 2013 02:29:14 +0000","Fri, 6 Jun 2008 21:07:05 +0000",43049893,Fixed release audit warnings and a few formatting warnings from checkstyle.,-0.6,-0.6,neutral
hadoop,930,comment_8,"Thanks for the review Chris. It's to do with efficiency of listing directories. If you use mime type then you can't tell the difference between files and directories when listing bucket keys. So you have to query each key in a directory which can be prohibitively slow. But if you use the _$folder$ suffix convention (which S3Fox uses too BTW) you can easily distinguish files and directories. The code should be doing this. I agree that it's useful - in fact, the other s3 filesystem needs updating to do this too. Thanks for the tip. The code does detect this condition, but it might be nice to try to workaround as you say (perhaps emitting a warning). Have you done this elsewhere?",design_debt,non-optimal_design,"Thu, 25 Jan 2007 14:48:52 +0000","Thu, 2 May 2013 02:29:14 +0000","Fri, 6 Jun 2008 21:07:05 +0000",43049893,"Thanks for the review Chris. Any reason you didn't use the mime type to denote directory files (as jets3t does)? It's to do with efficiency of listing directories. If you use mime type then you can't tell the difference between files and directories when listing bucket keys. So you have to query each key in a directory which can be prohibitively slow. But if you use the _$folder$ suffix convention (which S3Fox uses too BTW) you can easily distinguish files and directories. I believe MD5 checksum should be set on s3 put (via header), and verified on s3 get. The code should be doing this. I agree that it's useful - in fact, the other s3 filesystem needs updating to do this too. Sometimes 'legacy' buckets have underscores, might consider trying to survive them. Thanks for the tip. The code does detect this condition, but it might be nice to try to workaround as you say (perhaps emitting a warning). Have you done this elsewhere?",0.17625,0.1355769231,neutral
hadoop,930,comment_0,"Here's a patch for a native S3 filesystem. * Writes are supported. * The scheme is s3n making it completely independent of the existing block-based S3 filesystem. It might be possible to make a general (read-only) S3 filesystem that can read both types, but I haven't attempted that here (it can go in another Jira if needed). * Empty directories are written using the naming convention of appending ""_$folder$"" to the key. This is the approach taken by S3Fox, and - crucially for efficiency - it makes it possible to tell if a key represents a file or a directory from a list bucket operation. * There's a new unit test for the contract of FileSystem to ensure that different implementations are consistent. Both S3 filesystems and HDFS are tested using this test. It would be good to add other filesystems later. * Renames are not supported as S3 doesn't support them natively (yet). It would be possible to support renames by getting the client to copy the data out of S3 then back again. * The Jets3t library has been upgraded to the latest version (0.6.0)",requirement_debt,requirement_partially_implemented,"Thu, 25 Jan 2007 14:48:52 +0000","Thu, 2 May 2013 02:29:14 +0000","Fri, 6 Jun 2008 21:07:05 +0000",43049893,"Here's a patch for a native S3 filesystem. Writes are supported. The scheme is s3n making it completely independent of the existing block-based S3 filesystem. It might be possible to make a general (read-only) S3 filesystem that can read both types, but I haven't attempted that here (it can go in another Jira if needed). Empty directories are written using the naming convention of appending ""_$folder$"" to the key. This is the approach taken by S3Fox, and - crucially for efficiency - it makes it possible to tell if a key represents a file or a directory from a list bucket operation. There's a new unit test (FileSystemContractBaseTest) for the contract of FileSystem to ensure that different implementations are consistent. Both S3 filesystems and HDFS are tested using this test. It would be good to add other filesystems later. Renames are not supported as S3 doesn't support them natively (yet). It would be possible to support renames by getting the client to copy the data out of S3 then back again. The Jets3t library has been upgraded to the latest version (0.6.0)",0.1574166667,0.1574166667,neutral
hadoop,1034,comment_1,"More generally, in a bunch of hadoop code, only IOException are caught. (eg: same issue on I think a little cleanup is needed, especially while catching an exception in a run() of a Thread...",code_debt,low_quality_code,"Fri, 23 Feb 2007 13:45:50 +0000","Wed, 8 Jul 2009 16:42:18 +0000","Fri, 23 Feb 2007 20:27:58 +0000",24128,"More generally, in a bunch of hadoop code, only IOException are caught. (eg: same issue on DataNode.DataXceiveServer.run()) I think a little cleanup is needed, especially while catching an exception in a run() of a Thread...",0.25,0.125,negative
hadoop,1034,comment_2,"Agreed. All threads should catch Throwable at the top level and log them. *Sigh* I thought we had gone through all of the threads and fixed that problem. If you see any more, please file them as bugs.",code_debt,low_quality_code,"Fri, 23 Feb 2007 13:45:50 +0000","Wed, 8 Jul 2009 16:42:18 +0000","Fri, 23 Feb 2007 20:27:58 +0000",24128,"Agreed. All threads should catch Throwable at the top level and log them. Sigh I thought we had gone through all of the threads and fixed that problem. If you see any more, please file them as bugs.",0.1,0.1,negative
hadoop,1072,comment_1,"This is an incompatible change to a public API. So, if we choose to make it, we must deprecate the existing name for at least one release prior to removing it. Personally, I'm not convinced the naming hygiene is worth the hassle in this case, but I would not veto it.",code_debt,low_quality_code,"Tue, 6 Mar 2007 23:18:03 +0000","Thu, 11 Aug 2011 18:57:59 +0000","Thu, 11 Aug 2011 18:57:59 +0000",139865996,"This is an incompatible change to a public API. So, if we choose to make it, we must deprecate the existing name for at least one release prior to removing it. Personally, I'm not convinced the naming hygiene is worth the hassle in this case, but I would not veto it.",-0.02011111111,-0.02011111111,negative
hadoop,1147,comment_4,"+1 I was originally liked them, but they mostly end up creating noise. For example, I wrote the sort and word count examples that many people use for the start of their first map/reduce program and my name is in the @author. Someone didn't remove the comments and then handed off their program to someone else and it lead to confusion.",documentation_debt,low_quality_documentation,"Thu, 22 Mar 2007 20:14:04 +0000","Mon, 20 Aug 2007 18:11:50 +0000","Wed, 20 Jun 2007 22:45:50 +0000",7785106,"+1 I was originally liked them, but they mostly end up creating noise. For example, I wrote the sort and word count examples that many people use for the start of their first map/reduce program and my name is in the @author. Someone didn't remove the comments and then handed off their program to someone else and it lead to confusion.",0.2916666667,0.2916666667,negative
hadoop,1192,comment_0,The checksum patch did not overwrite the method getContentLength in This leads to the use of the default slower version of getContentLength defined in FileSystem. This patch fixed the du problem. It also fixed the dus problem by declaring the totalSize to be long.,code_debt,slow_algorithm,"Mon, 2 Apr 2007 20:42:22 +0000","Wed, 8 Jul 2009 16:42:24 +0000","Tue, 3 Apr 2007 18:17:31 +0000",77709,The checksum patch did not overwrite the method getContentLength in DistributedFileSystem. This leads to the use of the default slower version of getContentLength defined in FileSystem. This patch fixed the du problem. It also fixed the dus problem by declaring the totalSize to be long.,-0.2666666667,-0.2,neutral
hadoop,1192,comment_1,"+1 Code looks fine. While reviewing this I noticed that FsShell.dus() seem to be doing one level recursion into the paths it wants to du. I would have thought we either need not recurse at all or to recurse recurse fully. This patch does not change this logic. Hairong is taking a look at this. If there is some issue here (either correctness or just code efficiency), that could be a different jira.",code_debt,complex_code,"Mon, 2 Apr 2007 20:42:22 +0000","Wed, 8 Jul 2009 16:42:24 +0000","Tue, 3 Apr 2007 18:17:31 +0000",77709,"+1 Code looks fine. While reviewing this I noticed that FsShell.dus() seem to be doing one level recursion into the paths it wants to du. I would have thought we either need not recurse at all or to recurse recurse fully. This patch does not change this logic. Hairong is taking a look at this. If there is some issue here (either correctness or just code efficiency), that could be a different jira.",0.2804285714,0.2804285714,neutral
hadoop,1245,comment_8,"+1 This looks reasonable to me. Note that this is potentially incompatible, since previously folks could set the number of tasks per node globally at the jobtracker, now it is determined by the configuration of the tasktracker nodes. So we should probably either add a compatibility note (i.e., move this to the INCOMPATIBLE section of CHANGES.txt) or perhaps have a configuration parameter that enables the old behavior. I think a compatibility note is probably sufficient. Thoughts?",documentation_debt,outdated_documentation,"Wed, 11 Apr 2007 00:20:03 +0000","Wed, 8 Jul 2009 16:52:21 +0000","Wed, 24 Oct 2007 22:52:12 +0000",17015529,"+1 This looks reasonable to me. Note that this is potentially incompatible, since previously folks could set the number of tasks per node globally at the jobtracker, now it is determined by the configuration of the tasktracker nodes. So we should probably either add a compatibility note (i.e., move this to the INCOMPATIBLE section of CHANGES.txt) or perhaps have a configuration parameter that enables the old behavior. I think a compatibility note is probably sufficient. Thoughts?",0.2611666667,0.2611666667,neutral
hadoop,1245,comment_1,"I've also run into this, and came up with a patch that involved: - adding a 'maxTasks' value to the TaskTrackerStatus (set by the local - modifying JobTracker to track totalTaskCapacity instead of per-node maxTasks, and to use the particular task tracker's maxTasks value when deciding whether to assign it another task If this seems like a reasonable approach, I can do more testing and provide a patch.",test_debt,lack_of_tests,"Wed, 11 Apr 2007 00:20:03 +0000","Wed, 8 Jul 2009 16:52:21 +0000","Wed, 24 Oct 2007 22:52:12 +0000",17015529,"I've also run into this, and came up with a patch that involved: adding a 'maxTasks' value to the TaskTrackerStatus (set by the local mapred.tasktracker.tasks.maximum) modifying JobTracker to track totalTaskCapacity instead of per-node maxTasks, and to use the particular task tracker's maxTasks value when deciding whether to assign it another task If this seems like a reasonable approach, I can do more testing and provide a patch.",0.521,0.1389166667,neutral
hadoop,1251,comment_1,"This method is not specific to TaskTracker, i.e., it should work fine with LocalRunner too, right? So there ought to be a better place to put it. JobConf? JobClient?",architecture_debt,violation_of_modularity,"Thu, 12 Apr 2007 05:54:52 +0000","Thu, 2 May 2013 02:29:04 +0000","Mon, 16 Apr 2007 22:57:10 +0000",406938,"This method is not specific to TaskTracker, i.e., it should work fine with LocalRunner too, right? So there ought to be a better place to put it. JobConf? JobClient?",0.25225,0.25225,neutral
hadoop,1254,comment_1,"TestCheckpoint has one case where it creates a MiniDFSCluster but doesn't wait for it to be active. I'll file a patch for this. I wonder if this started showing up due to some new speed up or slow down in starting a NameNode and/or a DataNode, perhaps introduced by HADOOP-971 or HADOOP-1189...",code_debt,slow_algorithm,"Thu, 12 Apr 2007 17:50:45 +0000","Wed, 8 Jul 2009 16:42:25 +0000","Fri, 13 Apr 2007 17:43:25 +0000",85960,"TestCheckpoint has one case where it creates a MiniDFSCluster but doesn't wait for it to be active. I'll file a patch for this. I wonder if this started showing up due to some new speed up or slow down in starting a NameNode and/or a DataNode, perhaps introduced by HADOOP-971 or HADOOP-1189...",0.0235,0.0235,negative
hadoop,1367,comment_2,The distFrom variable has been made a ThreadLocal. This should get rid of the findbugs warnings,code_debt,multi-thread_correctness,"Mon, 14 May 2007 22:36:04 +0000","Mon, 20 Aug 2007 18:11:57 +0000","Wed, 27 Jun 2007 18:12:46 +0000",3785802,The distFrom variable has been made a ThreadLocal. This should get rid of the findbugs warnings,-0.3,-0.3,neutral
hadoop,1367,comment_0,This is actually protected by the FSNamesystem global lock. Have to address this Deferriissue when we go to fine-grain-locking model. Deferring to next release.,defect_debt,uncorrected_known_defects,"Mon, 14 May 2007 22:36:04 +0000","Mon, 20 Aug 2007 18:11:57 +0000","Wed, 27 Jun 2007 18:12:46 +0000",3785802,This is actually protected by the FSNamesystem global lock. Have to address this Deferriissue when we go to fine-grain-locking model. Deferring to next release.,0.15,0.15,neutral
hadoop,1367,comment_1,I do not think it is a critical bug. Deferring it to 0.14.,defect_debt,uncorrected_known_defects,"Mon, 14 May 2007 22:36:04 +0000","Mon, 20 Aug 2007 18:11:57 +0000","Wed, 27 Jun 2007 18:12:46 +0000",3785802,I do not think it is a critical bug. Deferring it to 0.14.,-0.1,-0.1,neutral
hadoop,1453,comment_4,Both your patches are for whereas your comment says that there are *two* places where a redundant exists() call is made: and ChecksumFileSystem. Can you pl clarify?,code_debt,low_quality_code,"Fri, 1 Jun 2007 23:24:43 +0000","Wed, 8 Jul 2009 16:42:29 +0000","Wed, 20 Jun 2007 21:30:25 +0000",1634742,Both your patches are for DistributedFileSystem whereas your comment says that there are two places where a redundant exists() call is made: DistributedFileSystem and ChecksumFileSystem. Can you pl clarify?,0.0,0.0,neutral
hadoop,1459,comment_17,In the medium term we should move all of Hadoop to use IP addresses instead of hostnames. I've filed the relevant bug in HADOOP-1487.,code_debt,low_quality_code,"Mon, 4 Jun 2007 10:09:30 +0000","Wed, 8 Jul 2009 16:41:57 +0000","Mon, 18 Jun 2007 20:39:50 +0000",1247420,In the medium term we should move all of Hadoop to use IP addresses instead of hostnames. I've filed the relevant bug in HADOOP-1487.,0.0,0.0,neutral
hadoop,1459,comment_3,"Could you please remove redundant import if you change DatanodeInfo. I think this is the right fix for the time being, although I'm not happy that # we should keep 2 different identifications for the nodes and that # we have different ways of node identification in different components of hadoop. My proposition would be to return back to hostnames instead of ip addresses. But this of course belongs to a different issue.",code_debt,dead_code,"Mon, 4 Jun 2007 10:09:30 +0000","Wed, 8 Jul 2009 16:41:57 +0000","Mon, 18 Jun 2007 20:39:50 +0000",1247420,"Could you please remove redundant import org.apache.hadoop.io.UTF8 if you change DatanodeInfo. I think this is the right fix for the time being, although I'm not happy that we should keep 2 different identifications for the nodes and that we have different ways of node identification in different components of hadoop. My proposition would be to return back to hostnames instead of ip addresses. But this of course belongs to a different issue.",-0.01025,-0.005125,neutral
hadoop,1459,comment_5,"Note that this increases serialization cost for any DatanodeInfo tranfer, which is pretty much most RPC. This will needs a protocol version change since this won't work with prev clients/datanodes.",design_debt,non-optimal_design,"Mon, 4 Jun 2007 10:09:30 +0000","Wed, 8 Jul 2009 16:41:57 +0000","Mon, 18 Jun 2007 20:39:50 +0000",1247420,"Note that this increases serialization cost for any DatanodeInfo tranfer, which is pretty much most RPC. This will needs a protocol version change since this won't work with prev clients/datanodes.",-0.2,-0.2,neutral
hadoop,1664,comment_0,"I am writing an admin guide for upgrading to Hadoop-0.14. will post it in couple of days. If you have any logs, please add them here. Upgrade and rollback procedure is same as before.",documentation_debt,low_quality_documentation,"Mon, 30 Jul 2007 14:54:35 +0000","Wed, 8 Jul 2009 16:42:33 +0000","Tue, 23 Oct 2007 20:58:27 +0000",7365832,"I am writing an admin guide for upgrading to Hadoop-0.14. will post it in couple of days. If you have any logs, please add them here. Upgrade and rollback procedure is same as before.",0.145,0.145,neutral
hadoop,1773,comment_6,"I don't see any error on fedora_core_x64 jdk_1.5 ant_1.7 Quoting a code snippet from build.xml <touch 2:00 pm"" <fileset dir=""${conf.dir}"" <fileset </touch What is the significance of ""01/25/1971 2:00 pm"" .. if the idea is to stamp the files with an older date.. then a better thing to do is <touch millis=""0"" <fileset dir=""${conf.dir}"" <fileset </touch This may not fix the issue.. its just a suggestion to make the build file easier to understand",design_debt,non-optimal_design,"Thu, 23 Aug 2007 22:01:21 +0000","Thu, 12 May 2016 18:22:44 +0000","Sat, 27 Apr 2013 00:33:01 +0000",179116300,"I don't see any error on fedora_core_x64 jdk_1.5 ant_1.7 Quoting a code snippet from build.xml <touch datetime=""01/25/1971 2:00 pm""> <fileset dir=""${conf.dir}"" includes=""*/.template""/> <fileset dir=""${contrib.dir}"" includes=""*/.template""/> </touch> What is the significance of ""01/25/1971 2:00 pm"" .. if the idea is to stamp the files with an older date.. then a better thing to do is <touch millis=""0""> <fileset dir=""${conf.dir}"" includes=""*/.template""/> <fileset dir=""${contrib.dir}"" includes=""*/.template""/> </touch> This may not fix the issue.. its just a suggestion to make the build file easier to understand",0.1875,0.075,neutral
hadoop,1926,comment_4,Fixed the javadoc oversight.,documentation_debt,low_quality_documentation,"Thu, 20 Sep 2007 11:51:41 +0000","Wed, 8 Jul 2009 16:52:26 +0000","Tue, 2 Oct 2007 22:16:48 +0000",1074307,Fixed the javadoc oversight.,-0.2,-0.2,neutral
hadoop,1961,comment_2,There are some more inconsistencies in copyToLocal() : For example : I will fix this as well. There is also an extra isDirectory() that is not required.,code_debt,low_quality_code,"Thu, 27 Sep 2007 23:33:00 +0000","Wed, 8 Jul 2009 16:42:11 +0000","Fri, 5 Oct 2007 00:09:01 +0000",606961,There are some more inconsistencies in copyToLocal() : For example : I will fix this as well. There is also an extra isDirectory() that is not required.,0.3155,0.3155,negative
hadoop,1961,comment_3,"There are at least three versions of copyToLocal : one in FsShell, one in ChecksumFileSystem, and FileUtil. All of these implement same logic and recursion.. but are slightly different. FsShell and ChecksumFS versions in turn invoke FileUtil version. We should remove FsShell and ChecksumFS, at least in 0.15 or 0.16.",code_debt,duplicated_code,"Thu, 27 Sep 2007 23:33:00 +0000","Wed, 8 Jul 2009 16:42:11 +0000","Fri, 5 Oct 2007 00:09:01 +0000",606961,"There are at least three versions of copyToLocal : one in FsShell, one in ChecksumFileSystem, and FileUtil. All of these implement same logic and recursion.. but are slightly different. FsShell and ChecksumFS versions in turn invoke FileUtil version. We should remove FsShell and ChecksumFS, at least in 0.15 or 0.16.",0.0,0.0,negative
hadoop,1961,comment_8,"+1 Patch looks good. Below are some minor thoughts. - I totally agree that there are redundant copyToLocal methods. See also HADOOP-1544. - In the case of rename failing, we know that the tmp file is good but cannot be renamed to dst. User can easily rename the tmp file. For the other exception cases, user don't know what to do to fix the problem. If we remove ""deleteOnExit"" flag, then we cannot easily tell whether the tmp file is perfect or not. - Should we not use deprecated API anymore? e.g.",code_debt,low_quality_code,"Thu, 27 Sep 2007 23:33:00 +0000","Wed, 8 Jul 2009 16:42:11 +0000","Fri, 5 Oct 2007 00:09:01 +0000",606961,"+1 Patch looks good. Below are some minor thoughts. I totally agree that there are redundant copyToLocal methods. See also HADOOP-1544. In the case of rename failing, we know that the tmp file is good but cannot be renamed to dst. User can easily rename the tmp file. For the other exception cases, user don't know what to do to fix the problem. If we remove ""deleteOnExit"" flag, then we cannot easily tell whether the tmp file is perfect or not. Should we not use deprecated API anymore? e.g. srcFS.isDirectory(src), srcFS.listPaths(src)",0.152,0.08866666667,negative
hadoop,1961,comment_11,I was dealing with multiple patches and totally missed running the unit tests for this. TestDFSShell faile because of the following change : There are no change the fix. Only TestDFSShell.java is changed. Also updated the patch to copy multiple directories.,test_debt,lack_of_tests,"Thu, 27 Sep 2007 23:33:00 +0000","Wed, 8 Jul 2009 16:42:11 +0000","Fri, 5 Oct 2007 00:09:01 +0000",606961,I was dealing with multiple patches and totally missed running the unit tests for this. TestDFSShell faile because of the following change : bin/hadoop fs -get dir1 localdir # and localdir exists 0.13 and 0.14.1 copy contents of dir1 into localdir 0.14.2 copies dir1 into localdir There are no change the fix. Only TestDFSShell.java is changed. Also updated the patch to copy multiple directories.,-0.08,-0.08,negative
hadoop,2148,comment_2,This patch optimizes and so that they perform the data-node blockMap lookup only once. The patch is pretty straightforward. I don't think we should benchmark this.,code_debt,slow_algorithm,"Sat, 3 Nov 2007 00:12:52 +0000","Wed, 8 Jul 2009 16:42:46 +0000","Wed, 19 Mar 2008 22:48:25 +0000",11918133,This patch optimizes FSDataset.getBlockFile() and FSDataset.getLength() so that they perform the data-node blockMap lookup only once. The patch is pretty straightforward. I don't think we should benchmark this.,0.06666666667,0.04,negative
hadoop,2148,comment_4,This fixes findBugs warnings. I could not reproduce test timeout in This test has a lot of test cases. My suspicion is that if Hudson runs slow it could run out of time on this.,code_debt,low_quality_code,"Sat, 3 Nov 2007 00:12:52 +0000","Wed, 8 Jul 2009 16:42:46 +0000","Wed, 19 Mar 2008 22:48:25 +0000",11918133,This fixes findBugs warnings. I could not reproduce test timeout in TestDFSStorageStateRecovery. This test has a lot of test cases. My suspicion is that if Hudson runs slow it could run out of time on this.,-0.2,-0.15,negative
hadoop,2148,comment_6,+1 This patch looks good. It removes a duplicate block map look up.,code_debt,duplicated_code,"Sat, 3 Nov 2007 00:12:52 +0000","Wed, 8 Jul 2009 16:42:46 +0000","Wed, 19 Mar 2008 22:48:25 +0000",11918133,+1 This patch looks good. It removes a duplicate block map look up.,0.288,0.288,negative
hadoop,2148,comment_7,I verified both test failures. They are not related to the patch imo. In the first case took too long. It finished only 47 cases out of 71 in 13 minutes when the junit framework terminated it. In the second case the cluster fell into a infinite loop trying to replicate a block. Filed HADOOP-3050 to investigate it.,test_debt,expensive_tests,"Sat, 3 Nov 2007 00:12:52 +0000","Wed, 8 Jul 2009 16:42:46 +0000","Wed, 19 Mar 2008 22:48:25 +0000",11918133,I verified both test failures. They are not related to the patch imo. In the first case TestDFSStorageStateRecovery took too long. It finished only 47 cases out of 71 in 13 minutes when the junit framework terminated it. In the second case the cluster fell into a infinite loop trying to replicate a block. Filed HADOOP-3050 to investigate it.,-0.05833333333,-0.05833333333,negative
hadoop,2181,comment_9,"Some comments: 1) The change in JobInProgress to do with wasRunning is problematic. In some cases, you might end up logging the split info more than once. 2) The method doesn't fit well in the StringUtils class. OTOH you could define it as a private method in TaskInProgress from where you call it.",design_debt,non-optimal_design,"Fri, 9 Nov 2007 18:27:51 +0000","Wed, 8 Jul 2009 16:52:31 +0000","Tue, 6 May 2008 11:36:27 +0000",15440916,"Some comments: 1) The change in JobInProgress to do with wasRunning is problematic. In some cases, you might end up logging the split info more than once. 2) The StringUtils.nodetoString method doesn't fit well in the StringUtils class. OTOH you could define it as a private method in TaskInProgress from where you call it.",-0.15175,-0.1214,negative
hadoop,2205,comment_0,I'd also like to use this jira to fix 2 minor typos in the website introduced by HADOOP-1917.,documentation_debt,low_quality_documentation,"Wed, 14 Nov 2007 16:06:37 +0000","Wed, 28 Nov 2007 22:32:47 +0000","Thu, 15 Nov 2007 09:09:40 +0000",61383,I'd also like to use this jira to fix 2 minor typos in the website introduced by HADOOP-1917.,0.0,0.0,neutral
hadoop,2205,comment_1,"Attached patch fixes some typos and broken links introduced by HADOOP-1917. I'll also ensure that website is updated, here is what my workspace looks like, after this patch:",documentation_debt,low_quality_documentation,"Wed, 14 Nov 2007 16:06:37 +0000","Wed, 28 Nov 2007 22:32:47 +0000","Thu, 15 Nov 2007 09:09:40 +0000",61383,"Attached patch fixes some typos and broken links introduced by HADOOP-1917. I'll also ensure that website is updated, here is what my workspace looks like, after this patch:",0.0,0.0,neutral
hadoop,2208,comment_10,"Some comments: You don't have to call setSendCounters in JobInProgress.java, and, you probably should rename the APIs getSendCounters and setSendCounters to something more intuitive.",code_debt,low_quality_code,"Thu, 15 Nov 2007 04:47:40 +0000","Wed, 8 Jul 2009 16:52:35 +0000","Thu, 3 Jan 2008 08:16:17 +0000",4246117,"Some comments: You don't have to call setSendCounters in JobInProgress.java, and, you probably should rename the APIs getSendCounters and setSendCounters to something more intuitive.",0.0,0.0,neutral
hadoop,2208,comment_12,"I'm pretty worried about the approach of this patch. It takes it from always sending the current values for the counters to just sending the ones that changed. That doesn't seem like an optimization that is likely to be important. Have you run large jobs that show this is important? My concern is that sending the deltas makes the system very vulnerable to losing or duplicating a message. My preference would be to have a boolean in the TaskStatus whether it should be sending the counters or not, but always send the current values of all counters. I'd also recommend against the current sendCounters and doSendCounters. I think your original names were better: Maybe they should be something like:",code_debt,low_quality_code,"Thu, 15 Nov 2007 04:47:40 +0000","Wed, 8 Jul 2009 16:52:35 +0000","Thu, 3 Jan 2008 08:16:17 +0000",4246117,"I'm pretty worried about the approach of this patch. It takes it from always sending the current values for the counters to just sending the ones that changed. That doesn't seem like an optimization that is likely to be important. Have you run large jobs that show this is important? My concern is that sending the deltas makes the system very vulnerable to losing or duplicating a message. My preference would be to have a boolean in the TaskStatus whether it should be sending the counters or not, but always send the current values of all counters. I'd also recommend against the current sendCounters and doSendCounters. I think your original names were better: {get,set}SendCounters. Maybe they should be something like: {get,set} IncludeCounters...",-0.01175,-0.01044444444,neutral
hadoop,2208,comment_8,"I see a potential bug with this patch: the TaskTracker _caches_ counters sent by the child-task till it's sent out to the JobTracker via the heartbeat. Hence, we need to *merge* the ones received from the child in not just over-write them as-is today. Minor nit: the merge of the counters probably belongs to rather than ...",code_debt,low_quality_code,"Thu, 15 Nov 2007 04:47:40 +0000","Wed, 8 Jul 2009 16:52:35 +0000","Thu, 3 Jan 2008 08:16:17 +0000",4246117,"I see a potential bug with this patch: the TaskTracker caches counters sent by the child-task till it's sent out to the JobTracker via the heartbeat. Hence, we need to merge the ones received from the child in TaskStatus.statusUpdate, not just over-write them as-is today. Minor nit: the merge of the counters probably belongs to TaskInProgress.recomputeProgress rather than TaskInProgress.updateStatus ...",0.0,0.0,neutral
hadoop,2402,comment_0,"Inserted a 64k to Effects compression similar to block, lzo compressed SequenceFile (~20%). For comparison, lzop (command line compression utility backed by lzo lib) uses 256k blocks (and a different file format) and compresses the same 100M sample by 60%. As an aside, with this applied, Zip compressed text files are written approximately 10% faster; uncompressed text about 4% faster.",code_debt,slow_algorithm,"Tue, 11 Dec 2007 02:03:47 +0000","Thu, 2 May 2013 02:29:11 +0000","Thu, 24 Jan 2008 23:23:49 +0000",3878402,"Inserted a 64k BufferedOutputStream to TextOutputFormat::getRecordWriter. Effects compression similar to block, lzo compressed SequenceFile (~20%). For comparison, lzop (command line compression utility backed by lzo lib) uses 256k blocks (and a different file format) and compresses the same 100M sample by 60%. As an aside, with this applied, Zip compressed text files are written approximately 10% faster; uncompressed text about 4% faster.",-0.06666666667,-0.05,neutral
hadoop,2402,comment_10,"Removed most whitespace changes, except those reformatting sections with lines greater than 80 chars",code_debt,low_quality_code,"Tue, 11 Dec 2007 02:03:47 +0000","Thu, 2 May 2013 02:29:11 +0000","Thu, 24 Jan 2008 23:23:49 +0000",3878402,"Removed most whitespace changes, except those reformatting sections with lines greater than 80 chars",0.5,0.5,neutral
hadoop,2402,comment_2,"I think this is probably the right approach, to not require the codecs themselves to return buffered streams. But the size of the buffer should be rather than a fixed 64k, no?",code_debt,low_quality_code,"Tue, 11 Dec 2007 02:03:47 +0000","Thu, 2 May 2013 02:29:11 +0000","Thu, 24 Jan 2008 23:23:49 +0000",3878402,"I think this is probably the right approach, to not require the codecs themselves to return buffered streams. But the size of the buffer should be io.file.buffer.size, rather than a fixed 64k, no?",0.2635,0.1054,neutral
hadoop,2402,comment_4,"We can't compress multiple buffers together with the lzo codec? It only compresses buffer-at-a-time? If so, then it should do the buffering & set the buffer size, since this is an lzo-specific issue. I don't think it's a bug for a codec to return a buffered stream if a particular buffer size is required to get good performance from that codec. If it's impossible to get lzo to compress data across buffers, and 64k or larger is required to get good compression, then it should mandate that buffer size, perhaps adding a new configuration parameter. Separately, we should consider whether to (a) unilaterally add an io.file.buffer.size buffer in TextOutputFormat, since it helps other codecs, or (b) assume that all codecs return appropriately buffered streams, and add a buffer in the Zip codec if it improves performance. If a io.file.buffer.size buffer=4k gives somewhat improved Zip performance, and a 64k buffer gives even better performance, I think that's okay. Performance should improve a bit by increasing at the expense of chewing up more memory per open file. The default setting should be for decent performance with minimal memory use.",code_debt,slow_algorithm,"Tue, 11 Dec 2007 02:03:47 +0000","Thu, 2 May 2013 02:29:11 +0000","Thu, 24 Jan 2008 23:23:49 +0000",3878402,"> For Lzo, this means it will compress no more than 4k at a time, yielding even less than 20% compression. We can't compress multiple buffers together with the lzo codec? It only compresses buffer-at-a-time? If so, then it should do the buffering & set the buffer size, since this is an lzo-specific issue. > This might be a better place to add some buffering, but then the codec will be returning a buffered stream. I don't think it's a bug for a codec to return a buffered stream if a particular buffer size is required to get good performance from that codec. If it's impossible to get lzo to compress data across buffers, and 64k or larger is required to get good compression, then it should mandate that buffer size, perhaps adding a new configuration parameter. Separately, we should consider whether to (a) unilaterally add an io.file.buffer.size buffer in TextOutputFormat, since it helps other codecs, or (b) assume that all codecs return appropriately buffered streams, and add a buffer in the Zip codec if it improves performance. If a io.file.buffer.size buffer=4k gives somewhat improved Zip performance, and a 64k buffer gives even better performance, I think that's okay. Performance should improve a bit by increasing io.file.buffer.size, at the expense of chewing up more memory per open file. The default setting should be for decent performance with minimal memory use.",0.09347777778,0.09510833333,neutral
hadoop,2402,comment_9,"Mostly looks ok, but there are too many unrelated white-space changes - hence, I'm cancelling this patch.",code_debt,low_quality_code,"Tue, 11 Dec 2007 02:03:47 +0000","Thu, 2 May 2013 02:29:11 +0000","Thu, 24 Jan 2008 23:23:49 +0000",3878402,"Mostly looks ok, but there are too many unrelated white-space changes - hence, I'm cancelling this patch.",0.75,0.75,neutral
hadoop,2424,comment_0,"With the right header, the existing LzoCodec should be compatible with lzop. It would probably be better implemented as an anyway.",requirement_debt,requirement_partially_implemented,"Fri, 14 Dec 2007 00:16:50 +0000","Fri, 4 Jan 2008 18:33:59 +0000","Fri, 14 Dec 2007 00:23:54 +0000",424,"With the right header, the existing LzoCodec should be compatible with lzop. It would probably be better implemented as an InputFormat/OutputFormat, anyway.",0.538,0.538,positive
hadoop,2776,comment_8,"I'm going to close this as won't fix. I don't think this is anything that we actually can fix here other than providing a complicated hostname mapping system for web interfaces. Part of the frustration I'm sure stems from a misunderstanding of what is actually happening: The slaves file is only used by the shell code to run ssh connections. It has absolutely zero impact on the core of Hadoop. Hadoop makes the perfectly valid assumption that the hostname the system tells us is a valid, network-connectable hostname. It is, from the inside of EC2. We have no way to know that you are attempting to connect from a completely different address that is being forwarded from some external entity. Proxying connections into a private network space is a perfectly valid solution.",defect_debt,uncorrected_known_defects,"Mon, 4 Feb 2008 16:59:32 +0000","Thu, 17 Jul 2014 20:16:05 +0000","Thu, 17 Jul 2014 20:16:05 +0000",203483793,"I'm going to close this as won't fix. I don't think this is anything that we actually can fix here other than providing a complicated hostname mapping system for web interfaces. Part of the frustration I'm sure stems from a misunderstanding of what is actually happening: The slaves file has the public names listed. The slaves file is only used by the shell code to run ssh connections. It has absolutely zero impact on the core of Hadoop. Resolving a public name inside EC2 returns the private IP (which would reverse to the internal DNS name). Hadoop makes the perfectly valid assumption that the hostname the system tells us is a valid, network-connectable hostname. It is, from the inside of EC2. We have no way to know that you are attempting to connect from a completely different address that is being forwarded from some external entity. Proxying connections into a private network space is a perfectly valid solution.",0.04791666667,0.0175,negative
hadoop,2776,comment_1,"Also note that the new EC2 scripts add SOCKS support: hadoop-ec2 proxy <cluster name By installing FoxyProxy or setting you socks firewall settings, you can browse your whole cluster. that said, Nate's suggestion might be less clunky... see:",design_debt,non-optimal_design,"Mon, 4 Feb 2008 16:59:32 +0000","Thu, 17 Jul 2014 20:16:05 +0000","Thu, 17 Jul 2014 20:16:05 +0000",203483793,"Also note that the new EC2 scripts add SOCKS support: hadoop-ec2 proxy <cluster name> By installing FoxyProxy or setting you socks firewall settings, you can browse your whole cluster. that said, Nate's suggestion might be less clunky... see: https://issues.apache.org/jira/browse/HADOOP-2410",0.2,0.2,negative
hadoop,2776,comment_7,Any more updates on this? Running Hadoop inside Amazon EC2 is super annoying because most of the links are broken (internal vs external addresses). This bug has been open for FOUR YEARS.,design_debt,non-optimal_design,"Mon, 4 Feb 2008 16:59:32 +0000","Thu, 17 Jul 2014 20:16:05 +0000","Thu, 17 Jul 2014 20:16:05 +0000",203483793,Any more updates on this? Running Hadoop inside Amazon EC2 is super annoying because most of the links are broken (internal vs external addresses). This bug has been open for FOUR YEARS.,-0.07311111111,-0.07311111111,negative
hadoop,2796,comment_0,"The proposed solution in the bug of adding a constant number to the script's exit code, in retrospect, seems like a bad idea. - It is not very intuitive. - There could be cases where because of the addition, some shells like bash which do modulo 256 on exit codes, could make the result become 0, which seems like a successful execution. - It causes an unreasonable dependency between HOD and user scripts, who need to remember this magic number. The requirements for this problem, to my understanding, are as follows: - Return a zero exit code for a completely successful operation (both hod and the script have worked fine) - Return a non-zero exit code for a failed operation (either hod or the script have failed). Users may not care for more than this. Did it work or not - In the event of a non-zero exit code where the user wants to know if his script failed, provide an easy, clear way to determine if it failed. On these lines, the attached patch does the following: - Returns a zero exit code on success. - Returns a non-zero exit code on failure of script or hod itself. - If the script returned a non-zero exit code, it writes the exit code from the script to a file 'script.exitcode' into the cluster directory. Users can simple check for this file's existence and determine if it is a script failure. - If it's a hod failure, no such file will exist.",design_debt,non-optimal_design,"Thu, 7 Feb 2008 13:12:36 +0000","Wed, 21 May 2008 20:05:50 +0000","Fri, 21 Mar 2008 14:26:42 +0000",3719646,"The proposed solution in the bug of adding a constant number to the script's exit code, in retrospect, seems like a bad idea. It is not very intuitive. There could be cases where because of the addition, some shells like bash which do modulo 256 on exit codes, could make the result become 0, which seems like a successful execution. It causes an unreasonable dependency between HOD and user scripts, who need to remember this magic number. The requirements for this problem, to my understanding, are as follows: Return a zero exit code for a completely successful operation (both hod and the script have worked fine) Return a non-zero exit code for a failed operation (either hod or the script have failed). Users may not care for more than this. Did it work or not In the event of a non-zero exit code where the user wants to know if his script failed, provide an easy, clear way to determine if it failed. On these lines, the attached patch does the following: Returns a zero exit code on success. Returns a non-zero exit code on failure of script or hod itself. If the script returned a non-zero exit code, it writes the exit code from the script to a file 'script.exitcode' into the cluster directory. Users can simple check for this file's existence and determine if it is a script failure. If it's a hod failure, no such file will exist.",0.0470952381,0.02792307692,negative
hadoop,2796,comment_1,"There are no test cases in this patch, because the commit of HADOOP-2848 missed committing the testHod.py file. This will cause a conflict now as the test cases should really be added to that file. Will submit test cases as part of a separate patch.",test_debt,lack_of_tests,"Thu, 7 Feb 2008 13:12:36 +0000","Wed, 21 May 2008 20:05:50 +0000","Fri, 21 Mar 2008 14:26:42 +0000",3719646,"There are no test cases in this patch, because the commit of HADOOP-2848 missed committing the testHod.py file. This will cause a conflict now as the test cases should really be added to that file. Will submit test cases as part of a separate patch.",-0.1,-0.1,negative
hadoop,2851,comment_3,Talked to Chris: these debug messages do not seem useful. So they are better removed. removed the debug messages and fixed all unchecked warning in Configuration.,code_debt,low_quality_code,"Tue, 19 Feb 2008 16:34:00 +0000","Tue, 28 Jul 2009 01:14:28 +0000","Tue, 28 Jul 2009 01:14:28 +0000",45304828,Talked to Chris: these debug messages do not seem useful. So they are better removed. 2851_20081103.patch: removed the debug messages and fixed all unchecked warning in Configuration.,-0.2,-0.15,negative
hadoop,3081,comment_1,"The non-static f, FsPermission permission) and the static fs, Path dir, FsPermission permission) look similar but they are different in semantic: - The non-static f, FsPermission permission) likes mkdir in Unix, it makes a directory with *umask*. - However, the static fs, Path dir, FsPermission permission) makes a directory with *absolute permission*. So I think it is better to change one of the method name (probably the static one), otherwise, it would be confusing.",code_debt,low_quality_code,"Mon, 24 Mar 2008 17:37:05 +0000","Wed, 8 Jul 2009 16:42:58 +0000","Fri, 19 Sep 2008 23:29:59 +0000",15486774,"The non-static FileSystem.mkdirs(Path f, FsPermission permission) and the static FileSystem.mkdirs(FileSystem fs, Path dir, FsPermission permission) look similar but they are different in semantic: The non-static FileSystem.mkdirs(Path f, FsPermission permission) likes mkdir in Unix, it makes a directory with umask. However, the static FileSystem.mkdirs(FileSystem fs, Path dir, FsPermission permission) makes a directory with absolute permission. So I think it is better to change one of the method name (probably the static one), otherwise, it would be confusing.",-0.3603333333,-0.2315238095,neutral
hadoop,3159,comment_5,"Digressing a little bit, the fundamental confusion seems to be that the key used by the cache needs scheme, authority, and username, but it only requires scheme and authority for look up.. so tries derive username some how. In long term, hopefully the interface itself gets fixed.",design_debt,non-optimal_design,"Thu, 3 Apr 2008 00:10:31 +0000","Thu, 17 Apr 2008 05:28:57 +0000","Fri, 4 Apr 2008 19:23:29 +0000",155578,"Digressing a little bit, the fundamental confusion seems to be that the key used by the cache needs scheme, authority, and username, but it only requires scheme and authority for look up.. so tries derive username some how. In long term, hopefully the interface itself gets fixed.",0.35,0.35,neutral
hadoop,3159,comment_8,"+1 The new patch is acceptable. Longer-term it seems to me we need a static URI) method, and the possibility to register different login methods for different URI schemes, e.g., The static method would then, when the configuration has no login information invoke the login method of the class named for that scheme, if any.",design_debt,non-optimal_design,"Thu, 3 Apr 2008 00:10:31 +0000","Thu, 17 Apr 2008 05:28:57 +0000","Fri, 4 Apr 2008 19:23:29 +0000",155578,"+1 The new patch is acceptable. Longer-term it seems to me we need a static login(Configuration, URI) method, and the possibility to register different login methods for different URI schemes, e.g., fs.login.hdfs=UnixUserGroupInformation. The static method would then, when the configuration has no login information invoke the login method of the class named for that scheme, if any.",-0.1095,-0.2188,neutral
hadoop,3198,comment_1,Some comments. 1) Declare a _private static final_ for {{MAX_DFS_RETRIES}} and initialize it to 10. Use this in the for loop. 2) Remove extra spaces after {{reporter}} (line 14 of the patch) 3) Sleeping for 1 sec needs to be argued. Btw a log message is required before waiting. 4) Some extra code slipped in (regarding the log message). 5) After 10 retries we should throw the exception rather than silently coming out of the loop (leading to null pointer exception). +_Points to ponder_+ Can we do a timeout based stuff where we wait for _shuffle-run-time / 2_ before bailing out and having multiple retries within this timeout. This will somehow make sure that we dont kill the reducer too early.,code_debt,low_quality_code,"Sat, 5 Apr 2008 19:30:54 +0000","Wed, 8 Jul 2009 16:52:43 +0000","Thu, 17 Apr 2008 16:42:38 +0000",1026704,Some comments. 1) Declare a private static final for MAX_DFS_RETRIES and initialize it to 10. Use this in the for loop. 2) Remove extra spaces after reporter (line 14 of the patch) 3) Sleeping for 1 sec needs to be argued. Btw a log message is required before waiting. 4) Some extra code slipped in (regarding the log message). 5) After 10 retries we should throw the exception rather than silently coming out of the loop (leading to null pointer exception). Points to ponder Can we do a timeout based stuff where we wait for shuffle-run-time / 2 before bailing out and having multiple retries within this timeout. This will somehow make sure that we dont kill the reducer too early.,0.06944444444,0.06944444444,neutral
hadoop,3198,comment_5,This will lead to very unmaintainable code. We absolutely do not want to have nested retries for different contexts.,code_debt,low_quality_code,"Sat, 5 Apr 2008 19:30:54 +0000","Wed, 8 Jul 2009 16:52:43 +0000","Thu, 17 Apr 2008 16:42:38 +0000",1026704,This will lead to very unmaintainable code. We absolutely do not want to have nested retries for different contexts.,-0.1,-0.1,neutral
hadoop,3198,comment_3,"HDFS client has a retry on exists. It is likely that it tried and failed the several times. That is perhaps fine for exists call in general. However, for this particular call in getRecordWriter in reduce rask, the cost of failure is too expensive. Thus, reduce task has to do something special. In this sense, I think it is reduce task's responsibility to further re-try. I am open for any suggestions to fix the problem. However, I am not convenced that re-try at rpc level is the right answer.",design_debt,non-optimal_design,"Sat, 5 Apr 2008 19:30:54 +0000","Wed, 8 Jul 2009 16:52:43 +0000","Thu, 17 Apr 2008 16:42:38 +0000",1026704,"HDFS client has a retry on exists. It is likely that it tried and failed the several times. That is perhaps fine for exists call in general. However, for this particular call in getRecordWriter in reduce rask, the cost of failure is too expensive. Thus, reduce task has to do something special. In this sense, I think it is reduce task's responsibility to further re-try. I am open for any suggestions to fix the problem. However, I am not convenced that re-try at rpc level is the right answer.",-0.0986875,-0.0986875,neutral
hadoop,3477,comment_1,"More details: == tar tzvf |awk '{print $6}'|sort|uniq -c|grep -v '1 ' 2 2 2 2 2 2 == This occurs because those files are included twice in build.xml, because there is no explicit exclude on those dirs, like there is for the others. The attached patch(1-line) adds such an exclude. This is keeping me from using the downloaded tarball as the orig.tar.gz for a debian package.",build_debt,build_others,"Sun, 1 Jun 2008 23:09:33 +0000","Fri, 22 Aug 2008 19:48:19 +0000","Wed, 11 Jun 2008 21:56:42 +0000",859629,"More details: == adam@zoot:~/code/upstream/nutch$ tar tzvf hadoop-0.17.0.tar.gz |awk ' {print $6} '|sort|uniq -c|grep -v '1 ' 2 hadoop-0.17.0/contrib/hod/bin/VERSION 2 hadoop-0.17.0/contrib/hod/bin/checknodes 2 hadoop-0.17.0/contrib/hod/bin/hod 2 hadoop-0.17.0/contrib/hod/bin/hodcleanup 2 hadoop-0.17.0/contrib/hod/bin/hodring 2 hadoop-0.17.0/contrib/hod/bin/ringmaster == This occurs because those files are included twice in build.xml, because there is no explicit exclude on those dirs, like there is for the others. The attached patch(1-line) adds such an exclude. This is keeping me from using the downloaded tarball as the orig.tar.gz for a debian package.",-0.05833333333,-0.01590909091,neutral
hadoop,3505,comment_0,"Actually changes in Hadoop 0.18 now allow HOD to take relative paths on the command line. Likewise, if an incorrect conf is present, the errors are reported back to the client. However, we should document that the tarball should not contain a modified conf, so that users know this proactively. In addition to the above, the following changes are identified after reviewing the bugs fixed for HOD in Hadoop 0.18: - HADOOP-3376: Hyperlink Maui in the documentation, crediting Cluster Resources for the software. Also, Torque and Maui should be capitalized in the documentation. - HADOOP-3464: Add 2 new error scenarios in the trouble shooting section. One should describe the error that comes when ringmaster fails, the other when the jobtracker fails. For the latter, we should mention that administrators can review other log messages in the ringmaster log to see which other machines had problems bringing up the jobtracker, apart from the one that is reported on the command line. - HADOOP-3483: Remove any limitations we have specified about creating a cluster directory. Now, we create it automatically. - HADOOP-3184: Document in the Config guide about parameter.",documentation_debt,low_quality_documentation,"Thu, 5 Jun 2008 22:07:49 +0000","Fri, 22 Aug 2008 19:50:40 +0000","Sat, 21 Jun 2008 00:27:40 +0000",1304391,"Actually changes in Hadoop 0.18 now allow HOD to take relative paths on the command line. Likewise, if an incorrect conf is present, the errors are reported back to the client. However, we should document that the tarball should not contain a modified conf, so that users know this proactively. In addition to the above, the following changes are identified after reviewing the bugs fixed for HOD in Hadoop 0.18: HADOOP-3376: Hyperlink Maui in the documentation, crediting Cluster Resources for the software. Also, Torque and Maui should be capitalized in the documentation. HADOOP-3464: Add 2 new error scenarios in the trouble shooting section. One should describe the error that comes when ringmaster fails, the other when the jobtracker fails. For the latter, we should mention that administrators can review other log messages in the ringmaster log to see which other machines had problems bringing up the jobtracker, apart from the one that is reported on the command line. HADOOP-3483: Remove any limitations we have specified about creating a cluster directory. Now, we create it automatically. HADOOP-3184: Document in the Config guide about ringmaster.max-master-failures parameter.",-0.128875,-0.08175,neutral
hadoop,3560,comment_2,i have not added tests since the test require creation of large input files.,test_debt,lack_of_tests,"Fri, 13 Jun 2008 20:20:56 +0000","Wed, 8 Jul 2009 16:41:31 +0000","Tue, 17 Jun 2008 05:15:34 +0000",291278,i have not added tests since the test require creation of large input files.,0.0,0.0,negative
hadoop,3836,comment_0,The test output says The output directory was not cleaned up after the test is done.,code_debt,low_quality_code,"Fri, 25 Jul 2008 20:13:51 +0000","Wed, 8 Jul 2009 16:52:55 +0000","Fri, 1 Aug 2008 22:45:04 +0000",613873,The test output says The output directory was not cleaned up after the test is done.,0.0,0.0,negative
hadoop,3836,comment_2,"I tried the patch. The test still fails. The problem is that there is a typo in line 180 (after the patch). ""reader.close()"" should be It works fine after fixed the typo. Also, the before is not needed.",documentation_debt,low_quality_documentation,"Fri, 25 Jul 2008 20:13:51 +0000","Wed, 8 Jul 2009 16:52:55 +0000","Fri, 1 Aug 2008 22:45:04 +0000",613873,"I tried the patch. The test still fails. The problem is that there is a typo in line 180 (after the patch). ""reader.close()"" should be ""seqReader.close()"". It works fine after fixed the typo. Also, the ""@SuppressWarnings( {""unchecked""} )"" before _testMultipleOutputs(...) is not needed.",0.00675,0.0050625,negative
hadoop,3905,comment_1,"In the patch. # I substantially simplified what used to be called by using existing hadoop class instead of a pair of classes import and # is now an abstract class, and is its implementation for storing edits in a file. # Introduced and its implementation called for reading edits from a file. # The rest of the {{FSEditLog}} remained unchanged. The idea here is that one should write implementations of and for a different type of persistent storage and the rest of the logic for the edits log should remain unchanged. Since I don't have other implementations ready yet it is hard to predict what else should be changed or abstracted. FSImage and FSEditLog heavily depend on the storage directory file names so may be that should be changed somehow in the future.",design_debt,non-optimal_design,"Wed, 6 Aug 2008 01:49:11 +0000","Wed, 8 Jul 2009 16:43:16 +0000","Sat, 16 Aug 2008 00:06:40 +0000",857849,"In the patch. I substantially simplified what used to be called EditLogOutputStream by using existing hadoop class org.apache.hadoop.io.DataOutputBuffer instead of a pair of classes import DataOutputStream and ByteArrayOutputStream EditLogOutputStream is now an abstract class, and EditLogFileOutputStream is its implementation for storing edits in a file. Introduced EditLogInputStream and its implementation called EditLogFileInputStream for reading edits from a file. The rest of the FSEditLog remained unchanged. The idea here is that one should write implementations of EditLogOutputStream and EditLogInputStream for a different type of persistent storage and the rest of the logic for the edits log should remain unchanged. Since I don't have other implementations ready yet it is hard to predict what else should be changed or abstracted. FSImage and FSEditLog heavily depend on the storage directory file names so may be that should be changed somehow in the future.",-0.06428571429,-0.04090909091,neutral
hadoop,3905,comment_3,I fixed one JavaDoc warning.,documentation_debt,low_quality_documentation,"Wed, 6 Aug 2008 01:49:11 +0000","Wed, 8 Jul 2009 16:43:16 +0000","Sat, 16 Aug 2008 00:06:40 +0000",857849,I fixed one JavaDoc warning.,-0.6,-0.6,neutral
hadoop,3925,comment_2,One of our users submitted a job that has a million mappers and million reducers. The JobTracker was runnign with 3GB heap. It went into 100% CPU usage (probably GC). Never came back to life even after 10 minutes. Is there a way (in the current release) to prevent this from happening?,design_debt,non-optimal_design,"Fri, 8 Aug 2008 06:13:55 +0000","Wed, 8 Jul 2009 16:52:57 +0000","Thu, 20 Nov 2008 07:59:21 +0000",8991926,One of our users submitted a job that has a million mappers and million reducers. The JobTracker was runnign with 3GB heap. It went into 100% CPU usage (probably GC). Never came back to life even after 10 minutes. Is there a way (in the current release) to prevent this from happening?,-0.04,-0.04,negative
hadoop,3957,comment_0,fixed unchecked warnings. Removed some not-so-useful methods and the use of deprecated API.,code_debt,dead_code,"Fri, 15 Aug 2008 00:06:03 +0000","Wed, 26 Sep 2012 13:59:09 +0000","Mon, 18 Aug 2008 20:54:41 +0000",334118,3957_20080814.patch: fixed unchecked warnings. Removed some not-so-useful methods and the use of deprecated API.,-0.05,-0.03333333333,negative
hadoop,3999,comment_1,"1. This would be good if it could be easily extended; rather than than a hard coded set of values, clients could add other (key,value) info for schedulers to use. Things like for cycle-scavenging task-trackers, and other extensions that custom schedulers could use. It could also integrate with diagnostics. 2. There's a danger here in trying to do a full grid scheduler. Why Danger? Hard to get right, there are other tools and products that can do a lot of this. Hadoop likes to push work near the data and works best if the work is all Java. 3. Developers are surprisingly bad about estimating workload, especially if you have a few layers between you and the MR jobs. The best metric for how intensive a job will be is ""what was like last time"".",design_debt,non-optimal_design,"Fri, 22 Aug 2008 09:42:15 +0000","Fri, 18 Jul 2014 22:22:39 +0000","Fri, 18 Jul 2014 22:22:39 +0000",186324024,"1. This would be good if it could be easily extended; rather than than a hard coded set of values, clients could add other (key,value) info for schedulers to use. Things like expected-availability for cycle-scavenging task-trackers, and other extensions that custom schedulers could use. It could also integrate with diagnostics. 2. There's a danger here in trying to do a full grid scheduler. Why Danger? Hard to get right, there are other tools and products that can do a lot of this. Hadoop likes to push work near the data and works best if the work is all Java. 3. Developers are surprisingly bad about estimating workload, especially if you have a few layers between you and the MR jobs. The best metric for how long/CPU-intensive/IO intensive a job will be is ""what was like last time"".",0.05619444444,0.05619444444,neutral
hadoop,4066,comment_1,I think it is better to keep this document in the README file. The reasons being: 1. The README is version-controlled and can change from version to version. 2. All contrib modules are *required* to have a README file.,documentation_debt,low_quality_documentation,"Thu, 4 Sep 2008 00:54:33 +0000","Wed, 8 Jul 2009 17:05:05 +0000","Thu, 4 Sep 2008 22:20:22 +0000",77149,I think it is better to keep this document in the README file. The reasons being: 1. The README is version-controlled and can change from version to version. 2. All contrib modules are required to have a README file.,0.1,0.1,neutral
hadoop,4066,comment_2,"Wiki is *not* an acceptable place for system documentation. In particular, it is not versioned or released.",documentation_debt,low_quality_documentation,"Thu, 4 Sep 2008 00:54:33 +0000","Wed, 8 Jul 2009 17:05:05 +0000","Thu, 4 Sep 2008 22:20:22 +0000",77149,"Wiki is not an acceptable place for system documentation. In particular, it is not versioned or released.",-0.328,-0.328,neutral
hadoop,4066,comment_3,I filed to cleanup the README and will delete the version specific stuff from the wiki.,documentation_debt,low_quality_documentation,"Thu, 4 Sep 2008 00:54:33 +0000","Wed, 8 Jul 2009 17:05:05 +0000","Thu, 4 Sep 2008 22:20:22 +0000",77149,I filed https://issues.apache.org/jira/browse/HADOOP-4076 to cleanup the README and will delete the version specific stuff from the wiki.,0.0,0.0,neutral
hadoop,4182,comment_1,"I agree with you that it is a problem at the application / user level. I only wanted to put a simple comment somewhere on the Hadoop Wiki that says that a line must end with an end of line delimiter. If not, user might get different behaviors as I explained earlier. This simple comment can keep a user from accidental un-expected results.",documentation_debt,low_quality_documentation,"Tue, 16 Sep 2008 05:36:36 +0000","Wed, 8 Jul 2009 17:05:24 +0000","Tue, 23 Sep 2008 16:58:13 +0000",645697,"I agree with you that it is a problem at the application / user level. I only wanted to put a simple comment somewhere on the Hadoop Wiki that says that a line must end with an end of line delimiter. If not, user might get different behaviors as I explained earlier. This simple comment can keep a user from accidental un-expected results.",0.0,0.0,neutral
hadoop,4300,comment_1,"I haven't tested on 0.19, but we've been using the Trash feature on our clusters for some time . Could you check if Namenode config also has the non-zero fs.trash.interval? - Client uses 'fs.trash.interval' to determine whether to delete files or move them to Trash. - Namenode uses 'fs.trash.interval' for running the expunge thread to clean up the users' Trash directories. So your files should be deleted between namenode's 'fs.trash.interval to 'fs.trash.interval * 2' period.",test_debt,lack_of_tests,"Mon, 29 Sep 2008 06:32:19 +0000","Wed, 8 Jul 2009 16:43:20 +0000","Fri, 17 Oct 2008 21:27:01 +0000",1608882,"I haven't tested on 0.19, but we've been using the Trash feature on our clusters for some time . Could you check if Namenode config also has the non-zero fs.trash.interval? Client uses 'fs.trash.interval' to determine whether to delete files or move them to Trash. Namenode uses 'fs.trash.interval' for running the expunge thread to clean up the users' Trash directories. So your files should be deleted between namenode's 'fs.trash.interval to 'fs.trash.interval * 2' period.",-0.01164285714,-0.01086666667,neutral
hadoop,4576,comment_10,Changing visibility of the constructor and removing the deprecation. ant test-patch output for the patch is :,code_debt,dead_code,"Mon, 3 Nov 2008 05:38:44 +0000","Wed, 8 Jul 2009 16:40:30 +0000","Fri, 5 Dec 2008 05:14:19 +0000",2763335,Changing visibility of the constructor and removing the deprecation. ant test-patch output for the patch is :,-0.25,-0.25,neutral
hadoop,4576,comment_5,"Comments : : 1) No need to import {{AtomicInteger}}. Also plz remove extra diffs. : 1) You cant simply change the constructor def. Overload it and deprecate the other if needed. 2) Extra diffs w.r.t. 3) Use {{StringUtils}} for formatting time. 1) The ordering doesnt seem right. You submit 5 jobs, try to assign tasks (which should be no-op) and then you init the jobs. 2) Shouldnt we also check/test the timing issue that after _poll-interval_ units of time the values are correct. Something like - add jobs - check the queue-sched-info - allow the jobs to be inited by the poller i.e wait for _poller-interval_ time - check again to see if the change is made. 2) Plz mark the start and end of a new sub-test using comments",code_debt,low_quality_code,"Mon, 3 Nov 2008 05:38:44 +0000","Wed, 8 Jul 2009 16:40:30 +0000","Fri, 5 Dec 2008 05:14:19 +0000",2763335,"Comments : JobQueuesManager.java : 1) No need to import AtomicInteger. Also plz remove extra diffs. CapacityTaskScheduler.java : 1) You cant simply change the constructor def. Overload it and deprecate the other if needed. 2) Extra diffs w.r.t. 3) + sb.append(String.format(""* Scheduling information can be off by "" + + ""maximum of %d seconds\n"", pollingInterval/1000)); Use StringUtils for formatting time. TestCapacityScheduler.java 1) + scheduler.assignTasks(tracker(""tt1"")); // heartbeat + p.selectJobsToInitialize(); The ordering doesnt seem right. You submit 5 jobs, try to assign tasks (which should be no-op) and then you init the jobs. 2) Shouldnt we also check/test the timing issue that after poll-interval units of time the values are correct. Something like add jobs check the queue-sched-info allow the jobs to be inited by the poller i.e wait for poller-interval time check again to see if the change is made. 2) Plz mark the start and end of a new sub-test using comments",-0.1152,-0.0329375,neutral
hadoop,4576,comment_9,"I think it is not necessary to deprecate the constructor of SchedulingInfo. It is a private static inner class and hence nobody should be affected ? It would just add more code to maintain. Can you please submit a new patch removing the deprecation and just changing the api ? Please also remember to remove the checks for null in the toString method, as the object is not expected to be null after this change. BTW, I spoke to Amar about this and we agree on this point now.",code_debt,dead_code,"Mon, 3 Nov 2008 05:38:44 +0000","Wed, 8 Jul 2009 16:40:30 +0000","Fri, 5 Dec 2008 05:14:19 +0000",2763335,"I think it is not necessary to deprecate the constructor of SchedulingInfo. It is a private static inner class and hence nobody should be affected ? It would just add more code to maintain. Can you please submit a new patch removing the deprecation and just changing the api ? Please also remember to remove the checks for null in the toString method, as the object is not expected to be null after this change. BTW, I spoke to Amar about this and we agree on this point now.",-0.02808333333,-0.02808333333,neutral
hadoop,4603,comment_9,"This is essentially fixed by using the native libraries in newer releases of Hadoop. The problem is that the native code is completely non-portable and the committer community has shown no real desire to make that code portable. I don't believe we should encourage folks to run without the native code because the performance is likely to be seriously horrendous. (My anecdotal experience says the NN in 0.20.20x is 10-15% slower compared to 0.20.2). At this point, I'd close this as won't fix, just like I did all of my portability JIRAs (including some with patches). I think pretending that we care about portability is sort of silly at this point when it has been demonstrated over and over that we don't.",design_debt,non-optimal_design,"Thu, 6 Nov 2008 21:21:29 +0000","Mon, 21 Jul 2014 16:56:19 +0000","Mon, 21 Jul 2014 16:56:19 +0000",179955290,"This is essentially fixed by using the native libraries in newer releases of Hadoop. The problem is that the native code is completely non-portable and the committer community has shown no real desire to make that code portable. I don't believe we should encourage folks to run without the native code because the performance is likely to be seriously horrendous. (My anecdotal experience says the NN in 0.20.20x is 10-15% slower compared to 0.20.2). At this point, I'd close this as won't fix, just like I did all of my portability JIRAs (including some with patches). I think pretending that we care about portability is sort of silly at this point when it has been demonstrated over and over that we don't.",-0.1506666667,-0.1506666667,negative
hadoop,4941,comment_0,remove the deprecated methods.,code_debt,dead_code,"Wed, 24 Dec 2008 19:01:56 +0000","Tue, 24 Aug 2010 20:34:43 +0000","Thu, 1 Jan 2009 18:07:36 +0000",687940,4941_20081224.patch: remove the deprecated methods.,0.0,0.0,neutral
hadoop,4985,comment_0,"remove unnecessary ""throws IOException""",code_debt,dead_code,"Wed, 7 Jan 2009 02:10:41 +0000","Wed, 12 Aug 2020 23:02:16 +0000","Mon, 12 Jan 2009 21:18:36 +0000",500875,"4985_20090106.patch: remove unnecessary ""throws IOException""",0.0,0.0,negative
hadoop,4985,comment_2,"- FSDirectory empty line change. - Javadoc for should not remove {{@throws IOException}} but rather replace it with {{@throws - file: "" ...)}} should be does not exist: "" ...)}} I once tried to unify this in the code, but it's now back with all different messages. - Additionally we can remove IOException in FSEditLog.close() and then and then",documentation_debt,low_quality_documentation,"Wed, 7 Jan 2009 02:10:41 +0000","Wed, 12 Aug 2020 23:02:16 +0000","Mon, 12 Jan 2009 21:18:36 +0000",500875,"FSDirectory empty line change. Javadoc for FSDirectory.setReplication() should not remove @throws IOException but rather replace it with @throws QuotaExceededException FileNotFoundException(""Unknown file: "" ...) should be FileNotFoundException(""File does not exist: "" ...) I once tried to unify this in the code, but it's now back with all different messages. Additionally we can remove IOException in FSEditLog.close() FSEditLog.existsNew() FSEditLog.logOpenFile() FSEditLog.getFsEditName() and then FSDirectory.persistBlocks() FSDirectory.closeFile() FSDirectory.setTimes() and then FSNamesystem.getBlockLocationsInternal()",-0.1,-0.01666666667,negative
hadoop,4997,comment_1,"Dhruba, we should also remove the unit tests that test the tmp directory does not get removed after restarting the cluster.",test_debt,expensive_tests,"Thu, 8 Jan 2009 20:21:26 +0000","Wed, 8 Jul 2009 16:43:29 +0000","Fri, 16 Jan 2009 23:56:50 +0000",704124,"Dhruba, we should also remove the unit tests that test the tmp directory does not get removed after restarting the cluster.",0.0,0.0,neutral
hadoop,5097,comment_0,"- change JspHelper.fsn from ""static"" to ""private final"" - change a few non-static methods in JspHelper to static",code_debt,low_quality_code,"Wed, 21 Jan 2009 22:37:59 +0000","Tue, 24 Aug 2010 20:35:10 +0000","Mon, 2 Feb 2009 18:43:39 +0000",1022740,"5097_20090121.patch: change JspHelper.fsn from ""static"" to ""private final"" change a few non-static methods in JspHelper to static",-0.4375,-0.2916666667,neutral
hadoop,5097,comment_1,added javadoc and re-organized some methods.,code_debt,low_quality_code,"Wed, 21 Jan 2009 22:37:59 +0000","Tue, 24 Aug 2010 20:35:10 +0000","Mon, 2 Feb 2009 18:43:39 +0000",1022740,5097_20090126.patch: added javadoc and re-organized some methods.,0.0,0.0,neutral
hadoop,5097,comment_2,"# Remove comment in FSNamesystem /** NameNode RPC address */ # I do not very much like that you add JspHelper as a NameNode members. I understand the intention is not to instantiate objects on the web UI, but may be we should rather consider making most of the methods / fields of JspHelper static instead of constructing the object. In any case it is better not to introduce helper variables in NamNode and DataNode.",code_debt,low_quality_code,"Wed, 21 Jan 2009 22:37:59 +0000","Tue, 24 Aug 2010 20:35:10 +0000","Mon, 2 Feb 2009 18:43:39 +0000",1022740,"Remove comment in FSNamesystem /** NameNode RPC address */ I do not very much like that you add JspHelper as a NameNode members. I understand the intention is not to instantiate objects on the web UI, but may be we should rather consider making most of the methods / fields of JspHelper static instead of constructing the object. In any case it is better not to introduce helper variables in NamNode and DataNode.",-0.07283333333,-0.07283333333,neutral
hadoop,5097,comment_3,changed all JspHelp methods to static and removed /** NameNode RPC address */,code_debt,low_quality_code,"Wed, 21 Jan 2009 22:37:59 +0000","Tue, 24 Aug 2010 20:35:10 +0000","Mon, 2 Feb 2009 18:43:39 +0000",1022740,5097_20090127.patch: changed all JspHelp methods to static and removed /** NameNode RPC address */,-0.875,-0.4375,neutral
hadoop,5097,comment_4,should be done in static. Thank Suresh for catching this.,code_debt,low_quality_code,"Wed, 21 Jan 2009 22:37:59 +0000","Tue, 24 Aug 2010 20:35:10 +0000","Mon, 2 Feb 2009 18:43:39 +0000",1022740,5097_20090127b.patch: UnixUserGroupInformation.saveToConf() should be done in static. Thank Suresh for catching this.,-0.2125,-0.1416666667,neutral
hadoop,5141,comment_4,"It doesn't have any specific dependency on the version, all we need is the latest version, since the vesion of json.jar that we are using currently is missing some classes.",architecture_debt,using_obsolete_technology,"Thu, 29 Jan 2009 09:08:08 +0000","Thu, 1 Oct 2009 07:37:54 +0000","Thu, 1 Oct 2009 07:37:54 +0000",21162586,"1. check that the 20080701 version isn't what is actually needed Giri, did you check this? It doesn't have any specific dependency on the version, all we need is the latest version, since the vesion of json.jar that we are using currently is missing some classes.",-0.2,-0.1,neutral
hadoop,5298,comment_4,"There is no difference in the logic. I agree that two tests are similar. The codes were duplicated in HADOOP-4284. It needs more works to refactor the codes now. Since these are only tests, I suggest we leave it and do the refactoring in the future.",code_debt,duplicated_code,"Fri, 20 Feb 2009 16:38:04 +0000","Thu, 23 Apr 2009 19:18:04 +0000","Mon, 9 Mar 2009 22:15:41 +0000",1489057,"> It looks like HADOOP-4695 is addressing the same issue, but uses a different approach. Is the difference significant? The two tests are nearly identical and could probably share more code than they currently do. There is no difference in the logic. I agree that two tests are similar. The codes were duplicated in HADOOP-4284. It needs more works to refactor the codes now. Since these are only tests, I suggest we leave it and do the refactoring in the future.",0.12,0.125,neutral
hadoop,5465,comment_1,"Two bugs in DFS contributed to the problem: (1). DataNode does not sync on modification to the counter ""xmitsInProgress"", which keeps track of the number of replication in progress. When two threads update the counter concurrently, race condition may occurs. The counter may change to be a non-zero value when no replication is going on. (2). Each DN is configured to have at most 2 replications in progress. When DN notifies NN that it has 1 replication in progress, NN should be able to send one block replication request to DN. But NN wrongly interprets the counter as the number of targets. When it sees that the block is scheduled to 2 targets but DN can only take 1, it sends an empty replication request to DN. As a result, blocking all replications from this DataNode. If the DataNode is the only source of an under-replicated block, the block will never get replicated. Fixing either (1) or (2) could fix the problem. I think (1) is more fundamental so I will fix (1) in this jira and file a different jira to fix (2).",code_debt,multi-thread_correctness,"Wed, 11 Mar 2009 23:03:36 +0000","Wed, 8 Jul 2009 16:43:33 +0000","Fri, 13 Mar 2009 19:59:42 +0000",161766,"Two bugs in DFS contributed to the problem: (1). DataNode does not sync on modification to the counter ""xmitsInProgress"", which keeps track of the number of replication in progress. When two threads update the counter concurrently, race condition may occurs. The counter may change to be a non-zero value when no replication is going on. (2). Each DN is configured to have at most 2 replications in progress. When DN notifies NN that it has 1 replication in progress, NN should be able to send one block replication request to DN. But NN wrongly interprets the counter as the number of targets. When it sees that the block is scheduled to 2 targets but DN can only take 1, it sends an empty replication request to DN. As a result, blocking all replications from this DataNode. If the DataNode is the only source of an under-replicated block, the block will never get replicated. Fixing either (1) or (2) could fix the problem. I think (1) is more fundamental so I will fix (1) in this jira and file a different jira to fix (2).",-0.07723076923,-0.07723076923,neutral
hadoop,5561,comment_3,"would recommend having an Ant property javadoc.memory that is set to 512m in the build file, but can be overridden by people with problems (or 64 bit JVMs) without having to patch the build file. It could also be used by all build files",design_debt,non-optimal_design,"Mon, 23 Mar 2009 23:49:25 +0000","Tue, 24 Aug 2010 20:36:43 +0000","Thu, 26 Mar 2009 00:25:57 +0000",174992,"would recommend having an Ant property javadoc.memory that is set to 512m in the build file, but can be overridden by people with problems (or 64 bit JVMs) without having to patch the build file. It could also be used by all build files",0.0,0.0,neutral
hadoop,5657,comment_4,It'd help code readability if some comments are added on why two values are emitted per map and the logic the testcase is employing for validation (verbose comments for the arithmetic is what I mean *smile*).,code_debt,low_quality_code,"Sat, 11 Apr 2009 00:16:20 +0000","Tue, 24 Aug 2010 20:37:05 +0000","Wed, 29 Apr 2009 04:11:21 +0000",1569301,It'd help code readability if some comments are added on why two values are emitted per map and the logic the testcase is employing for validation (verbose comments for the arithmetic is what I mean smile).,0.4,0.4,neutral
hadoop,5657,comment_6,Adds some extra comments; no functional changes,code_debt,low_quality_code,"Sat, 11 Apr 2009 00:16:20 +0000","Tue, 24 Aug 2010 20:37:05 +0000","Wed, 29 Apr 2009 04:11:21 +0000",1569301,Adds some extra comments; no functional changes,0.0,0.0,neutral
hadoop,5657,comment_2,Emits only two additional records per map; less expensive key updates. Detects known-bad case when merging a combination of in-memory and on-disk segments.,design_debt,non-optimal_design,"Sat, 11 Apr 2009 00:16:20 +0000","Tue, 24 Aug 2010 20:37:05 +0000","Wed, 29 Apr 2009 04:11:21 +0000",1569301,Emits only two additional records per map; less expensive key updates. Detects known-bad case when merging a combination of in-memory and on-disk segments.,-0.05,-0.05,neutral
hadoop,5701,comment_0,"Consider that a cluster have 2000 map slots and jobs submitted in the following sequence: |1:00pm|JobA|1500 maps, each map runs 24 hours| |1:30pm|JobB|1000 maps, each map runs 2 hours| |1:40pm|JobC|3000 maps, each map runs 10 minutes| Then, all 1500 maps in JobA got scheduled and only 500 map slots remained in the cluster at 1pm. 30 minutes later, JobB came and only 500 maps slots got scheduled. At 1:40pm, JobC came but no maps got scheduled until some maps in JobB finished 2 hours later. In this cases, JobA always has 75% of the capacity, JobB and JobC never able to obtain 1/N of the capacity. If JobA has 2000 maps, other jobs have to wait for maps in JobA to finish and have no progress in 24 hours.",design_debt,non-optimal_design,"Fri, 17 Apr 2009 18:49:24 +0000","Wed, 8 Jul 2009 16:41:03 +0000","Fri, 12 Jun 2009 00:07:10 +0000",4771066,"Consider that a cluster have 2000 map slots and jobs submitted in the following sequence: Then, all 1500 maps in JobA got scheduled and only 500 map slots remained in the cluster at 1pm. 30 minutes later, JobB came and only 500 maps slots got scheduled. At 1:40pm, JobC came but no maps got scheduled until some maps in JobB finished 2 hours later. In this cases, JobA always has 75% of the capacity, JobB and JobC never able to obtain 1/N of the capacity. If JobA has 2000 maps, other jobs have to wait for maps in JobA to finish and have no progress in 24 hours.",-0.272,-0.272,neutral
hadoop,5824,comment_0,+1 for removing it if it is not used.,code_debt,dead_code,"Wed, 13 May 2009 21:53:52 +0000","Mon, 3 Feb 2014 10:40:35 +0000","Thu, 14 May 2009 18:13:11 +0000",73159,+1 for removing it if it is not used.,0.0,0.0,neutral
hadoop,6182,comment_0,"with this patch the releaseaudit warnings is down to 1. [rat:report] 1 Unknown Licenses [rat:report] [rat:report] Unapproved licenses: [rat:report] this is an empty file. I'm not sure why we have this empty java files. If we can remove this empty java file, we can get rid of this releaaseaudit warning. Thanks,",code_debt,dead_code,"Mon, 10 Aug 2009 05:55:27 +0000","Tue, 24 Aug 2010 20:39:20 +0000","Mon, 17 Aug 2009 04:27:18 +0000",599511,"with this patch the releaseaudit warnings is down to 1. [rat:report] 1 Unknown Licenses [rat:report] ******************************* [rat:report] Unapproved licenses: [rat:report] src/java/org/apache/hadoop/fs/LengthFileChecksum.java LengthFileChecksum.java this is an empty file. I'm not sure why we have this empty java files. If we can remove this empty java file, we can get rid of this releaaseaudit warning. Thanks,",-0.1092,-0.1208571429,negative
hadoop,6182,comment_7,"Looks like the original patch inserted text blocks into the XML files ahead of the block: <?xml version=""1.0""? <?xml-stylesheet type=""text/xsl"" I corrected this for checkstyle in HADOOP-6185, but because the error is pervasive, it fails a wide variety of unit tests.",code_debt,low_quality_code,"Mon, 10 Aug 2009 05:55:27 +0000","Tue, 24 Aug 2010 20:39:20 +0000","Mon, 17 Aug 2009 04:27:18 +0000",599511,"Looks like the original patch inserted text blocks into the XML files ahead of the block: <?xml version=""1.0""?> <?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?> I corrected this for checkstyle in HADOOP-6185, but because the error is pervasive, it fails a wide variety of unit tests.",-0.15,-0.1,negative
hadoop,6182,comment_8,Let's remove the file separately once the build is stable.,code_debt,dead_code,"Mon, 10 Aug 2009 05:55:27 +0000","Tue, 24 Aug 2010 20:39:20 +0000","Mon, 17 Aug 2009 04:27:18 +0000",599511,"> Looks like LengthFileChecksum was removed in HADOOP-3981, although the source file wasn't deleted. ... Let's remove the file separately once the build is stable.",0.4,0.2,negative
hadoop,6198,comment_0,I'd argue that we may want to support both interfaces. The fact that the current path-based filtering retrieves a FileStatus object first is an implementation detail and it is conceivable that a different FS implementation may be more efficient to support path-based filtering than file-status based filtering.,code_debt,slow_algorithm,"Tue, 18 Aug 2009 05:46:36 +0000","Thu, 24 Jul 2014 19:57:11 +0000","Thu, 24 Jul 2014 19:57:11 +0000",155657435,I'd argue that we may want to support both interfaces. The fact that the current path-based filtering retrieves a FileStatus object first is an implementation detail and it is conceivable that a different FS implementation may be more efficient to support path-based filtering than file-status based filtering.,0.1095,0.1095,neutral
hadoop,6198,comment_2,I am not sure why it would be a burden for FS implementors. listStatus would be implemented by FileSystem and FS implementors should only override them when the default implementation is less efficient (and the implementors have the incentive of making them fast).,code_debt,slow_algorithm,"Tue, 18 Aug 2009 05:46:36 +0000","Thu, 24 Jul 2014 19:57:11 +0000","Thu, 24 Jul 2014 19:57:11 +0000",155657435,I am not sure why it would be a burden for FS implementors. listStatus would be implemented by FileSystem and FS implementors should only override them when the default implementation is less efficient (and the implementors have the incentive of making them fast).,-0.05,-0.05,neutral
hadoop,6198,comment_1,"Granted, but supporting two filtering interfaces will likely cause maintenance headaches and be more of a burden to a FS implementor; I'd rather pick an API and not support all possible variants. If a FileSystem is more efficient with path-based filtering, it can still work with FileStatus objects, either populating them lazily, filling them with defaults (what many shims do anyway), or even failing if a user queries unsupported data. Since globStatus returns FileStatus objects, any implementation will need to construct them for the set of accepted Paths, anyway. Given that the API seems biased toward FileStatus objects, I'd rather endure a penalty for the hypothetical FS that doesn't return this information, rather than maintain two separate filtering APIs.",design_debt,non-optimal_design,"Tue, 18 Aug 2009 05:46:36 +0000","Thu, 24 Jul 2014 19:57:11 +0000","Thu, 24 Jul 2014 19:57:11 +0000",155657435,"Granted, but supporting two filtering interfaces will likely cause maintenance headaches and be more of a burden to a FS implementor; I'd rather pick an API and not support all possible variants. If a FileSystem is more efficient with path-based filtering, it can still work with FileStatus objects, either populating them lazily, filling them with defaults (what many shims do anyway), or even failing if a user queries unsupported data. Since globStatus returns FileStatus objects, any implementation will need to construct them for the set of accepted Paths, anyway. Given that the API seems biased toward FileStatus objects, I'd rather endure a penalty for the hypothetical FS that doesn't return this information, rather than maintain two separate filtering APIs.",-0.0699375,-0.0699375,neutral
hadoop,6220,comment_5,"Would adding to the throws list of HttpServer::start, and rethrowing, be clearer? should be reserved for, well, interrupted I/O. Have you seen wrapped in logs or other evidence that would support this change? It is difficult to evaluate this without corresponding changes to the callers. What one might do with this is speculative in its current form.",code_debt,low_quality_code,"Thu, 27 Aug 2009 12:01:14 +0000","Thu, 12 May 2016 18:22:56 +0000","Wed, 28 Sep 2011 20:38:10 +0000",65867816,"If we want callers to distinguish InterruptedExceptions from IOEs, then this exception should be extracted [...] Would adding InterruptedException to the throws list of HttpServer::start, and rethrowing, be clearer? InterruptedIOException should be reserved for, well, interrupted I/O. Have you seen wrapped InterruptedExceptions in logs or other evidence that would support this change? It is difficult to evaluate this without corresponding changes to the callers. What one might do with this is speculative in its current form.",-0.2536666667,-0.1505416667,negative
hadoop,6220,comment_0,This turns an interrupt into an includes the original exception as the cause. No test. This is a tricky one to set up a test for as you need to block the startup and then interrupt the starting thread.,test_debt,lack_of_tests,"Thu, 27 Aug 2009 12:01:14 +0000","Thu, 12 May 2016 18:22:56 +0000","Wed, 28 Sep 2011 20:38:10 +0000",65867816,This turns an interrupt into an InterruptedIOException; includes the original exception as the cause. No test. This is a tricky one to set up a test for as you need to block the startup and then interrupt the starting thread.,-0.2333333333,-0.2333333333,negative
hadoop,6220,comment_11,No tests as it's so hard to recreate this situation in a test; you need one thread sleeping and another interrupting.,test_debt,lack_of_tests,"Thu, 27 Aug 2009 12:01:14 +0000","Thu, 12 May 2016 18:22:56 +0000","Wed, 28 Sep 2011 20:38:10 +0000",65867816,No tests as it's so hard to recreate this situation in a test; you need one thread sleeping and another interrupting.,0.05,0.05,negative
hadoop,6220,comment_14,+1 voting by self no tests as there is no easy way to generate the race condition.,test_debt,lack_of_tests,"Thu, 27 Aug 2009 12:01:14 +0000","Thu, 12 May 2016 18:22:56 +0000","Wed, 28 Sep 2011 20:38:10 +0000",65867816,+1 voting by self no tests as there is no easy way to generate the race condition.,-0.292,-0.292,negative
hadoop,6229,comment_2,Took Nikolas' suggestion. Created new class Turns out mapred already has such class. After this patch is committed I will remove mapred's one and make it use this one (from common). Also I will need to change HDFS to use this new exception instead of FileNotFound.,code_debt,low_quality_code,"Wed, 2 Sep 2009 00:40:39 +0000","Wed, 26 Apr 2017 08:44:34 +0000","Mon, 7 Sep 2009 13:27:32 +0000",478013,Took Nikolas' suggestion. Created new class FileAlreadyExestsException. Turns out mapred already has such class. After this patch is committed I will remove mapred's one and make it use this one (from common). Also I will need to change HDFS to use this new exception instead of FileNotFound.,0.125,0.1,neutral
hadoop,6229,comment_4,I believe the original code isn't defensive enough and there's a possibility for NPE to be thrown if {{Path f}} happens to be {{null}}. I'd add an extra check for this as the very first line of the method. Looks good otherwise.,code_debt,low_quality_code,"Wed, 2 Sep 2009 00:40:39 +0000","Wed, 26 Apr 2017 08:44:34 +0000","Mon, 7 Sep 2009 13:27:32 +0000",478013,I believe the original code isn't defensive enough and there's a possibility for NPE to be thrown if Path f happens to be null. I'd add an extra check for this as the very first line of the method. Looks good otherwise.,0.092,0.092,neutral
hadoop,6229,comment_8,"+1 This will be tested by the test being introduced in HDFS-303, right? Minor nit: we tend not to add serialVersionUID fields. (Any warnings in IDEs should be turned off.)",code_debt,low_quality_code,"Wed, 2 Sep 2009 00:40:39 +0000","Wed, 26 Apr 2017 08:44:34 +0000","Mon, 7 Sep 2009 13:27:32 +0000",478013,"+1 This will be tested by the test being introduced in HDFS-303, right? Minor nit: we tend not to add serialVersionUID fields. (Any warnings in IDEs should be turned off.)",-0.085,-0.085,neutral
hadoop,6229,comment_12,I've just committed this. Thanks Boris! Could you update the release note with the new behaviour please?,documentation_debt,outdated_documentation,"Wed, 2 Sep 2009 00:40:39 +0000","Wed, 26 Apr 2017 08:44:34 +0000","Mon, 7 Sep 2009 13:27:32 +0000",478013,I've just committed this. Thanks Boris! Could you update the release note with the new behaviour please?,0.3666666667,0.3666666667,neutral
hadoop,6279,comment_0,"Does not include unit tests - this is an existing open issue HADOOP-3634. This patch is a one-liner, though. Manual verification steps: 1) Launch daemon with configured in 2) Go to /metrics on web UI 3) Verify totalMemoryM is present",test_debt,lack_of_tests,"Tue, 22 Sep 2009 23:01:36 +0000","Tue, 24 Aug 2010 20:39:55 +0000","Thu, 8 Oct 2009 18:54:24 +0000",1367568,"Does not include unit tests - this is an existing open issue HADOOP-3634. This patch is a one-liner, though. Manual verification steps: 1) Launch daemon with NoEmitMetricsContext configured in hadoop-metrics.properties 2) Go to /metrics on web UI 3) Verify totalMemoryM is present",0.1666666667,0.125,neutral
hadoop,6364,comment_4,"# It might make sense to have options for the namenode and job tracker, something with different config names for each, so a single configuration can define both. # It could still be important for some people to say ""come up on all interfaces, localhost included"", which could imply it should be a list. # There's also the problem that the namenode has historically been very fussy about what hostname was used to refer to it",code_debt,low_quality_code,"Thu, 5 Nov 2009 17:30:50 +0000","Thu, 22 Mar 2012 06:02:21 +0000","Wed, 2 Nov 2011 18:22:22 +0000",62815892,"It might make sense to have options for the namenode and job tracker, something with different config names for each, so a single configuration can define both. It could still be important for some people to say ""come up on all interfaces, localhost included"", which could imply it should be a list. There's also the problem that the namenode has historically been very fussy about what hostname was used to refer to it",-0.1687777778,-0.1687777778,neutral
hadoop,6364,comment_1,"This sorta-kinda exists in the undocumented configuration slave.host.name, right?",documentation_debt,low_quality_documentation,"Thu, 5 Nov 2009 17:30:50 +0000","Thu, 22 Mar 2012 06:02:21 +0000","Wed, 2 Nov 2011 18:22:22 +0000",62815892,"This sorta-kinda exists in the undocumented configuration slave.host.name, right?",-0.02433333333,-0.02433333333,neutral
hadoop,6364,comment_2,I don't know. I've never tried it since it is undocumented. :),documentation_debt,low_quality_documentation,"Thu, 5 Nov 2009 17:30:50 +0000","Thu, 22 Mar 2012 06:02:21 +0000","Wed, 2 Nov 2011 18:22:22 +0000",62815892,I don't know. I've never tried it since it is undocumented.,0.1333333333,0.0,neutral
hadoop,6413,comment_3,Updated to JUnit 4 (annotation) style,code_debt,low_quality_code,"Sat, 5 Dec 2009 00:41:05 +0000","Tue, 24 Aug 2010 20:41:09 +0000","Tue, 15 Dec 2009 00:39:08 +0000",863883,Updated to JUnit 4 (annotation) style,0.0,0.0,neutral
hadoop,6457,comment_0,"Do note that users can be in more than one group, so hadoop.group.name should probably be hadoop.group.names, and be comma-delimited.",code_debt,low_quality_code,"Fri, 18 Dec 2009 10:32:04 +0000","Tue, 29 Jul 2014 21:24:15 +0000","Tue, 29 Jul 2014 21:24:15 +0000",145536731,"Do note that users can be in more than one group, so hadoop.group.name should probably be hadoop.group.names, and be comma-delimited.",-0.1,-0.1,neutral
hadoop,6584,comment_8,Small patch to fix javadoc warnings introduced into Y20 branch. Not to be committed.,code_debt,low_quality_code,"Mon, 22 Feb 2010 02:12:58 +0000","Mon, 12 Dec 2011 06:20:06 +0000","Sat, 3 Jul 2010 00:03:05 +0000",11310607,Small patch to fix javadoc warnings introduced into Y20 branch. Not to be committed.,-0.55,-0.55,negative
hadoop,6632,comment_3,A minor fix for the MR side to reuse filesystem handles,code_debt,low_quality_code,"Sun, 14 Mar 2010 04:28:53 +0000","Mon, 12 Dec 2011 06:19:10 +0000","Tue, 20 Jul 2010 00:51:35 +0000",11046162,A minor fix for the MR side to reuse filesystem handles,0.0,0.0,neutral
hadoop,6658,comment_5,New patch which excludes annotations that are marked Private or LimitedPrivate. Also removes Javadoc and RAT warnings.,documentation_debt,low_quality_documentation,"Wed, 24 Mar 2010 04:14:18 +0000","Wed, 28 Mar 2018 20:17:32 +0000","Thu, 22 Apr 2010 20:48:54 +0000",2565276,New patch which excludes annotations that are marked Private or LimitedPrivate. Also removes Javadoc and RAT warnings.,-0.3,-0.3,neutral
hadoop,6727,comment_1,"Just updating throws clauses and java docs, hence the lack of tests.",code_debt,low_quality_code,"Tue, 27 Apr 2010 23:42:05 +0000","Tue, 24 Aug 2010 20:42:57 +0000","Mon, 3 May 2010 17:39:29 +0000",496644,"Just updating throws clauses and java docs, hence the lack of tests.",-0.4,-0.4,negative
hadoop,6730,comment_6,"Review of the patch: * Lines 47-52: These notes aren't necessary since the test writers should know about @Before and @After. Also, the provided methods (setUp() and tearDown()) conflict with the provided names. * Lines 55 & 56: These values should be all capitalized and final. Alternatively, it may be good to provide these values via functions so that implementing classes may override as needed. * Line 64: catch should be on the same line as closing brace, per our coding style * Line 70: fc should not be declared static. It is the responsibility of each implementing class to provide an instance of it. Also, fc should probably be protected * Line 91: Please provide assert message for assertion failure * Since this is a test that the copy method worked, we should prove it by comparing the contents of the copied file with the original. * Lines 135-138: This code: is a no-op and should be removed. Nits: * Rather than Assert.assertTrue() you can do a static import of * Line 57: Remove extra line * Line 47: ""Since this a junit 4"" ->""Since this is a junit 4 test""",code_debt,low_quality_code,"Wed, 28 Apr 2010 18:50:09 +0000","Mon, 12 Dec 2011 06:19:03 +0000","Sat, 1 May 2010 21:09:36 +0000",267567,"Review of the patch: Lines 47-52: These notes aren't necessary since the test writers should know about @Before and @After. Also, the provided methods (setUp() and tearDown()) conflict with the provided names. Lines 55 & 56: These values should be all capitalized and final. Alternatively, it may be good to provide these values via functions so that implementing classes may override as needed. Line 64: catch should be on the same line as closing brace, per our coding style Line 70: fc should not be declared static. It is the responsibility of each implementing class to provide an instance of it. Also, fc should probably be protected Line 91: Please provide assert message for assertion failure Since this is a test that the copy method worked, we should prove it by comparing the contents of the copied file with the original. Lines 135-138: This code: is a no-op and should be removed. Nits: Rather than Assert.assertTrue() you can do a static import of org.junit.Assert. {assertTrue|*} . Line 57: Remove extra line Line 47: ""Since this a junit 4"" ->""Since this is a junit 4 test""",0.17135,0.1223928571,neutral
hadoop,6730,comment_8,Updated patch with changes. 1. Removed unnecessary tearDown method. 2. Changed file read write to bytes from String.,code_debt,dead_code,"Wed, 28 Apr 2010 18:50:09 +0000","Mon, 12 Dec 2011 06:19:03 +0000","Sat, 1 May 2010 21:09:36 +0000",267567,Updated patch with changes. 1. Removed unnecessary tearDown method. 2. Changed file read write to bytes from String.,0.0,0.0,neutral
hadoop,6730,comment_9,Updated patch. Removed static from fc initialization. Removed invalid comments.,documentation_debt,low_quality_documentation,"Wed, 28 Apr 2010 18:50:09 +0000","Mon, 12 Dec 2011 06:19:03 +0000","Sat, 1 May 2010 21:09:36 +0000",267567,Updated patch. Removed static from fc initialization. Removed invalid comments.,-0.2916666667,-0.2916666667,neutral
hadoop,6730,comment_3,"Hey Ravi, per the previous comment I left the getFileStatus outside the try block so the is now swallowed silently and is thrown up to copy (whose API wants This deserves a comment. The patch just has util classes - perhaps you forgot to add the test class in your diff?",test_debt,lack_of_tests,"Wed, 28 Apr 2010 18:50:09 +0000","Mon, 12 Dec 2011 06:19:03 +0000","Sat, 1 May 2010 21:09:36 +0000",267567,"Hey Ravi, per the previous comment I left the getFileStatus outside the try block so the FileNotFoundException is now swallowed silently and is thrown up to copy (whose API wants FileNotFoundThrown). This deserves a comment. The patch just has util classes - perhaps you forgot to add the test class in your diff?",-0.2,-0.1333333333,neutral
hadoop,6794,comment_2,"The following need to be added to the patch. - configuration property is not present in common-project but is present and needed in mapreduce. - Nine settings for 'Job Summary Appender' are present and needed in mapreduce project but are not there in common project. - Two settings for mapreduce task logs are present and needed in mapreduce project but are not there in common project. -- -- - One more thing: copying this file over from mapreduce will not work because six settings for 'Security appender' are in common but not present in the mapreduce project's log4j.properties file. May be we should also split both the above files per project. But that may be long term, will open a separate ticket if you agree. Other issues - Minor: bin/rcc still refers to hadoop-core at one place. - when i run `ant binary` in hdfs bin directory isn't getting copied into the packaged binary, neither are the hdfs script files copied. - when i run `ant binary` in mapreduce, files aren't getting copied into the packaged bin directory Verified the rest of the changes in all the three patches, they look good. Still couldn't get the cluster up with different installation directories, will be trying in the meanwhile..",design_debt,non-optimal_design,"Mon, 31 May 2010 21:42:07 +0000","Tue, 24 Aug 2010 20:43:17 +0000","Thu, 10 Jun 2010 23:48:08 +0000",871561,"The following need to be added to the patch. conf/hadoop-policy.xml.template: security.admin.operations.protocol.acl configuration property is not present in common-project but is present and needed in mapreduce. conf/log4j.properties Nine settings for 'Job Summary Appender' are present and needed in mapreduce project but are not there in common project. Two settings for mapreduce task logs are present and needed in mapreduce project but are not there in common project. hadoop.tasklog.iscleanup=false log4j.appender.TLA.isCleanup=${hadoop.tasklog.iscleanup} One more thing: copying this file over from mapreduce will not work because six settings for 'Security appender' are in common but not present in the mapreduce project's log4j.properties file. May be we should also split both the above files per project. But that may be long term, will open a separate ticket if you agree. Other issues Minor: bin/rcc still refers to hadoop-core at one place. when i run `ant binary` in hdfs bin directory isn't getting copied into the packaged binary, neither are the hdfs script files copied. when i run `ant binary` in mapreduce, files aren't getting copied into the packaged bin directory Verified the rest of the changes in all the three patches, they look good. Still couldn't get the cluster up with different installation directories, will be trying in the meanwhile..",-0.04626190476,-0.01117307692,neutral
hadoop,6826,comment_8,"hi amar, this API never made it to any ""release"" of Hadoop. Better to remove it now than to ship a release with a bad API.",code_debt,low_quality_code,"Tue, 15 Jun 2010 21:21:00 +0000","Thu, 2 May 2013 02:29:32 +0000","Tue, 22 Jun 2010 04:15:36 +0000",543276,"hi amar, this API never made it to any ""release"" of Hadoop. Better to remove it now than to ship a release with a bad API.",-0.025,-0.025,negative
hadoop,6869,comment_3,"- Looks good with one nit. I'd suggest to have methods with fewer arguments simply call ones with more args. E.g. in this case instead of duplicating their exact implementation. You have done similar in already. - Also, in you already have a ref to the filesystem so you can just call its {{createFile}} methods instead of implementing your own logic just call to have new file created for you. - also you are writing some content into this file which seems to be an inadvertent side-effect. Doesn't look like a good idea.",code_debt,low_quality_code,"Wed, 21 Jul 2010 17:05:23 +0000","Mon, 27 Sep 2010 04:14:15 +0000","Wed, 28 Jul 2010 03:23:10 +0000",555467,"Looks good with one nit. I'd suggest to have methods with fewer arguments simply call ones with more args. E.g. in this case instead of duplicating their exact implementation. You have done similar in DaemonProtocolAspect already. Also, in DaemonProtocolAspect you already have a ref to the filesystem so you can just call its createFile methods instead of implementing your own logic just call fs.createNewFile() to have new file created for you. also you are writing some content into this file which seems to be an inadvertent side-effect. Doesn't look like a good idea.",0.2126,0.132875,neutral
hadoop,6869,comment_5,"Hi Cos, I do agree with you. However my requirement is creating the user define files and folders in task attempt id folder with different kind of permissions while job is running. Later while cleaning up, i just wanted make sure whether all the user defined files and folders are cleaned up or not for that job. So that I have defined the above methods for fulfilling my requirement. As my understand, i don't think so, the FileSystem is providing the no such straight forward and createNewFolder) like you mentioned. In the four methods, the first method creates the files with full permission.Suppose if user wants to give his own permissions while creating file in that case he can use the second method.Same way for folder creation also. I will remove this part in the code.",code_debt,dead_code,"Wed, 21 Jul 2010 17:05:23 +0000","Mon, 27 Sep 2010 04:14:15 +0000","Wed, 28 Jul 2010 03:23:10 +0000",555467,"Hi Cos, I do agree with you. However my requirement is creating the user define files and folders in task attempt id folder with different kind of permissions while job is running. Later while cleaning up, i just wanted make sure whether all the user defined files and folders are cleaned up or not for that job. So that I have defined the above methods for fulfilling my requirement. As my understand, i don't think so, the FileSystem is providing the no such straight forward methods(createNewFile and createNewFolder) like you mentioned. In the four methods, the first method creates the files with full permission.Suppose if user wants to give his own permissions while creating file in that case he can use the second method.Same way for folder creation also. also you are writing some content into this file which seems to be an inadvertent side-effect. Doesn't look like a good idea I will remove this part in the code.",0.1111111111,0.0724,neutral
hadoop,6869,comment_6,This reasoning sounds like an argument to move this functionality up the stack to the MR cluster concrete implementation. But let's suppose they are useful enough and let them be in the Common. I do understand that. What I have said is that you need a generic enough method which does most of needed functionality (i.e. create a file with specified permissions) and a wrapper around it which creates a file with all permissions. The implementation of the latter is essentially a call to the former with *all* permissions being passed. This is already done in this very patch for the implementation of methods. Look at your own code.,code_debt,low_quality_code,"Wed, 21 Jul 2010 17:05:23 +0000","Mon, 27 Sep 2010 04:14:15 +0000","Wed, 28 Jul 2010 03:23:10 +0000",555467,"my requirement is creating the user define files and folders in task attempt id folder with different kind of permissions while job is running This reasoning sounds like an argument to move this functionality up the stack to the MR cluster concrete implementation. But let's suppose they are useful enough and let them be in the Common. In the four methods, the first method creates the files with full permission.Suppose if user wants to give his own permissions while creating file in that case he can use the second method.Same way for folder creation also. I do understand that. What I have said is that you need a generic enough method which does most of needed functionality (i.e. create a file with specified permissions) and a wrapper around it which creates a file with all permissions. The implementation of the latter is essentially a call to the former with all permissions being passed. This is already done in this very patch for the implementation of createFolder(..) methods. Look at your own code.",0.2142857143,0.15,neutral
hadoop,6869,comment_4,In continuation of the comment above: since there's {{getFS()}} method which exposes filesystem API from a particular daemon then creating additional 4 wrappers around the standard API doesn't make much sense to me. These wrappers are too obvious and then will simply crowd Herriot API without adding much value.,design_debt,non-optimal_design,"Wed, 21 Jul 2010 17:05:23 +0000","Mon, 27 Sep 2010 04:14:15 +0000","Wed, 28 Jul 2010 03:23:10 +0000",555467,In continuation of the comment above: since there's getFS() method which exposes filesystem API from a particular daemon then creating additional 4 wrappers around the standard API doesn't make much sense to me. These wrappers are too obvious and then will simply crowd Herriot API without adding much value.,-0.1,-0.1,neutral
hadoop,6885,comment_4,Updated the patch to fix the IPC javadoc warnings. The earlier warnings are seen when using javadoc-dev. Both targets build cleanly on trunk with this patch. Thanks Jakob.,documentation_debt,low_quality_documentation,"Wed, 28 Jul 2010 01:46:25 +0000","Mon, 12 Dec 2011 06:19:06 +0000","Wed, 18 Aug 2010 21:51:40 +0000",1886715,Updated the patch to fix the IPC javadoc warnings. The earlier warnings are seen when using javadoc-dev. Both targets build cleanly on trunk with this patch. Thanks Jakob.,-0.2,-0.2,neutral
hadoop,6925,comment_5,"Forgot to mention that I verified TestCodec passed on trunk and the merge to branch-0.21. Previous comment has a typo, meant *branch-0.21* not branch-0.20.",documentation_debt,low_quality_documentation,"Tue, 24 Aug 2010 22:09:57 +0000","Mon, 12 Dec 2011 06:18:39 +0000","Tue, 24 Aug 2010 22:48:49 +0000",2332,"Forgot to mention that I verified TestCodec passed on trunk and the merge to branch-0.21. Previous comment has a typo, meant branch-0.21 not branch-0.20.",0.03,0.03,neutral
hadoop,6925,comment_0,Patch fixes the read() implementation to correctly mask with 0xff before upcasting to int. Also augments the unit test to check the single-byte read() function - the new test fails before this patch.,test_debt,low_coverage,"Tue, 24 Aug 2010 22:09:57 +0000","Mon, 12 Dec 2011 06:18:39 +0000","Tue, 24 Aug 2010 22:48:49 +0000",2332,Patch fixes the read() implementation to correctly mask with 0xff before upcasting to int. Also augments the unit test to check the single-byte read() function - the new test fails before this patch.,0.05,0.05,neutral
hadoop,6936,comment_5,"Fixed the FAQ links. The HowToConfigure page is highly outdated. I do not know if it still serves a purpose beyond the tutorial (which explains how to too). It could be possibly set up to redirect to a current documentation. But as Nicholas notes, this is all doable by anyone signed into the Wiki (free account signups, just gotta enter captchas). Filing a JIRA for this is not required.",documentation_debt,outdated_documentation,"Wed, 1 Sep 2010 19:38:57 +0000","Sun, 12 Jun 2011 18:55:25 +0000","Sun, 12 Jun 2011 18:52:36 +0000",24534819,"Fixed the FAQ links. The HowToConfigure page is highly outdated. I do not know if it still serves a purpose beyond the tutorial (which explains how to too). It could be possibly set up to redirect to a current documentation. But as Nicholas notes, this is all doable by anyone signed into the Wiki (free account signups, just gotta enter captchas). Filing a JIRA for this is not required.",0.1051666667,0.1051666667,negative
hadoop,6988,comment_1,"At a minimum, -1 on the environment variable. Shouldn't HADOOP_CLIENT_OPTS be sufficient for passing extra -D params? We have an abundance of environment variables that users can't handle as it is.",code_debt,low_quality_code,"Mon, 4 Oct 2010 23:28:30 +0000","Tue, 24 May 2011 19:12:40 +0000","Tue, 24 May 2011 19:12:40 +0000",20029450,"At a minimum, -1 on the environment variable. Shouldn't HADOOP_CLIENT_OPTS be sufficient for passing extra -D params? We have an abundance of environment variables that users can't handle as it is.",-0.2083333333,-0.2083333333,negative
hadoop,6988,comment_3,"Grr. I really wish we'd stop creating pet environment variables. This is ridiculous. Can we remove this env var as part of this JIRA? What takes precendence the env var or the jobconf setting? What is the interaction? If the answer is ""we have to look at the code"" then we've failed. It makes much more sense to have to support a comma delimited set (to be consistent with the rest of the job conf. Never mind that colon is the traditional directory delimiter on OS X.)",code_debt,low_quality_code,"Mon, 4 Oct 2010 23:28:30 +0000","Tue, 24 May 2011 19:12:40 +0000","Tue, 24 May 2011 19:12:40 +0000",20029450,"Grr. I really wish we'd stop creating pet environment variables. This is ridiculous. Can we remove this env var as part of this JIRA? What takes precendence the env var or the mapreduce.job.credentials.binary jobconf setting? What is the interaction? If the answer is ""we have to look at the code"" then we've failed. It makes much more sense to have mapreduce.job.credentials.binary to support a comma delimited set (to be consistent with the rest of the job conf. Never mind that colon is the traditional directory delimiter on OS X.)",-0.06666666667,-0.04,negative
hadoop,6988,comment_4,"The environment variable should *not* be multi-valued. It is used to communicate the job's token store to sub-processes of the task. Since a task can't be in more than one job, there isn't any need. What is the use case for having multiple token files? The rest of the lists use commas, so this should be the same. Wouldn't it be easier to write a tool that allows you to combine multiple token files together into a single one?",code_debt,low_quality_code,"Mon, 4 Oct 2010 23:28:30 +0000","Tue, 24 May 2011 19:12:40 +0000","Tue, 24 May 2011 19:12:40 +0000",20029450,"The environment variable should not be multi-valued. It is used to communicate the job's token store to sub-processes of the task. Since a task can't be in more than one job, there isn't any need. What is the use case for having multiple token files? The rest of the lists use commas, so this should be the same. Wouldn't it be easier to write a tool that allows you to combine multiple token files together into a single one?",-0.08333333333,-0.08333333333,negative
hadoop,6988,comment_9,"I'm sorry, but it is completely short sighted to have a single use env var like this. If we need to modify the tasks environment for something else, are we going to introduce another environment variable? How many are too many?",design_debt,non-optimal_design,"Mon, 4 Oct 2010 23:28:30 +0000","Tue, 24 May 2011 19:12:40 +0000","Tue, 24 May 2011 19:12:40 +0000",20029450,"I'm sorry, but it is completely short sighted to have a single use env var like this. If we need to modify the tasks environment for something else, are we going to introduce another environment variable? How many are too many?",-0.15,-0.15,negative
hadoop,7057,comment_1,For apparent reasons - typo correction - patch doesn't provide any tests. I'm going to commit this for it is trivial.,documentation_debt,low_quality_documentation,"Wed, 1 Dec 2010 00:59:34 +0000","Tue, 15 Nov 2011 00:50:50 +0000","Wed, 1 Dec 2010 01:44:24 +0000",2690,For apparent reasons - typo correction - patch doesn't provide any tests. I'm going to commit this for it is trivial.,0.1,0.1,negative
hadoop,7098,comment_0,"+1 The lack of a definition looks like an oversight (these variables were introduced in HADOOP-2551). is already honoured by bin/mapred, so no changes are needed there. Bernd, have you tested this manually? There's currently no easy way to write an automated test for this change.",test_debt,lack_of_tests,"Tue, 11 Jan 2011 17:24:40 +0000","Tue, 15 Nov 2011 00:50:20 +0000","Fri, 4 Mar 2011 05:37:08 +0000",4450348,"+1 The lack of a HADOOP_TASKTRACKER_OPTS definition looks like an oversight (these variables were introduced in HADOOP-2551). HADOOP_TASKTRACKER_OPTS is already honoured by bin/mapred, so no changes are needed there. Bernd, have you tested this manually? There's currently no easy way to write an automated test for this change.",0.2115,0.2115,neutral
hadoop,7098,comment_1,"I did test this manually indeed. The patch is in active use in my env. There are things like which might help setting up a test, but just for this small improvement it seem like overkill.",test_debt,lack_of_tests,"Tue, 11 Jan 2011 17:24:40 +0000","Tue, 15 Nov 2011 00:50:20 +0000","Fri, 4 Mar 2011 05:37:08 +0000",4450348,"I did test this manually indeed. The patch is in active use in my env. There are things like http://code.google.com/p/jbash/ which might help setting up a test, but just for this small improvement it seem like overkill.",0.2236666667,0.2236666667,neutral
hadoop,7118,comment_3,"+1 LGTM. I would also convert the test to JUnitv4 but one can't have everything, I guess ;)",test_debt,expensive_tests,"Tue, 25 Jan 2011 22:51:30 +0000","Mon, 12 Dec 2011 06:19:06 +0000","Wed, 26 Jan 2011 07:08:28 +0000",29818,"+1 LGTM. I would also convert the test to JUnitv4 but one can't have everything, I guess",0.2,0.0,neutral
hadoop,7154,comment_13,"- I know this bug is pretty old, but do you mind doing me the favor of explaining this statement: Perhaps I am revealing too much of my naivety, but what issues the vmem size presents nor the reasons are necessarily obvious to me. The reason I ask is not directly related to this JIRA nor even Hadoop. I am just trying to learn more about the glibc change and its potential impacts. I've noticed high virtual memory size in another Java-based application (a Zabbix agent process if you care) and I'm struggling slightly to decide if I should worry about it. presents what appears to me to be a rational explanation as to why the virtual memory size shouldn't matter too much. I could push on Zabbix to implement a change to set MALLOC_ARENA_MAX and I feel relatively confident the change wouldn't hurt anything but I'm not sure it would actually help anything either. The Zabbix agent appears to be performing fine and the only reason I noticed the high vmem size was because someone pointed me to this JIRA and I did an audit looking for processes with virtual memory sizes that looked suspicious. I guess the biggest problem I have with the affect the glibc change has on reported vmem size is that it seems to make vmem size meaningless where previously you could get some idea about what a process was doing from its vmem size but your comment suggests maybe there are other things I should be concerned about as well. If you could share those with me I would greatly appreciate it and perhaps others will benefit as well. Thanks!",defect_debt,uncorrected_known_defects,"Fri, 25 Feb 2011 00:48:56 +0000","Tue, 21 Apr 2015 21:28:17 +0000","Tue, 8 Mar 2011 23:51:52 +0000",1033376,"tlipcon - I know this bug is pretty old, but do you mind doing me the favor of explaining this statement: We've observed a DN process using 14GB of vmem for only 300M of resident set. This causes all kinds of nasty issues for obvious reasons. Perhaps I am revealing too much of my naivety, but what issues the vmem size presents nor the reasons are necessarily obvious to me. The reason I ask is not directly related to this JIRA nor even Hadoop. I am just trying to learn more about the glibc change and its potential impacts. I've noticed high virtual memory size in another Java-based application (a Zabbix agent process if you care) and I'm struggling slightly to decide if I should worry about it. http://journal.siddhesh.in/posts/malloc-per-thread-arenas-in-glibc.html presents what appears to me to be a rational explanation as to why the virtual memory size shouldn't matter too much. I could push on Zabbix to implement a change to set MALLOC_ARENA_MAX and I feel relatively confident the change wouldn't hurt anything but I'm not sure it would actually help anything either. The Zabbix agent appears to be performing fine and the only reason I noticed the high vmem size was because someone pointed me to this JIRA and I did an audit looking for processes with virtual memory sizes that looked suspicious. I guess the biggest problem I have with the affect the glibc change has on reported vmem size is that it seems to make vmem size meaningless where previously you could get some idea about what a process was doing from its vmem size but your comment suggests maybe there are other things I should be concerned about as well. If you could share those with me I would greatly appreciate it and perhaps others will benefit as well. Thanks!",0.05465,-0.02494444444,neutral
hadoop,7154,comment_10,"I was very confused by this discussion and dug into it a bit more; here's what I learned. The takeaway is, ARENA_MAX=4 is a win for Java apps. # Java doesn't use {{malloc()}} for object allocations; instead it uses its own directly {{mmap()}}ed arenas. # however, a few things such as direct {{ByteBuffer}}s do end up calling malloc on arbitrary threads. There's not much thread locality in the use of such buffers. As a result, the glibc arena allocator is using a lot of VSS to optimize a codepath that's not very hot. So decreasing the number of arenas is a win, overall, even though it will increase contention (the malloc arena locks are pretty cold so this doesn't matter much) and potentially increase cache churn. But fewer arenas should decrease total cache footprint by increasing reuse.",design_debt,non-optimal_design,"Fri, 25 Feb 2011 00:48:56 +0000","Tue, 21 Apr 2015 21:28:17 +0000","Tue, 8 Mar 2011 23:51:52 +0000",1033376,"I was very confused by this discussion and dug into it a bit more; here's what I learned. The takeaway is, ARENA_MAX=4 is a win for Java apps. Java doesn't use malloc() for object allocations; instead it uses its own directly {{mmap()}}ed arenas. however, a few things such as direct {{ByteBuffer}}s do end up calling malloc on arbitrary threads. There's not much thread locality in the use of such buffers. As a result, the glibc arena allocator is using a lot of VSS to optimize a codepath that's not very hot. So decreasing the number of arenas is a win, overall, even though it will increase contention (the malloc arena locks are pretty cold so this doesn't matter much) and potentially increase cache churn. But fewer arenas should decrease total cache footprint by increasing reuse.",0.131125,0.131125,neutral
hadoop,7323,comment_1,This looks good. The only nit I noticed is that the javadoc on codecsByName in is incorrect.,documentation_debt,low_quality_documentation,"Mon, 23 May 2011 22:55:09 +0000","Wed, 8 Jun 2011 11:15:35 +0000","Tue, 7 Jun 2011 18:32:27 +0000",1280238,This looks good. The only nit I noticed is that the javadoc on codecsByName in CompressionCodecFactory is incorrect.,0.388,0.388,positive
hadoop,7375,comment_0,AFS#getFileStatus is now called instead of which means fixRelativePart is no longer used to make the path absolute in FileContext relative to the working dir before passing the path to AFS right? Nit: lines 568 and 2231 need indenting. Otherwise looks great.,code_debt,low_quality_code,"Fri, 10 Jun 2011 05:54:28 +0000","Tue, 15 Nov 2011 00:50:44 +0000","Sun, 12 Jun 2011 01:36:27 +0000",157319,"AFS#getFileStatus is now called instead of FileContext#getFileStatus, which means fixRelativePart is no longer used to make the path absolute in FileContext relative to the working dir before passing the path to AFS right? Nit: lines 568 and 2231 need indenting. Otherwise looks great.",0.01216666667,0.01216666667,neutral
hadoop,7375,comment_4,"Ah, never mind, I thought it was previously calling FC#getFileStatus. +1 feel free to address the nits directly in the commit since it's just indentation.",code_debt,low_quality_code,"Fri, 10 Jun 2011 05:54:28 +0000","Tue, 15 Nov 2011 00:50:44 +0000","Sun, 12 Jun 2011 01:36:27 +0000",157319,"Ah, never mind, I thought it was previously calling FC#getFileStatus. +1 feel free to address the nits directly in the commit since it's just indentation.",0.1,0.1,neutral
hadoop,7483,comment_1,"Well, as allen said on HADOOP-6605, ""My technical objection is that there is a high likelihood of getting this wrong."" There's a serious risk of the JAVA_HOME autodetector becoming a major maintenance mess. This is the beginning.",design_debt,non-optimal_design,"Mon, 25 Jul 2011 20:05:01 +0000","Thu, 17 Mar 2016 16:41:40 +0000","Thu, 17 Mar 2016 16:41:40 +0000",146608599,"Well, as allen said on HADOOP-6605, ""My technical objection is that there is a high likelihood of getting this wrong."" There's a serious risk of the JAVA_HOME autodetector becoming a major maintenance mess. This is the beginning.",0.06494444444,0.06494444444,negative
hadoop,7620,comment_2,"I agree with Alejandro. Build turnaround time is a big pain, and especially on non-SSD, or even worse, NFS, the javadoc build takes quite a while. Lots of iops to create all those .html files. To check that the javadoc changes don't introduce issues, we have test-patch.",design_debt,non-optimal_design,"Fri, 9 Sep 2011 18:01:17 +0000","Sun, 26 Apr 2015 01:26:52 +0000","Sun, 26 Apr 2015 01:26:52 +0000",114420335,"I agree with Alejandro. Build turnaround time is a big pain, and especially on non-SSD, or even worse, NFS, the javadoc build takes quite a while. Lots of iops to create all those .html files. To check that the javadoc changes don't introduce issues, we have test-patch.",-0.03666666667,-0.03666666667,negative
hadoop,7684,comment_7,rpm package doesnt seem to include the historyserver and secondarynamenode init scripts. looks like and init.d script should be added to the list of init.d scripts in the spec file.,code_debt,low_quality_code,"Tue, 27 Sep 2011 01:02:35 +0000","Wed, 19 Oct 2011 00:26:06 +0000","Tue, 4 Oct 2011 00:45:09 +0000",603754,rpm package doesnt seem to include the historyserver and secondarynamenode init scripts. looks like hadoop-historyserver and hadoop-secondarynamnode init.d script should be added to the list of init.d scripts in the spec file.,0.0,0.0,neutral
hadoop,7971,comment_6,Updated with better error msg and exit status .,code_debt,low_quality_code,"Thu, 12 Jan 2012 20:16:35 +0000","Mon, 5 Mar 2012 02:48:49 +0000","Tue, 17 Jan 2012 07:01:11 +0000",384276,Updated with better error msg and exit status .,0.05,0.05,neutral
hadoop,7974,comment_0,Patch that uses a get-parent call instead of hacking with strings.,code_debt,low_quality_code,"Fri, 13 Jan 2012 23:50:17 +0000","Mon, 5 Mar 2012 02:49:12 +0000","Wed, 8 Feb 2012 02:09:28 +0000",2168351,Patch that uses a get-parent call instead of hacking with strings.,0.0,0.0,neutral
hadoop,7985,comment_0,In * Touchz.java should be renamed to Touch.java to avoid compilation everytime. * Record IO generated test files which were compiled everytime. We could modify RccTask.java to doCompile only when sourceFile is newer than the destination file These are just 2 seconds of the 24 seconds needed to build hadoop-common. So obviously not substantial. Will keep looking. I'm beginning to think running from JARs might not be the best idea to have a quick dev cycle. Maybe I should try running from the target/classes directories and skip building the jar altogether.,design_debt,non-optimal_design,"Thu, 19 Jan 2012 21:14:41 +0000","Thu, 15 Aug 2013 19:02:33 +0000","Thu, 15 Aug 2013 19:02:33 +0000",49585672,In hadoop-common-project/hadoop-common: Touchz.java should be renamed to Touch.java to avoid compilation everytime. Record IO generated test files which were compiled everytime. We could modify RccTask.java to doCompile only when sourceFile is newer than the destination file These are just 2 seconds of the 24 seconds needed to build hadoop-common. So obviously not substantial. Will keep looking. I'm beginning to think running from JARs might not be the best idea to have a quick dev cycle. Maybe I should try running from the target/classes directories and skip building the jar altogether.,0.0475,0.0475,neutral
hadoop,7985,comment_1,all protobuf compilation sections in pom.xml can be modified like this I am now trying to optimize mvn -P-cbuild -DskipTests -X compile Even here jsps are being compiled into java files every time unnecessarily,design_debt,non-optimal_design,"Thu, 19 Jan 2012 21:14:41 +0000","Thu, 15 Aug 2013 19:02:33 +0000","Thu, 15 Aug 2013 19:02:33 +0000",49585672,all protobuf compilation sections in pom.xml can be modified like this I am now trying to optimize mvn -P-cbuild -Dmaven.javadoc.skip -DskipTests -X compile Even here jsps are being compiled into java files every time unnecessarily,-0.25,-0.125,neutral
hadoop,7985,comment_3,Just a lil' bit of improvement. Still a long ways to go,design_debt,non-optimal_design,"Thu, 19 Jan 2012 21:14:41 +0000","Thu, 15 Aug 2013 19:02:33 +0000","Thu, 15 Aug 2013 19:02:33 +0000",49585672,Just a lil' bit of improvement. Still a long ways to go,0.46625,0.46625,neutral
hadoop,8084,comment_2,No I haven't done benchmarks. But it seemed to make sense to avoid a copy if it could be avoided.,code_debt,slow_algorithm,"Fri, 17 Feb 2012 01:31:13 +0000","Wed, 23 May 2012 20:15:43 +0000","Tue, 21 Feb 2012 05:20:49 +0000",359376,No I haven't done benchmarks. But it seemed to make sense to avoid a copy if it could be avoided.,-0.1,-0.1,neutral
hadoop,8100,comment_0,This JIRA applies to trunk as well. Would you please provide a PATCH for trunk? On the patch: For the key names I'd remove 'authentication' as these properties can be used for different things than authentication. Testcase is missing.,test_debt,lack_of_tests,"Wed, 22 Feb 2012 21:56:04 +0000","Thu, 17 Jul 2014 15:04:14 +0000","Thu, 17 Jul 2014 15:04:14 +0000",75661690,This JIRA applies to trunk as well. Would you please provide a PATCH for trunk? On the patch: For the key names I'd remove 'authentication' as these properties can be used for different things than authentication. Testcase is missing.,0.10775,0.10775,neutral
hadoop,8124,comment_0,"- removes Syncable.sync(); - removes the deprecated - removes unnecessary ""throws IOException"" declarations.",code_debt,dead_code,"Thu, 1 Mar 2012 00:09:47 +0000","Thu, 12 May 2016 18:22:41 +0000","Thu, 1 Mar 2012 23:53:06 +0000",85399,"c8124_20120229.patch: removes Syncable.sync(); removes the deprecated FSDataOutputStream(out); removes unnecessary ""throws IOException"" declarations.",0.25,0.1666666667,neutral
hadoop,8209,comment_3,"Patch looks pretty good to me, Eli. Just a few small comments: # Not obvious to me why we have these static version methods in the Storage class, which themselves just delegate to static methods of the VersionInfo class. # Recommend adding additional detail to the AssertionErrors, including the revisions and versions that didn't match. # Recommend adding an explanation to the DN log message about why the communication is being allowed, e.g.: ""... because versions match exactly ('"" + version + ""') and is enabled."" Ditto for TT. # Similarly the log message explaining why communication isn't being allowed might mention whether the check failed because of strict revision checking, or relaxed version checking. # Why call the new method ""getInfoVersion"" in JobTracker? getVersion, as was done in Storage, seems to make more sense to me. # In I don't think you actually test that different revisions are still disallowed by default, since you change both the revision and version simultaneously in the test.",code_debt,low_quality_code,"Sun, 25 Mar 2012 22:17:38 +0000","Wed, 24 Oct 2012 19:12:09 +0000","Thu, 12 Apr 2012 22:06:37 +0000",1554539,"Patch looks pretty good to me, Eli. Just a few small comments: Not obvious to me why we have these static version methods in the Storage class, which themselves just delegate to static methods of the VersionInfo class. Recommend adding additional detail to the AssertionErrors, including the revisions and versions that didn't match. Recommend adding an explanation to the DN log message about why the communication is being allowed, e.g.: ""... because versions match exactly ('"" + version + ""') and hadoop.relaxed.worker.version.check is enabled."" Ditto for TT. Similarly the log message explaining why communication isn't being allowed might mention whether the check failed because of strict revision checking, or relaxed version checking. Why call the new method ""getInfoVersion"" in JobTracker? getVersion, as was done in Storage, seems to make more sense to me. In TestTaskTrackerVersionCheck#testDefaultVersionCheck, I don't think you actually test that different revisions are still disallowed by default, since you change both the revision and version simultaneously in the test.",0.1753888889,0.1521923077,neutral
hadoop,8209,comment_5,"I reviewed the delta, and it largely looks good. One tiny nit: looks like you variously spelled the word ""disallow"" either as ""dissallow"" or ""dissalow"". Patch looks good otherwise - +1. Please do also run the branch-1 test suite on the latest patch before committing.",code_debt,low_quality_code,"Sun, 25 Mar 2012 22:17:38 +0000","Wed, 24 Oct 2012 19:12:09 +0000","Thu, 12 Apr 2012 22:06:37 +0000",1554539,"I reviewed the delta, and it largely looks good. One tiny nit: looks like you variously spelled the word ""disallow"" either as ""dissallow"" or ""dissalow"". Patch looks good otherwise - +1. Please do also run the branch-1 test suite on the latest patch before committing.",0.438,0.438,neutral
hadoop,8209,comment_6,"Thanks ATM. Will fix the spelling misstake, running the full suite now.",documentation_debt,low_quality_documentation,"Sun, 25 Mar 2012 22:17:38 +0000","Wed, 24 Oct 2012 19:12:09 +0000","Thu, 12 Apr 2012 22:06:37 +0000",1554539,"Thanks ATM. Will fix the spelling misstake, running the full suite now.",0.2,0.2,neutral
hadoop,8210,comment_8,"I think it's worth changing the return type of this function to LinkedHashSet, so it's clear that the ordering here is on purpose. Perhaps also add a comment here saying something like: - Nits: please un-abbreviate ""first"" for better readability. Also, ""e.g."" instead of ""Eg."" -- or just say ""For example"" - I think it's more idiomatic to just put the postincrement inside the []s - - there's a small spurious whitespace change in NetUtils.java - looks like the pom change is still in this patch (redundant with HADOOP-8211)",code_debt,low_quality_code,"Mon, 26 Mar 2012 00:10:15 +0000","Thu, 2 May 2013 02:29:51 +0000","Mon, 2 Apr 2012 19:07:17 +0000",673022,"I think it's worth changing the return type of this function to LinkedHashSet, so it's clear that the ordering here is on purpose. Perhaps also add a comment here saying something like: Nits: please un-abbreviate ""first"" for better readability. Also, ""e.g."" instead of ""Eg.""  or just say ""For example"" I think it's more idiomatic to just put the postincrement inside the []s there's a small spurious whitespace change in NetUtils.java looks like the pom change is still in this patch (redundant with HADOOP-8211)",0.0728,0.0728,neutral
hadoop,8245,comment_0,"For problem #1, the solution is the same as is already done in some other test cases. We just need to add a workaround to clear the ZK MBeans before running the tearDown method. It's a hack, but in the absense of a fix for ZOOKEEPER-1438, it's about all we can do. I spent some time investigating problem #2. The bug is as follows: - these test cases create a new and call on it before running the main body of the tests. Although they don't call {{joinElection()}}, the creation of the elector does create a {{zkClient}} object with an associated Watcher. - in the test case, we shut down and restart ZK. This causes the above Watcher instance to fire its Disconnected and then Connected events. There was a bug in the handling of the Connected event that would cause it to re-monitor the lock znode regardless of whether it was previously in the election. - So, when ZK comes back up, there was not two but *three* electors racing for the lock. However, two of the electors actually corresponded to the same dummy service. In some cases this race would be resolved in such a way that the test timed out. I don't think this is a problem in practice, since the ""formatZK"" call runs in its own JVM in the current code. However, it's worth fixing to get the tests to not be flaky, and to have a more reasonable behavior. There are several fixes to be done: - Add extra asserts for to catch cases where we might accidentally re-join the election when we weren't supposed to be in it. - Fix the handling of the ""Connected"" event to only re-join if the elector wants to be in the election - Cause exceptions thrown by watcher callbacks to be propagated back as fatal errors Will post a patch momentarily.",architecture_debt,using_obsolete_technology,"Wed, 4 Apr 2012 03:04:16 +0000","Wed, 4 Apr 2012 19:21:28 +0000","Wed, 4 Apr 2012 19:21:28 +0000",58632,"For problem #1, the solution is the same as is already done in some other test cases. We just need to add a workaround to clear the ZK MBeans before running the tearDown method. It's a hack, but in the absense of a fix for ZOOKEEPER-1438, it's about all we can do. I spent some time investigating problem #2. The bug is as follows: these test cases create a new ActiveStandbyElector, and call ActiveStandbyElector.ensureBaseNode() on it before running the main body of the tests. Although they don't call joinElection(), the creation of the elector does create a zkClient object with an associated Watcher. in the testZookeeperFailure test case, we shut down and restart ZK. This causes the above Watcher instance to fire its Disconnected and then Connected events. There was a bug in the handling of the Connected event that would cause it to re-monitor the lock znode regardless of whether it was previously in the election. So, when ZK comes back up, there was not two but three electors racing for the lock. However, two of the electors actually corresponded to the same dummy service. In some cases this race would be resolved in such a way that the test timed out. I don't think this is a problem in practice, since the ""formatZK"" call runs in its own JVM in the current code. However, it's worth fixing to get the tests to not be flaky, and to have a more reasonable behavior. There are several fixes to be done: Add extra asserts for wantToBeInElection to catch cases where we might accidentally re-join the election when we weren't supposed to be in it. Fix the handling of the ""Connected"" event to only re-join if the elector wants to be in the election Cause exceptions thrown by watcher callbacks to be propagated back as fatal errors Will post a patch momentarily.",-0.001743589744,-0.05194117647,neutral
hadoop,8245,comment_1,Attached patch fixes the flaky behavior for me. I looped for 30+ minutes and didn't see failures after applying this.,test_debt,flaky_test,"Wed, 4 Apr 2012 03:04:16 +0000","Wed, 4 Apr 2012 19:21:28 +0000","Wed, 4 Apr 2012 19:21:28 +0000",58632,Attached patch fixes the flaky behavior for me. I looped TestZKFailoverController for 30+ minutes and didn't see failures after applying this.,0.2,0.2,neutral
hadoop,8316,comment_0,"Patch attached. - update to NullAppender in log4j.properties, it is set explicitly by default in the bin and env scripts, so this is mostly a nop - and now default to the NullAppender in log4j.properties. Update hdfs.audit.logger in hadoop-env.sh to match. This is being made configurable in HADOOP-8224. mapred.audit.logger is not set in the bin or env scripts and is dead code, filed HADOOP-8392 for that (and to hookup RM/NM). Testing, verified the hdfs audit log is no longer automatically created and logged to when run from a tarball install.",code_debt,dead_code,"Thu, 26 Apr 2012 02:34:24 +0000","Thu, 11 Oct 2012 17:45:05 +0000","Fri, 11 May 2012 19:25:54 +0000",1356690,"Patch attached. update hadoop.security.logger to NullAppender in log4j.properties, it is set explicitly by default in the bin and env scripts, so this is mostly a nop hdfs/mapred.audit.logger and now default to the NullAppender in log4j.properties. Update hdfs.audit.logger in hadoop-env.sh to match. This is being made configurable in HADOOP-8224. mapred.audit.logger is not set in the bin or env scripts and is dead code, filed HADOOP-8392 for that (and to hookup RM/NM). Testing, verified the hdfs audit log is no longer automatically created and logged to when run from a tarball install.",0.05,0.005882352941,neutral
hadoop,8341,comment_3,"Addressing the missing license statement in findbugs exclude file. The test passes for me locally. I think it is caused by running with an older version of an HDFS jar, so I am going to upload the new patch and see if that fixes it.",documentation_debt,low_quality_documentation,"Tue, 1 May 2012 20:09:22 +0000","Thu, 12 May 2016 18:25:56 +0000","Tue, 8 May 2012 13:24:35 +0000",580513,"Addressing the missing license statement in findbugs exclude file. The test passes for me locally. I think it is caused by running with an older version of an HDFS jar, so I am going to upload the new patch and see if that fixes it.",-0.1,-0.1,negative
hadoop,8358,comment_0,"This patch is in HADOOP cause its slightly wide/crossproject. Lemme know if I should split it though. Changes summary: * The NodeManager log (from where I noticed this first) showed this cause there was a hdfs-default.xml config set for this deprecated prop, and its web filter loaded that up. Cleaned up hdfs-default.xml. * The old property lookup existed in JspHelper in HDFS. Changed that to use the new property. * Cleaned up the usage of constants for this property, via classes instead of its own constant refs. * Added new prop and default to core-default.xml",code_debt,low_quality_code,"Fri, 4 May 2012 07:44:56 +0000","Thu, 11 Oct 2012 17:45:09 +0000","Mon, 28 May 2012 15:42:38 +0000",2102262,"This patch is in HADOOP cause its slightly wide/crossproject. Lemme know if I should split it though. Changes summary: The NodeManager log (from where I noticed this first) showed this cause there was a hdfs-default.xml config set for this deprecated prop, and its web filter loaded that up. Cleaned up hdfs-default.xml. The old property lookup existed in JspHelper in HDFS. Changed that to use the new property. Cleaned up the usage of constants for this property, via CommonConfigurationKeys classes instead of its own constant refs. Added new prop and default to core-default.xml",-0.1363636364,-0.1363636364,neutral
hadoop,8358,comment_4,Any further comments on the patch? Its quite trivial a change and helps remove unnecessary WARN noise.,code_debt,dead_code,"Fri, 4 May 2012 07:44:56 +0000","Thu, 11 Oct 2012 17:45:09 +0000","Mon, 28 May 2012 15:42:38 +0000",2102262,Any further comments on the patch? Its quite trivial a change and helps remove unnecessary WARN noise.,0.4375,0.4375,neutral
hadoop,8358,comment_10,"Failing test is unrelated to this change. Findbugs succeeded this time, so the previous issue was something else on trunk at the time or was a flaky result.",test_debt,flaky_test,"Fri, 4 May 2012 07:44:56 +0000","Thu, 11 Oct 2012 17:45:09 +0000","Mon, 28 May 2012 15:42:38 +0000",2102262,"Failing test org.apache.hadoop.fs.viewfs.TestViewFsTrash is unrelated to this change. Findbugs succeeded this time, so the previous issue was something else on trunk at the time or was a flaky result.",0.1,0.02857142857,neutral
hadoop,8431,comment_6,"It should just print the usage. Eg in the following I'd remove the ERROR log, the backtrace and the ""Invalid arguments"" log.",code_debt,low_quality_code,"Thu, 24 May 2012 01:51:09 +0000","Thu, 11 Oct 2012 17:45:04 +0000","Fri, 7 Sep 2012 01:27:55 +0000",9157006,"It should just print the usage. Eg in the following I'd remove the ERROR log, the IllegalArgumentException backtrace and the ""Invalid arguments"" log.",-0.2,-0.2,neutral
hadoop,8633,comment_4,The changes look OK. I'm not sure I really like TargetFileSystem. I just don't see a lot of benefit from making TargetFileSystem. I think it would be cleaner to just have your own list of Paths to delete and then delete them in the finally block. Having it be a FileSystem just seems confusing to me. Because TargetFileSystem is never added to the FileSystem cache I assume that if a System.exit is called somewhere while processing the command the TargetFileSystem will not be closed and the files will not actually be deleted on exit. Is this important?,code_debt,low_quality_code,"Mon, 30 Jul 2012 14:17:13 +0000","Thu, 12 May 2016 18:25:59 +0000","Wed, 1 Aug 2012 14:04:40 +0000",172047,The changes look OK. I'm not sure I really like TargetFileSystem. I just don't see a lot of benefit from making TargetFileSystem. I think it would be cleaner to just have your own list of Paths to delete and then delete them in the finally block. Having it be a FileSystem just seems confusing to me. Because TargetFileSystem is never added to the FileSystem cache I assume that if a System.exit is called somewhere while processing the command the TargetFileSystem will not be closed and the files will not actually be deleted on exit. Is this important?,0.015625,0.015625,negative
hadoop,8741,comment_3,On version 2.x and above this is no longer an issue. The affected documentations are 1.0.4 and 1.2.1. These versions are EOL.,defect_debt,uncorrected_known_defects,"Tue, 28 Aug 2012 14:27:13 +0000","Wed, 24 May 2017 09:57:48 +0000","Wed, 24 May 2017 09:57:47 +0000",149455834,On version 2.x and above this is no longer an issue. The affected documentations are 1.0.4 and 1.2.1. These versions are EOL.,-0.05,-0.05,neutral
hadoop,8843,comment_4,"Looks good to me, except for one small nit: it would be worth a javadoc comment on OLD_CHECKPOINT saying something like: /** Format of checkpoint directories used prior to Hadoop 0.23. */ and then a small comment at the point where it's used, like // Check for old-style checkpoint directories left over after an upgrade from Hadoop 1.x Otherwise I think people may be confused what ""old"" refers to, a few years down the line.",code_debt,low_quality_code,"Tue, 25 Sep 2012 20:27:48 +0000","Wed, 3 Sep 2014 23:21:16 +0000","Wed, 26 Sep 2012 17:39:25 +0000",76297,"Looks good to me, except for one small nit: it would be worth a javadoc comment on OLD_CHECKPOINT saying something like: /** Format of checkpoint directories used prior to Hadoop 0.23. */ and then a small comment at the point where it's used, like // Check for old-style checkpoint directories left over after an upgrade from Hadoop 1.x Otherwise I think people may be confused what ""old"" refers to, a few years down the line.",0.2556666667,0.2556666667,positive
hadoop,8866,comment_1,"While you're at it, why not get rid of size() and numeric iteration too? eg:",code_debt,low_quality_code,"Tue, 25 Sep 2012 22:41:01 +0000","Fri, 15 Feb 2013 13:11:50 +0000","Sat, 29 Sep 2012 01:00:52 +0000",267591,"While you're at it, why not get rid of size() and numeric iteration too? eg:",0.0,0.0,neutral
hadoop,8895,comment_1,"Daryn, Thanks for your input. Though it is not absolutely necessary for {{TokenRenewer}} to be an interface, I felt it would make things simpler when working on HDFS-4009. For instance, if a could implement {{TokenRenewer}}, we might not need Now that we are removing completely - {{HADOOP-8891}} - it won't be immediately applicable. However, I was not sure why it should be an abstract class. Do you think we should leave it as is? Is there an advantage of abstract class over interface? I am uploading a patch with my proposed changes.",code_debt,low_quality_code,"Sat, 6 Oct 2012 03:55:55 +0000","Mon, 3 Nov 2014 18:33:59 +0000","Tue, 9 Oct 2012 18:33:46 +0000",311871,"Daryn, Thanks for your input. Though it is not absolutely necessary for TokenRenewer to be an interface, I felt it would make things simpler when working on HDFS-4009. For instance, if a WebHdfsFileSystem could implement TokenRenewer, we might not need DelegationTokenRenewer.Renewable. Now that we are removing DelegationTokenRenewer completely - HADOOP-8891 - it won't be immediately applicable. However, I was not sure why it should be an abstract class. Do you think we should leave it as is? Is there an advantage of abstract class over interface? I am uploading a patch with my proposed changes.",0.128,0.09955555556,neutral
hadoop,8912,comment_7,"Ah, got it. Thanks for the explanation. It will help for those developers who clone the git repo and `git add ...' their files before generating a patch. In that case, it will help somewhat to commit this, but it won't solve the problem 100% of the time. Raja, do you happen to know if there's a semantically equivalent thing we could do for svn, to ensure that committers don't check in bad line endings?",design_debt,non-optimal_design,"Wed, 10 Oct 2012 18:09:30 +0000","Wed, 3 Sep 2014 23:07:19 +0000","Fri, 12 Oct 2012 05:02:44 +0000",125594,"Ah, got it. Thanks for the explanation. It will help for those developers who clone the git repo and `git add ...' their files before generating a patch. In that case, it will help somewhat to commit this, but it won't solve the problem 100% of the time. Raja, do you happen to know if there's a semantically equivalent thing we could do for svn, to ensure that committers don't check in bad line endings?",0.28,0.28,neutral
hadoop,8929,comment_2,"Sorry, should have explained the patch in more detail: - made it Comparable and changed the HashMap to TreeMap so that the printout is in ascending percentile order. Given that this map is always very small, and snapshot() is only called once a minute or so, the runtime/memory differences between treemap and hashmap should be negligible. - changed the behavior to return null instead of throw, because all the catching, etc, got pretty ugly. In implementing toString, I figured I'd clean up the other behavior along the way.",code_debt,low_quality_code,"Mon, 15 Oct 2012 19:46:19 +0000","Thu, 12 May 2016 18:22:35 +0000","Tue, 16 Oct 2012 06:07:18 +0000",37259,"Sorry, should have explained the patch in more detail: made it Comparable and changed the HashMap to TreeMap so that the printout is in ascending percentile order. Given that this map is always very small, and snapshot() is only called once a minute or so, the runtime/memory differences between treemap and hashmap should be negligible. changed the behavior to return null instead of throw, because all the catching, etc, got pretty ugly. In implementing toString, I figured I'd clean up the other behavior along the way.",-0.06666666667,-0.05,neutral
hadoop,8929,comment_6,Summary and the bug description needs update based on this [comment |,documentation_debt,outdated_documentation,"Mon, 15 Oct 2012 19:46:19 +0000","Thu, 12 May 2016 18:22:35 +0000","Tue, 16 Oct 2012 06:07:18 +0000",37259,Summary and the bug description needs update based on this comment,0.0,0.0,neutral
hadoop,9056,comment_1,"Looks good in general! I have the following questions and comments. # It seems this patch only contains changes that made compiling hadoop.dll available on Windows. Some functionality was missing. For example, POSIX.chmod was not ported over from branch-1-win; so were some other IO methods. I also notice some new functions were introduced in trunk compared with branch-1. For example, and What is the plan for those missing functions? Do we plan to port them over in other JIRAs? # file is not needed. # Snappy does not work on Windows right now. It may be better to exclude snappy related files from Windows build. # I think it is better to use WINDOWS instead of  _WIN32 macro in some places because we explicitly defined the UNIX and WINDOWS macros in the beginning of It will make it easier in the future if we want to change the definition of the macros. # Some changes in SecureIOUtils and Datanode are not ported over. Again I think this is related to point 1 above.",build_debt,build_others,"Fri, 16 Nov 2012 23:43:43 +0000","Thu, 2 May 2013 02:30:56 +0000","Wed, 12 Dec 2012 19:15:45 +0000",2230322,"Looks good in general! I have the following questions and comments. It seems this patch only contains changes that made compiling hadoop.dll available on Windows. Some functionality was missing. For example, POSIX.chmod was not ported over from branch-1-win; so were some other IO methods. I also notice some new functions were introduced in trunk compared with branch-1. For example, posixFadviseIfPossible() and POSIX.posix_fadvis(). What is the plan for those missing functions? Do we plan to port them over in other JIRAs? native.vcxproj.user file is not needed. Snappy does not work on Windows right now. It may be better to exclude snappy related files from Windows build. I think it is better to use WINDOWS instead of  _WIN32 macro in some places because we explicitly defined the UNIX and WINDOWS macros in the beginning of org_apache_hadoop.h. It will make it easier in the future if we want to change the definition of the macros. Some changes in SecureIOUtils and Datanode are not ported over. Again I think this is related to point 1 above.",-0.00078125,0.01193181818,neutral
hadoop,9165,comment_1,"Junping, I'd suggest splitting the YARN and MAPREDUCE docs. I am in favor of YARN-291 instead of TT changes mainly because it flows so much smoothly in YARN and also how complicated the TT changes could turn out to be. Please close this as invalid as you already have YARN and MAPREDUCE specific tickets. Tx.",documentation_debt,low_quality_documentation,"Sun, 23 Dec 2012 08:01:38 +0000","Mon, 1 Apr 2013 21:22:11 +0000","Thu, 27 Dec 2012 09:15:56 +0000",350058,"Junping, I'd suggest splitting the YARN and MAPREDUCE docs. I am in favor of YARN-291 instead of TT changes mainly because it flows so much smoothly in YARN and also how complicated the TT changes could turn out to be. Please close this as invalid as you already have YARN and MAPREDUCE specific tickets. Tx.",0.13125,0.13125,neutral
hadoop,9218,comment_0,Several methods on ipc.Server and RPC use type Writable *internally* as a wrapper so that they work across multiple RpcEngine kinds. The request on the wire is in Protobuf for the Protocbuf Rpc Engine. Also the basic rpc headers are also in protobuf. Patch adds javadoc and also changes the names of wrappers.,documentation_debt,low_quality_documentation,"Wed, 16 Jan 2013 00:44:10 +0000","Tue, 27 Aug 2013 22:06:41 +0000","Fri, 15 Feb 2013 02:27:46 +0000",2598216,Several methods on ipc.Server and RPC use type Writable internally as a wrapper so that they work across multiple RpcEngine kinds. The request on the wire is in Protobuf for the Protocbuf Rpc Engine. Also the basic rpc headers are also in protobuf. Patch adds javadoc and also changes the names of wrappers.,0.02,0.02,neutral
hadoop,9254,comment_4,"+1, lgtm. One minor nit: has an extra license comment in the middle of the imports which I'll cleanup on checkin.",code_debt,low_quality_code,"Mon, 28 Jan 2013 10:06:33 +0000","Thu, 12 May 2016 18:27:10 +0000","Wed, 2 Oct 2013 20:55:31 +0000",21379738,"+1, lgtm. One minor nit: BloomFilterCommonTester has an extra license comment in the middle of the imports which I'll cleanup on checkin.",0.0,0.0,neutral
hadoop,9259,comment_0,straightforward hardening of teardown logic,design_debt,non-optimal_design,"Wed, 16 Jan 2013 10:53:53 +0000","Thu, 18 Aug 2016 18:35:36 +0000","Mon, 25 Mar 2013 13:13:56 +0000",5883603,straightforward hardening of teardown logic,0.0,0.0,neutral
hadoop,9267,comment_0,"Patch for {{hadoop}} and {{hdfs}}. Ran TestHDFSCLI and TestCLI successfully. No unit tests since this is kind of trivial, but I could wrangle it if desired.",test_debt,lack_of_tests,"Thu, 31 Jan 2013 03:29:16 +0000","Thu, 12 May 2016 18:21:53 +0000","Fri, 22 Feb 2013 18:38:00 +0000",1955324,"Patch for hadoop and hdfs. Ran TestHDFSCLI and TestCLI successfully. No unit tests since this is kind of trivial, but I could wrangle it if desired.",0.1813333333,0.1813333333,positive
hadoop,9278,comment_0,"The invalid HAR URI is caused here in On Windows, this creates a path that includes the drive specifier. Later, this gets used to construct a HAR URI for testing, which isn't valid, because it doesn't adhere to the the protocol-host format used by HAR URIs. The file handle leak happens in This method opens the _masterindex file, but not all code paths guarantee that the file will be closed. For example, parsing an invalid version throws an exception before the _masterindex file gets closed. There is a test that covers this case, so it leaks a file handle when that test runs. On Windows, this ultimately causes tests to fail during their post-test cleanup, because Windows file locking behavior causes the delete to fail.",code_debt,low_quality_code,"Mon, 4 Feb 2013 23:00:17 +0000","Thu, 12 May 2016 18:22:52 +0000","Tue, 5 Feb 2013 21:26:06 +0000",80749,"The invalid HAR URI is caused here in TestHarFileSystemBasics: On Windows, this creates a path that includes the drive specifier. Later, this gets used to construct a HAR URI for testing, which isn't valid, because it doesn't adhere to the the protocol-host format used by HAR URIs. The file handle leak happens in HarFileSystem#HarMetaData#parseMetaData: This method opens the _masterindex file, but not all code paths guarantee that the file will be closed. For example, parsing an invalid version throws an exception before the _masterindex file gets closed. There is a test that covers this case, so it leaks a file handle when that test runs. On Windows, this ultimately causes tests to fail during their post-test cleanup, because Windows file locking behavior causes the delete to fail.",-0.1,-0.1,negative
hadoop,9305,comment_0,"Here's a simple patch which addresses the issue. The only behavior changes are to conditionally load the AIX64LoginModule and the UsernamePrincipal classes if we're on a 64-bit AIX box, instead of the AIXLoginModule and AIXPrincipal classes. This patch also refactors the getOsPrincipalClass a little bit to reduce some code repetition. No tests are included since to test this properly would require an AIX box. I tested this manually by running with both 32-bit and 64-bit AIX clients and confirming that it works as expected, both with and without Kerberos enabled. Without the patch only 32-bit clients will work. I also ensured there are no regressions by testing the Hadoop client with both IBM Java and Sun Java on Linux both with and without Kerberos enabled. Everything worked as expected.",code_debt,duplicated_code,"Wed, 13 Feb 2013 18:48:14 +0000","Tue, 27 Aug 2013 22:06:30 +0000","Wed, 13 Feb 2013 19:48:07 +0000",3593,"Here's a simple patch which addresses the issue. The only behavior changes are to conditionally load the AIX64LoginModule and the UsernamePrincipal classes if we're on a 64-bit AIX box, instead of the AIXLoginModule and AIXPrincipal classes. This patch also refactors the getOsPrincipalClass a little bit to reduce some code repetition. No tests are included since to test this properly would require an AIX box. I tested this manually by running with both 32-bit and 64-bit AIX clients and confirming that it works as expected, both with and without Kerberos enabled. Without the patch only 32-bit clients will work. I also ensured there are no regressions by testing the Hadoop client with both IBM Java and Sun Java on Linux both with and without Kerberos enabled. Everything worked as expected.",-0.0023125,-0.0023125,neutral
hadoop,9305,comment_1,"+1 pending jenkins. ATM, what about opening a JIRA to clean this spaghetti of conditionals replacing it with a MAP and a simple struct having the needed settings?",code_debt,low_quality_code,"Wed, 13 Feb 2013 18:48:14 +0000","Tue, 27 Aug 2013 22:06:30 +0000","Wed, 13 Feb 2013 19:48:07 +0000",3593,"+1 pending jenkins. ATM, what about opening a JIRA to clean this spaghetti of conditionals replacing it with a MAP and a simple struct having the needed settings?",0.2,0.2,neutral
hadoop,9309,comment_2,"Thanks, Arpit. I tested this on Windows and Ubuntu with native build, and it worked great. Here are a couple of comments. I am +1 for the patch after removal of some unneeded #includes unless those #includes are there for some reason that I missed. (See below for details.) I believe this setting would disable the warning across the whole project, right? Another option could be to upgrade our lz4.c. It looks like there have been some recent changes to address warnings seen when compiling on Windows: I'm sure that would be a much larger scope though, so it's probably best to treat it as a separate jira. Is it necessary to add these #includes to NativeCodeLoader.c? I tried removing them, and I was still able to compile.",code_debt,dead_code,"Thu, 14 Feb 2013 21:18:45 +0000","Thu, 21 Feb 2013 18:58:39 +0000","Thu, 21 Feb 2013 18:45:01 +0000",595576,"Thanks, Arpit. I tested this on Windows and Ubuntu with native build, and it worked great. Here are a couple of comments. I am +1 for the patch after removal of some unneeded #includes unless those #includes are there for some reason that I missed. (See below for details.) I believe this setting would disable the warning across the whole project, right? Another option could be to upgrade our lz4.c. It looks like there have been some recent changes to address warnings seen when compiling on Windows: http://code.google.com/p/lz4/source/detail?r=75&path=/trunk/lz4.c I'm sure that would be a much larger scope though, so it's probably best to treat it as a separate jira. Is it necessary to add these #includes to NativeCodeLoader.c? I tried removing them, and I was still able to compile.",0.1083636364,0.1083636364,positive
hadoop,9336,comment_1,"The semantics will be returning the UGI of the connection, so it will always report the UGI of the original user making the connection, not of any subsequent {{UGI.doAs}} calls. However, this jira will not universally affect anything that doesn't explicitly use it. I intend for only the {{FSNamesystem}} audit calls to currently use it to reduce the significant performance bottlenecks we are encountering. I'm contemplating filing another jira for {{UGI.doAs}} to cache a stack of UGIs. That would greatly accelerate in general.",code_debt,slow_algorithm,"Tue, 26 Feb 2013 20:22:15 +0000","Thu, 12 May 2016 18:21:47 +0000","Thu, 28 Feb 2013 22:07:17 +0000",179102,"The semantics will be returning the UGI of the connection, so it will always report the UGI of the original user making the connection, not of any subsequent UGI.doAs calls. However, this jira will not universally affect anything that doesn't explicitly use it. I intend for only the FSNamesystem audit calls to currently use it to reduce the significant performance bottlenecks we are encountering. I'm contemplating filing another jira for UGI.doAs to cache a stack of UGIs. That would greatly accelerate UGI.getCurrentUser in general.",0.02857142857,0.025,neutral
hadoop,9336,comment_7,"+1 on the code change, but more detailed comment on getRemoteUser() will be nice, in order to avoid potential misuse.",documentation_debt,low_quality_documentation,"Tue, 26 Feb 2013 20:22:15 +0000","Thu, 12 May 2016 18:21:47 +0000","Thu, 28 Feb 2013 22:07:17 +0000",179102,"+1 on the code change, but more detailed comment on getRemoteUser() will be nice, in order to avoid potential misuse.",0.175,0.175,neutral
hadoop,9369,comment_4,"Hi Karthik, the patch seems fine to me, and I agree that the patch is so simple and writing a test sufficiently difficult that it seems unnecessary to write a test for this. One question - can you comment on the ramifications of this issue? How does it manifest itself? And what triggers it?",test_debt,lack_of_tests,"Wed, 6 Mar 2013 03:18:43 +0000","Mon, 3 Nov 2014 18:33:57 +0000","Thu, 7 Mar 2013 23:57:01 +0000",160698,"Hi Karthik, the patch seems fine to me, and I agree that the patch is so simple and writing a test sufficiently difficult that it seems unnecessary to write a test for this. One question - can you comment on the ramifications of this issue? How does it manifest itself? And what triggers it?",0.03333333333,0.03333333333,positive
hadoop,9419,comment_0,"Never mind. I created a patch, and it is completely useless in fixing this problem. The tasks still OOM because the codec itself is so small and the MergeManager creates new codecs so quickly that on a job with lots of reduces it literally uses up all of the address space with direct byte buffers. Some of the processes get killed by the NM for going over the virtual address space before they OOM. We could try and have the CodecPool detect that the codec is doing the wrong thing and ""correct"" it for the codec, but that is too heavy handed in my opinion.",design_debt,non-optimal_design,"Tue, 19 Mar 2013 18:34:28 +0000","Tue, 19 Mar 2013 21:20:44 +0000","Tue, 19 Mar 2013 21:20:44 +0000",9976,"Never mind. I created a patch, and it is completely useless in fixing this problem. The tasks still OOM because the codec itself is so small and the MergeManager creates new codecs so quickly that on a job with lots of reduces it literally uses up all of the address space with direct byte buffers. Some of the processes get killed by the NM for going over the virtual address space before they OOM. We could try and have the CodecPool detect that the codec is doing the wrong thing and ""correct"" it for the codec, but that is too heavy handed in my opinion.",-0.0813,-0.0813,negative
hadoop,9440,comment_3,Why do we get a protobuf exception in the first place? We shouldn't be modifying the test to capture an IOException as we don't expect that to happen. Please correct my understanding if am wrong.,design_debt,non-optimal_design,"Thu, 28 Mar 2013 07:13:55 +0000","Fri, 17 Jan 2014 09:25:22 +0000","Fri, 12 Jul 2013 06:43:44 +0000",9156589,Why do we get a protobuf exception in the first place? We shouldn't be modifying the test to capture an IOException as we don't expect that to happen. Please correct my understanding if am wrong.,0.09166666667,0.09166666667,negative
hadoop,9508,comment_1,"If the code itself is confusing, perhaps you could use this jira to add comments/fix comments.",code_debt,low_quality_code,"Thu, 25 Apr 2013 22:41:43 +0000","Thu, 12 May 2016 18:22:06 +0000","Thu, 25 Apr 2013 23:56:12 +0000",4469,"If the code itself is confusing, perhaps you could use this jira to add comments/fix comments.",-0.437,-0.437,neutral
hadoop,9544,comment_6,"Thanks very much for the review and commit. Just to document it, my full test run came back successful. There was just 1 failure in which is known to be a flaky test.",test_debt,flaky_test,"Fri, 3 May 2013 17:22:11 +0000","Wed, 15 May 2013 05:16:05 +0000","Fri, 3 May 2013 22:47:32 +0000",19521,"Thanks very much for the review and commit. Just to document it, my full test run came back successful. There was just 1 failure in TestBalancerWithNodeGroup, which is known to be a flaky test.",0.2166666667,0.2166666667,positive
hadoop,9669,comment_3,"Thanks, Haohui. Some comments: 1. please try to keep the original javadoc for the same named methods 2. can you make ""State state"" as final? 3. please fix the javadoc /** check if the rest of data has more than <len""len"" is not visible in generated javadoc 4. readFixedOpaque still has a copy not sure if it's possible to generat a read-only bytebuffer from another bytebuffer 5. it would be nice to remove the extra copy for writeFixedOpaque For 4 and 5, I am ok if you think it's out of scope of this JIRA.",code_debt,low_quality_code,"Tue, 25 Jun 2013 19:43:32 +0000","Tue, 24 Sep 2013 23:33:30 +0000","Wed, 18 Sep 2013 01:03:59 +0000",7276827,"Thanks, Haohui. Some comments: 1. please try to keep the original javadoc for the same named methods 2. can you make ""State state"" as final? 3. please fix the javadoc /** check if the rest of data has more than <len> bytes */ ""len"" is not visible in generated javadoc 4. readFixedOpaque still has a copy not sure if it's possible to generat a read-only bytebuffer from another bytebuffer 5. it would be nice to remove the extra copy for writeFixedOpaque For 4 and 5, I am ok if you think it's out of scope of this JIRA.",0.1898125,0.1898125,neutral
hadoop,9669,comment_6,"+1. New patch looks good. Nit: I noticed that most NFS related RPC call response size is less than 256 bytes, so we can have as 256 instead of 512.",code_debt,low_quality_code,"Tue, 25 Jun 2013 19:43:32 +0000","Tue, 24 Sep 2013 23:33:30 +0000","Wed, 18 Sep 2013 01:03:59 +0000",7276827,"+1. New patch looks good. Nit: I noticed that most NFS related RPC call response size is less than 256 bytes, so we can have DEFAULT_INITIAL_CAPACITY as 256 instead of 512.",0.2586666667,0.092,neutral
hadoop,9669,comment_0,Here is a more version that utilizes Java's ByteBuffer. It should be more efficient. The APIs are compatible with the previous version. The APIs might not be ideal towards an efficient implementation of XDR serialization / deserialization. I'm tracking my proposals to the APIs in a separate JIRA.,design_debt,non-optimal_design,"Tue, 25 Jun 2013 19:43:32 +0000","Tue, 24 Sep 2013 23:33:30 +0000","Wed, 18 Sep 2013 01:03:59 +0000",7276827,Here is a more version that utilizes Java's ByteBuffer. It should be more efficient. The APIs are compatible with the previous version. The APIs might not be ideal towards an efficient implementation of XDR serialization / deserialization. I'm tracking my proposals to the APIs in a separate JIRA.,0.125,0.125,neutral
hadoop,9748,comment_0,Minor change. This helps reduce unnecessary class lock contention between such common methods as and etc.,design_debt,non-optimal_design,"Thu, 18 Jul 2013 17:40:12 +0000","Thu, 12 May 2016 18:27:32 +0000","Fri, 19 Jul 2013 14:08:36 +0000",73704,"Minor change. This helps reduce unnecessary class lock contention between such common methods as UGI#getGroupNames and UGI.getCurrentUser, etc.",0.2,0.1333333333,neutral
hadoop,9748,comment_4,"ensureInitialized() forced many frequently called methods to unconditionally acquire the class lock. This patch certainly reduces the lock contention, but the highly contended getCurrentUser() can still block many threads. I understand this is being addressed in HADOOP-9749. There may be something else we can do to improve this for namenode. According to the ""worst"" jstack of a busy namenode I took, there were 29 BLOCKED handler threads. All of them were blocked on the UGI class lock. Here is the breakdown: - 2 ensureInitialized() - from non static synchronized methods. This Jira will unblock these. - 27 getCurrentUser() Among the 27 threads that were blocked at getCurrentUser(), - 18 - from in most namenode RPC methods - 8 - getBlockLocations() - 1 I think FSPermissionChecker can be modified to be created with a passed in UGI. FSNamesystem can the one already stored in RPC server by calling getRemoteUser(). This will eliminate a bulk of getCurrentUser() calls from namenode RPC handlers. A similar change can be made to mkdirs. Block token generation is not as straightforward. Even without it we can eliminate majority of the calls. We could potentially do the same for other RPC servers. I will file a HDFS jira for this. HADOOP-9749 is still needed since getCurrentUser() is used everywhere beyond namenode RPC server. +1 for this patch.",design_debt,non-optimal_design,"Thu, 18 Jul 2013 17:40:12 +0000","Thu, 12 May 2016 18:27:32 +0000","Fri, 19 Jul 2013 14:08:36 +0000",73704,"ensureInitialized() forced many frequently called methods to unconditionally acquire the class lock. This patch certainly reduces the lock contention, but the highly contended getCurrentUser() can still block many threads. I understand this is being addressed in HADOOP-9749. There may be something else we can do to improve this for namenode. According to the ""worst"" jstack of a busy namenode I took, there were 29 BLOCKED handler threads. All of them were blocked on the UGI class lock. Here is the breakdown: 2 ensureInitialized() - from non static synchronized methods. This Jira will unblock these. 27 getCurrentUser() Among the 27 threads that were blocked at getCurrentUser(), 18 FSPermissionChecker() - from FSNamesystem#getPermissionChecker() in most namenode RPC methods 8 BlockTokenSecretManager#generateToken() - getBlockLocations() 1 NameNodeRpcServer.mkdirs I think FSPermissionChecker can be modified to be created with a passed in UGI. FSNamesystem can the one already stored in RPC server by calling getRemoteUser(). This will eliminate a bulk of getCurrentUser() calls from namenode RPC handlers. A similar change can be made to mkdirs. Block token generation is not as straightforward. Even without it we can eliminate majority of the calls. We could potentially do the same for other RPC servers. I will file a HDFS jira for this. HADOOP-9749 is still needed since getCurrentUser() is used everywhere beyond namenode RPC server. +1 for this patch.",-0.03844117647,-0.03439473684,neutral
hadoop,9763,comment_4,"Nicholas, this should be moved to hadoop-common, right?",architecture_debt,violation_of_modularity,"Sun, 21 Jul 2013 10:16:02 +0000","Tue, 27 Aug 2013 22:06:38 +0000","Wed, 24 Jul 2013 06:49:54 +0000",246832,"Nicholas, this should be moved to hadoop-common, right?",0.527,0.527,neutral
hadoop,9763,comment_5,GSet and LightWeightGSet are currently in hdfs. I agree that we should move them to common. Let's do it in a separated issue?,architecture_debt,violation_of_modularity,"Sun, 21 Jul 2013 10:16:02 +0000","Tue, 27 Aug 2013 22:06:38 +0000","Wed, 24 Jul 2013 06:49:54 +0000",246832,GSet and LightWeightGSet are currently in hdfs. I agree that we should move them to common. Let's do it in a separated issue?,0.06666666667,0.06666666667,neutral
hadoop,9763,comment_13,- adds more cases to the short test; - disables the longer test; - randomizes initial value of currentTestTime and the increments; - adds slightly more comments in,code_debt,low_quality_code,"Sun, 21 Jul 2013 10:16:02 +0000","Tue, 27 Aug 2013 22:06:38 +0000","Wed, 24 Jul 2013 06:49:54 +0000",246832,c9763_20130724.patch: adds more cases to the short test; disables the longer test; randomizes initial value of currentTestTime and the increments; adds slightly more comments in TestLightWeightCache.,0.0,0.0,neutral
hadoop,9763,comment_6,"The patch looks good to me. Some thoughts after discussing with : 1. Maybe we do not need to refresh a cache entry's access time and position in the priority queue when the entry is accessed, since the expected timeout on the client side is based on the time the first request is sent. 2. It would be better if we can add an upper limit for the size of the GSet. This can guarantee NN continues to function even when a large amount of retry requests come within a short period of time.",design_debt,non-optimal_design,"Sun, 21 Jul 2013 10:16:02 +0000","Tue, 27 Aug 2013 22:06:38 +0000","Wed, 24 Jul 2013 06:49:54 +0000",246832,"The patch looks good to me. Some thoughts after discussing with sureshms: 1. Maybe we do not need to refresh a cache entry's access time and position in the priority queue when the entry is accessed, since the expected timeout on the client side is based on the time the first request is sent. 2. It would be better if we can add an upper limit for the size of the GSet. This can guarantee NN continues to function even when a large amount of retry requests come within a short period of time.",0.246,0.246,neutral
hadoop,9763,comment_12,"The patch looks very good to me. Only some minors: 1. it would be better if more javadoc/comments can be added to such as the general steps for each check process, and what is tested for each step. 2. Maybe we can assign an initial random value for",documentation_debt,low_quality_documentation,"Sun, 21 Jul 2013 10:16:02 +0000","Tue, 27 Aug 2013 22:06:38 +0000","Wed, 24 Jul 2013 06:49:54 +0000",246832,"The patch looks very good to me. Only some minors: 1. TestLightWeightCache.java, it would be better if more javadoc/comments can be added to TestLightWeightCache, such as the general steps for each check process, and what is tested for each step. 2. Maybe we can assign an initial random value for LightWeightCacheTestCase#currentTestTime?",0.2615,0.2179166667,neutral
hadoop,9763,comment_0,adds LightWeightCache. Still need to test it.,test_debt,lack_of_tests,"Sun, 21 Jul 2013 10:16:02 +0000","Tue, 27 Aug 2013 22:06:38 +0000","Wed, 24 Jul 2013 06:49:54 +0000",246832,h5017_20130721.patch: adds LightWeightCache. Still need to test it.,0.1405,0.09366666667,neutral
hadoop,9763,comment_10,adds more tests.,test_debt,lack_of_tests,"Sun, 21 Jul 2013 10:16:02 +0000","Tue, 27 Aug 2013 22:06:38 +0000","Wed, 24 Jul 2013 06:49:54 +0000",246832,c9763_20130723b.patch: adds more tests.,0.0,0.0,neutral
hadoop,9883,comment_8,"Hi Abin! Thanks for your contribution! Please refer to Matt's comment (from when I first started :-)) Also, please consider reading . The two things I'd recommend are using 2 spaces for indentation and following the 80 char limit per line. I noticed that the change was made as part of MAPREDUCE-181 . Before that it used to be what you are proposing. Do you understand why that change might / might not be relevant any more? I can review the test code once we finalize the src/main code.",code_debt,dead_code,"Mon, 19 Aug 2013 15:53:23 +0000","Tue, 12 May 2015 22:07:20 +0000","Tue, 12 May 2015 22:07:11 +0000",54540828,"Hi Abin! Thanks for your contribution! Please refer to Matt's comment (from when I first started ) https://issues.apache.org/jira/browse/HDFS-2011?focusedCommentId=13041707&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13041707 Also, please consider reading http://wiki.apache.org/hadoop/HowToContribute . The two things I'd recommend are using 2 spaces for indentation and following the 80 char limit per line. I noticed that the change was made as part of MAPREDUCE-181 . Before that it used to be what you are proposing. Do you understand why that change might / might not be relevant any more? I can review the test code once we finalize the src/main code.",0.1416666667,0.125,positive
hadoop,9896,comment_10,"Hi , I think this is not the root cause. I run restRetryProxy alone and also get the timeout, this probably has nothing to do with socket leak. Here is the error message. You can see client retried twice and got stuck",code_debt,low_quality_code,"Wed, 21 Aug 2013 23:21:54 +0000","Thu, 12 May 2016 18:21:44 +0000","Tue, 3 Sep 2013 21:10:43 +0000",1115329,"Hi chuanliu, I think this is not the root cause. I run restRetryProxy alone and also get the timeout, this probably has nothing to do with socket leak. Here is the error message. You can see client retried twice and got stuck",-0.275,-0.275,neutral
hadoop,9896,comment_14,"I may found the race condition in ipc.Client and have a fix, please see the detail in [HADOOP-9916]",code_debt,low_quality_code,"Wed, 21 Aug 2013 23:21:54 +0000","Thu, 12 May 2016 18:21:44 +0000","Tue, 3 Sep 2013 21:10:43 +0000",1115329,"I may found the race condition in ipc.Client and have a fix, please see the detail in HADOOP-9916",0.1,0.1,neutral
hadoop,9896,comment_11,"Another run timeout, this time with a bit more log I suspect there is race condition in Client or Server causing this.",design_debt,non-optimal_design,"Wed, 21 Aug 2013 23:21:54 +0000","Thu, 12 May 2016 18:21:44 +0000","Tue, 3 Sep 2013 21:10:43 +0000",1115329,"Another run timeout, this time with a bit more log I suspect there is race condition in Client or Server causing this.",-0.2,-0.2,neutral
hadoop,9896,comment_12,", I think the timeout is a separate issue. In our case, if we run testRetryProxy alone, it never fails. However when running all the test cases in the test class together, we will always get the JVM crash error due to testRetryProxy hangs. It could be a race condition. What is your OS and configuration?  suspects your guys never run into our problem because you have faster machines. So it could also be that our Linux VM is slower and we never run into this timeout issue.",design_debt,non-optimal_design,"Wed, 21 Aug 2013 23:21:54 +0000","Thu, 12 May 2016 18:21:44 +0000","Tue, 3 Sep 2013 21:10:43 +0000",1115329,"macrokigol, I think the timeout is a separate issue. In our case, if we run testRetryProxy alone, it never fails. However when running all the test cases in the test class together, we will always get the JVM crash error due to testRetryProxy hangs. It could be a race condition. What is your OS and configuration? shanyu suspects your guys never run into our problem because you have faster machines. So it could also be that our Linux VM is slower and we never run into this timeout issue.",-0.04285714286,-0.04285714286,neutral
hadoop,9896,comment_3,I investigated this problem a little bit. I think there is some socket leak in the test code. Attach a patch that fix the leaking issue in the test by explicitly closing client and server at the end of each test case.,design_debt,non-optimal_design,"Wed, 21 Aug 2013 23:21:54 +0000","Thu, 12 May 2016 18:21:44 +0000","Tue, 3 Sep 2013 21:10:43 +0000",1115329,I investigated this problem a little bit. I think there is some socket leak in the test code. Attach a patch that fix the leaking issue in the test by explicitly closing client and server at the end of each test case.,-0.2,-0.2,neutral
hadoop,9896,comment_7,"+1 Thanks Chuan! I tried the patch and it works fine! Not sure when lingering client thread is causing this problem though. Looks like sometimes the Server response was probably consumed by other client thus the testRetryProxy()'s client is waiting forever. But anyway, this patch fixed the TestIPC test case and I think they are good fix.",design_debt,non-optimal_design,"Wed, 21 Aug 2013 23:21:54 +0000","Thu, 12 May 2016 18:21:44 +0000","Tue, 3 Sep 2013 21:10:43 +0000",1115329,"+1 Thanks Chuan! I tried the patch and it works fine! Not sure when lingering client thread is causing this problem though. Looks like sometimes the Server response was probably consumed by other client thus the testRetryProxy()'s client is waiting forever. But anyway, this patch fixed the TestIPC test case and I think they are good fix.",0.5152,0.5152,neutral
hadoop,9896,comment_8,Attach a new patch. A 30 sec timeout is added to each test case. The unit test can pass consistently on my single core Ubuntu box. So I assume the timeout is enough for the test cases.,design_debt,non-optimal_design,"Wed, 21 Aug 2013 23:21:54 +0000","Thu, 12 May 2016 18:21:44 +0000","Tue, 3 Sep 2013 21:10:43 +0000",1115329,Attach a new patch. A 30 sec timeout is added to each test case. The unit test can pass consistently on my single core Ubuntu box. So I assume the timeout is enough for the test cases.,0.28125,0.28125,neutral
hadoop,9929,comment_3,I don't believe the globbing code I cleaned up for v23+ has these issues. You want want to use it for reference.,code_debt,low_quality_code,"Tue, 3 Sep 2013 16:09:44 +0000","Mon, 24 Feb 2014 20:58:05 +0000","Thu, 19 Sep 2013 02:10:04 +0000",1332020,I don't believe the globbing code I cleaned up for v23+ has these issues. You want want to use it for reference.,0.1,0.1,negative
hadoop,10106,comment_3,You seemed to have unnecessarily moved the doRead() function's location in the server.java file. Can you please leave it in its original place in the file and please resubmit the patch.,architecture_debt,violation_of_modularity,"Sat, 16 Nov 2013 00:41:24 +0000","Mon, 24 Feb 2014 20:58:37 +0000","Mon, 16 Dec 2013 22:14:35 +0000",2669591,You seemed to have unnecessarily moved the doRead() function's location in the server.java file. Can you please leave it in its original place in the file and please resubmit the patch.,0.02222222222,0.02222222222,negative
hadoop,10139,comment_6,"This is a really useful update and should take care of a common complaint on the mailing lists. My comments: # I think the pseudo-distributed instructions result in the job being executed locally instead of being submitted to yarn so its not a complete example. We need minimal yarn configuration to demonstrate job execution on yarn. # should be Same with {{start-yarn.sh}} # {{HADOOP_PREFIX}} seems to be necessary, else {{HADOOP_CONF_DIR}} gets initialized to /etc/hadoop and this breaks the DataNode, for one. We should document setting {{HADOOP_PREFIX}}. # Typo: _quickly perform single operations using Hadoop_; _single_ should be _simple_? # Windows setup is quite different from Linux and the steps given here would be confusing for someone attempting to install on Windows. Can you please add a link to the Windows setup wiki page and clarify that the instructions given here are for Linux only? Users tend to take the Linux instructions and attempt to run them in Cygwin which never works as expected. Nitpick comments, not that important: # Instead of ""Format a new perhaps we can just say Format the filesystem. # It may help readability if steps in the execution section were a numbered list. What do you think? # It would be nice if the bullet points after ""Now you are ready to start your Hadoop cluster in one of the three supported modes were links to subsequent sections, not a big deal.",documentation_debt,low_quality_documentation,"Mon, 2 Dec 2013 18:54:30 +0000","Thu, 4 Sep 2014 01:16:47 +0000","Thu, 30 Jan 2014 19:31:21 +0000",5099811,"This is a really useful update and should take care of a common complaint on the mailing lists. My comments: I think the pseudo-distributed instructions result in the job being executed locally instead of being submitted to yarn so its not a complete example. We need minimal yarn configuration to demonstrate job execution on yarn. bin/start-dfs.sh should be sbin/start-dfs.sh. Same with start-yarn.sh HADOOP_PREFIX seems to be necessary, else HADOOP_CONF_DIR gets initialized to /etc/hadoop and this breaks the DataNode, for one. We should document setting HADOOP_PREFIX. Typo: quickly perform single operations using Hadoop; single should be simple? Windows setup is quite different from Linux and the steps given here would be confusing for someone attempting to install on Windows. Can you please add a link to the Windows setup wiki page https://wiki.apache.org/hadoop/Hadoop2OnWindows and clarify that the instructions given here are for Linux only? Users tend to take the Linux instructions and attempt to run them in Cygwin which never works as expected. Nitpick comments, not that important: Instead of ""Format a new distributed-filesystem perhaps we can just say Format the filesystem. It may help readability if steps in the execution section were a numbered list. What do you think? It would be nice if the bullet points after ""Now you are ready to start your Hadoop cluster in one of the three supported modes were links to subsequent sections, not a big deal.",-0.1005595238,-0.08281372549,positive
hadoop,10175,comment_2,"Hi, Chuan. The patch looks good. There is a very minor typo on the test name: I think this patch will be ready to commit after that's corrected.",documentation_debt,low_quality_documentation,"Thu, 19 Dec 2013 02:32:53 +0000","Thu, 12 May 2016 18:21:39 +0000","Mon, 23 Dec 2013 18:43:17 +0000",403824,"Hi, Chuan. The patch looks good. There is a very minor typo on the test name: testMakeQulifiedPath. I think this patch will be ready to commit after that's corrected.",0.3795,0.284625,positive
hadoop,10175,comment_3,"Thanks for reviewing, Chris! Attaching a new patch that corrects the typo.",documentation_debt,low_quality_documentation,"Thu, 19 Dec 2013 02:32:53 +0000","Thu, 12 May 2016 18:21:39 +0000","Mon, 23 Dec 2013 18:43:17 +0000",403824,"Thanks for reviewing, Chris! Attaching a new patch that corrects the typo.",0.2,0.2,positive
hadoop,10175,comment_4,+1 for the patch. Thanks for fixing that typo. I'll commit this.,documentation_debt,low_quality_documentation,"Thu, 19 Dec 2013 02:32:53 +0000","Thu, 12 May 2016 18:21:39 +0000","Mon, 23 Dec 2013 18:43:17 +0000",403824,+1 for the patch. Thanks for fixing that typo. I'll commit this.,0.2,0.2,positive
hadoop,10214,comment_0,"A simple fix. I think it was introduced by: r1556103 | vinodkv | 2014-01-07 09:56:11 +0800 (Tue, 07 Jan 2014) | 2 lines YARN-1029. Added embedded leader election in the ResourceManager. Contributed by Karthik Kambatla. The above change made public, so the multi-threaded warning be observed.",code_debt,multi-thread_correctness,"Thu, 9 Jan 2014 04:10:55 +0000","Thu, 12 May 2016 18:23:20 +0000","Thu, 9 Jan 2014 06:38:25 +0000",8850,"A simple fix. I think it was introduced by: r1556103 | vinodkv | 2014-01-07 09:56:11 +0800 (Tue, 07 Jan 2014) | 2 lines YARN-1029. Added embedded leader election in the ResourceManager. Contributed by Karthik Kambatla. The above change made terminateConnection() public, so the multi-threaded warning be observed.",-0.12,-0.12,neutral
hadoop,10225,comment_4,"I checked on Maven central, and it looks like the 2.x release pretty much have javadoc and source artifacts: We're missing javadoc for 3.0.0-alpha1 though, so let's update the affects/targets accordingly.",documentation_debt,outdated_documentation,"Sun, 12 Jan 2014 14:05:14 +0000","Thu, 10 Jan 2019 18:37:42 +0000","Thu, 10 Jan 2019 18:37:42 +0000",157609948,"I checked on Maven central, and it looks like the 2.x release pretty much have javadoc and source artifacts: http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22org.apache.hadoop%22%20AND%20a%3A%22hadoop-hdfs%22 We're missing javadoc for 3.0.0-alpha1 though, so let's update the affects/targets accordingly.",-0.05,-0.05,neutral
hadoop,10291,comment_0,The root cause is that s property used by some test cases has a side effect. The tests set The broken test case assumed that it will be set when other tests are invoked. The fix is to explicitly set the property instead of depending on the execution order.,design_debt,non-optimal_design,"Mon, 27 Jan 2014 17:04:03 +0000","Thu, 12 May 2016 18:27:23 +0000","Wed, 29 Jan 2014 04:44:35 +0000",128432,The root cause is that s property used by some test cases has a side effect. The tests set (NetUtils.addStaticResolution) The broken test case assumed that it will be set when other tests are invoked. The fix is to explicitly set the property instead of depending on the execution order.,-0.325,-0.24375,negative
hadoop,10498,comment_7,"Thanks Daryn for the confirmation. A nice to have. I do not have strong argument for this. The header name could be configurable instead of hard coding to ""X-Forwarded-For"". The default value being ""X-Forwarded-For"".",code_debt,low_quality_code,"Mon, 14 Apr 2014 16:07:30 +0000","Thu, 12 May 2016 18:21:56 +0000","Tue, 15 Apr 2014 15:27:58 +0000",84028,"daryn Thanks Daryn for the confirmation. A nice to have. I do not have strong argument for this. The header name could be configurable instead of hard coding to ""X-Forwarded-For"". The default value being ""X-Forwarded-For"".",0.025,0.025,positive
hadoop,10499,comment_19,"Hi, Sorry for the inconvenience this caused. Is it possible to patch HBase to stop calling the 3-arg form of that was removed? The third argument was unused anyway, so this won't change any functionality in HBase. Additionally, this class is currently annotated {{Private}}, so I expected we didn't need to check for downstream impacts before I committed. That means one of two additional changes needs to happen: # Change HBase to stop calling altogether. I'm not really sure what alternative you would have though, so maybe this isn't feasible. # Change the annotation to {{LimitedPrivate}} for HBase, so devs know to check for downstream impacts on interface-breaking changes in the future. What are your thoughts on this?",code_debt,low_quality_code,"Mon, 14 Apr 2014 17:10:42 +0000","Wed, 3 Sep 2014 20:36:28 +0000","Wed, 16 Apr 2014 23:18:20 +0000",194858,"Hi, yuzhihong@gmail.com. Sorry for the inconvenience this caused. Is it possible to patch HBase to stop calling the 3-arg form of ProxyUsers#authorize that was removed? The third argument was unused anyway, so this won't change any functionality in HBase. Additionally, this class is currently annotated Private, so I expected we didn't need to check for downstream impacts before I committed. That means one of two additional changes needs to happen: Change HBase to stop calling ProxyUsers#authorize altogether. I'm not really sure what alternative you would have though, so maybe this isn't feasible. Change the annotation to LimitedPrivate for HBase, so devs know to check for downstream impacts on interface-breaking changes in the future. What are your thoughts on this?",-0.253125,-0.225,neutral
hadoop,10499,comment_3,"Although it makes sense to remove an unused parameter, I wonder if the parameter should be honored? It would allow different rpc services to have different proxy superuser configurations as opposed to the current static configuration... It probably made sense when daemons generally had only one rpc service. I don't have a strong opinion because I suppose it could be by another jira.",code_debt,low_quality_code,"Mon, 14 Apr 2014 17:10:42 +0000","Wed, 3 Sep 2014 20:36:28 +0000","Wed, 16 Apr 2014 23:18:20 +0000",194858,"Although it makes sense to remove an unused parameter, I wonder if the parameter should be honored? It would allow different rpc services to have different proxy superuser configurations as opposed to the current static configuration... It probably made sense when daemons generally had only one rpc service. I don't have a strong opinion because I suppose it could be re-added/implemented by another jira.",0.03883333333,0.03883333333,neutral
hadoop,10499,comment_6,"I believe , the static nature of the ProxyUser capability should be addressed separately, if there is a requirement for different services to have different ProxyUser capabilities. HADOOP-10448 would make it easier since it will move the state out of ProxyUsers. I am trying to clean up the interface exposed by _ProxyUsers_ in order to work on HADOOP-10448 and that's why this jira.",code_debt,low_quality_code,"Mon, 14 Apr 2014 17:10:42 +0000","Wed, 3 Sep 2014 20:36:28 +0000","Wed, 16 Apr 2014 23:18:20 +0000",194858,"I believe , the static nature of the ProxyUser capability should be addressed separately, if there is a requirement for different services to have different ProxyUser capabilities. HADOOP-10448 would make it easier since it will move the state out of ProxyUsers. I am trying to clean up the interface exposed by ProxyUsers in order to work on HADOOP-10448 and that's why this jira.",0.05541666667,0.05541666667,neutral
hadoop,10501,comment_2,"It should be okay as long as it is called after Server#start(). Adding {{synchronized}} won't do much good, but won't hurt either.",design_debt,non-optimal_design,"Mon, 14 Apr 2014 23:36:34 +0000","Sat, 7 Mar 2015 23:17:57 +0000","Sat, 7 Mar 2015 23:17:57 +0000",28251683,"It should be okay as long as it is called after Server#start(). Adding synchronized won't do much good, but won't hurt either.",0.6915,0.6915,neutral
hadoop,10526,comment_1,"The patch looks good except a typo: ""Oveerriding"". I changed the target to be 2.5 and trunk. Although this bug is present in other branches, This is where you intend to fix this bug. ""Fix version"" is filled in after check-in.",documentation_debt,low_quality_documentation,"Mon, 5 Dec 2011 15:12:56 +0000","Wed, 3 Sep 2014 20:36:28 +0000","Mon, 21 Apr 2014 19:22:44 +0000",75010188,"The patch looks good except a typo: ""Oveerriding"". I changed the target to be 2.5 and trunk. Although this bug is present in other branches, This is where you intend to fix this bug. ""Fix version"" is filled in after check-in.",0.194,0.194,positive
hadoop,10673,comment_10,"Thanks for the updating, Ming. It looks better approach to me. One additional point: if we can make the variables final, we can remove null check in {{Server#stop()}}.",code_debt,low_quality_code,"Mon, 9 Jun 2014 18:01:13 +0000","Mon, 1 Dec 2014 03:07:25 +0000","Tue, 15 Jul 2014 23:15:40 +0000",3129267,"Thanks for the updating, Ming. It looks better approach to me. One additional point: if we can make the variables final, we can remove null check in Server#stop().",0.3,0.3,positive
hadoop,10673,comment_7,", thank you for the suggestion. I think this improvement is useful, too. One minor comment: both {{rpcMetrics}} and are not final field, so how about adding null check before accessing them?",code_debt,low_quality_code,"Mon, 9 Jun 2014 18:01:13 +0000","Mon, 1 Dec 2014 03:07:25 +0000","Tue, 15 Jul 2014 23:15:40 +0000",3129267,"mingma, thank you for the suggestion. I think this improvement is useful, too. One minor comment: both rpcMetrics and rpcDetailedMetrics are not final field, so how about adding null check before accessing them?",0.307,0.307,positive
hadoop,10673,comment_0,The patch updates the existing rpc metrics in the case of exception. HDFS unit tests will be updated to cover this.,test_debt,lack_of_tests,"Mon, 9 Jun 2014 18:01:13 +0000","Mon, 1 Dec 2014 03:07:25 +0000","Tue, 15 Jul 2014 23:15:40 +0000",3129267,The patch updates the existing rpc metrics in the case of exception. HDFS unit tests will be updated to cover this.,0.0,0.0,positive
hadoop,10681,comment_17,Can we do the same for {{Lz4Compressor}} and I noticed a significant performance overhead using {{Lz4Compressor}} for compared to due to the same problem.,code_debt,slow_algorithm,"Wed, 11 Jun 2014 17:41:42 +0000","Thu, 8 Sep 2016 06:49:18 +0000","Sun, 5 Oct 2014 14:49:19 +0000",10012057,Can we do the same for Lz4Compressor and Bzip2Compressor? I noticed a significant performance overhead using Lz4Compressor for hbase.client.rpc.compressor compared to SnappyCompressor due to the same problem.,-0.1,-0.04,negative
hadoop,10681,comment_1,"Added a SnappyCode impl into hive, which should come before in the classpath for hive+tez. Tested out TPC-H Query5, which has a spill-merge on the JOIN. Query times went from 539.8 seconds to 464.89 seconds, mostly from speedup to a single reducer stage.",code_debt,slow_algorithm,"Wed, 11 Jun 2014 17:41:42 +0000","Thu, 8 Sep 2016 06:49:18 +0000","Sun, 5 Oct 2014 14:49:19 +0000",10012057,"Added a SnappyCode impl into hive, which should come before in the classpath for hive+tez. Tested out TPC-H Query5, which has a spill-merge on the JOIN. Query times went from 539.8 seconds to 464.89 seconds, mostly from speedup to a single reducer stage.",0.1271666667,0.1271666667,negative
hadoop,10681,comment_7,Address findbugs warnings,code_debt,low_quality_code,"Wed, 11 Jun 2014 17:41:42 +0000","Thu, 8 Sep 2016 06:49:18 +0000","Sun, 5 Oct 2014 14:49:19 +0000",10012057,Address findbugs warnings,-0.6,-0.6,negative
hadoop,10681,comment_3,"The patch removes unsafe synchronized blocks from the individual methods in Snappy and Zlib codecs. This synchronization is slow and when used in the most common pattern for CompressionCodec is still thread-unsafe for sharing streams with loops running across multiple threads and would ideally require explicit code { while { 0, buffer.length); } } to get correct stateful behaviour. As code exists today it is not thread-safe and does slow lock-prefix x86_64 instructions. The JNI library below in SnappyCodec.c actually does its own locking mutexes for the actual critical sections within.",design_debt,non-optimal_design,"Wed, 11 Jun 2014 17:41:42 +0000","Thu, 8 Sep 2016 06:49:18 +0000","Sun, 5 Oct 2014 14:49:19 +0000",10012057,"The patch removes unsafe synchronized blocks from the individual methods in Snappy and Zlib codecs. This synchronization is slow and when used in the most common pattern for CompressionCodec is still thread-unsafe for sharing streams with loops running across multiple threads and would ideally require explicit code synchronized(compressor) { while (!compressor.finished()) { compressor.compress(buffer, 0, buffer.length); } } to get correct stateful behaviour. As code exists today it is not thread-safe and does slow lock-prefix x86_64 instructions. The JNI library below in SnappyCodec.c actually does its own locking mutexes for the actual critical sections within.",0.04408333333,0.02938888889,negative
hadoop,10681,comment_10,"The synchronized blocks would've made a lot of sense if setInput() or was atomic. Since it only reads part of the data (64kb or so) in for an invocation, the user has never been able to use this with multiple threads safely. To make sure this was never used with threading in something like HBase, I cross-checked & HBase has an unsynchronized improved version of gzip which writes its own header/trailer chunks without synchronization.",requirement_debt,non-functional_requirements_not_fully_satisfied,"Wed, 11 Jun 2014 17:41:42 +0000","Thu, 8 Sep 2016 06:49:18 +0000","Sun, 5 Oct 2014 14:49:19 +0000",10012057,"The synchronized blocks would've made a lot of sense if setInput() or decompress/compress() was atomic. Since it only reads part of the data (64kb or so) in for an invocation, the user has never been able to use this with multiple threads safely. To make sure this was never used with threading in something like HBase, I cross-checked & HBase has an unsynchronized improved version of gzip which writes its own header/trailer chunks without synchronization. https://github.com/apache/hbase/blob/c61cb7fb55124547a36a6ef56afaec43676039f8/hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/ReusableStreamGzipCodec.java#L100",-0.08133333333,-0.061,negative
hadoop,10752,comment_2,"Minor nit: I'd really like for this to say that the ifdef is for ARM. At least, I'd never guess that this was ARM without a for a while.",code_debt,low_quality_code,"Thu, 26 Jun 2014 08:54:13 +0000","Tue, 31 Mar 2015 08:43:07 +0000","Tue, 31 Mar 2015 08:43:06 +0000",24018533,"Minor nit: I'd really like for this to say that the ifdef is for ARM. At least, I'd never guess that this was ARM without a hint/searching/pondering for a while.",0.0,0.0,neutral
hadoop,10819,comment_2,"Looks like it could maybe be this error, in which case it is a missing build-time dependency in hadoop",build_debt,under-declared_dependencies,"Sat, 12 Jul 2014 07:28:34 +0000","Tue, 4 Dec 2018 06:53:26 +0000","Tue, 4 Dec 2018 06:53:26 +0000",138756292,"Looks like it could maybe be this error, in which case it is a missing build-time dependency in hadoop http://stackoverflow.com/questions/19823184/maven-compiler-plugin-3-x",-0.4,-0.4,negative
hadoop,11117,comment_1,stack/message of little or no value,code_debt,low_quality_code,"Mon, 22 Sep 2014 20:42:40 +0000","Wed, 8 Oct 2014 16:11:45 +0000","Wed, 8 Oct 2014 16:11:45 +0000",1366145,stack/message of little or no value,0.0,0.0,negative
hadoop,11117,comment_5,"I really wish we could catch these and not throw a stack trace to figure out why/what/where it broke. From an end user perspective, these stacks are completely unfriendly.",code_debt,low_quality_code,"Mon, 22 Sep 2014 20:42:40 +0000","Wed, 8 Oct 2014 16:11:45 +0000","Wed, 8 Oct 2014 16:11:45 +0000",1366145,"I really wish we could catch these and not throw a stack trace to figure out why/what/where it broke. From an end user perspective, these stacks are completely unfriendly.",0.06575,0.06575,negative
hadoop,11117,comment_8,"Some of the test failures are spurious, the only regression appears to be These tests are failing because the test code is doing an {{assertEquals}} on the expected string; the propagation of the underlying exception message is breaking this comparison. Fix is what could have been done in the first place: use as the probe",code_debt,low_quality_code,"Mon, 22 Sep 2014 20:42:40 +0000","Wed, 8 Oct 2014 16:11:45 +0000","Wed, 8 Oct 2014 16:11:45 +0000",1366145,"Some of the test failures are spurious, the only regression appears to be TestUserGroupInformation These tests are failing because the test code is doing an assertEquals on the expected string; the propagation of the underlying exception message is breaking this comparison. Fix is what could have been done in the first place: use String.contains() as the probe",-0.2445,-0.163,negative
hadoop,11117,comment_9,patch -002 which patches the test to make it less brittle to exception strings,code_debt,low_quality_code,"Mon, 22 Sep 2014 20:42:40 +0000","Wed, 8 Oct 2014 16:11:45 +0000","Wed, 8 Oct 2014 16:11:45 +0000",1366145,patch -002 which patches the test to make it less brittle to exception strings,0.0,0.0,negative
hadoop,11117,comment_7,I agree ... but this is something minimal which can go in with little/no work though it looks like a couple of tests are not picking up the changed text. What we could do long term is replicate some of the stuff we did for the networking errors: point to wiki pages. But how is anyone going to make sense of a ? I don't want to write the wiki page for that.,test_debt,low_coverage,"Mon, 22 Sep 2014 20:42:40 +0000","Wed, 8 Oct 2014 16:11:45 +0000","Wed, 8 Oct 2014 16:11:45 +0000",1366145,I agree ... but this is something minimal which can go in with little/no work though it looks like a couple of tests are not picking up the changed text. What we could do long term is replicate some of the stuff we did for the networking errors: point to wiki pages. But how is anyone going to make sense of a NoMatchException ? I don't want to write the wiki page for that.,-0.1416666667,-0.1416666667,negative
hadoop,11201,comment_4,"Ah, now it makes sense. In that case, I think renaming to {{justPaths}} would help clarify the code.",code_debt,low_quality_code,"Tue, 14 Oct 2014 08:39:59 +0000","Fri, 10 Apr 2015 20:04:30 +0000","Wed, 19 Nov 2014 01:13:11 +0000",3083592,"The main thing going on there is extracting paths from, potentially absolute, input URI's. Ah, now it makes sense. In that case, I think renaming to justPaths would help clarify the code.",0.2,0.1333333333,positive
hadoop,11313,comment_2,"Hi, Tsuyoshi. This is a good idea. Thank you for posting a patch. I have a few recommendations: # Instead of running it through {{hadoop jar}} with the full class name, the shorter way to run this command is {{hadoop checknative}}. I recommend using the shorter form in the documentation. # Related to the above, since it's part of our CLI, I recommend adding documentation to too. Let's include mention of the {{-a}} option. Without {{-a}}, the command only checks for With {{-a}}, the command checks for all the libraries that hadoop-common dynamically links to. # I think it would be more informative for the sample here to show what the output looks like when the native code is found. In particular, it displays the absolute path to each library it finds, which can be very helpful. Here is a sample from one of my dev environments:",documentation_debt,low_quality_documentation,"Tue, 18 Nov 2014 07:50:04 +0000","Fri, 24 Apr 2015 22:49:02 +0000","Sun, 7 Dec 2014 04:14:26 +0000",1628662,"Hi, Tsuyoshi. This is a good idea. Thank you for posting a patch. I have a few recommendations: Instead of running it through hadoop jar with the full class name, the shorter way to run this command is hadoop checknative. I recommend using the shorter form in the documentation. Related to the above, since it's part of our CLI, I recommend adding documentation to CommandsManual.apt.vm too. Let's include mention of the -a option. Without -a, the command only checks for libhadoop.so/hadoop.dll. With -a, the command checks for all the libraries that hadoop-common dynamically links to. I think it would be more informative for the sample here to show what the output looks like when the native code is found. In particular, it displays the absolute path to each library it finds, which can be very helpful. Here is a sample from one of my dev environments:",0.2364545455,0.1734,positive
hadoop,11313,comment_8,"+1 for the patch. I committed this to trunk and branch-2. Tsuyoshi, thank you for contributing this documentation improvement.",documentation_debt,low_quality_documentation,"Tue, 18 Nov 2014 07:50:04 +0000","Fri, 24 Apr 2015 22:49:02 +0000","Sun, 7 Dec 2014 04:14:26 +0000",1628662,"+1 for the patch. I committed this to trunk and branch-2. Tsuyoshi, thank you for contributing this documentation improvement.",0.3236666667,0.3236666667,positive
hadoop,11355,comment_2,"+1 LGTM, small nit for the future, you can use when checking exception text. Will commit shortly, thanks Arun.",code_debt,low_quality_code,"Fri, 5 Dec 2014 18:05:17 +0000","Fri, 24 Apr 2015 22:49:00 +0000","Fri, 5 Dec 2014 20:02:02 +0000",7005,"+1 LGTM, small nit for the future, you can use GenericTestUtils.assertExceptionContains when checking exception text. Will commit shortly, thanks Arun.",0.15,0.1,positive
hadoop,11379,comment_0,In this patch I fixed warnings raised by findbugs 3 against hadoop-auth and They're all encoding related warnings.,code_debt,low_quality_code,"Tue, 9 Dec 2014 19:07:55 +0000","Fri, 24 Apr 2015 22:48:55 +0000","Tue, 9 Dec 2014 21:09:20 +0000",7285,In this patch I fixed warnings raised by findbugs 3 against hadoop-auth and hadoop-auth-examples. They're all encoding related warnings.,-0.6,-0.6,neutral
hadoop,11409,comment_4,"Thanks for the patch, Gera! When we throw the exception due to lacking a scheme, it would be very helpful to users to mention the property where the bad URI originated to show them what needs to be changed. The unit test should have an Assert.fail call or something similar after the getFileContext call, otherwise the lack of throwing an exception for a bad fs URI will still pass the test. ""Excpected"" s/b ""Expected""",test_debt,low_coverage,"Mon, 15 Dec 2014 23:03:47 +0000","Fri, 10 Apr 2015 20:04:49 +0000","Thu, 18 Dec 2014 21:30:27 +0000",253600,"Thanks for the patch, Gera! When we throw the exception due to lacking a scheme, it would be very helpful to users to mention the property where the bad URI originated to show them what needs to be changed. The unit test should have an Assert.fail call or something similar after the getFileContext call, otherwise the lack of throwing an exception for a bad fs URI will still pass the test. ""Excpected"" s/b ""Expected""",0.02655,0.02655,positive
hadoop,11437,comment_0,"Hello  I did not find author tag, ""DistCp Version 2"" is pressent ,here Version 2 can be removed..Please correct me if I am wrong..",documentation_debt,low_quality_documentation,"Fri, 19 Dec 2014 17:33:31 +0000","Thu, 12 May 2016 18:27:56 +0000","Wed, 11 Feb 2015 23:48:18 +0000",4688087,"Hello aw I did not find author tag, ""DistCp Version 2"" is pressent ,here Version 2 can be removed..Please correct me if I am wrong..",0.09166666667,0.09166666667,negative
hadoop,11437,comment_1,Look at the very bottom of the readme: That should be removed.,documentation_debt,outdated_documentation,"Fri, 19 Dec 2014 17:33:31 +0000","Thu, 12 May 2016 18:27:56 +0000","Wed, 11 Feb 2015 23:48:18 +0000",4688087,Look at the very bottom of the readme: That should be removed.,0.0,0.0,negative
hadoop,11437,comment_3,Hi  Removed from readme and documentation guide..,documentation_debt,outdated_documentation,"Fri, 19 Dec 2014 17:33:31 +0000","Thu, 12 May 2016 18:27:56 +0000","Wed, 11 Feb 2015 23:48:18 +0000",4688087,"Hi aw We should also remove all the ""DistCp V2"" stuff and just make it distcp since there is no distcp v1 anymore Removed from readme and documentation guide..",0.0,0.0,negative
hadoop,11523,comment_3,"Hi Duo. Thank you for the patch. The current patch would acquire the lease for all {{rename}} operations, covering both block blobs and page blobs. During initial development of atomic rename, we were careful to limit the scope to only page blobs (typically used by HBase logs). Existing applications using block blobs might not be expecting the leases, so I think we need to make sure the lease acquisition only happens after checking In the error handling, if an exception is thrown from the try block, and then another exception is also thrown from freeing the lease, then this would drop the original exception and throw the exception from freeing the lease. I suspect in general the main exception from the try block is going to be more interesting for root cause analysis, so I recommend throwing that one and just logging the one from freeing the lease, like so: In addition to the unit tests, I ran the test suite against a live Azure storage account and confirmed that everything passed. The release audit warning from Jenkins is unrelated to this patch.",design_debt,non-optimal_design,"Thu, 29 Jan 2015 18:57:37 +0000","Thu, 22 Oct 2015 00:48:25 +0000","Fri, 30 Jan 2015 01:08:39 +0000",22262,"Hi Duo. Thank you for the patch. The current patch would acquire the lease for all rename operations, covering both block blobs and page blobs. During initial development of atomic rename, we were careful to limit the scope to only page blobs (typically used by HBase logs). Existing applications using block blobs might not be expecting the leases, so I think we need to make sure the lease acquisition only happens after checking AzureNativeFileSystemStore#isAtomicRenameKey. In the error handling, if an exception is thrown from the try block, and then another exception is also thrown from freeing the lease, then this would drop the original exception and throw the exception from freeing the lease. I suspect in general the main exception from the try block is going to be more interesting for root cause analysis, so I recommend throwing that one and just logging the one from freeing the lease, like so: In addition to the unit tests, I ran the test suite against a live Azure storage account and confirmed that everything passed. The release audit warning from Jenkins is unrelated to this patch.",-0.01,-0.03583333333,neutral
hadoop,11523,comment_9,"Thanks, . I have just 2 more minor nitpicks. I suggest changing this: to this: This way, we'll log the full stack trace on lease free errors and have more information for troubleshooting. This is just a minor style point, but for the following, the keyword should be inline with the closing brace. Instead of this: We do this: Instead of this: We do this:",design_debt,non-optimal_design,"Thu, 29 Jan 2015 18:57:37 +0000","Thu, 22 Oct 2015 00:48:25 +0000","Fri, 30 Jan 2015 01:08:39 +0000",22262,"Thanks, onpduo. I have just 2 more minor nitpicks. I suggest changing this: to this: This way, we'll log the full stack trace on lease free errors and have more information for troubleshooting. This is just a minor style point, but for the following, the keyword should be inline with the closing brace. Instead of this: We do this: Instead of this: We do this:",0.06,0.06,neutral
hadoop,11585,comment_0,"* fixed broken ""Samplers"" section title * added newlines for ease of editing.",documentation_debt,low_quality_documentation,"Wed, 11 Feb 2015 21:50:33 +0000","Thu, 12 May 2016 18:23:45 +0000","Thu, 12 Feb 2015 00:15:20 +0000",8687,"fixed broken ""Samplers"" section title added newlines for ease of editing.",0.025,0.025,neutral
hadoop,11607,comment_1,"+1 Note that as s3a uses SLF4J for its log API, it can switch to {{log.info(""item {}"", value)}} for terser/more efficient logging logic (we still recommend wrapping debug statements though, just to skip string creation. If you switch to that mode, I'd be even happier. But I'm OK with what you've done so far",code_debt,low_quality_code,"Tue, 17 Feb 2015 21:30:07 +0000","Fri, 10 Apr 2015 20:04:36 +0000","Fri, 20 Feb 2015 21:59:09 +0000",260942,"+1 Note that as s3a uses SLF4J for its log API, it can switch to log.info(""item {}"", value) for terser/more efficient logging logic (we still recommend wrapping debug statements though, just to skip string creation. If you switch to that mode, I'd be even happier. But I'm OK with what you've done so far",0.272625,0.272625,positive
hadoop,11658,comment_2,Thanks  for the report and the patch! Two comments: * Can we use in * Some inserted lines are longer than 80 characters. Would you render these?,code_debt,low_quality_code,"Mon, 2 Mar 2015 05:47:26 +0000","Fri, 10 Apr 2015 20:04:33 +0000","Mon, 2 Mar 2015 09:15:54 +0000",12508,Thanks drankye for the report and the patch! Two comments: Can we use IO_COMPRESSION_CODECS_KEY in TestCodecFactory.java? Some inserted lines are longer than 80 characters. Would you render these?,0.1333333333,0.18,positive
hadoop,11677,comment_3,Added missing import. Could not find any test class for HTTPServer2. I will try to add tests for this class,test_debt,lack_of_tests,"Thu, 5 Mar 2015 09:57:13 +0000","Tue, 30 Aug 2016 01:30:40 +0000","Mon, 23 Nov 2015 03:58:15 +0000",22701662,Added missing import. Could not find any test class for HTTPServer2. I will try to add tests for this class,-0.1333333333,-0.1333333333,negative
hadoop,11730,comment_4,"patch applied; tested against s3 EU. Given the nature of these problems, it may be good to start thinking about whether we can do things with better simulate failures; the test here is a good start, though we may want more complex policies...mockito might be the tool to reach for.",test_debt,low_coverage,"Thu, 19 Mar 2015 23:22:57 +0000","Sat, 31 Oct 2020 00:26:01 +0000","Thu, 23 Apr 2015 20:45:10 +0000",3014533,"patch applied; tested against s3 EU. Given the nature of these problems, it may be good to start thinking about whether we can do things with better simulate failures; the test here is a good start, though we may want more complex policies...mockito might be the tool to reach for.",0.3565,0.3565,neutral
hadoop,11740,comment_0,"This initial patch simply removes {{ErasureEncoder}} and {{ErasureDecoder}}. I think the following further simplifications are possible: # We can get rid of {{ErasureCoder}} since it has a single subclass now # Similarly, maybe we can get rid of since provides enough abstraction anyway # If {{ECBlockGroup}} can provide erased indices, we can further combine encoding and decoding classes",code_debt,low_quality_code,"Mon, 23 Mar 2015 20:10:41 +0000","Wed, 30 Sep 2015 19:38:02 +0000","Fri, 3 Apr 2015 22:23:49 +0000",958388,"This initial patch simply removes ErasureEncoder and ErasureDecoder. I think the following further simplifications are possible: We can get rid of ErasureCoder since it has a single subclass now (AbstractErasureCoder Similarly, maybe we can get rid of ErasureCodingStep since AbstractErasureCodingStep provides enough abstraction anyway If ECBlockGroup can provide erased indices, we can further combine encoding and decoding classes",0.375,0.375,positive
hadoop,11740,comment_1,"Thanks  for the good thoughts. Yes it's often a good question to think about either interface or abstract class in Java. My feeling is that if it's in a framework, subject to be pluggable and implemented by customers, an interface would be good to have. So I guess we could keep {{ErasureCoder}} interface, and convert interface to a class. I'm not sure, as erased indices have to be be computed according to input blocks and output blocks just for decoders, and encoders don't have the related logics. Currently in RS coder the decoding is rather simple but I believe it will be much complicated for codes like LRC and Hitchhiker, so separating encoding class and decoding class is desired.",code_debt,low_quality_code,"Mon, 23 Mar 2015 20:10:41 +0000","Wed, 30 Sep 2015 19:38:02 +0000","Fri, 3 Apr 2015 22:23:49 +0000",958388,"Thanks zhz for the good thoughts. We can get rid of ErasureCoder since it has a single subclass now (AbstractErasureCoder) Yes it's often a good question to think about either interface or abstract class in Java. My feeling is that if it's in a framework, subject to be pluggable and implemented by customers, an interface would be good to have. So I guess we could keep ErasureCoder interface, and convert ErasureCodingStep interface to a class. If ECBlockGroup can provide erased indices, we can further combine encoding and decoding classes I'm not sure, as erased indices have to be be computed according to input blocks and output blocks just for decoders, and encoders don't have the related logics. Currently in RS coder the decoding is rather simple but I believe it will be much complicated for codes like LRC and Hitchhiker, so separating encoding class and decoding class is desired.",0.3328888889,0.3679583333,positive
hadoop,11740,comment_7,"Thanks for the update. I looked the new patch, just two minor comments: 1. In the test codes, may be better to use {{ErasureCoder}} instead of or since the interface type is good enough, which is why we're here. With this refining, from caller's point of view, nothing different from between encoder and decoder, so it should use the common interface. 2. Those unnecessary Javadoc are there to conform Javadoc conventions and format. In future someone may fill them. I suggest we don't remove them, you can find so many in the project.",code_debt,low_quality_code,"Mon, 23 Mar 2015 20:10:41 +0000","Wed, 30 Sep 2015 19:38:02 +0000","Fri, 3 Apr 2015 22:23:49 +0000",958388,"Thanks for the update. I looked the new patch, just two minor comments: 1. In the test codes, may be better to use ErasureCoder instead of AbstractErasureEncoder or AbstractErasureDecoder since the interface type is good enough, which is why we're here. With this refining, from caller's point of view, nothing different from between encoder and decoder, so it should use the common interface. 2. Those unnecessary Javadoc are there to conform Javadoc conventions and format. In future someone may fill them. I suggest we don't remove them, you can find so many in the project.",0.14375,0.14375,positive
hadoop,11740,comment_6,Thanks Kai for the review! The updated patch addresses the issue in the test code. I also made a pass of the and removed unnecessary Javadoc,documentation_debt,outdated_documentation,"Mon, 23 Mar 2015 20:10:41 +0000","Wed, 30 Sep 2015 19:38:02 +0000","Fri, 3 Apr 2015 22:23:49 +0000",958388,Thanks Kai for the review! The updated patch addresses the issue in the test code. I also made a pass of the TestErasureCoderBase and removed unnecessary Javadoc,0.1333333333,0.1333333333,positive
hadoop,11740,comment_8,"Thanks Kai for the review! I read the test classes again and figured out more about the structure. Let me know if it looks OK now. Regarding Javadoc I believe we shouldn't have empty statements when merging to trunk. If a parameter or return value is self-descriptory, I think it's better not to add a Javadoc than adding an empty one. But let's discuss that separately since it's not in the scope of this JIRA.",documentation_debt,low_quality_documentation,"Mon, 23 Mar 2015 20:10:41 +0000","Wed, 30 Sep 2015 19:38:02 +0000","Fri, 3 Apr 2015 22:23:49 +0000",958388,"Thanks Kai for the review! I read the test classes again and figured out more about the structure. Let me know if it looks OK now. Regarding Javadoc I believe we shouldn't have empty statements when merging to trunk. If a parameter or return value is self-descriptory, I think it's better not to add a Javadoc than adding an empty one. But let's discuss that separately since it's not in the scope of this JIRA.",0.2833333333,0.2833333333,positive
hadoop,11837,comment_4,LGTM. Minor nits: Change to +1 once addressed.,code_debt,low_quality_code,"Wed, 15 Apr 2015 19:43:59 +0000","Fri, 24 Apr 2015 22:49:17 +0000","Fri, 17 Apr 2015 18:00:35 +0000",166596,LGTM. Minor nits: Change to +1 once addressed.,0.25,0.25,neutral
hadoop,11862,comment_0,", technically KMS is a proxy for an actual key provider, that, in addition to the keys, generates EDEKs for keys and decrypts them to corresponding DEKs. It also caches the keys in memory. It delegates the Key operations to a backing keyprovider specified by the specified in the *kms-site.xml* conf file. The only concrete implementation (shipped with hadoop) of a KeyProvider is currently the Consider the following deployment scenario : # *KMS1* configured with as ""jcek://file@..."". and thus will delegate to a # *KMS2* configured with as ""kms://http@KMS1.."" and thus will delegate Key operations to KMS1 but will provide generate/decrypt operations. It also caches the Keys for faster access # *KMS3* ALSO configured with as ""kms://http@KMS1.."" and thus will, like KMS2 delegate Key operations to KMS1 but will provide generate/decrypt operations. Now if we set the to the special loadbalancing url : then all requests will be loadbalanced across KMS2 and KMS3 and keys will be shared.",design_debt,non-optimal_design,"Wed, 22 Apr 2015 02:34:43 +0000","Fri, 21 May 2021 09:03:54 +0000","Tue, 28 Jun 2016 22:47:29 +0000",37483966,"dengxiumao, technically KMS is a proxy for an actual key provider, that, in addition to creating/storing/deleting the keys, generates EDEKs for keys and decrypts them to corresponding DEKs. It also caches the keys in memory. It delegates the Key store/retrieve/delete operations to a backing keyprovider specified by the hadoop.kms.key.provider.uri specified in the kms-site.xml conf file. The only concrete implementation (shipped with hadoop) of a KeyProvider is currently the JavaKeyStoreProvider. Consider the following deployment scenario : KMS1 configured with hadoop.kms.key.provider.uri as ""jcek://file@..."". and thus will delegate to a JavaKeyStoreProvider KMS2 configured with hadoop.kms.key.provider.uri as ""kms://http@KMS1.."" and thus will delegate Key create/store/rollover/delete operations to KMS1 but will provide generate/decrypt operations. It also caches the Keys for faster access KMS3 ALSO configured with hadoop.kms.key.provider.uri as ""kms://http@KMS1.."" and thus will, like KMS2 delegate Key create/store/rollover/delete operations to KMS1 but will provide generate/decrypt operations. Now if we set the hadoop.security.key.provider.path to the special loadbalancing url : ""kms://http@KM1_HOST;KMS2_HOST:16000/kms"", then all requests will be loadbalanced across KMS2 and KMS3 and keys will be shared.",0.10625,0.1051724138,neutral
hadoop,11862,comment_1,"hi , thank you for your reply. yes, the scenario as you said, actually will be loadbalanced and will shared accross KMS instances. But, it's not High Available(HA), there are 2 senarios: 1. if the KMS1 goes down, KMS2 and KMS3 will not available. 2. if the kms.keystore file was delete, the files encrypted by the keys in kms.keystore won't be read. So, I think if keys have several replicas, like HDFS replicas mechanism, it will be really HA. ps. maybe I should modify the title more clearly.",design_debt,non-optimal_design,"Wed, 22 Apr 2015 02:34:43 +0000","Fri, 21 May 2021 09:03:54 +0000","Tue, 28 Jun 2016 22:47:29 +0000",37483966,"hi asuresh, thank you for your reply. yes, the scenario as you said, actually will be loadbalanced and will shared accross KMS instances. But, it's not High Available(HA), there are 2 senarios: 1. if the KMS1 goes down, KMS2 and KMS3 will not available. 2. if the kms.keystore file was delete, the files encrypted by the keys in kms.keystore won't be read. So, I think if keys have several replicas, like HDFS replicas mechanism, it will be really HA. ps. maybe I should modify the title more clearly.",-0.0092,-0.0092,neutral
hadoop,11862,comment_2,", Hmmm.. it is not HA for some operations (create / rollover / delete) most other operations including get / encrypt / decrypt, they should be (since the keys are actually cached by KMS.. and EDEKs generated by 1 KMS can be decrypted by the other.. if the EZ key version is in cache). But yes, i agree, there is no replica stored anywhere.. so for catastrophic failures where the backing KMS does not come up, you lose data. I would expect an enterprise deployment to use a more robust production quality key store for which you can easily write a KeyProvider and use it as a backing key store. But yes, we dont ship one with hadoop.",design_debt,non-optimal_design,"Wed, 22 Apr 2015 02:34:43 +0000","Fri, 21 May 2021 09:03:54 +0000","Tue, 28 Jun 2016 22:47:29 +0000",37483966,"dengxiumao, But, it's not High Available(HA), there are 2 senarios: Hmmm.. it is not HA for some operations (create / rollover / delete) most other operations including get / encrypt / decrypt, they should be (since the keys are actually cached by KMS.. and EDEKs generated by 1 KMS can be decrypted by the other.. if the EZ key version is in cache). But yes, i agree, there is no replica stored anywhere.. so for catastrophic failures where the backing KMS does not come up, you lose data. I would expect an enterprise deployment to use a more robust production quality key store for which you can easily write a KeyProvider and use it as a backing key store. But yes, we dont ship one with hadoop.",0.1725,0.135,neutral
hadoop,11880,comment_1,"Looking at the tests, turns out some tests do expect hard-coded paths for minidfs cluster, so the cluster comes back up in the same run this is going to be fun. Either revert with a hard-coded path and expect parallel tests to fail, or extend the dfs builder to add an operation to set the subdir, which would be retained over a test case/test suite. The revert is the short-term option, but mini dfs will need to be fixed for reliable hdfs test runs",code_debt,low_quality_code,"Mon, 27 Apr 2015 14:52:15 +0000","Fri, 1 Sep 2017 15:22:29 +0000","Fri, 1 Sep 2017 15:22:29 +0000",74133014,"Looking at the tests, turns out some tests do expect hard-coded paths for minidfs cluster, so the cluster comes back up in the same run this is going to be fun. Either revert with a hard-coded path and expect parallel tests to fail, or extend the dfs builder to add an operation to set the subdir, which would be retained over a test case/test suite. The revert is the short-term option, but mini dfs will need to be fixed for reliable hdfs test runs",0.3443333333,0.3443333333,neutral
hadoop,11880,comment_4,"None of the above. I thought you might have an opinion on what the next step is. Frankly, I'm half-leaning towards setting a different hard-coded path and disabling parallel tests in HDFS again since it's pretty clear that the unit tests are pretty screwed up based upon investigation. :( We still need to fix it, though.",test_debt,low_coverage,"Mon, 27 Apr 2015 14:52:15 +0000","Fri, 1 Sep 2017 15:22:29 +0000","Fri, 1 Sep 2017 15:22:29 +0000",74133014,"None of the above. I thought you might have an opinion on what the next step is. Frankly, I'm half-leaning towards setting a different hard-coded path and disabling parallel tests in HDFS again since it's pretty clear that the unit tests are pretty screwed up based upon stevel@apache.org's investigation. We still need to fix it, though.",-0.034875,0.05025,neutral
hadoop,11880,comment_5,"Got it. Thanks! I just commented in HDFS-9263 that the Maven build, when running tests in parallel mode, automatically creates a separate test directory for each concurrent test process to use. Thus, the isolation is achieved at the Maven build level, and there is no requirement for {{MiniDFSCluster}} itself to generate a unique workspace. Of course, that only holds true if all tests are playing nice and using the properties passed down by Maven to determine their test path. I'd like to explore fixing this by backtracing from the bad paths shown in the RAT report to try to find the offending tests. Given that I think is correct already, I suspect we'll find the problems are in tests that don't use {{MiniDFSCluster}}, or in tests that explicitly configure their own data directories instead of relying on I'm reluctant to revert the test parallelization. If it's more urgent to complete this cutover first though, then I understand.",test_debt,expensive_tests,"Mon, 27 Apr 2015 14:52:15 +0000","Fri, 1 Sep 2017 15:22:29 +0000","Fri, 1 Sep 2017 15:22:29 +0000",74133014,"Got it. Thanks! Either revert with a hard-coded path and expect parallel tests to fail, or extend the dfs builder to add an operation to set the subdir, which would be retained over a test case/test suite. I just commented in HDFS-9263 that the Maven build, when running tests in parallel mode, automatically creates a separate test directory for each concurrent test process to use. Thus, the isolation is achieved at the Maven build level, and there is no requirement for MiniDFSCluster itself to generate a unique workspace. Of course, that only holds true if all tests are playing nice and using the properties passed down by Maven to determine their test path. I'd like to explore fixing this by backtracing from the bad paths shown in the RAT report to try to find the offending tests. Given that I think MiniDFSCluster#getBaseDirectory is correct already, I suspect we'll find the problems are in tests that don't use MiniDFSCluster, or in tests that explicitly configure their own data directories instead of relying on MiniDFSCluster#getBaseDirectory. I'm reluctant to revert the test parallelization. If it's more urgent to complete this cutover first though, then I understand.",0.02004166667,0.01853333333,neutral
hadoop,11880,comment_7,"The naughty test is The problem is specific to the HDFS-9263 patch, so I'll add more details over there.",test_debt,flaky_test,"Mon, 27 Apr 2015 14:52:15 +0000","Fri, 1 Sep 2017 15:22:29 +0000","Fri, 1 Sep 2017 15:22:29 +0000",74133014,"The naughty test is TestMiniDFSCluster. The problem is specific to the HDFS-9263 patch, so I'll add more details over there.",-0.4,-0.2,neutral
hadoop,12002,comment_3,"It'd be better to loop over all the executables and print out all of them that aren't found, then give the -1 and return. Right now, if someone is missing several of them they'll probably have to run through multiple times.",code_debt,low_quality_code,"Tue, 19 May 2015 22:55:18 +0000","Thu, 5 Nov 2015 22:39:39 +0000","Thu, 5 Nov 2015 22:39:39 +0000",14687061,"It'd be better to loop over all the executables and print out all of them that aren't found, then give the -1 and return. Right now, if someone is missing several of them they'll probably have to run through multiple times.",0.28175,0.28175,neutral
hadoop,12002,comment_4,"Thanks , I revised the patch. -02 * cover Sean's feedback * make temporary variables in for loops local * examine the executable paths with -f, not only -x",code_debt,low_quality_code,"Tue, 19 May 2015 22:55:18 +0000","Thu, 5 Nov 2015 22:39:39 +0000","Thu, 5 Nov 2015 22:39:39 +0000",14687061,"Thanks busbey, I revised the patch. -02 cover Sean's feedback make temporary variables in for loops local examine the executable paths with -f, not only -x",0.5125,0.5125,neutral
hadoop,12002,comment_7,"* Generate a -1 jira table for every missing executable. This simplifies the code and gets rid of a lot of excess variables. * We can drop the extra line feed, so just just use echo instead of the more complex printf * This isn't java, we don't need camelCase or really long names. ;) (e.g., findbugsExecutable) [Yes, i recognize I didn't purge them out the first time, but we should make the effort to get rid of them when we can.] * Remove the extra check: -x should fail if the file doesn't exist.",code_debt,complex_code,"Tue, 19 May 2015 22:55:18 +0000","Thu, 5 Nov 2015 22:39:39 +0000","Thu, 5 Nov 2015 22:39:39 +0000",14687061,"Generate a -1 jira table for every missing executable. This simplifies the code and gets rid of a lot of excess variables. We can drop the extra line feed, so just just use echo instead of the more complex printf This isn't java, we don't need camelCase or really long names. (e.g., findbugsExecutable) [Yes, i recognize I didn't purge them out the first time, but we should make the effort to get rid of them when we can.] Remove the extra check: -x should fail if the file doesn't exist.",0.0325,0.0125,neutral
hadoop,12021,comment_3,"Lewis, could you give a little more detail of your Nutch usecase? It's also worth noting that while we provide the description in core-default.xml / hdfs-default.xml / etc for documentation, but probably not in user-provided config files. The -default.xml files are already included in our JARs, so it shouldn't increase dependency size. Loading them in will, however, increase in-memory size, which is probably a concern for some user app.",documentation_debt,low_quality_documentation,"Sat, 23 May 2015 00:36:20 +0000","Mon, 29 Aug 2016 22:19:43 +0000","Sun, 31 May 2015 20:05:45 +0000",761365,"Lewis, could you give a little more detail of your Nutch usecase? It's also worth noting that while we provide the description in core-default.xml / hdfs-default.xml / etc for documentation, but probably not in user-provided config files. The -default.xml files are already included in our JARs, so it shouldn't increase dependency size. Loading them in will, however, increase in-memory size, which is probably a concern for some user app.",-0.15,-0.15,neutral
hadoop,12021,comment_6,Hi Folks. The suggestions here have convinced me that the place to do this is over in Nutch as it is certainly application specific and just causing too much overhead in Hadoop. The text is not required in-memory within the Hadoop Configuration object.,documentation_debt,low_quality_documentation,"Sat, 23 May 2015 00:36:20 +0000","Mon, 29 Aug 2016 22:19:43 +0000","Sun, 31 May 2015 20:05:45 +0000",761365,Hi Folks. The suggestions here have convinced me that the place to do this is over in Nutch as it is certainly application specific and just causing too much overhead in Hadoop. The text is not required in-memory within the Hadoop Configuration object.,0.1456666667,0.1456666667,neutral
hadoop,12268,comment_2,"Nice catch, ! The fix looks good to me. Two comments: 1. Would you remove unused imports in and 2. Would you please fix the following javadoc comment? ""concat"" should be ""append"".",code_debt,low_quality_code,"Fri, 24 Jul 2015 07:54:01 +0000","Tue, 30 Aug 2016 01:24:49 +0000","Sat, 1 Aug 2015 04:59:35 +0000",680734,"Nice catch, zxu! The fix looks good to me. Two comments: 1. Would you remove unused imports in TestHDFSContractAppend and AbstractContractAppendTest? 2. Would you please fix the following javadoc comment? ""concat"" should be ""append"".",0.3001666667,0.2572857143,positive
hadoop,12371,comment_2,"thanks.. however .. what if I simply want to wipe out trash ? On my testing cluster, with rarely small storage, I am running jobs generating some data and deleting them over and over again, the trash easily blows up the storage and my HDFS becomes unavailable. Does it make sense to provide an force option to clean up trash ? E.g hadoop fs -expunge -f",design_debt,non-optimal_design,"Tue, 1 Sep 2015 08:39:01 +0000","Wed, 9 Sep 2015 02:24:18 +0000","Wed, 9 Sep 2015 02:24:18 +0000",668717,"xyao thanks.. however .. what if I simply want to wipe out trash ? On my testing cluster, with rarely small storage, I am running jobs generating some data and deleting them over and over again, the trash easily blows up the storage and my HDFS becomes unavailable. Does it make sense to provide an force option to clean up trash ? E.g hadoop fs -expunge -f",0.05,0.05,neutral
hadoop,12458,comment_1,"Failed tests aren't related. Thanks for the changes! +1, committing shortly. Quick notes: - Please do not set a Fix Version. Use Target Version field instead. The Fix Version must indicate only the branches where it has *already* been committed to. The former is to indicate requests of branches it must go to, so is more appropriate. - For more typo corrections in future, please also feel free to roll up multiple corrections into the same patch.",documentation_debt,low_quality_documentation,"Sat, 3 Oct 2015 03:15:58 +0000","Tue, 30 Aug 2016 01:22:46 +0000","Sat, 3 Oct 2015 13:10:24 +0000",35666,"Failed tests aren't related. Thanks for the changes! +1, committing shortly. Quick notes: Please do not set a Fix Version. Use Target Version field instead. The Fix Version must indicate only the branches where it has already been committed to. The former is to indicate requests of branches it must go to, so is more appropriate. For more typo corrections in future, please also feel free to roll up multiple corrections into the same patch.",0.19,0.2104166667,neutral
hadoop,12460,comment_2,"Corrected the checkstyle errors and white spaces & Attached the patch, Please review",code_debt,low_quality_code,"Tue, 6 Oct 2015 14:07:39 +0000","Tue, 30 Aug 2016 01:22:43 +0000","Mon, 19 Oct 2015 06:47:59 +0000",1096820,"Corrected the checkstyle errors and white spaces & Attached the patch, Please review",-0.1,-0.1,neutral
hadoop,12460,comment_4,"Thanks  Few comments, 1. Identation can be same as other lines and \n not required at the end. 2. Document need to be updated for the {{get}} command instead of {{copyToLocal}} 3. {{TestCLI}} failure should be fixed.",documentation_debt,outdated_documentation,"Tue, 6 Oct 2015 14:07:39 +0000","Tue, 30 Aug 2016 01:22:43 +0000","Mon, 19 Oct 2015 06:47:59 +0000",1096820,"Thanks jagadesh.kiran Few comments, 1. Identation can be same as other lines and \n not required at the end. 2. Document need to be updated for the get command instead of copyToLocal 3. TestCLI failure should be fixed.",0.0,0.0,neutral
hadoop,12505,comment_10,"Yes, because it means unpredictable behavior. Unpredictable behavior almost always turns into a security hole. It's trivial to construct a group that turns into ../.. (or whatever) in the path structure if I'm interpreting the output of hadoop fs -ls. That's very very bad. (that said: it'd be an awesome crack. Change the default user's group and watch everyone nuke their own files...) The NFS folks added some code to do it, but didn't really integrate it correctly. Expedience always trumps correctness. :(",code_debt,low_quality_code,"Fri, 23 Oct 2015 16:08:23 +0000","Mon, 28 Mar 2016 16:48:44 +0000","Mon, 28 Mar 2016 16:16:19 +0000",13565276,"I'm curious then about what is your stance on JniBasedUnixGroupsMapping. Do you see it as a bug that it works correctly with non-Unix-compliant names? Yes, because it means unpredictable behavior. Unpredictable behavior almost always turns into a security hole. It's trivial to construct a group that turns into ../.. (or whatever) in the path structure if I'm interpreting the output of hadoop fs -ls. That's very very bad. (that said: it'd be an awesome crack. Change the default user's group and watch everyone nuke their own files...) In Hadoop, we don't have access to a canonical UID/GID, The NFS folks added some code to do it, but didn't really integrate it correctly. Expedience always trumps correctness.",-0.068,0.0188,negative
hadoop,12505,comment_5,"The Hadoop code itself does not consistently enforce POSIX compliance for user names or group names. It's more a function of the hosting OS. For example, Harsh has pointed out that for operators using the JNI-based implementation instead of shell-based, Hadoop users already can show up with membership in groups that don't have a POSIX-compliant name. I've seen that for Windows deployments, and it sounds like the use case here was a Linux deployment connected to Active Directory. The group mapping alone is not sufficient to enforce POSIX compliance either. This is only used to populate the user's set of groups. It does not control other input paths for group names. For example, WebHDFS and Java API calls can accept names with spaces. Here is an example, tested on Mac. This kind of data of course complicates parsing of shell output, and I imagine many operators would prefer to enforce POSIX-compliant names by policy. However, I don't believe Hadoop has taken responsibility for that enforcement. I don't think the existing implementation of really provides any POSIX compliance benefits, at least not intentionally. In the example given, it would split the group ""Domain Users"" on spaces and decide to put the user into 2 groups: ""Domain"" and ""Users"". While those split names don't have spaces, they're also still not POSIX compliant because of the capital letters, and more importantly, they're completely erroneous. Hopefully the split isn't putting anyone into a real group where they don't really belong. I see this as a bug rather than a POSIX compliance feature. I would prefer to see the -1 lifted and have the bug fixed. That said, I also see it as low priority, since the majority of deployments I see use the JNI-based implementation now, which does not have the bug.",code_debt,low_quality_code,"Fri, 23 Oct 2015 16:08:23 +0000","Mon, 28 Mar 2016 16:48:44 +0000","Mon, 28 Mar 2016 16:16:19 +0000",13565276,"The Hadoop code itself does not consistently enforce POSIX compliance for user names or group names. It's more a function of the hosting OS. For example, Harsh has pointed out that for operators using the JNI-based implementation instead of shell-based, Hadoop users already can show up with membership in groups that don't have a POSIX-compliant name. I've seen that for Windows deployments, and it sounds like the use case here was a Linux deployment connected to Active Directory. The group mapping alone is not sufficient to enforce POSIX compliance either. This is only used to populate the user's set of groups. It does not control other input paths for group names. For example, WebHDFS and Java FileSystem#setOwner API calls can accept names with spaces. Here is an example, tested on Mac. This kind of data of course complicates parsing of shell output, and I imagine many operators would prefer to enforce POSIX-compliant names by policy. However, I don't believe Hadoop has taken responsibility for that enforcement. I don't think the existing implementation of ShellBasedUnixGroupsMapping really provides any POSIX compliance benefits, at least not intentionally. In the example given, it would split the group ""Domain Users"" on spaces and decide to put the user into 2 groups: ""Domain"" and ""Users"". While those split names don't have spaces, they're also still not POSIX compliant because of the capital letters, and more importantly, they're completely erroneous. Hopefully the split isn't putting anyone into a real group where they don't really belong. I see this as a bug rather than a POSIX compliance feature. I would prefer to see the -1 lifted and have the bug fixed. That said, I also see it as low priority, since the majority of deployments I see use the JNI-based implementation now, which does not have the bug.",-0.02106944444,-0.02106944444,negative
hadoop,12505,comment_3,"yeah, it'll break the universe in really awful ways if we accept groups with spaces. Think about folks parsing hadoop fs -ls for example. or the REST interfaces. or ACLs. or ... ugh!",design_debt,non-optimal_design,"Fri, 23 Oct 2015 16:08:23 +0000","Mon, 28 Mar 2016 16:48:44 +0000","Mon, 28 Mar 2016 16:16:19 +0000",13565276,"yeah, it'll break the universe in really awful ways if we accept groups with spaces. Think about folks parsing hadoop fs -ls for example. or the REST interfaces. or ACLs. or ... ugh!",-0.09333333333,-0.09333333333,negative
hadoop,12701,comment_0,"After the fix, mvn for entire Hadoop tree took 01:27 min on my Macbook Pro. In comparison, it took 38.677 s before.",code_debt,slow_algorithm,"Mon, 11 Jan 2016 01:03:40 +0000","Tue, 30 Aug 2016 01:20:10 +0000","Sat, 7 May 2016 00:21:36 +0000",10106276,"After the fix, mvn checkstyle:checkstyle for entire Hadoop tree took 01:27 min on my Macbook Pro. In comparison, it took 38.677 s before.",0.0,0.0,neutral
hadoop,12733,comment_2,"RE: ASF license warnings This looks like a bunch of generated files, not checked in files.",code_debt,low_quality_code,"Sat, 23 Jan 2016 07:16:45 +0000","Wed, 4 Jan 2017 23:04:55 +0000","Wed, 4 Jan 2017 05:21:00 +0000",29973855,"RE: ASF license warnings This looks like a bunch of generated files, not checked in files.",-0.6,-0.6,negative
hadoop,12829,comment_1,"Thank you, . I can't think of any reason why this thread should swallow without a trace. It is not performing any operations that should inherently generate INE, as far as I can see, and if someone else sends an INE we ought to... interrupt the thread. Can you include the stack trace so that this exception has a better chance of getting noticed? Also, capitalize the error? +1 pending those changes",code_debt,low_quality_code,"Sat, 20 Feb 2016 01:19:42 +0000","Mon, 16 Dec 2019 10:04:45 +0000","Tue, 23 Feb 2016 19:33:42 +0000",324840,"Thank you, gchanan. I can't think of any reason why this thread should swallow InterruptedException without a trace. It is not performing any operations that should inherently generate INE, as far as I can see, and if someone else sends an INE we ought to... interrupt the thread. Can you include the stack trace so that this exception has a better chance of getting noticed? Also, capitalize the error? +1 pending those changes",0.008333333333,0.008333333333,negative
hadoop,12829,comment_11,"Hello guys, I hit the same thread leak issue in FLINK-15239. In our use case, a Flink cluster is started w/o Hadoop dependencies, and accepts jobs submitted from various clients. If a job needs to access Hadoop, client will include Hadoop jars as its dependencies. On Flink side, we create a class loader to load all the dependencies of each job. As a result, we start a thread each time we load the Hadoop classes. While the change here made the thread respond to interrupt exception, it seems there's no one to interrupt the thread. So I guess the thread only dies when the JVM exits. If that's the case, is it possible to provide a way to explicitly interrupt the thread?",design_debt,non-optimal_design,"Sat, 20 Feb 2016 01:19:42 +0000","Mon, 16 Dec 2019 10:04:45 +0000","Tue, 23 Feb 2016 19:33:42 +0000",324840,"Hello guys, I hit the same thread leak issue in FLINK-15239. In our use case, a Flink cluster is started w/o Hadoop dependencies, and accepts jobs submitted from various clients. If a job needs to access Hadoop, client will include Hadoop jars as its dependencies. On Flink side, we create a class loader to load all the dependencies of each job. As a result, we start a StatisticsDataReferenceCleaner thread each time we load the Hadoop classes. While the change here made the thread respond to interrupt exception, it seems there's no one to interrupt the thread. So I guess the thread only dies when the JVM exits. If that's the case, is it possible to provide a way to explicitly interrupt the thread?",-0.05,-0.05,negative
hadoop,12829,comment_0,"Attached a patch. Doesn't include any tests -- not sure exactly what to test. I internally tested by changing STATS_DATA_CLEANER to package-private and writing the following test: which passes with the change and hangs without it. I'm unclear on if hadoop even wants something like this, since I'm not up to speed on how hadoop handles JVM reuse for unit tests.",test_debt,lack_of_tests,"Sat, 20 Feb 2016 01:19:42 +0000","Mon, 16 Dec 2019 10:04:45 +0000","Tue, 23 Feb 2016 19:33:42 +0000",324840,"Attached a patch. Doesn't include any tests  not sure exactly what to test. I internally tested by changing STATS_DATA_CLEANER to package-private and writing the following test: which passes with the change and hangs without it. I'm unclear on if hadoop even wants something like this, since I'm not up to speed on how hadoop handles JVM reuse for unit tests.",0.070375,0.070375,negative
hadoop,12837,comment_3,"Thanks  for sharing the details. Yes, it is referring to a directory. Would it be possible for you to suggest some workaround since there are no plans to fix it as you mentioned.",defect_debt,uncorrected_known_defects,"Wed, 24 Feb 2016 09:22:18 +0000","Fri, 28 Apr 2017 09:57:07 +0000","Fri, 28 Apr 2017 09:57:07 +0000",37067689,"Thanks cnauroth for sharing the details. Yes, it is referring to a directory. Would it be possible for you to suggest some workaround since there are no plans to fix it as you mentioned.",0.2,0.2,neutral
hadoop,12837,comment_8,"Well, the above mentioned workaround of the _SUCCESS file works in my case since the content of the directory in question isn't expected to change after it is created. However in case of frequently updating directory contents that won't work. For that case one needs to dig deeper in the directory / sub-directories and determine mtime of each file and finally return the max value as mtime of the directory, however, that would be an expensive operation, particularly in case of huge directories / subdirectories. For now the workaround seems to be working for me. You may want to keep this ticket in backlog if this happens to find priority. Feel free to close if otherwise. Thanks guys for sharing your inputs. Regards, Jagdish",defect_debt,uncorrected_known_defects,"Wed, 24 Feb 2016 09:22:18 +0000","Fri, 28 Apr 2017 09:57:07 +0000","Fri, 28 Apr 2017 09:57:07 +0000",37067689,"Well, the above mentioned workaround of the _SUCCESS file works in my case since the content of the directory in question isn't expected to change after it is created. However in case of frequently updating directory contents that won't work. For that case one needs to dig deeper in the directory / sub-directories and determine mtime of each file and finally return the max value as mtime of the directory, however, that would be an expensive operation, particularly in case of huge directories / subdirectories. For now the workaround seems to be working for me. You may want to keep this ticket in backlog if this happens to find priority. Feel free to close if otherwise. Thanks guys for sharing your inputs. Regards, Jagdish",0.22909375,0.22909375,neutral
hadoop,12864,comment_1,"Actually, it looks like bin/rcc should have been removed with HADOOP-10474.",code_debt,dead_code,"Wed, 2 Mar 2016 18:16:48 +0000","Wed, 29 Jun 2016 03:30:40 +0000","Tue, 28 Jun 2016 21:25:19 +0000",10206511,"Actually, it looks like bin/rcc should have been removed with HADOOP-10474.",0.0,0.0,negative
hadoop,12946,comment_0,Patch 001: * Make the thread object static. * Use a reference count to track how many instances depend on the thread. * Ensure the thread is started on the first start call. * Ensure the thread is only stopped on the last stop call * Move AbstractService specific test cases from TestJvmMetrics to a new TestAbstractService class. * Add JvmPauseMonitor test cases.,code_debt,low_quality_code,"Sun, 20 Mar 2016 18:14:55 +0000","Tue, 10 Jan 2017 01:41:24 +0000","Tue, 10 Jan 2017 01:41:24 +0000",25514789,Patch 001: Make the thread object static. Use a reference count to track how many instances depend on the thread. Ensure the thread is started on the first start call. Ensure the thread is only stopped on the last stop call Move AbstractService specific test cases from TestJvmMetrics to a new TestAbstractService class. Add JvmPauseMonitor test cases.,-0.1170333333,-0.1170333333,neutral
hadoop,13011,comment_10,"line 129 "" This is done by leveraging the Hadoop filesystem abstraction within the provider implementation."" can this be rephrased into something concise without the word ""leveraging""? line 140, backquote line 14, same for `cat 158. Remove the `you`; make it less personal. ""Organisations ? users? "" can you quote ""side files"" not a term I'd encountered before",documentation_debt,low_quality_documentation,"Sat, 9 Apr 2016 13:53:33 +0000","Tue, 30 Aug 2016 01:16:28 +0000","Fri, 22 Apr 2016 05:12:37 +0000",1091944,"line 129 "" This is done by leveraging the Hadoop filesystem abstraction within the provider implementation."" can this be rephrased into something concise without the word ""leveraging""? line 140, backquote hadoop.security.credstore.java-keystore-provider.password-file line 14, same for `cat 158. Remove the `you`; make it less personal. ""Organisations ? users? "" can you quote ""side files"" not a term I'd encountered before",-0.07142857143,-0.04545454545,neutral
hadoop,13030,comment_11,Pretty much this exact same code exists in httpfs.sh ....,code_debt,duplicated_code,"Sat, 16 Apr 2016 01:20:05 +0000","Tue, 30 Aug 2016 01:16:21 +0000","Thu, 28 Apr 2016 00:14:36 +0000",1032871,Pretty much this exact same code exists in httpfs.sh ....,0.19075,0.19075,neutral
hadoop,13030,comment_2,Patch 2 fixes the shellcheck warnings.,code_debt,low_quality_code,"Sat, 16 Apr 2016 01:20:05 +0000","Tue, 30 Aug 2016 01:16:21 +0000","Thu, 28 Apr 2016 00:14:36 +0000",1032871,Patch 2 fixes the shellcheck warnings.,-0.6,-0.6,neutral
hadoop,13030,comment_4,"Patch 3 adds my first part of comment to the function, for better readability.",code_debt,low_quality_code,"Sat, 16 Apr 2016 01:20:05 +0000","Tue, 30 Aug 2016 01:16:21 +0000","Thu, 28 Apr 2016 00:14:36 +0000",1032871,"Patch 3 adds my first part of comment to the function, for better readability.",0.5,0.5,neutral
hadoop,13039,comment_4,"Hi , I think we can remove this string ""as potentially malicious"". The patch looks good otherwise. Thanks!",code_debt,dead_code,"Tue, 19 Apr 2016 22:33:30 +0000","Fri, 6 Jan 2017 01:01:38 +0000","Wed, 27 Apr 2016 03:49:39 +0000",623769,"Hi liuml07, I think we can remove this string ""as potentially malicious"". The patch looks good otherwise. Thanks!",0.392,0.392,neutral
hadoop,13039,comment_12,This is for adding missing documentation for config property. No code change was involved.,documentation_debt,outdated_documentation,"Tue, 19 Apr 2016 22:33:30 +0000","Fri, 6 Jan 2017 01:01:38 +0000","Wed, 27 Apr 2016 03:49:39 +0000",623769,The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. This is for adding missing documentation for config property. No code change was involved.,-0.2,0.0,neutral
hadoop,13039,comment_5,"Per-offline discussion with , the v1 patch refined the documentation to make it clearer.",documentation_debt,low_quality_documentation,"Tue, 19 Apr 2016 22:33:30 +0000","Fri, 6 Jan 2017 01:01:38 +0000","Wed, 27 Apr 2016 03:49:39 +0000",623769,"Per-offline discussion with arpitagarwal, the v1 patch refined the documentation to make it clearer.",0.0,0.0,neutral
hadoop,13233,comment_0,Moved to Hadoop Common project because this code is in hadoop-common module.,architecture_debt,violation_of_modularity,"Thu, 2 Jun 2016 14:10:25 +0000","Mon, 13 Feb 2017 18:51:34 +0000","Mon, 13 Feb 2017 18:38:06 +0000",22134461,Moved to Hadoop Common project because this code is in hadoop-common module.,0.0,0.0,negative
hadoop,13233,comment_4,"TestKDiag succeeded on my machine, I suspect it's a random failure. checkstyle could be fixed, but it would either look weird (indented differently than the other lines in Stat.DESCRIPTION) or I would have to reindent a few more lines.",code_debt,low_quality_code,"Thu, 2 Jun 2016 14:10:25 +0000","Mon, 13 Feb 2017 18:51:34 +0000","Mon, 13 Feb 2017 18:38:06 +0000",22134461,"TestKDiag succeeded on my machine, I suspect it's a random failure. checkstyle could be fixed, but it would either look weird (indented differently than the other lines in Stat.DESCRIPTION) or I would have to reindent a few more lines.",-0.1333333333,-0.1333333333,negative
hadoop,13233,comment_8,+1 No need to fix checkstyle warning. Failed tests not reproducible.,code_debt,low_quality_code,"Thu, 2 Jun 2016 14:10:25 +0000","Mon, 13 Feb 2017 18:51:34 +0000","Mon, 13 Feb 2017 18:38:06 +0000",22134461,+1 No need to fix checkstyle warning. Failed tests not reproducible.,0.1,0.1,negative
hadoop,13365,comment_0,"I think the plan of attach should be: 1. Let users keep \_OPTS in hadoop-env.sh, etc. 2. Detect if it's not already an array by checking \_OPTS[1]. 3. If not, convert to array using eval (bash 3.x compat) This way, if users want to use \_OPTS in array format (which, for long lists of options, is significantly easier to work with), they can.",design_debt,non-optimal_design,"Mon, 11 Jul 2016 15:06:56 +0000","Sat, 1 Sep 2018 20:06:01 +0000","Sat, 1 Sep 2018 20:06:01 +0000",67582745,"I think the plan of attach should be: 1. Let users keep _OPTS in hadoop-env.sh, etc. 2. Detect if it's not already an array by checking _OPTS[1]. 3. If not, convert to array using eval (bash 3.x compat) This way, if users want to use _OPTS in array format (which, for long lists of options, is significantly easier to work with), they can.",0.05714285714,0.05714285714,neutral
hadoop,13365,comment_2,"-00: * first pass There's a lot happening here, so let's go through it: * adding some helper routines in hadoop-functions to: ** convert strings to arrays if the array doesn't already exist ** add to arrays based upon a key to dedupe * convert almost all internal users of HADOOP_OPTS and xyz_OPTS to use the array form * update some pre-existing doc references * add several unit tests * rewrite existing unit tests to use the array form To do: * more/better docs * figure out what to do about catalina? * get HADOOP-13341 committed, since this code is several times larger without it * more testing",documentation_debt,low_quality_documentation,"Mon, 11 Jul 2016 15:06:56 +0000","Sat, 1 Sep 2018 20:06:01 +0000","Sat, 1 Sep 2018 20:06:01 +0000",67582745,"-00: first pass There's a lot happening here, so let's go through it: adding some helper routines in hadoop-functions to: convert strings to arrays if the array doesn't already exist add to arrays based upon a key to dedupe convert almost all internal users of HADOOP_OPTS and xyz_OPTS to use the array form update some pre-existing doc references add several unit tests rewrite existing unit tests to use the array form To do: more/better docs figure out what to do about catalina? get HADOOP-13341 committed, since this code is several times larger without it more testing",0.1666666667,0.1666666667,neutral
hadoop,13365,comment_4,"One of the feedbacks from HADOOP-13341 from was that the docs should include an example of _OPTS ordering. We have an opportunity to de-dupe here. If this JIRA does end up de-duping, then that should be documented. If it doesn't de-dupe, then the docs should specifically give an example of ordering.",documentation_debt,low_quality_documentation,"Mon, 11 Jul 2016 15:06:56 +0000","Sat, 1 Sep 2018 20:06:01 +0000","Sat, 1 Sep 2018 20:06:01 +0000",67582745,"One of the feedbacks from HADOOP-13341 from stevel@apache.org was that the docs should include an example of _OPTS ordering. We have an opportunity to de-dupe here. If this JIRA does end up de-duping, then that should be documented. If it doesn't de-dupe, then the docs should specifically give an example of ordering.",0.225,0.225,neutral
hadoop,13386,comment_7,"1.9.0 has removed a couple of dependencies, including upgrading to Jackson 2.x. We also got rid of paranamer. Avro does only depend on Jackson, Jackson-Databind and Commons-compress. You can see the dependencies here:",build_debt,over-declared_dependencies,"Mon, 18 Jul 2016 23:23:06 +0000","Sat, 27 Jan 2024 06:37:07 +0000","Sat, 26 Mar 2022 11:32:04 +0000",179410138,"1.9.0 has removed a couple of dependencies, including upgrading to Jackson 2.x. We also got rid of paranamer. Avro does only depend on Jackson, Jackson-Databind and Commons-compress. You can see the dependencies here: https://mvnrepository.com/artifact/org.apache.avro/avro/1.8.2 https://mvnrepository.com/artifact/org.apache.avro/avro/1.9.0",0.0,0.0,neutral
hadoop,13386,comment_8,"OK. We will need to tag as incompatible for precompiled avro code. But it is better, and with fewer dependencies, actually an improvement on what we get today.",build_debt,over-declared_dependencies,"Mon, 18 Jul 2016 23:23:06 +0000","Sat, 27 Jan 2024 06:37:07 +0000","Sat, 26 Mar 2022 11:32:04 +0000",179410138,"OK. We will need to tag as incompatible for precompiled avro code. But it is better, and with fewer dependencies, actually an improvement on what we get today.",0.4236666667,0.4236666667,neutral
hadoop,13386,comment_10,"Real issue is that it forces people downstream to recompile their code too, if they have this version of avro on their CP. If they exclude it, then you can't use the compiled classes in Hadoop -but there aren't that many. Maybe: tag as incompatible and explain what to do: recompile or exclude",design_debt,non-optimal_design,"Mon, 18 Jul 2016 23:23:06 +0000","Sat, 27 Jan 2024 06:37:07 +0000","Sat, 26 Mar 2022 11:32:04 +0000",179410138,"Real issue is that it forces people downstream to recompile their code too, if they have this version of avro on their CP. If they exclude it, then you can't use the compiled classes in Hadoop -but there aren't that many. Maybe: tag as incompatible and explain what to do: recompile or exclude",-0.1333333333,-0.1333333333,neutral
hadoop,13386,comment_0,"successor to HADOOP-12527; given the discourse there I'm going to change the title of that one and close this as a duplicate. see also: for some coverage of the problem. We aren't scared of Avro, but do need to take care of its transitive dependencies. Help there showing what they are and testing all down the stack is very much appreciated",test_debt,low_coverage,"Mon, 18 Jul 2016 23:23:06 +0000","Sat, 27 Jan 2024 06:37:07 +0000","Sat, 26 Mar 2022 11:32:04 +0000",179410138,"successor to HADOOP-12527; given the discourse there I'm going to change the title of that one and close this as a duplicate. see also: http://steveloughran.blogspot.co.uk/2016/05/fear-of-dependencies.html for some coverage of the problem. We aren't scared of Avro, but do need to take care of its transitive dependencies. Help there showing what they are and testing all down the stack is very much appreciated",0.03816666667,0.03816666667,neutral
hadoop,13529,comment_5,some incremental updates: 1. add docs 2. remove UserInfo 3. remove comments for overrie function 4. codestyle issue,code_debt,low_quality_code,"Mon, 22 Aug 2016 01:05:09 +0000","Mon, 6 Nov 2017 07:53:16 +0000","Fri, 26 Aug 2016 04:40:22 +0000",358513,kakagou some incremental updates: 1. add docs 2. remove UserInfo 3. remove comments for overrie function 4. codestyle issue,0.0,0.0,neutral
hadoop,13638,comment_11,That was copied from an existing test case.,code_debt,duplicated_code,"Wed, 21 Sep 2016 21:04:16 +0000","Wed, 2 Oct 2019 17:15:08 +0000","Mon, 26 Sep 2016 20:12:54 +0000",428918,That was copied from an existing test case.,0.0,0.0,neutral
hadoop,13638,comment_9,Checkstyle warnings can't be removed unless we refactor test methods.,code_debt,low_quality_code,"Wed, 21 Sep 2016 21:04:16 +0000","Wed, 2 Oct 2019 17:15:08 +0000","Mon, 26 Sep 2016 20:12:54 +0000",428918,Checkstyle warnings can't be removed unless we refactor test methods.,-0.6,-0.6,neutral
hadoop,13732,comment_1,"Hi Mike, if we need to use a more recent version of Maven, then we also need to update BUILDING.txt. Could you comment on the availability of the required Maven version on a few common OSs? e.g. RHEL6, 7, Ubuntu 12/14/16.",documentation_debt,outdated_documentation,"Tue, 18 Oct 2016 18:47:19 +0000","Sat, 22 Oct 2016 00:07:53 +0000","Fri, 21 Oct 2016 23:41:59 +0000",276880,"Hi Mike, if we need to use a more recent version of Maven, then we also need to update BUILDING.txt. Could you comment on the availability of the required Maven version on a few common OSs? e.g. RHEL6, 7, Ubuntu 12/14/16.",0.0,0.0,neutral
hadoop,13732,comment_2,"I'd have to make a dependency-check specific note in BUILDING.txt, which seems a little awkard. (The normal build isn't affected, of course.) I'll see what I can do. My only alternative idea is a comment around this plugin in pom.xml. I do agree it needs to be documented somewhere. * I don't even think that maven is _available_ on RHEL 6.6 * My RHEL 7.2 machine looks like it would use version 3.0.5-16 * My Ubuntu 16.04 machine is using 3.3.9 * Looks like Ubuntu 14.04 uses 3.0.5-1 The maven release history page is at",documentation_debt,outdated_documentation,"Tue, 18 Oct 2016 18:47:19 +0000","Sat, 22 Oct 2016 00:07:53 +0000","Fri, 21 Oct 2016 23:41:59 +0000",276880,"I'd have to make a dependency-check specific note in BUILDING.txt, which seems a little awkard. (The normal build isn't affected, of course.) I'll see what I can do. My only alternative idea is a comment around this plugin in pom.xml. I do agree it needs to be documented somewhere. I don't even think that maven is available on RHEL 6.6 My RHEL 7.2 machine looks like it would use version 3.0.5-16 My Ubuntu 16.04 machine is using 3.3.9 Looks like Ubuntu 14.04 uses 3.0.5-1 The maven release history page is at https://maven.apache.org/docs/history.html",0.1125,0.1125,neutral
hadoop,13768,comment_4,"Looking at the following codes: 1. Please give {{l}} a more readable name. 2. Can you give some comments to explain some bit about the procedure? I (probably others) wouldn't know why it's like that without querying the SDK's manual. I know it now, there're 2 modes in the operation, one mode to return successfully deleted objects, and the other returning the deleting-failed objects. You're using the latter, and use a loop to try some times to delete and delete the failed-to-delete objects.",code_debt,low_quality_code,"Fri, 28 Oct 2016 08:27:17 +0000","Fri, 24 Nov 2017 08:43:36 +0000","Fri, 10 Feb 2017 06:56:06 +0000",9066529,"Looking at the following codes: 1. Please give l a more readable name. 2. Can you give some comments to explain some bit about the procedure? I (probably others) wouldn't know why it's like that without querying the SDK's manual. I know it now, there're 2 modes in the ossClient.deleteObjects operation, one mode to return successfully deleted objects, and the other returning the deleting-failed objects. You're using the latter, and use a loop to try some times to delete and delete the failed-to-delete objects.",-0.0125,-0.0109375,neutral
hadoop,13768,comment_6,"assuming that the file limit is always1000, why not just list the path in 1000 blocks and issue delete requests in that size. There are ultimate limits to the size of responses in path listings (max size of an HTTP request), and inevitably heap problems well before then.",design_debt,non-optimal_design,"Fri, 28 Oct 2016 08:27:17 +0000","Fri, 24 Nov 2017 08:43:36 +0000","Fri, 10 Feb 2017 06:56:06 +0000",9066529,"assuming that the file limit is always1000, why not just list the path in 1000 blocks and issue delete requests in that size. There are ultimate limits to the size of responses in path listings (max size of an HTTP request), and inevitably heap problems well before then.",-0.2948333333,-0.2948333333,neutral
hadoop,13991,comment_0,"Musaddique thank your for your post and details on a fix. I'm sorry to say we aren't going to take this. That's not because there's anything wrong with it, but because we've stopped doing any work on s3n other than any emergency security work, putting all our effort into S3a. Leaving s3n alone means that we have a reference s3 connector that is pretty much guaranteed not to have any regressions, while in s3a we can do more leading edge stuff. S3a does have retry logic, a lot built into the Amazon S3 library itself, with some extra bits to deal with things that aren't retried that well (e.g. final commit of a multipart upload). # please switch to s3a as soon as you can. If you are using Hadoop 2.7.3, its stable enough for use. # and, if you want to improve s3a, please get involved on that code, ideally look at the work in HADOOP-11694 to see what to look forward to in Hadoop 2.8, and HADOOP-13204 to see the todo list where help is really welcome and that includes help testing. thanks,",defect_debt,uncorrected_known_defects,"Sat, 14 Jan 2017 18:41:34 +0000","Fri, 24 Feb 2017 14:07:36 +0000","Fri, 24 Feb 2017 14:07:36 +0000",3525962,"Musaddique thank your for your post and details on a fix. I'm sorry to say we aren't going to take this. That's not because there's anything wrong with it, but because we've stopped doing any work on s3n other than any emergency security work, putting all our effort into S3a. Leaving s3n alone means that we have a reference s3 connector that is pretty much guaranteed not to have any regressions, while in s3a we can do more leading edge stuff. S3a does have retry logic, a lot built into the Amazon S3 library itself, with some extra bits to deal with things that aren't retried that well (e.g. final commit of a multipart upload). please switch to s3a as soon as you can. If you are using Hadoop 2.7.3, its stable enough for use. and, if you want to improve s3a, please get involved on that code, ideally look at the work in HADOOP-11694 to see what to look forward to in Hadoop 2.8, and HADOOP-13204 to see the todo list where help is really welcome and that includes help testing. thanks,",0.1673439153,0.1673439153,negative
hadoop,14314,comment_0,"Thanks for filing this JIRA. Since we didn't find official link to replace the old one, suggest to simply remove it.",documentation_debt,outdated_documentation,"Mon, 17 Apr 2017 21:38:32 +0000","Thu, 23 Aug 2018 13:21:29 +0000","Thu, 23 Aug 2018 13:07:10 +0000",42564518,"templedf Thanks for filing this JIRA. Since we didn't find official link to replace the old one, suggest to simply remove it.",0.2,0.2,neutral
hadoop,14351,comment_4,Thanks  for reviewing the patch. Attached another patch containing the checkstyle fixes excluding the fix for the {{FileLength}} related warning. The below findbugs warning is unrelated to changes done in the patch which was introduced in [Commit of HADOOP-10809,code_debt,low_quality_code,"Tue, 25 Apr 2017 15:53:17 +0000","Wed, 26 Apr 2017 23:53:39 +0000","Wed, 26 Apr 2017 20:58:05 +0000",104688,"Thanks liuml07 for reviewing the patch. Attached another patch containing the checkstyle fixes excluding the fix for the FileLength related warning. The below findbugs warning is unrelated to changes done in the patch which was introduced in Commit #2217e2f8ff418b88eac6ad36cafe3a9795a11f40 of HADOOP-10809 Useless object stored in variable keysToUpdateAsFolder of method org.apache.hadoop.fs.azure.NativeAzureFileSystem.mkdirs(Path, FsPermission, boolean)",-0.1333333333,-0.0045,positive
hadoop,14359,comment_2,"+1, the patch looks good to me. I confirmed that the modules do not have transitive commons-httpclient dependency. Unfortunately, not yet. Now hadoop-yarn-project and module have transitive commons-httpclient dependency via hbase-server. * hadoop-yarn-project - HBASE-16267 removed the dependency and the fix version is 2.0.0.",build_debt,over-declared_dependencies,"Thu, 27 Apr 2017 08:06:23 +0000","Wed, 2 Oct 2019 17:16:03 +0000","Mon, 1 May 2017 06:24:10 +0000",339467,"+1, the patch looks good to me. I confirmed that the modules do not have transitive commons-httpclient dependency. I'd like to get rid of commons-httpclient and make downstream applications aware it will no longer appear in Hadoop code. Unfortunately, not yet. Now hadoop-yarn-project and yarn-server-timelineserver-hbase module have transitive commons-httpclient dependency via hbase-server. hadoop-yarn-project -> hadoop-yarn-server -> hbase-server:1.2.4 -> commons-httpclient:3.1.0 HBASE-16267 removed the dependency and the fix version is 2.0.0.",0.0052,0.08414285714,positive
hadoop,14359,comment_3,Thanks for the pointer! I really wish we could get rid of commons-httpclient in Hadoop 3. ran mvn dependency:tree and I see commons-httpclient 3 is exposed in two jars: [INFO] [INFO] +- [INFO] | +- [INFO] | | \- [INFO] | +- [INFO] | +- [INFO] [INFO] +- [INFO] | +- [INFO] | | \- [INFO] | +- [INFO] | +-,code_debt,dead_code,"Thu, 27 Apr 2017 08:06:23 +0000","Wed, 2 Oct 2019 17:16:03 +0000","Mon, 1 May 2017 06:24:10 +0000",339467,Thanks for the pointer! I really wish we could get rid of commons-httpclient in Hadoop 3. ran mvn dependency:tree and I see commons-httpclient 3 is exposed in two jars: [INFO] org.apache.hadoop:hadoop-yarn-server-timelineservice-hbase:jar:3.0.0-alpha3-SNAPSHOT [INFO] +- org.apache.hbase:hbase-server:jar:1.2.4:compile [INFO] | +- org.apache.hbase:hbase-procedure:jar:1.2.4:compile [INFO] | | - org.apache.hbase:hbase-common:jar:tests:1.2.4:test [INFO] | +- org.apache.hbase:hbase-prefix-tree:jar:1.2.4:runtime [INFO] | +- commons-httpclient:commons-httpclient:jar:3.1:compile [INFO] org.apache.hadoop:hadoop-yarn-server-timelineservice-hbase-tests:jar:3.0.0-alpha3-SNAPSHOT [INFO] +- org.apache.hbase:hbase-server:jar:1.2.4:compile [INFO] | +- org.apache.hbase:hbase-procedure:jar:1.2.4:compile [INFO] | | - org.apache.hbase:hbase-common:jar:tests:1.2.4:test [INFO] | +- org.apache.hbase:hbase-prefix-tree:jar:1.2.4:runtime [INFO] | +- commons-httpclient:commons-httpclient:jar:3.1:compile,0.1333333333,0.01481481481,positive
hadoop,14479,comment_13,"The problem was that Intel didn't have anything that wasn't locked behind an email wall to get to the source tree. It was pretty much impossible for us to get anything that was reliable. FWIW, this isn't the only native component that isn't getting tested due to missing dependencies or just plain brokenness in the CMakefiles. I'll likely upgrade the Dockerfile bits to handle multiple OSes and architectures at some point. When I do that, dependencies will almost certainly get revisited. (I don't think anyone else is really paying attention to this stuff. Looking forward to the Jenkins upgrade, cuz a lot of non-Yetus stuff is just going to flat out break.)",build_debt,build_others,"Fri, 2 Jun 2017 14:27:32 +0000","Sat, 19 Aug 2017 02:50:34 +0000","Thu, 29 Jun 2017 03:28:35 +0000",2293263,"We're supposed to be running the ISA-L tests as part of precommit, so I'm not sure what happened here. The problem was that Intel didn't have anything that wasn't locked behind an email wall to get to the source tree. It was pretty much impossible for us to get anything that was reliable. FWIW, this isn't the only native component that isn't getting tested due to missing dependencies or just plain brokenness in the CMakefiles. I'll likely upgrade the Dockerfile bits to handle multiple OSes and architectures at some point. When I do that, dependencies will almost certainly get revisited. (I don't think anyone else is really paying attention to this stuff. Looking forward to the Jenkins upgrade, cuz a lot of non-Yetus stuff is just going to flat out break.)",0.1785238095,0.1562083333,neutral
hadoop,14479,comment_0,"Thanks for the report . I tried this locally too and ran into the same problem. We're supposed to be running the ISA-L tests as part of precommit, so I'm not sure what happened here. Seems like there's a test gap for ISA-L vs. the Java coder (time to revisit HDFS-11066?)  /  could you assist with debugging this?",test_debt,low_coverage,"Fri, 2 Jun 2017 14:27:32 +0000","Sat, 19 Aug 2017 02:50:34 +0000","Thu, 29 Jun 2017 03:28:35 +0000",2293263,"Thanks for the report Ayappan. I tried this locally too and ran into the same problem. We're supposed to be running the ISA-L tests as part of precommit, so I'm not sure what happened here. Seems like there's a test gap for ISA-L vs. the Java coder (time to revisit HDFS-11066?) Sammi / drankye could you assist with debugging this?",0.0,0.0,neutral
hadoop,14479,comment_14,"did we ever file that JIRA to get ISA-L re-enabled? ISA-L is required for production usage of EC, so having testing is really important.",test_debt,low_coverage,"Fri, 2 Jun 2017 14:27:32 +0000","Sat, 19 Aug 2017 02:50:34 +0000","Thu, 29 Jun 2017 03:28:35 +0000",2293263,"drankye did we ever file that JIRA to get ISA-L re-enabled? ISA-L is required for production usage of EC, so having testing is really important.",0.2,0.2,neutral
hadoop,14479,comment_3,"Hi , I agree, it would be good to fill the test gap. Let's revisit HDFS-11066. Thanks!",test_debt,low_coverage,"Fri, 2 Jun 2017 14:27:32 +0000","Sat, 19 Aug 2017 02:50:34 +0000","Thu, 29 Jun 2017 03:28:35 +0000",2293263,"Hi andrew.wang, We're supposed to be running the ISA-L tests as part of precommit, so I'm not sure what happened here. Seems like there's a test gap for ISA-L vs. the Java coder (time to revisit HDFS-11066?) I agree, it would be good to fill the test gap. Let's revisit HDFS-11066. Thanks!",0.296,0.148,neutral
hadoop,14479,comment_4,"Hmm, I thought the test gap was majorly caused by the precommit missing ISA-L building. Quite some time ago it was done in HADOOP-12626 but removed in HADOOP-13342. I thought we should bring it back and re-enable the ISA-L building & tests in precommit. Andrew, do you think so? If sounds good, we can fire a new issue to do it. Thanks!",test_debt,low_coverage,"Fri, 2 Jun 2017 14:27:32 +0000","Sat, 19 Aug 2017 02:50:34 +0000","Thu, 29 Jun 2017 03:28:35 +0000",2293263,"Hmm, I thought the test gap was majorly caused by the precommit missing ISA-L building. Quite some time ago it was done in HADOOP-12626 but removed in HADOOP-13342. I thought we should bring it back and re-enable the ISA-L building & tests in precommit. Andrew, do you think so? If sounds good, we can fire a new issue to do it. Thanks!",0.03133333333,0.03133333333,neutral
hadoop,14479,comment_8,"First of all, these three failed unit tests are not related with ISA-L version change. There are actually hidden issues. The implementation of native XOR encoder/decoder has array out of index issue which cause the JVM crash. One failed unit test is to test underlying encoder reuse by execution testCoding twice, while the underlying encoder is released explicitly by By review the context, I think is not the right place to release the encoder. 3. One failed unit test is because it doesn't consider the native RS coder situation. I have tried the patch locally. , thanks for reporting this. You can have a try with the new patch. Besides, I will close HADOOP-14593 later since I have merged fixes for 3 unit cases into one patch.",test_debt,expensive_tests,"Fri, 2 Jun 2017 14:27:32 +0000","Sat, 19 Aug 2017 02:50:34 +0000","Thu, 29 Jun 2017 03:28:35 +0000",2293263,"First of all, these three failed unit tests are not related with ISA-L version change. There are actually hidden issues. 1.TestHHXORErasureCoder The implementation of native XOR encoder/decoder has array out of index issue which cause the JVM crash. 2.TestRSErasureCoder One failed unit test is to test underlying encoder reuse by execution testCoding twice, while the underlying encoder is released explicitly by ErasureEncodingStep.finish. By review the context, I think ErasureEncodingStep.finish is not the right place to release the encoder. 3. TestCodecRawCoderMapping One failed unit test is because it doesn't consider the native RS coder situation. I have tried the patch locally. Ayappan, thanks for reporting this. You can have a try with the new patch. Besides, I will close HADOOP-14593 later since I have merged fixes for 3 unit cases into one patch.",-0.1403888889,-0.1151333333,neutral
hadoop,14870,comment_2,"Got the tests working here...they were actually running, though the use of JUnit4 rules to name threads was amplifying confusion. Seen a test failure",code_debt,low_quality_code,"Fri, 15 Sep 2017 17:50:21 +0000","Fri, 27 Oct 2017 11:57:26 +0000","Fri, 27 Oct 2017 11:57:11 +0000",3607610,"Got the tests working here...they were actually running, though the use of JUnit4 rules to name threads was amplifying confusion. Seen a test failure",0.06666666667,0.06666666667,negative
hadoop,14942,comment_1,LGTM +1 cleanup code is always good to make robust,code_debt,low_quality_code,"Wed, 11 Oct 2017 00:41:37 +0000","Fri, 20 Oct 2017 22:01:24 +0000","Fri, 20 Oct 2017 21:29:45 +0000",852488,LGTM +1 cleanup code is always good to make robust,0.588,0.588,positive
hadoop,15476,comment_1,"Thanks for catching this . Can we just remove this log message? There is another message below: For wildcard address, localAddr will be null. So perhaps we can fix this other log message to print wildcard if localAddr is null.",code_debt,low_quality_code,"Thu, 17 May 2018 19:31:54 +0000","Wed, 1 Aug 2018 20:07:56 +0000","Wed, 1 Aug 2018 19:34:06 +0000",6566532,"Thanks for catching this ajayydv. Can we just remove this log message? There is another message below: For wildcard address, localAddr will be null. So perhaps we can fix this other log message to print wildcard if localAddr is null.",0.1125,0.1125,neutral
hadoop,15486,comment_0,There's no need for a config. Just make the lock always be fair. Updates to the topology should be rare compared to block placements so the throughput degradation of the fair lock will be minimal.,code_debt,multi-thread_correctness,"Mon, 21 May 2018 19:28:58 +0000","Mon, 23 Jul 2018 17:14:03 +0000","Wed, 23 May 2018 17:35:51 +0000",166013,There's no need for a config. Just make the lock always be fair. Updates to the topology should be rare compared to block placements so the throughput degradation of the fair lock will be minimal.,0.1666666667,0.1666666667,neutral
hadoop,15569,comment_0,"Patch 001 * document assumed roles better, in particularly, the permissions you need for S3Guard read and S3Guard admin. * Change structure to avoid listing which commands need which perms, and be a bit expansive in what is needed, to line up for future changes (e.g. get/set object tags) * I have tested the s3guard stuff with an assumed role.",code_debt,low_quality_code,"Thu, 28 Jun 2018 19:19:44 +0000","Tue, 10 Jul 2018 16:58:49 +0000","Tue, 10 Jul 2018 16:58:49 +0000",1028345,"Patch 001 document assumed roles better, in particularly, the permissions you need for S3Guard read and S3Guard admin. Change structure to avoid listing which commands need which perms, and be a bit expansive in what is needed, to line up for future changes (e.g. get/set object tags) I have tested the s3guard stuff with an assumed role.",-0.0949,-0.0949,neutral
hadoop,15577,comment_0,"We don't need the 0-rename stuff, because distcp, except in the --atomic mode, isn't trying to do atomic operations. What we do need, is for distcp to not upload to a temp file and rename each one into place: remove that and for non-atomic uploads you eliminate the O(data) delay after each upload. Closing as a duplicate of that. *as that JIRA has no code/tests, I would support anyone who sat down to do implement the feature* There's also lots of work going on with HDFS to have an explicit multipart upload mechanism for filesystems, which can be used for a block-by-block upload to S3, this would improve distcp upload perf on files in HDFS > 1 block, as the blocks could be uploaded in parallel with locality. Keep an eye on that",test_debt,lack_of_tests,"Mon, 2 Jul 2018 22:14:48 +0000","Tue, 3 Jul 2018 14:20:20 +0000","Tue, 3 Jul 2018 14:20:20 +0000",57932,"We don't need the 0-rename stuff, because distcp, except in the --atomic mode, isn't trying to do atomic operations. What we do need, is for distcp to not upload to a temp file and rename each one into place: remove that and for non-atomic uploads you eliminate the O(data) delay after each upload. Closing as a duplicate of that. as that JIRA has no code/tests, I would support anyone who sat down to do implement the feature There's also lots of work going on with HDFS to have an explicit multipart upload mechanism for filesystems, which can be used for a block-by-block upload to S3, this would improve distcp upload perf on files in HDFS > 1 block, as the blocks could be uploaded in parallel with locality. Keep an eye on that",0.07795555556,0.07795555556,neutral
hadoop,15645,comment_0,"Patch 001 * better setup of implementations, especially bucket config, where per-bucket settings are cleared * and both check the metastore type of the test FS, skip if not correct -and include details on store in the message. * testDiff test erases test FS in testDiff setup through rawFS and metastore FS, in case it gets contaminated This addresses the double setup of dynamodb being picked up on a local test run, the ddb metastore getting tainted with stuff which shouldn't be there, and the s3 bucket getting stuff into it which the getFileSystem() fs doesn't see/delete.",design_debt,non-optimal_design,"Wed, 1 Aug 2018 00:16:58 +0000","Mon, 13 Aug 2018 21:12:02 +0000","Mon, 13 Aug 2018 11:22:35 +0000",1076737,"Patch 001 better setup of AbstractS3GuardToolTestBase implementations, especially bucket config, where per-bucket settings are cleared ITestS3GuardToolDynamoDB and ITestS3GuardToolLocal both check the metastore type of the test FS, skip if not correct -and include details on store in the message. testDiff test erases test FS in testDiff setup through rawFS and metastore FS, in case it gets contaminated This addresses the double setup of dynamodb being picked up on a local test run, the ddb metastore getting tainted with stuff which shouldn't be there, and the s3 bucket getting stuff into it which the getFileSystem() fs doesn't see/delete.",-0.14375,-0.14375,neutral
hadoop,15742,comment_7,"Other than checkstyle issue, +1 for others.",code_debt,low_quality_code,"Tue, 11 Sep 2018 08:04:59 +0000","Tue, 18 Sep 2018 03:14:56 +0000","Tue, 18 Sep 2018 03:12:47 +0000",587268,"Other than checkstyle issue, +1 for others.",0.0,0.0,neutral
hadoop,15859,comment_0,Attached a patch that removes the JNI setting of the remaining field per Ben's analysis above and cleans up the naming re: objects vs. classes in the JNI function arguments.,code_debt,low_quality_code,"Tue, 16 Oct 2018 18:31:51 +0000","Wed, 7 Nov 2018 01:34:32 +0000","Wed, 17 Oct 2018 19:48:06 +0000",90975,Attached a patch that removes the JNI setting of the remaining field per Ben's analysis above and cleans up the naming re: objects vs. classes in the JNI function arguments.,0.0,0.0,neutral
hadoop,16044,comment_0,"Following up for -HADOOP-15662, c-ompared to L173: - Updated the format to: *Unknown host name: %s. Retrying to resolve the host name...* - Replace *""ex.getMessage()*"" with although they both return host name string, but the *getHost()* seems to be more readable code. - Removed the else if logic as it breaks the current exception trace. Test: testUnknownHost() would take about 14 minutes due to the retry, I verified the warning format in the console: I haven't come up with a good way to retrieve the log msg for test, any suggestions? Testes against US west account passed: All tests passed my US west account: XNS account oauth 35, 0, Errors: 0, Skipped: 0 324, 0, Errors: 0, Skipped: 22 168, 0, Errors: 0, Skipped: 21 XNS account sharedKey: 35, 0, Errors: 0, Skipped: 0 324, 0, Errors: 0, Skipped: 20 168, 0, Errors: 0, Skipped: 15 non-xns account sharedKe: 35, 0, Errors: 0, Skipped: 0 324, 0, Errors: 0, Skipped: 206 168, 0, Errors: 0, Skipped: 15",code_debt,low_quality_code,"Fri, 11 Jan 2019 18:02:56 +0000","Mon, 14 Jan 2019 20:03:40 +0000","Mon, 14 Jan 2019 19:47:06 +0000",265450,"Following up for -HADOOP-15662, c-ompared to HADOOP-15662-002.patch: L173: Updated the format to: Unknown host name: %s. Retrying to resolve the host name... Replace ""ex.getMessage()"" with ""httpOperation.getUrl().getHost())"", although they both return host name string, but the getHost()seems to be more readable code. Removed the else if logic as it breaks the current exception trace. Test: testUnknownHost() would take about 14 minutes due to the retry, I verified the warning format in the console: I haven't come up with a good way to retrieve the log msg for test, any suggestions? Testes against US west account passed: All tests passed my US west account: XNS account oauth Tests run: 35, Failures: 0, Errors: 0, Skipped: 0 Tests run: 324, Failures: 0, Errors: 0, Skipped: 22 Tests run: 168, Failures: 0, Errors: 0, Skipped: 21 XNS account sharedKey: Tests run: 35, Failures: 0, Errors: 0, Skipped: 0 Tests run: 324, Failures: 0, Errors: 0, Skipped: 20 Tests run: 168, Failures: 0, Errors: 0, Skipped: 15 non-xns account sharedKe: Tests run: 35, Failures: 0, Errors: 0, Skipped: 0 Tests run: 324, Failures: 0, Errors: 0, Skipped: 206 Tests run: 168, Failures: 0, Errors: 0, Skipped: 15",-0.02505,-0.01565625,neutral
hadoop,16044,comment_3,"This is a silly question, but why is being retried? Because JVMs do have a history of caching negative DNS lookup results. Looking at the latest javadocs, is set to 10s, so it will eventually come up, but you do have to trust DNS to be updating its records Elsewhere i the code we're just treating this as unrecoverable. Which may not be correct action in a world of dynamicness...but if we do start changing this policy, we should think about doing it consistently everywhere (including HA failover events)",design_debt,non-optimal_design,"Fri, 11 Jan 2019 18:02:56 +0000","Mon, 14 Jan 2019 20:03:40 +0000","Mon, 14 Jan 2019 19:47:06 +0000",265450,"This is a silly question, but why is UnknownHostException being retried? Because JVMs do have a history of caching negative DNS lookup results. Looking at the latest javadocs, networkaddress.cache.negative.ttl is set to 10s, so it will eventually come up, but you do have to trust DNS to be updating its records Elsewhere i the code we're just treating this as unrecoverable. Which may not be correct action in a world of dynamicness...but if we do start changing this policy, we should think about doing it consistently everywhere (including HA failover events)",0.09,0.00625,neutral
hadoop,16044,comment_4,"ABFS has been retrying on since it previewed because our understanding is that this exception is thrown for transient name resolution failures. Our retry policy last longer than the typical DNS TTL (or negative cache) of 5 minutes, so the driver could recover and enable a long running task to complete successfully. WASB also retries for these. I expect ADL retries too, although have not confirmed. Mostly we do this for status quo, I mean, it is less likely to cause a regression if we keep the current behavior. With that said, if you have evidence this is a bad design, we should change it. I see that we do the opposite for S3, but I don't know what led to that decision nor do I have a good sense for the behavior in the wild, so I don't know what's best. Certainly retrying is not going to increase the recovery time on the node in question.",design_debt,non-optimal_design,"Fri, 11 Jan 2019 18:02:56 +0000","Mon, 14 Jan 2019 20:03:40 +0000","Mon, 14 Jan 2019 19:47:06 +0000",265450,"ABFS has been retrying onUnknownHostException since it previewed because our understanding is that this exception is thrown for transient name resolution failures. Our retry policy last longer than the typical DNS TTL (or negative cache) of 5 minutes, so the driver could recover and enable a long running task to complete successfully.WASB also retries for these. I expect ADL retries too, although have not confirmed. Mostly we do this for status quo, I mean, it is less likely to cause a regression if we keep the current behavior. With that said, ifyou have evidence this is a bad design, we should change it. I see thatwe do the opposite for S3, but I don't know what led to that decision nor do I have a good sense for the behavior in the wild, so I don't know what's best. Certainly retrying is not going to increase the recovery time on the node in question.",-0.1557291667,-0.1557291667,neutral
hadoop,16044,comment_5,"bq With that said, if you have evidence this is a bad design, we should change it. no, I don't think it is a bad design. I'm curious. In a classic physical deployment, DNS failures are a bad sign. And, because the JVM cached -ve DNS results *forever* , spinning never fixed things. If the JVMs have stopped doing this, then in a dynamic world, this makes sense. I wondering something broader, which is: is the assumption that not worth retrying"" no longer valid? And if so, what to do about all those existing uses?",design_debt,non-optimal_design,"Fri, 11 Jan 2019 18:02:56 +0000","Mon, 14 Jan 2019 20:03:40 +0000","Mon, 14 Jan 2019 19:47:06 +0000",265450,"bq With that said, if you have evidence this is a bad design, we should change it. no, I don't think it is a bad design. I'm curious. In a classic physical deployment, DNS failures are a bad sign. And, because the JVM cached -ve DNS results forever , spinning never fixed things. If the JVMs have stopped doing this, then in a dynamic world, this makes sense. I wondering something broader, which is: is the assumption that ""UnknownHostException not worth retrying"" no longer valid? And if so, what to do about all those existing uses?",-0.1125,-0.1125,neutral
hadoop,16044,comment_6,"Good questions, and the answers are complicated by JVM implementations which may not honor DNS TTL for negative caching. Testing with fault injection could shed light here, but not sure how easy/difficult this is? Could we commit this change while we dig into this more?",test_debt,lack_of_tests,"Fri, 11 Jan 2019 18:02:56 +0000","Mon, 14 Jan 2019 20:03:40 +0000","Mon, 14 Jan 2019 19:47:06 +0000",265450,"Good questions, and the answers are complicated by JVM implementations which may nothonorDNS TTL for negative caching. Testing with fault injection could shed light here, but not sure how easy/difficult this is? Could we commit this change while we dig into this more?",0.151,0.151,neutral
hadoop,16160,comment_0,"If AdlFS contract test is not enabled, {{fs}} is null in When calling it calls and then it calls {{fs.delete()}}, finally NPE occurs. This issue is fixed by HADOOP-14170 in 2.9+, so I'll backport HADOOP-14170 to fix this issue.",test_debt,lack_of_tests,"Mon, 4 Mar 2019 09:57:24 +0000","Mon, 4 Mar 2019 11:05:53 +0000","Mon, 4 Mar 2019 11:05:53 +0000",4109,"If AdlFS contract test is not enabled, fs is null in TestAdlFileSystemContractLive#setUp. When calling TestAdlFileSystemContractLive#tearDown(), it calls FileSystemContractBaseTest#tearDown and then it calls fs.delete(), finally NPE occurs. This issue is fixed by HADOOP-14170 in 2.9+, so I'll backport HADOOP-14170 to fix this issue.",0.0,0.0,negative
hadoop,16207,comment_1,"could be more fundamental as in ""I'm not sure the committers are correctly telling S3Guard about parent directories"". After each PUT is manifest, we call finishedWrite() , but that seems to do more about purging spurious deleted files, rather than adding dir entries into S3Guard. Provided mkdirs() is called Proposed: build a list of all directories which need to exist as part of a job commit, and only create those entries The other strategy is for to mkdir on the parent. That's a bit more expensive though. Better to not worry about whether it exists and do all this stuff during job commit only",design_debt,non-optimal_design,"Tue, 26 Mar 2019 13:07:38 +0000","Fri, 4 Oct 2019 13:27:40 +0000","Fri, 4 Oct 2019 13:17:44 +0000",16589406,"could be more fundamental as in ""I'm not sure the committers are correctly telling S3Guard about parent directories"". After each PUT is manifest, we call finishedWrite() , but that seems to do more about purging spurious deleted files, rather than adding dir entries into S3Guard. Provided mkdirs() is called Proposed: build a list of all directories which need to exist as part of a job commit, and only create those entries The other strategy is for initiateMultipartUpload() to mkdir on the parent. That's a bit more expensive though. Better to not worry about whether it exists and do all this stuff during job commit only",-0.1525333333,-0.1525333333,neutral
hadoop,16207,comment_8,"Also, to run the tests in parallel - the jobs need to start using a different directory name. Currently, all of them use testMRJob (The method name in the common class that all tests inherit from). The issue with the local dir conflict is a MR configuration afaik (Likely the MR tmp dir config property). YARN clusters should already be able to run in parallel (different ports, random dir names, etc) I'd also be careful trying to run too many of these in parallel, given the amount of memory they consume. Maybe a different parallelism flag for any tests running on a cluster? How about simplifying the code and letting the tests reside in the same class, which makes the code easier to read and allows sharing a cluster more easily. Haven't seen the WIP patch - but sharing a cluster across different tests, which may or may not trigger at the same time seems like it may cause problems. The tests also use a 1 s sleep for the InconsistentFS to get into a consistent state. That can lead to flakiness in the tests. A higher sleep, unless InconsistentFS can be set up with an actual waitForConsistency method which is not time based.",test_debt,flaky_test,"Tue, 26 Mar 2019 13:07:38 +0000","Fri, 4 Oct 2019 13:27:40 +0000","Fri, 4 Oct 2019 13:17:44 +0000",16589406,"Also, to run the tests in parallel - the jobs need to start using a different directory name. Currently, all of them use testMRJob (The method name in the common class that all tests inherit from). The issue with the local dir conflict is a MR configuration afaik (Likely the MR tmp dir config property). YARN clusters should already be able to run in parallel (different ports, random dir names, etc) I'd also be careful trying to run too many of these in parallel, given the amount of memory they consume. Maybe a different parallelism flag for any tests running on a cluster? How about simplifying the code and letting the tests reside in the same class, which makes the code easier to read and allows sharing a cluster more easily. Haven't seen the WIP patch - but sharing a cluster across different tests, which may or may not trigger at the same time seems like it may cause problems. The tests also use a 1 s sleep for the InconsistentFS to get into a consistent state. That can lead to flakiness in the tests. A higher sleep, unless InconsistentFS can be set up with an actual waitForConsistency method which is not time based.",-0.09324,-0.09324,neutral
hadoop,16226,comment_1,thanks for the patch . Maybe use a simple double-slash comment to avoid this checkstyle issue: +1 pending ^,code_debt,low_quality_code,"Mon, 1 Apr 2019 23:59:49 +0000","Sun, 5 Apr 2020 08:54:10 +0000","Wed, 3 Apr 2019 04:19:35 +0000",101986,thanks for the patch ajisakaa. Maybe use a simple double-slash comment to avoid this checkstyle issue: +1 pending ^,0.05,0.05,positive
hadoop,16226,comment_2,Thanks  for the review. Added a period to fix the checkstyle warning.,code_debt,low_quality_code,"Mon, 1 Apr 2019 23:59:49 +0000","Sun, 5 Apr 2020 08:54:10 +0000","Wed, 3 Apr 2019 04:19:35 +0000",101986,Thanks jira.shegalov for the review. Added a period to fix the checkstyle warning.,-0.1,-0.06666666667,positive
hadoop,16265,comment_1,Maybe I should move it to hadoop-common project.,architecture_debt,violation_of_modularity,"Thu, 18 Apr 2019 01:57:25 +0000","Mon, 22 Apr 2019 15:32:30 +0000","Mon, 22 Apr 2019 15:22:36 +0000",393911,Maybe I should move it to hadoop-common project.,0.0,0.0,neutral
hadoop,16265,comment_0,"It is a little wired to return only the raw literal number when default time units and literal number is provided. It may mislead us to unexpected behave if we just take into account its name and parameters. Though changing long 10 to string '10s' will return expected result, I think it should return the same result when long 10 and default time unit SECOND is given.",code_debt,low_quality_code,"Thu, 18 Apr 2019 01:57:25 +0000","Mon, 22 Apr 2019 15:32:30 +0000","Mon, 22 Apr 2019 15:22:36 +0000",393911,"It is a little wired to return only the raw literal number when default time units and literal number is provided. It may mislead us to unexpected behave if we just take into account its name and parameters.Though changing long 10 to string '10s' will return expected result, I think it should return the same result when long 10 and default time unit SECOND is given.",-0.5333333333,-0.5333333333,neutral
hadoop,16265,comment_4,Thanks  for catching this! Should we use {{defaultUnit}} instead of in the latest patch v2?,code_debt,low_quality_code,"Thu, 18 Apr 2019 01:57:25 +0000","Mon, 22 Apr 2019 15:32:30 +0000","Mon, 22 Apr 2019 15:22:36 +0000",393911,Thanks starphin for catching this! Should we use defaultUnit instead of TimeUnit.SECONDS in the latest patch v2?,0.225,0.15,neutral
hadoop,16265,comment_2,"Hey , this is a good catch. It looks like  and I missed this when doing HDFS-14346. Would it not be simpler to just do: It seems a little weird to me to convert a number to a string just so that we can re-parse it into a number.",design_debt,non-optimal_design,"Thu, 18 Apr 2019 01:57:25 +0000","Mon, 22 Apr 2019 15:32:30 +0000","Mon, 22 Apr 2019 15:22:36 +0000",393911,"Hey starphin, this is a good catch. It looks like csun and I missed this when doing HDFS-14346. Would it not be simpler to just do: It seems a little weird to me to convert a number to a string just so that we can re-parse it into a number.",0.2586666667,0.2586666667,neutral
hadoop,16291,comment_1,Thanks . Thought it was strange the docs were wrong for this long. Was going to ask for a sanity check on this JIRA but you beat me to it.,documentation_debt,low_quality_documentation,"Thu, 2 May 2019 20:59:59 +0000","Fri, 3 May 2019 16:34:41 +0000","Fri, 3 May 2019 16:28:04 +0000",70085,Thanks daryn. Thought it was strange the docs were wrong for this long. Was going to ask for a sanity check on this JIRA but you beat me to it.,0.2666666667,0.2666666667,negative
hadoop,16359,comment_5,yes I tried the patch on 2.9 and it worked - before the patch the native libs are not bundled under Not sure how to test this - does other codecs have unit tests for the bundling part?,test_debt,lack_of_tests,"Tue, 11 Jun 2019 03:15:20 +0000","Wed, 2 Oct 2019 17:14:48 +0000","Fri, 14 Jun 2019 22:24:05 +0000",328125,weichiu yes I tried the patch on 2.9 and it worked - before the patch the native libs are not bundled under $HADOOP_HOME/lib/native. Not sure how to test this - does other codecs have unit tests for the bundling part?,0.2,0.1,neutral
hadoop,16435,comment_8,I don't think this is about a single session - these metrics are only used in the [IPC If you create an server and then shut it down - would you expect its metrics to be retained? (IMHO: if it silently retaines any information related to the actual instance that's a resource leak),code_debt,low_quality_code,"Wed, 17 Jul 2019 09:08:42 +0000","Tue, 30 Jul 2019 00:46:46 +0000","Tue, 30 Jul 2019 00:46:46 +0000",1093084,I don't think this is about a single session - these metrics are only used in the IPC Server. If you create an server and then shut it down - would you expect its metrics to be retained? (IMHO: if it silently retaines any information related to the actual instance that's a resource leak),-0.152,-0.032,neutral
hbase,23,comment_16,"FYI, leave out the CHANGES.txt changes. Your patch will fail if someone has made a commit ahead of yours. I tried the patch. Looks really good. I love that we're now showing server names and thanks for renaming catalog tables section. Would suggest that no reason catalog tables shouldn't be clickable as user tables are. Usually will be one region only but even so, the new Table page shows where that region is located and gives a never-before available quick-path to the hosting region server. The Is Split column in the table will probably never be true especially when we are doing this: Would suggest you remove it (Sorry - -my fault for suggesting it in the first place) Would you mind fixing the requests calcuation? Its kinda weird at the moment. Its requests per (default 3 seconds). It should be showing requests per second. Thats what people expect. On the code, FYI, the hadoop convention is two-spaces for tabs. Regards your TODO, that you've duplicated code until we add MetaTable, thats fine. Was wondering if the .META. table was made of multiple regions and your scanner had to cross from one to the other, do you handle that case? Otherwise, the patch is great.",code_debt,duplicated_code,"Wed, 21 Nov 2007 23:00:46 +0000","Fri, 22 Aug 2008 21:13:04 +0000","Wed, 14 May 2008 19:08:23 +0000",15106057,"FYI, leave out the CHANGES.txt changes. Your patch will fail if someone has made a commit ahead of yours. I tried the patch. Looks really good. I love that we're now showing server names and thanks for renaming catalog tables section. Would suggest that no reason catalog tables shouldn't be clickable as user tables are. Usually will be one region only but even so, the new Table page shows where that region is located and gives a never-before available quick-path to the hosting region server. The Is Split column in the table will probably never be true especially when we are doing this: Would suggest you remove it (Sorry - -my fault for suggesting it in the first place) Would you mind fixing the requests calcuation? Its kinda weird at the moment. Its requests per hbase.regionsserver.msginterval (default 3 seconds). It should be showing requests per second. Thats what people expect. On the code, FYI, the hadoop convention is two-spaces for tabs. Regards your TODO, that you've duplicated code until we add MetaTable, thats fine. Was wondering if the .META. table was made of multiple regions and your scanner had to cross from one to the other, do you handle that case? Otherwise, the patch is great.",0.03313157895,0.02997619048,neutral
hbase,23,comment_17,"It think so. The first loop scans over each region of .META. Regards the calculations, I searched in the whole source and it doesn't seem to be used, only referred to in the text so I think I will just remove it. Correct me if I'm wrong. So that means that they are clickable? I'm confused.",code_debt,dead_code,"Wed, 21 Nov 2007 23:00:46 +0000","Fri, 22 Aug 2008 21:13:04 +0000","Wed, 14 May 2008 19:08:23 +0000",15106057,"Was wondering if the .META. table was made of multiple regions and your scanner had to cross from one to the other, do you handle that case? It think so. The first loop scans over each region of .META. Regards the calculations, I searched ""hbase.regionserver.msginterval"" in the whole source and it doesn't seem to be used, only referred to in the text so I think I will just remove it. Correct me if I'm wrong. Would suggest that no reason catalog tables shouldn't be clickable as user tables are. So that means that they are clickable? I'm confused.",-0.0125,-0.006730769231,neutral
hbase,23,comment_18,"Review please. Things that changed : + Request load is now expressed in req/sec. + ROOT and META tables are clickable. The fact that these tables are not handled like user tables internally impacts the way I could show information. Suggestions appreciated. + Small indentation change. - Removed ""is split"" information on regions.",code_debt,low_quality_code,"Wed, 21 Nov 2007 23:00:46 +0000","Fri, 22 Aug 2008 21:13:04 +0000","Wed, 14 May 2008 19:08:23 +0000",15106057,"Review please. Things that changed : + Request load is now expressed in req/sec. + ROOT and META tables are clickable. The fact that these tables are not handled like user tables internally impacts the way I could show information. Suggestions appreciated. + Small indentation change. Removed ""is split"" information on regions.",0.1,0.08571428571,neutral
hbase,239,comment_3,Committed w/ below message. Resolving. M bin/hbase Had a hard-coded name for the hbase jar. Fix so allows for version in jar name.,code_debt,low_quality_code,"Tue, 28 Aug 2007 21:56:48 +0000","Mon, 4 Feb 2008 18:41:16 +0000","Wed, 29 Aug 2007 04:31:51 +0000",23703,Committed w/ below message. Resolving. M bin/hbase Had a hard-coded name for the hbase jar. Fix so allows for version in jar name.,0.175,0.175,neutral
hbase,467,comment_4,I'm fine w/ all being under o.a.h.h.r rather than under subpackages under o.a.h.h.r -- especially if its loads of work. Introducing o.a.h.h.r package is sufficient improvement over what we had previous. Good stuff.,architecture_debt,violation_of_modularity,"Sun, 24 Feb 2008 05:57:43 +0000","Fri, 22 Aug 2008 21:35:04 +0000","Sun, 24 Feb 2008 22:10:11 +0000",58348,I'm fine w/ all being under o.a.h.h.r rather than under subpackages under o.a.h.h.r  especially if its loads of work. Introducing o.a.h.h.r package is sufficient improvement over what we had previous. Good stuff.,0.6198333333,0.6198333333,negative
hbase,467,comment_3,"On second thought, I am going to put HBASE-467 on the back burner until HBASE-469 is completed. Since the original patch was based on unrefactored code, the changes will be easier to apply now than later.",code_debt,low_quality_code,"Sun, 24 Feb 2008 05:57:43 +0000","Fri, 22 Aug 2008 21:35:04 +0000","Sun, 24 Feb 2008 22:10:11 +0000",58348,"On second thought, I am going to put HBASE-467 on the back burner until HBASE-469 is completed. Since the original patch was based on unrefactored code, the changes will be easier to apply now than later.",0.0,0.0,negative
hbase,467,comment_6,There is just too much intermodule coupling to do this properly without breaking encapsulation.,code_debt,low_quality_code,"Sun, 24 Feb 2008 05:57:43 +0000","Fri, 22 Aug 2008 21:35:04 +0000","Sun, 24 Feb 2008 22:10:11 +0000",58348,There is just too much intermodule coupling to do this properly without breaking encapsulation.,-0.5,-0.5,negative
hbase,467,comment_1,"This is looking ugly. HRegion references a number of methods in HStore and HStoreFile. HStore references static methods from HRegion. HRegion and HStore are also tightly coupled with HLog. We can probably factor out the inner classes of Hregion, HStore and HStoreFile into but trying to tease these apart using the current plan will either be a) a ton of work or b) turn out to not be possible. Comments, please!",design_debt,non-optimal_design,"Sun, 24 Feb 2008 05:57:43 +0000","Fri, 22 Aug 2008 21:35:04 +0000","Sun, 24 Feb 2008 22:10:11 +0000",58348,"This is looking ugly. HRegion references a number of methods in HStore and HStoreFile. HStore references static methods from HRegion. HRegion and HStore are also tightly coupled with HLog. We can probably factor out the inner classes of Hregion, HStore and HStoreFile into o.a.h.h.regionserver. {region,store,storefile(??)} , but trying to tease these apart using the current plan will either be a) a ton of work or b) turn out to not be possible. Comments, please!",-0.2458333333,-0.184375,negative
hbase,467,comment_2,"Looking at the issue further, it appears that making subpackages for region, store, etc inner classes is not going to work well. Although many of the inner classes are static, in order for the parent class to access them, too much would have to be made public. Just factoring out inner classes (into and making them package scope would yield the highest containment.",design_debt,non-optimal_design,"Sun, 24 Feb 2008 05:57:43 +0000","Fri, 22 Aug 2008 21:35:04 +0000","Sun, 24 Feb 2008 22:10:11 +0000",58348,"Looking at the issue further, it appears that making subpackages for region, store, etc inner classes is not going to work well. Although many of the inner classes are static, in order for the parent class to access them, too much would have to be made public. Just factoring out inner classes (into o.a.h.h.regionserver) and making them package scope would yield the highest containment.",-0.1367777778,-0.1367777778,negative
hbase,798,comment_11,"Jim, I may have misspoken in my explanation. There are only single HRS methods that take all arguments. Well, there are two of deleteAll but there's different HR implementations. public void deleteAll(final byte [] regionName, final byte [] row, final byte [] column, final long timestamp, final long lockId) public void deleteAll(final byte [] regionName, final byte [] row, final long timestamp, final long lockId) public void deleteFamily(byte [] regionName, byte [] row, byte [] family, long timestamp, final long lockId) public void batchUpdate(final byte [] regionName, BatchUpdate b, final long lockId)",code_debt,duplicated_code,"Wed, 6 Aug 2008 04:29:41 +0000","Sun, 13 Sep 2009 22:33:29 +0000","Wed, 13 Aug 2008 02:35:03 +0000",597922,"Jim, I may have misspoken in my explanation. There are only single HRS methods that take all arguments. Well, there are two of deleteAll but there's different HR implementations. public void deleteAll(final byte [] regionName, final byte [] row, final byte [] column, final long timestamp, final long lockId) public void deleteAll(final byte [] regionName, final byte [] row, final long timestamp, final long lockId) public void deleteFamily(byte [] regionName, byte [] row, byte [] family, long timestamp, final long lockId) public void batchUpdate(final byte [] regionName, BatchUpdate b, final long lockId)",0.15775,0.15775,neutral
hbase,798,comment_16,"Reviewed patch. -1 because: - should renew the lease if it is passed an outstanding lock id - should use as it is much more efficient than - In HRegion, factor out the multiple occurrances of: into a private or protected method, such as: - isRowLocked should be protected or private",code_debt,slow_algorithm,"Wed, 6 Aug 2008 04:29:41 +0000","Sun, 13 Sep 2009 22:33:29 +0000","Wed, 13 Aug 2008 02:35:03 +0000",597922,"Reviewed patch. -1 because: HRegionServer.getLockFromId should renew the lease if it is passed an outstanding lock id HRegionServer.rowlocks should use java.util.concurrent.ConcurrentHashMap as it is much more efficient than Collections.synchronizedMap(HashMap) In HRegion, factor out the multiple occurrances of: into a private or protected method, such as: isRowLocked should be protected or private",0.2333333333,0.15,neutral
hbase,798,comment_3,"Here's my first go. This adds lockRow and unlockRow methods to the client/HTable. The return type of lockRow is a new client object RowLock (contains long lockid and byte[] row). unlockRow takes a RowLock as argument. Also adds new versions of commit, deleteAll, and deleteFamily that also can take RowLock's. Within HRS I created new versions of the same functions which take the actual lock ids: long in HRS (randomized id as with scannerids, used for leases, etc), Integer in HR (existing type used for row locks). Existing functions, which do not explicitly take locks, now call the same HR functions but with nulls as row locks. Internally HR checks if it was passed a valid row lock or not. If it's a null, it obtains a row lock and releases it at the end of the call. If it's valid, it makes use of the lock, and does nothing at the end. Otherwise an exception is thrown. Code has not been completely cleaned up, but this demonstrates how the client row locks are being implemented.",code_debt,low_quality_code,"Wed, 6 Aug 2008 04:29:41 +0000","Sun, 13 Sep 2009 22:33:29 +0000","Wed, 13 Aug 2008 02:35:03 +0000",597922,"Here's my first go. This adds lockRow and unlockRow methods to the client/HTable. The return type of lockRow is a new client object RowLock (contains long lockid and byte[] row). unlockRow takes a RowLock as argument. Also adds new versions of commit, deleteAll, and deleteFamily that also can take RowLock's. Within HRS I created new versions of the same functions which take the actual lock ids: long in HRS (randomized id as with scannerids, used for leases, etc), Integer in HR (existing type used for row locks). Existing functions, which do not explicitly take locks, now call the same HR functions but with nulls as row locks. Internally HR checks if it was passed a valid row lock or not. If it's a null, it obtains a row lock and releases it at the end of the call. If it's valid, it makes use of the lock, and does nothing at the end. Otherwise an exception is thrown. Code has not been completely cleaned up, but this demonstrates how the client row locks are being implemented.",0.0115,0.0115,neutral
hbase,798,comment_1,"After discussing with Jim on IRC, there won't be very many additional functions exposed through HTable. It will actually just be one new version of each of: batchUpdate, deleteAll x 2, and deleteFamily. So it would be those 4, plus the lockRow/unlockRow. You think it would be okay to include just those in HTable w/o subclassing it?",design_debt,non-optimal_design,"Wed, 6 Aug 2008 04:29:41 +0000","Sun, 13 Sep 2009 22:33:29 +0000","Wed, 13 Aug 2008 02:35:03 +0000",597922,"After discussing with Jim on IRC, there won't be very many additional functions exposed through HTable. It will actually just be one new version of each of: batchUpdate, deleteAll x 2, and deleteFamily. So it would be those 4, plus the lockRow/unlockRow. You think it would be okay to include just those in HTable w/o subclassing it?",0.425,0.425,neutral
hbase,798,comment_2,"I""m fine w/ that. You might want to subclass anyways just because it groups this new functionality nicely.",design_debt,non-optimal_design,"Wed, 6 Aug 2008 04:29:41 +0000","Sun, 13 Sep 2009 22:33:29 +0000","Wed, 13 Aug 2008 02:35:03 +0000",597922,"I""m fine w/ that. You might want to subclass anyways just because it groups this new functionality nicely.",0.3,0.3,neutral
hbase,847,comment_1,"Patch for the issue. OK, this is my first big(-ish) patch, so I am sure I am missing something :) Anyway, updates hbase as Jim Kellerman suggested. RowResult#getRow-s don't have any documentation yet. I will update them with a later patch. I also want to update scanners so that you can ask for multiple versions from them too (not done yet). (Also includes patch from HBASE-892.)",documentation_debt,outdated_documentation,"Wed, 27 Aug 2008 20:36:12 +0000","Sun, 13 Sep 2009 22:26:26 +0000","Wed, 3 Dec 2008 01:06:24 +0000",8397012,"Patch for the issue. OK, this is my first big(-ish) patch, so I am sure I am missing something Anyway, updates hbase as Jim Kellerman suggested. RowResult#getRow-s don't have any documentation yet. I will update them with a later patch. I also want to update scanners so that you can ask for multiple versions from them too (not done yet). (Also includes patch from HBASE-892.)",0.07291666667,0.06388888889,neutral
hbase,847,comment_2,"Patch does not apply. Patches must be in svn diff format to be accepted. Please add a test case to demonstrate that getting multiple versions works (should also include multiple versions with timestamp specified) Please do not include a patch for HBASE-52 and HBASE-33 in this patch. Even though they are similar, changes to scanners are more difficult. We try to limit the scope of a single patch in general. Insure that the sub issues of this Jira, HBASE-857, HBASE-31 and HBASE-44 are addressed. Thanks.",test_debt,lack_of_tests,"Wed, 27 Aug 2008 20:36:12 +0000","Sun, 13 Sep 2009 22:26:26 +0000","Wed, 3 Dec 2008 01:06:24 +0000",8397012,"Patch does not apply. Patches must be in svn diff format to be accepted. Please add a test case to demonstrate that getting multiple versions works (should also include multiple versions with timestamp specified) Please do not include a patch for HBASE-52 and HBASE-33 in this patch. Even though they are similar, changes to scanners are more difficult. We try to limit the scope of a single patch in general. Insure that the sub issues of this Jira, HBASE-857, HBASE-31 and HBASE-44 are addressed. Thanks.",0.1976190476,0.1976190476,neutral
hbase,991,comment_0,"Cleanup of the mapreduce examples. Started new Will point folks at examples in here since its hard keeping up examples that have been modified so they'll sit in javadoc. Also changed HbaseMapWritable so it can take byte [] for values, not just Writable. Makes sense passing byte [] rather than make a new, temporary to go from map to reduce.",documentation_debt,low_quality_documentation,"Tue, 11 Nov 2008 06:24:39 +0000","Sun, 13 Sep 2009 22:26:32 +0000","Mon, 17 Nov 2008 02:28:58 +0000",504259,"Cleanup of the mapreduce examples. Started new src/examples/mapred. Will point folks at examples in here since its hard keeping up examples that have been modified so they'll sit in javadoc. Also changed HbaseMapWritable so it can take byte [] for values, not just Writable. Makes sense passing byte [] rather than make a new, temporary ImmutableBytesWritable, to go from map to reduce.",0.075,0.06,neutral
hbase,1089,comment_5,Committed. Thanks for the nice patch Samuel. We might consider adding this to hbase 0.19.1. Could be used to indicate cruft in the hbase.rootdir; e.g. there is loads of cruft in pset hbase.rootdir since its been migrated across multiple versions.,design_debt,non-optimal_design,"Wed, 24 Dec 2008 18:03:58 +0000","Sun, 13 Sep 2009 22:24:17 +0000","Mon, 19 Jan 2009 21:45:12 +0000",2259674,Committed. Thanks for the nice patch Samuel. We might consider adding this to hbase 0.19.1. Could be used to indicate cruft in the hbase.rootdir; e.g. there is loads of cruft in pset hbase.rootdir since its been migrated across multiple versions.,0.1854166667,0.1854166667,positive
hbase,1271,comment_3,"* LocalHBaseCluster - is not so general thing... as I understand it is used for testing.. never for real cluster... so my change will simplify this testing by providing automatic another port binding. * HRS InfoServer is really general thing... but port of InfoServer is not so critical... we can document it like a future... ""Automatic InfoServer port binding"". And if you want explicit port setting we can add console param for HRS start without this changes we cant star't several HRS on one node with info servers * thx... will fix my spelling",code_debt,complex_code,"Fri, 20 Mar 2009 05:00:09 +0000","Sun, 13 Sep 2009 22:24:29 +0000","Wed, 29 Apr 2009 23:15:43 +0000",3521734,"LocalHBaseCluster - is not so general thing... as I understand it is used for testing.. never for real cluster... so my change will simplify this testing by providing automatic another port binding. HRS InfoServer is really general thing... but port of InfoServer is not so critical... we can document it like a future... ""Automatic InfoServer port binding"". And if you want explicit port setting we can add console param for HRS start without this changes we cant star't several HRS on one node with info servers thx... will fix my spelling",0.05625,0.05625,neutral
hbase,1271,comment_2,"Evgeny, - I like your approach with the ++port, but we need to note that you are changing it for the general use case, not just the tests. Any use of RegionServer or LocalHBaseCluster will now have the ++port logic in it. This means that a user who sets his port in a config to 5000 may spin up a server on port 5002 instead of producing an error. It's not clear to me that that is a good change necessarily? - LOG.info(""Faild binding Master to ... should fix spelling of ""Failed"" Looks great on the ZooKeeper side of things and otherwise. Thanks for taking this on :).",documentation_debt,low_quality_documentation,"Fri, 20 Mar 2009 05:00:09 +0000","Sun, 13 Sep 2009 22:24:29 +0000","Wed, 29 Apr 2009 23:15:43 +0000",3521734,"Evgeny, I like your approach with the ++port, but we need to note that you are changing it for the general use case, not just the tests. Any use of RegionServer or LocalHBaseCluster will now have the ++port logic in it. This means that a user who sets his port in a config to 5000 may spin up a server on port 5002 instead of producing an error. It's not clear to me that that is a good change necessarily? LOG.info(""Faild binding Master to ... should fix spelling of ""Failed"" Looks great on the ZooKeeper side of things and otherwise. Thanks for taking this on .",0.00819047619,0.02247619048,neutral
hbase,1271,comment_8,* moved port++ from LocalHBaseCluster to MiniHBaseCluster level * fixed spelling :) * Still auto port binding for Info Server. I think it is really useful.,documentation_debt,low_quality_documentation,"Fri, 20 Mar 2009 05:00:09 +0000","Sun, 13 Sep 2009 22:24:29 +0000","Wed, 29 Apr 2009 23:15:43 +0000",3521734,moved port++ from LocalHBaseCluster to MiniHBaseCluster level fixed spelling Still auto port binding for Info Server. I think it is really useful.,0.42025,0.3905,neutral
hbase,1309,comment_2,"In my opinion HBase should not dictate to users what they can or cannot store into the system. I did not say anything about empty keys, which of course makes no sense. The particular circumstance in this place is a crawling application that stores content indexed by SHA-1 hash but also includes a family 'url:' which stores, for each unique object according to hash, the list of URLs where it was encountered. The URLs are stored as column qualifiers to produce a list of unique values with no need for scanning for duplicate elimination etc. There is no need to store data also because all the data is already in the key.",design_debt,non-optimal_design,"Sun, 5 Apr 2009 02:18:31 +0000","Sun, 13 Sep 2009 22:24:31 +0000","Tue, 7 Apr 2009 18:23:08 +0000",230677,"In my opinion HBase should not dictate to users what they can or cannot store into the system. I did not say anything about empty keys, which of course makes no sense. The particular circumstance in this place is a crawling application that stores content indexed by SHA-1 hash but also includes a family 'url:' which stores, for each unique object according to hash, the list of URLs where it was encountered. The URLs are stored as column qualifiers to produce a list of unique values with no need for scanning for duplicate elimination etc. There is no need to store data also because all the data is already in the key.",0.1692,0.1692,neutral
hbase,1558,comment_1,"here's a prototype fix, but we need tests.",test_debt,lack_of_tests,"Sun, 21 Jun 2009 03:42:47 +0000","Sun, 13 Sep 2009 22:24:45 +0000","Mon, 22 Jun 2009 18:10:11 +0000",138444,"here's a prototype fix, but we need tests.",0.0,0.0,neutral
hbase,1655,comment_1,"Patch looks pretty good. Small nitpicky issues with some needless reorderings of stuff and also some tabbing oddities. Can be fixed on commit. I remember why we're using TreeMap now instead of HashMap. HashMap with byte[] as a key does not work with default comparator, and you cannot pass one in. There is no real downside to using TreeMap unless you have hundreds or thousands of tables, and even then logarithmic on the client-side on the order of 100-1000 is negligible. More efficient in memory but a little dirtier on the GC... however this is almost exclusively client-side (and there's ever only one) so really makes no difference IMO. So, back to TreeMap. The API seems to have grown quite a bit. Do we need all these permutations of getTable() ? Could we drop the String taking ones besides the simplest one? (This is why we don't support String in most of the HBase API now, leads to very long and ugly APIs) The default getTable(byte [] tableName) also requires instantiating a new each time internally, even if we are reusing an existing HTable... Whether there is a significant overhead or not to that, we should avoid it when unnecessary. Typical usage is probably to not supply your own HBC, so if someone wants that level of control, give them the ability to build the Pool themselves rather than expose the friendlier, higher-level methods with all the different ways to instantiate the pool. Blah, this is better said in a new patch....",code_debt,low_quality_code,"Tue, 14 Jul 2009 19:52:59 +0000","Wed, 12 Aug 2020 23:02:36 +0000","Fri, 17 Jul 2009 19:26:30 +0000",257611,"Patch looks pretty good. Small nitpicky issues with some needless reorderings of stuff and also some tabbing oddities. Can be fixed on commit. I remember why we're using TreeMap now instead of HashMap. HashMap with byte[] as a key does not work with default comparator, and you cannot pass one in. There is no real downside to using TreeMap unless you have hundreds or thousands of tables, and even then logarithmic on the client-side on the order of 100-1000 is negligible. More efficient in memory but a little dirtier on the GC... however this is almost exclusively client-side (and there's ever only one) so really makes no difference IMO. So, back to TreeMap. The API seems to have grown quite a bit. Do we need all these permutations of getTable() ? Could we drop the String taking ones besides the simplest one? (This is why we don't support String in most of the HBase API now, leads to very long and ugly APIs) The default getTable(byte [] tableName) also requires instantiating a new HBaseConfiguration() each time internally, even if we are reusing an existing HTable... Whether there is a significant overhead or not to that, we should avoid it when unnecessary. Typical usage is probably to not supply your own HBC, so if someone wants that level of control, give them the ability to build the Pool themselves rather than expose the friendlier, higher-level methods with all the different ways to instantiate the pool. Blah, this is better said in a new patch....",0.08128571429,0.08128571429,neutral
hbase,1655,comment_2,"This doesn't include any stargate stuff, only issues there are the tab issues, reordering, and whether to expose the pools or not. I'm not sure we want to remove the ability to work with HTablePool directly or not. This patch re-publics a bunch of stuff, cuts down on the API quite a bit, and reduces re-instantiation of HBC when getting pools that already exist and will always reuse the original when getting tables. We should definitely keep that. Otherwise the changes are open to discussion, I'm not particularly sold on my own patch, just providing it so we have a point of reference for discussion. The biggest thing to figure out is whether we even expose HTablePool non-statically to the user. If not, we go down the path of a much longer easy-to-use API... /me sleeping on it",code_debt,low_quality_code,"Tue, 14 Jul 2009 19:52:59 +0000","Wed, 12 Aug 2020 23:02:36 +0000","Fri, 17 Jul 2009 19:26:30 +0000",257611,"This doesn't include any stargate stuff, only issues there are the tab issues, reordering, and whether to expose the pools or not. I'm not sure we want to remove the ability to work with HTablePool directly or not. This patch re-publics a bunch of stuff, cuts down on the API quite a bit, and reduces re-instantiation of HBC when getting pools that already exist and will always reuse the original when getting tables. We should definitely keep that. Otherwise the changes are open to discussion, I'm not particularly sold on my own patch, just providing it so we have a point of reference for discussion. The biggest thing to figure out is whether we even expose HTablePool non-statically to the user. If not, we go down the path of a much longer easy-to-use API... /me sleeping on it",-0.197047619,-0.197047619,neutral
hbase,1655,comment_4,"A few questions/comments on the comments: - Why does the key to a HashMap need a comparator? - I much prefer the type of String vs byte[] for the tableName throughout all the method signatures and as a key to the internal map, but I tried to keep it as-is because it seemed to fit more with other HBase code. Can we just change it to String throughout and go back to the HashMap? - I am not a fan of the current API either. I'd much prefer that the HTablePool be instantiated by a client and we get rid of the static methods and static Map altogether. This fits in much more nicely for people like me who are using IoC containers like Spring. It also allows the ability to have multiple HTablePool instances, maybe each with their own configuration (which is currently just the max size). - Sorry about the tab/spaces issue. I didn't clean it up carefully enough. - Sorry about the reordering of imports. I use Eclipse which does this automatically and I was too lazy to try to restore the order back to the way it was. I'll try harder next time. So what is the next step? Should I make a new patch with an HTablePool that is meant to be instantiated and w/o the static methods?",code_debt,low_quality_code,"Tue, 14 Jul 2009 19:52:59 +0000","Wed, 12 Aug 2020 23:02:36 +0000","Fri, 17 Jul 2009 19:26:30 +0000",257611,"A few questions/comments on the comments: Why does the key to a HashMap need a comparator? I much prefer the type of String vs byte[] for the tableName throughout all the method signatures and as a key to the internal map, but I tried to keep it as-is because it seemed to fit more with other HBase code. Can we just change it to String throughout and go back to the HashMap? I am not a fan of the current API either. I'd much prefer that the HTablePool be instantiated by a client and we get rid of the static methods and static Map altogether. This fits in much more nicely for people like me who are using IoC containers like Spring. It also allows the ability to have multiple HTablePool instances, maybe each with their own configuration (which is currently just the max size). Sorry about the tab/spaces issue. I didn't clean it up carefully enough. Sorry about the reordering of imports. I use Eclipse which does this automatically and I was too lazy to try to restore the order back to the way it was. I'll try harder next time. So what is the next step? Should I make a new patch with an HTablePool that is meant to be instantiated and w/o the static methods?",-0.1090416667,-0.1315595238,neutral
hbase,1655,comment_11,"I've submitted another patch, that reflects a different approach taken to improve HTablePool. In the new approach, there will be no static methods at all. Instead, the normal usage of the class will be to instantiate an HTablePool directly and use the getTable and putTable methods to get an HTable and return an HTable to the pool. The pool class will automatically instantiate a new HTable in the case that no HTable for the given table name is in the pool OR if the internal pool for that table name is full (has more elements than its configured max size). The default max size is Integer.MAX_VALUE. While we lose the convenience of calling static methods, we gain a design with less code that is easy to understand, easy to use in an IoC container, and easy to unit test. This patch also includes a small modification to HTable that allows an instantiation to HTable with a null HBaseConfiguration object. This makes it possible to unit test classes such as HTablePool that internally use an HTable without requiring an environment in which the HTable can get and use a connection. Finally, this patch includes modifications to the stargate package reflecting the new HTablePool API. Let me know if this is suitable for inclusion in the project. Thanks to Jon Gray and Stack for their help and input.",design_debt,non-optimal_design,"Tue, 14 Jul 2009 19:52:59 +0000","Wed, 12 Aug 2020 23:02:36 +0000","Fri, 17 Jul 2009 19:26:30 +0000",257611,"I've submitted another patch, HBASE-1655-v3.patch, that reflects a different approach taken to improve HTablePool. In the new approach, there will be no static methods at all. Instead, the normal usage of the class will be to instantiate an HTablePool directly and use the getTable and putTable methods to get an HTable and return an HTable to the pool. The pool class will automatically instantiate a new HTable in the case that no HTable for the given table name is in the pool OR if the internal pool for that table name is full (has more elements than its configured max size). The default max size is Integer.MAX_VALUE. While we lose the convenience of calling static methods, we gain a design with less code that is easy to understand, easy to use in an IoC container, and easy to unit test. This patch also includes a small modification to HTable that allows an instantiation to HTable with a null HBaseConfiguration object. This makes it possible to unit test classes such as HTablePool that internally use an HTable without requiring an environment in which the HTable can get and use a connection. Finally, this patch includes modifications to the stargate package reflecting the new HTablePool API. Let me know if this is suitable for inclusion in the project. Thanks to Jon Gray and Stack for their help and input.",0.2026,0.1870153846,neutral
hbase,1655,comment_6,"- Even though TreeMap uses the comparator rather than the equals method to compare keys, using a byte[] as the key seems to break the contract of a java.util.Map. The last paragraph of the [Map says that impls are free to avoid calling equals() but the containsKey(Object key) method should ""return true if and only if this map contains a mapping for a key k such that (key==null ? k==null : key.equals(k))"". As you mentioned, you'd need a byte[] wrapper in order to be compliant. So a byte[] key will work in a TreeMap (and not in a HashMap), but we'd break the Map contract. Seems better to just use String and HashMap which works well and satisfies the Map contract. - Allowing both the static methods and the public construction would bring me back to the complaint I had originally with this class. :) It becomes confusing for a user trying to quickly understand how he/she is supposed to interact with this class. I guess this can all be explained away with documentation, but, it doesn't feel ""good"". - Also, yes, there would be a problem with re-instantiating the static map multiple times, but this could be prevented by implementing a Singleton pattern so that the static access operates on an internal singleton instance with a member Map rather than a static Map. - I agree that there is no reason to construct HBC's for each invocation. An internal HBC instance can be constructed and reused. - Is there a checkstyle.xml file for HBase? That would make it easy to check my code for formatting problems before submitting patches.",design_debt,non-optimal_design,"Tue, 14 Jul 2009 19:52:59 +0000","Wed, 12 Aug 2020 23:02:36 +0000","Fri, 17 Jul 2009 19:26:30 +0000",257611,"Even though TreeMap uses the comparator rather than the equals method to compare keys, using a byte[] as the key seems to break the contract of a java.util.Map. The last paragraph of the Map API says that impls are free to avoid calling equals() but the containsKey(Object key) method should ""return true if and only if this map contains a mapping for a key k such that (key==null ? k==null : key.equals(k))"". As you mentioned, you'd need a byte[] wrapper in order to be compliant. So a byte[] key will work in a TreeMap (and not in a HashMap), but we'd break the Map contract. Seems better to just use String and HashMap which works well and satisfies the Map contract. Allowing both the static methods and the public construction would bring me back to the complaint I had originally with this class. It becomes confusing for a user trying to quickly understand how he/she is supposed to interact with this class. I guess this can all be explained away with documentation, but, it doesn't feel ""good"". Also, yes, there would be a problem with re-instantiating the static map multiple times, but this could be prevented by implementing a Singleton pattern so that the static access operates on an internal singleton instance with a member Map rather than a static Map. I agree that there is no reason to construct HBC's for each invocation. An internal HBC instance can be constructed and reused. Is there a checkstyle.xml file for HBase? That would make it easy to check my code for formatting problems before submitting patches.",0.1284951531,0.04042724868,neutral
hbase,1655,comment_7,".bq So a byte[] key will work in a TreeMap (and not in a HashMap), but we'd break the Map contract. Seems better to just use String and HashMap which works well and satisfies the Map contract. Looking at the TreeMap equals implementation, yes, it breaks the above contract for Map. We're impure (insert lots of sack cloth and ashes, self-flagellation, etc., here -- smile). We are zealous about using byte [] everywhere. As Jon allows above, client-side, we can relax some. IMO, we should not be allowing public construction and static methods. +1 on Singleton to prevent multiple instantiations. There is no checkstyle (maybe there is one up in hadoop). In general 80 characters per line and two spaces for tab. Anything else we'll take. Don't worry about it too much.",design_debt,non-optimal_design,"Tue, 14 Jul 2009 19:52:59 +0000","Wed, 12 Aug 2020 23:02:36 +0000","Fri, 17 Jul 2009 19:26:30 +0000",257611,".bq So a byte[] key will work in a TreeMap (and not in a HashMap), but we'd break the Map contract. Seems better to just use String and HashMap which works well and satisfies the Map contract. Looking at the TreeMap equals implementation, yes, it breaks the above contract for Map. We're impure (insert lots of sack cloth and ashes, self-flagellation, etc., here  smile). We are zealous about using byte [] everywhere. As Jon allows above, client-side, we can relax some. IMO, we should not be allowing public construction and static methods. +1 on Singleton to prevent multiple instantiations. There is no checkstyle (maybe there is one up in hadoop). In general 80 characters per line and two spaces for tab. Anything else we'll take. Don't worry about it too much.",0.2391153846,0.2391153846,neutral
hbase,1655,comment_12,"I'm +1 on the changes described. I'm not a big fan of putting a big comment in the HTable constructor javadoc about being able to pass it a null. IMO Something like that belongs in a // comment in the code, not exposed in the javadoc of the most client-facing class we have. It's for developers, not users. Users who pass a null HBC will soon learn that this will not work, why do they care about unit testing?",documentation_debt,low_quality_documentation,"Tue, 14 Jul 2009 19:52:59 +0000","Wed, 12 Aug 2020 23:02:36 +0000","Fri, 17 Jul 2009 19:26:30 +0000",257611,"I'm +1 on the changes described. I'm not a big fan of putting a big comment in the HTable constructor javadoc about being able to pass it a null. IMO Something like that belongs in a // comment in the code, not exposed in the javadoc of the most client-facing class we have. It's for developers, not users. Users who pass a null HBC will soon learn that this will not work, why do they care about unit testing?",-0.0644,-0.0644,neutral
hbase,1770,comment_1,Patch looks good. Javadoc is a little bit odd because it seems like it's asking the client to call flushCommits rather than it happening automatically. Can fix on commit. Should we put this into 0.20 branch and trunk?,documentation_debt,low_quality_documentation,"Mon, 17 Aug 2009 22:32:50 +0000","Sun, 13 Sep 2009 22:24:54 +0000","Mon, 17 Aug 2009 23:44:31 +0000",4301,Patch looks good. Javadoc is a little bit odd because it seems like it's asking the client to call flushCommits rather than it happening automatically. Can fix on commit. Should we put this into 0.20 branch and trunk?,0.144,0.144,neutral
hbase,1990,comment_10,"The concept of is good, but unfortunately it might not work even for simpler use cases. Let's discuss the following example: The above example forces you to pick a data type for values at the instantiation of the Put object. But in most cases (at least in our software) we have different data types in a row such as Long, String, Custom Object etc. Even a typical relational database table always have multiple data types in a row. If you exclude the value and keep the value as byte array, it should be sufficient for 80% of the use cases. (Even though we have many columns where the column name is not a string, they are a minority)",design_debt,non-optimal_design,"Wed, 18 Nov 2009 21:48:59 +0000","Sat, 11 Jun 2022 23:15:15 +0000","Mon, 12 May 2014 00:40:37 +0000",141274298,"The concept of TestHTableGenerics.java is good, but unfortunately it might not work even for simpler use cases. Let's discuss the following example: The above example forces you to pick a data type for values at the instantiation of the Put object. But in most cases (at least in our software) we have different data types in a row such as Long, String, Custom Object etc. Even a typical relational database table always have multiple data types in a row. If you exclude the value and keep the value as byte array, it should be sufficient for 80% of the use cases. (Even though we have many columns where the column name is not a string, they are a minority)",-0.1171388889,-0.1004047619,neutral
hbase,2068,comment_13,"+1 Works great! There are few now obsolete imports, like in the changed Statistics classes. Could remove on commit. Otherwise please commit.",code_debt,low_quality_code,"Tue, 22 Dec 2009 13:46:58 +0000","Fri, 12 Oct 2012 06:14:13 +0000","Mon, 4 Jan 2010 21:02:15 +0000",1149317,"+1 Works great! There are few now obsolete imports, like in the changed Statistics classes. Could remove on commit. Otherwise please commit.",0.15,0.15,positive
hbase,2068,comment_14,Committed. Thanks for the patches lads. I removed the unused import. Added a couple of licenses too. Excellent.,code_debt,low_quality_code,"Tue, 22 Dec 2009 13:46:58 +0000","Fri, 12 Oct 2012 06:14:13 +0000","Mon, 4 Jan 2010 21:02:15 +0000",1149317,Committed. Thanks for the patches lads. I removed the unused import. Added a couple of licenses too. Excellent.,0.33,0.33,positive
hbase,2068,comment_2,Another inconsistency is that MasterMetrics uses a MetricsIntValue and the uses a MetricsRate class I suggest after fixing MetricsRate to replace the MasterMetrics one with it as well. I am also wondering if it would have been better to call it MetricsIntRate to indicate the size of the internal counter.,code_debt,low_quality_code,"Tue, 22 Dec 2009 13:46:58 +0000","Fri, 12 Oct 2012 06:14:13 +0000","Mon, 4 Jan 2010 21:02:15 +0000",1149317,Another inconsistency is that MasterMetrics uses a MetricsIntValue and the RegionsServerMetrics uses a MetricsRate class I suggest after fixing MetricsRate to replace the MasterMetrics one with it as well. I am also wondering if it would have been better to call it MetricsIntRate to indicate the size of the internal counter.,0.5655,0.5655,positive
hbase,2247,comment_1,-1 on adding a new API that duplicates another but with one distinction. Make this a Scan option instead.,code_debt,duplicated_code,"Mon, 22 Feb 2010 23:02:02 +0000","Sat, 11 Jun 2022 23:07:17 +0000","Sun, 26 Jan 2014 20:04:33 +0000",123886951,-1 on adding a new API that duplicates another but with one distinction. Make this a Scan option instead.,0.0,0.0,neutral
hbase,2295,comment_10,I agree isEmpty is better than size... I'll just put fix in under this issue. Thanks Todd. I changed it to do this instead on trunk and branch:,code_debt,low_quality_code,"Fri, 5 Mar 2010 23:31:34 +0000","Fri, 20 Nov 2015 12:43:37 +0000","Tue, 9 Mar 2010 17:11:23 +0000",322789,I agree isEmpty is better than size... I'll just put fix in under this issue. Thanks Todd. I changed it to do this instead on trunk and branch:,0.25,0.25,positive
hbase,2295,comment_6,"@Dhruba If you want to use a SortedSet and pass a comparator instead, so you can retain byte [] elements, there is a bytes comparator here: Otherwise, patch looks good. Regards TRUNK, its been mavenized so to build it, you need maven -- see -- and then to build and run tests do something like mvn install",design_debt,non-optimal_design,"Fri, 5 Mar 2010 23:31:34 +0000","Fri, 20 Nov 2015 12:43:37 +0000","Tue, 9 Mar 2010 17:11:23 +0000",322789,"@Dhruba If you want to use a SortedSet and pass a comparator instead, so you can retain byte [] elements, there is a bytes comparator here: http://hadoop.apache.org/hbase/docs/r0.20.3/api/org/apache/hadoop/hbase/util/Bytes.html#BYTES_COMPARATOR Otherwise, patch looks good. Regards TRUNK, its been mavenized so to build it, you need maven  see http://wiki.apache.org/hadoop/Hbase/MavenPrimer  and then to build and run tests do something like mvn install",0.0585,0.0585,positive
hbase,2341,comment_8,Cosmin suggests that we instrument the code with a coverage tool while running a long-running cluster test. We can then identify edge cases / dark corners better (and write tests to exercise them),test_debt,lack_of_tests,"Wed, 17 Mar 2010 23:25:26 +0000","Sat, 11 Jun 2022 23:22:01 +0000","Wed, 16 Jul 2014 20:45:12 +0000",136675186,Cosmin suggests that we instrument the code with a coverage tool while running a long-running cluster test. We can then identify edge cases / dark corners better (and write tests to exercise them),0.03125,0.03125,neutral
hbase,2555,comment_0,Patch removes constant and two unused HCD methods which use constant. HBase does not support anything but HFiles right now.,code_debt,dead_code,"Sun, 16 May 2010 21:36:30 +0000","Fri, 20 Nov 2015 12:42:41 +0000","Sun, 16 May 2010 22:47:17 +0000",4247,Patch removes constant and two unused HCD methods which use constant. HBase does not support anything but HFiles right now.,0.03175,0.03175,negative
hbase,2585,comment_2,"At a minimum, lets fix the dumb NPE and throw a better exception. Looking at code, it looks like the RS will just retry later to report the split (See ~#544 in HRegionServer).",code_debt,low_quality_code,"Thu, 20 May 2010 15:20:38 +0000","Sat, 11 Jun 2022 23:34:32 +0000","Wed, 16 Jul 2014 20:50:42 +0000",131175004,"At a minimum, lets fix the dumb NPE and throw a better exception. Looking at code, it looks like the RS will just retry later to report the split (See ~#544 in HRegionServer).",-0.025,-0.025,negative
hbase,2694,comment_6,"Final patch for commit. Includes changes from Todd's comments, added a few licenses, and also fixed mixed newlines on final patch file.",code_debt,low_quality_code,"Tue, 8 Jun 2010 18:39:57 +0000","Fri, 20 Nov 2015 12:43:18 +0000","Sat, 12 Jun 2010 01:20:41 +0000",283244,"Final patch for commit. Includes changes from Todd's comments, added a few licenses, and also fixed mixed newlines on final patch file.",0.1,0.1,neutral
hbase,2925,comment_10,You may have left out a couple of print statements in TestHCM. Not sure if that was intentional. Otherwise it's good to go.,code_debt,low_quality_code,"Tue, 17 Aug 2010 21:27:52 +0000","Fri, 20 Nov 2015 12:42:13 +0000","Mon, 6 Sep 2010 15:52:27 +0000",1707875,You may have left out a couple of print statements in TestHCM. Not sure if that was intentional. Otherwise it's good to go.,0.0,0.0,neutral
hbase,2925,comment_6,"@stack, thank you for picking up this issue. Feel free to reuse the code provided in in any way you want. (Let me know if this is not enough I could submit another file with Apache license added.) Back to the discussion, I agree that removing the {{hasCode()}} and {{equals()}} methods does resolve this issue. However, I wonder if it defeats the purpose of having a connection cache from the first place. You also alluded to the problem of the configuration settings changing after the connection is open, which, if we rely solely on Object equality, could lead to subtle error or unexpected behavior. Hence my suggestion to pin down the configuration once the TableServers instance is created. The disadvantage there is that small/unrelated changes in the configuration will invalidate the cache entry as well. If that is deemed a real problem given the common use cases, I was thinking maybe we define ""equality"" of the configuration for the purposes of caching as the equality of all properties that start with {{hbase.}} or {{zk.}} or any other prefix whose properties could be used as possible connection parameters. Not sure if the comparison becomes too slow in that case, but what is more important is whether correctness is compromised if we make the assumption that only properties starting with those prefixes affect the connection.",design_debt,non-optimal_design,"Tue, 17 Aug 2010 21:27:52 +0000","Fri, 20 Nov 2015 12:42:13 +0000","Mon, 6 Sep 2010 15:52:27 +0000",1707875,"@stack, thank you for picking up this issue. Feel free to reuse the code provided in SimpleHConnectionManagerLeakReplicator.java in any way you want. (Let me know if this is not enough I could submit another file with Apache license added.) Back to the discussion, I agree that removing the hasCode() and equals() methods does resolve this issue. However, I wonder if it defeats the purpose of having a connection cache from the first place. You also alluded to the problem of the configuration settings changing after the connection is open, which, if we rely solely on Object equality, could lead to subtle error or unexpected behavior. Hence my suggestion to pin down the configuration once the TableServers instance is created. The disadvantage there is that small/unrelated changes in the configuration will invalidate the cache entry as well. If that is deemed a real problem given the common use cases, I was thinking maybe we define ""equality"" of the configuration for the purposes of caching as the equality of all properties that start with hbase. or zk. or any other prefix whose properties could be used as possible connection parameters. Not sure if the comparison becomes too slow in that case, but what is more important is whether correctness is compromised if we make the assumption that only properties starting with those prefixes affect the connection.",0.09033333333,0.09876923077,neutral
hbase,2925,comment_7,"@Robert ...I wonder if it defeats the purpose of having a connection cache from the first place. As I see it, the main benefit to the cache is saving on region lookups, setup of zk connection, and master proxy setup. Having the likes of the following config cached is secondary: e.g. In fact, I'm now thinking that if a user changes any of the above in a Configuration that is being used as a key in that its ok if we don't notice the change as long as we doc the fact that the Configuration is read on instantiation of the HTable and then no more. I don't tthink this should 'surprise' the user too much and they can just go create new HTable with the new Configuration if they really want their new config. to take hold (Making things work this way is similar to what you suggest only you would copy the Configuration before adding it as a key). As to your last suggestion, I think we should move away from trying to equate Configurations at all; there be daemons that way. Let me know what you think. If you are agreeable, I'll work up the patch some more mostly adding doc clarifying what we've agreed here.",documentation_debt,outdated_documentation,"Tue, 17 Aug 2010 21:27:52 +0000","Fri, 20 Nov 2015 12:42:13 +0000","Mon, 6 Sep 2010 15:52:27 +0000",1707875,"@Robert ...I wonder if it defeats the purpose of having a connection cache from the first place. As I see it, the main benefit to the cache is saving on region lookups, setup of zk connection, and master proxy setup. Having the likes of the following config cached is secondary: e.g. In fact, I'm now thinking that if a user changes any of the above in a Configuration that is being used as a key in HCM#HBASE_INSTANCES, that its ok if we don't notice the change as long as we doc the fact that the Configuration is read on instantiation of the HTable and then no more. I don't tthink this should 'surprise' the user too much and they can just go create new HTable with the new Configuration if they really want their new config. to take hold (Making things work this way is similar to what you suggest only you would copy the Configuration before adding it as a key). As to your last suggestion, I think we should move away from trying to equate Configurations at all; there be daemons that way. Let me know what you think. If you are agreeable, I'll work up the patch some more mostly adding doc clarifying what we've agreed here.",0.3023958333,0.3023958333,neutral
hbase,2925,comment_9,"This version of the patch just adds javadoc to HTable explaining the advantage of shared Configuration -- the sharing of zookeeper connection, cache of region locations, etc. If you have a minute, give it a gander Robert and if its good w/ you, I'll go ahead and commit.",documentation_debt,outdated_documentation,"Tue, 17 Aug 2010 21:27:52 +0000","Fri, 20 Nov 2015 12:42:13 +0000","Mon, 6 Sep 2010 15:52:27 +0000",1707875,"This version of the patch just adds javadoc to HTable explaining the advantage of shared Configuration  the sharing of zookeeper connection, cache of region locations, etc. If you have a minute, give it a gander Robert and if its good w/ you, I'll go ahead and commit.",0.4575,0.4575,neutral
hbase,2925,comment_4,"Here's a start. Its not done yet but shows direction: i.e. removing hashcode and equals from HBC. TODO, is test that prove that HBASE-1251 is still fixed (I think thing to do here is just read configs down in TableServers out of the conf each time rather than once up front so if number of retries is changed mid-use, subsequent invocations will pick up new config. -- let me see).",requirement_debt,requirement_partially_implemented,"Tue, 17 Aug 2010 21:27:52 +0000","Fri, 20 Nov 2015 12:42:13 +0000","Mon, 6 Sep 2010 15:52:27 +0000",1707875,"Here's a start. Its not done yet but shows direction: i.e. removing hashcode and equals from HBC. TODO, is test that prove that HBASE-1251 is still fixed (I think thing to do here is just read configs down in TableServers out of the conf each time rather than once up front so if number of retries is changed mid-use, subsequent invocations will pick up new config.  let me see).",-0.001375,-0.001833333333,neutral
hbase,3181,comment_0,"Working on a document that goes over all this stuff, but I'd like stack to give a go at my current patch. There's a few fairly big fixes, not just to timeouts but also to server shutdown handling, and I'd like to see if it fixes the issues he's been seeing. Putting patch up on RB.",documentation_debt,low_quality_documentation,"Sun, 31 Oct 2010 19:28:35 +0000","Fri, 20 Nov 2015 12:42:59 +0000","Tue, 2 Nov 2010 02:05:45 +0000",110230,"Working on a document that goes over all this stuff, but I'd like stack to give a go at my current patch. There's a few fairly big fixes, not just to timeouts but also to server shutdown handling, and I'd like to see if it fixes the issues he's been seeing. Putting patch up on RB.",0.4166666667,0.4166666667,positive
hbase,3257,comment_1,"Patch was put to review board, but it's not forwarded to jira. Here is the review request: Summary: Coprocessors: Extend server side integration API to include HLog operations Coprocessor based extensions should be able to: - Observe, rewrite, or skip WALEdits as they are being written to the WAL - Write arbitrary content into WALEdits - Act on contents of WALEdits in the regionserver context during reconstruction Code changes: - a new coprocessor interface WALCPObserver is added which provides preWALWrite() and postWALWrite() upcalls to HLog, before and after at doWrite(). - added 2 new upcalls for RegionObserver for WAL replay, preWALRestore() and postWALRestore(). - a sample implementation -- -- was create which can add, remove, modify WALEdit before writing it to WAL. - test cases which use to test WAL write and replay. - added coprocessor loading at TestHLog to make sure it doesn't affect HLog. I need feedback for: - The new cp interface name -- WALCPObserver -- is not perfect. The ideal name is WALObserver but it's been used already. Other options include HLogObserver(H is not preferred), - No support for monitor master log splitting. I don't have a use case to support the requirement right now.",code_debt,low_quality_code,"Sun, 21 Nov 2010 19:43:23 +0000","Fri, 20 Nov 2015 12:42:27 +0000","Mon, 7 Feb 2011 23:01:50 +0000",6751107,"Patch was put to review board, but it's not forwarded to jira. Here is the review request: https://review.cloudera.org/r/1515/ Summary: Coprocessors: Extend server side integration API to include HLog operations Coprocessor based extensions should be able to: Observe, rewrite, or skip WALEdits as they are being written to the WAL Write arbitrary content into WALEdits Act on contents of WALEdits in the regionserver context during reconstruction Code changes: a new coprocessor interface WALCPObserver is added which provides preWALWrite() and postWALWrite() upcalls to HLog, before and after HLog.write.append(), at doWrite(). added 2 new upcalls for RegionObserver for WAL replay, preWALRestore() and postWALRestore(). a sample implementation  SampleRegionWALObserver  was create which can add, remove, modify WALEdit before writing it to WAL. test cases which use SampleRegionWALObserver to test WAL write and replay. added coprocessor loading at TestHLog to make sure it doesn't affect HLog. I need feedback for: The new cp interface name  WALCPObserver  is not perfect. The ideal name is WALObserver but it's been used already. Other options include HLogObserver(H is not preferred), LogObserver(confusion). No support for monitor master log splitting. I don't have a use case to support the requirement right now.",-0.1596944444,-0.1108846154,neutral
hbase,3510,comment_1,Same comment as last time - it's difficult to get to the RS name from here. So I just added the port - that makes it consistent with the other IPC threads.,design_debt,non-optimal_design,"Mon, 7 Feb 2011 20:44:11 +0000","Fri, 20 Nov 2015 12:43:36 +0000","Mon, 7 Feb 2011 22:02:23 +0000",4692,Same comment as last time - it's difficult to get to the RS name from here. So I just added the port - that makes it consistent with the other IPC threads.,-0.1,-0.1,neutral
hbase,3673,comment_1,"The patch did seem to make a difference in our particular use case, in terms of the average time it took to get a htable from the pool. For the sake of a more formal evaluation, I put together a benchmark (see attached test case), which did show noticeable difference. Specifically, when you've a htable pool of size 150, and a worker pool of 100 threads, where each thread does a get followed by a put a million times, the total time spent was 7775614 ms with the patch, as opposed to 12654820 ms without. That's turns out to be a 40% improvement. That said, using different htable pools for different use cases (e.g., different htables) might be the way to go, as that will tend to reduce the level of concurrency on any given htable pool.",code_debt,slow_algorithm,"Fri, 18 Mar 2011 22:20:04 +0000","Fri, 20 Nov 2015 12:40:39 +0000","Thu, 24 Mar 2011 22:28:49 +0000",518925,"The patch did seem to make a difference in our particular use case, in terms of the average time it took to get a htable from the pool. For the sake of a more formal evaluation, I put together a benchmark (see attached test case), which did show noticeable difference. Specifically, when you've a htable pool of size 150, and a worker pool of 100 threads, where each thread does a get followed by a put a million times, the total time spent was 7775614 ms with the patch, as opposed to 12654820 ms without. That's turns out to be a 40% improvement. That said, using different htable pools for different use cases (e.g., different htables) might be the way to go, as that will tend to reduce the level of concurrency on any given htable pool.",0.4084,0.4084,neutral
hbase,3696,comment_6,"Resolving as implemented by HBASE-11218 (which should go in soon) at least for standalone. Data loss on fs has been doc'd also. Hopefully HBASE-11218 will do pseudo distributed mode too. If not, doc will need tweaking. Can do in new issue.",documentation_debt,outdated_documentation,"Thu, 24 Mar 2011 01:10:36 +0000","Sun, 12 Jun 2022 00:55:50 +0000","Wed, 28 May 2014 04:08:13 +0000",100321057,"Resolving as implemented by HBASE-11218 (which should go in soon) at least for standalone. Data loss on fs has been doc'd also. Hopefully HBASE-11218 will do pseudo distributed mode too. If not, doc will need tweaking. Can do in new issue.",0.15,0.15,neutral
hbase,3807,comment_5,Removing the unused code for determining time elapsed in RegionServerMetrics : int seconds = - if (seconds == 0) { seconds = 1; } Will upload the patch ASAP,code_debt,dead_code,"Thu, 21 Apr 2011 06:28:51 +0000","Fri, 20 Nov 2015 12:43:21 +0000","Wed, 10 Aug 2011 19:43:28 +0000",9638077,Removing the unused code for determining time elapsed in RegionServerMetrics : int seconds = (int)((System.currentTimeMillis() - this.lastUpdate)/1000); if (seconds == 0) { seconds = 1; } Will upload the patch ASAP,0.0,0.0,neutral
hbase,4016,comment_0,"Your code is storing strings in the cells, but expects a big-endian encoded long, not a string.",design_debt,non-optimal_design,"Tue, 21 Jun 2011 23:07:46 +0000","Fri, 20 Nov 2015 11:53:05 +0000","Tue, 28 Jun 2011 23:36:32 +0000",606526,"Your code is storing strings in the cells, but incrementColumnValue expects a big-endian encoded long, not a string.",0.2,0.2,neutral
hbase,4088,comment_1,Small fix to logging message -- check for null before getting list size.,code_debt,low_quality_code,"Tue, 12 Jul 2011 17:41:15 +0000","Fri, 20 Nov 2015 11:53:16 +0000","Wed, 13 Jul 2011 04:20:05 +0000",38330,Small fix to logging message  check for null before getting list size.,0.0,0.0,neutral
hbase,4437,comment_3,Move to 0.20.205.0 hadoop. Also includes edits to our jsp and jamon templates commenting out DOCTYPE to get around bug where css is served as text/html. See tail of HBASE-2110 for discussion.,design_debt,non-optimal_design,"Mon, 19 Sep 2011 16:59:13 +0000","Fri, 20 Nov 2015 11:55:14 +0000","Mon, 24 Oct 2011 21:56:44 +0000",3041851,Move to 0.20.205.0 hadoop. Also includes edits to our jsp and jamon templates commenting out DOCTYPE to get around bug where css is served as text/html. See tail of HBASE-2110 for discussion.,0.0,0.0,neutral
hbase,4740,comment_6,"@Stack Yeah, 0 is actually the original behavior in the pre-HBASE-4552 version it I think would just eat exceptions and bail out without completing. It is more complicated because of bulk atomicity. Will update boolean if it works -- there is some template checking in another place so assumed it needed boxed type. The difference is that the version uses a different instance. I'll refactor to exclude that portion and require it in the test. I tried the previous version with a small data set on psuedo-dist cluster and live cluster. For this particular patch I tried this one by looping the relevant unit tests 100 times and seeing that they passed all the time. I haven't tested this exact version on real cluster.",code_debt,low_quality_code,"Thu, 3 Nov 2011 17:58:14 +0000","Fri, 20 Nov 2015 11:53:11 +0000","Tue, 8 Nov 2011 14:40:38 +0000",420144,"@Stack Yeah, 0 is actually the original behavior in the pre-HBASE-4552 version it I think would just eat exceptions and bail out without completing. It is more complicated because of bulk atomicity. Will update boolean if it works  there is some template checking in another place so assumed it needed boxed type. The difference is that the version uses a different LoadIncrementalHandlers instance. I'll refactor to exclude that portion and require it in the test. I tried the previous version with a small data set on psuedo-dist cluster and live cluster. For this particular patch I tried this one by looping the relevant unit tests 100 times and seeing that they passed all the time. I haven't tested this exact version on real cluster.",-0.1235,-0.1235,neutral
hbase,4778,comment_3,"@Ted: In this particular case, I think we ended up finding that HDFS wasn't reading truncated HFiles properly. Overall though, the idea is to prioritize data integrity & consistency over availability. We shouldn't be silently opening regions with missing data. We should instead understand why the data is missing. If someone wants to add a flag and allow this to happen, then that's fine.",code_debt,low_quality_code,"Sat, 12 Nov 2011 02:57:05 +0000","Fri, 12 Oct 2012 05:34:56 +0000","Mon, 14 Nov 2011 22:29:43 +0000",243158,"@Ted: In this particular case, I think we ended up finding that HDFS wasn't reading truncated HFiles properly. Overall though, the idea is to prioritize data integrity & consistency over availability. We shouldn't be silently opening regions with missing data. We should instead understand why the data is missing. If someone wants to add a 'data.loss.acceptable' flag and allow this to happen, then that's fine.",0.14,0.03123809524,negative
hbase,4804,comment_2,Haha.. I have a spelling problem and a tendency to omit words which may be incurable. :),documentation_debt,low_quality_documentation,"Wed, 16 Nov 2011 23:18:06 +0000","Fri, 20 Nov 2015 11:52:12 +0000","Wed, 16 Nov 2011 23:24:47 +0000",401,Haha.. I have a spelling problem and a tendency to omit words which may be incurable.,0.25,0.1,negative
hbase,5052,comment_0,"We could work on better cleaning the region name, but I think it is far better to depend on something like the hashed name",design_debt,non-optimal_design,"Fri, 16 Dec 2011 08:44:39 +0000","Fri, 20 Nov 2015 11:55:28 +0000","Sat, 7 Jan 2012 22:22:08 +0000",1949849,"We could work on better cleaning the region name, but I think it is far better to depend on something like the hashed name (HRehionInfo.hashCode()).",0.5333333333,0.2666666667,neutral
hbase,5110,comment_2,"Ah, I missed that thread... I just wanted to clarify if this is for readability or performance... do you see this function getting called a lot in a write workload? Your comments on the mailing list thread indicate that it's performance sensitive, but I don't see how that would be the case.",code_debt,slow_algorithm,"Fri, 30 Dec 2011 19:01:01 +0000","Sun, 12 Jun 2022 20:02:21 +0000","Tue, 21 Apr 2015 00:50:03 +0000",104305742,"Ah, I missed that thread... I just wanted to clarify if this is for readability or performance... do you see this function getting called a lot in a write workload? Your comments on the mailing list thread indicate that it's performance sensitive, but I don't see how that would be the case.",-0.2,-0.2,neutral
hbase,5110,comment_3,"I see it a lot in the heavy write scenario when major compaction occurs in the background, but to be realistic even when i see this method called 2000 times during a test of 5 hours, across a cluster of 10 RS (each RS log contains +/-200 calls of this method), i don't think this method present a performance problem. So for my point of view this is more readability issue before it becomes a performance problem. It is strange to me to see code asking each time in an iteration for object existence especially if creating the object is not heavy task.",code_debt,low_quality_code,"Fri, 30 Dec 2011 19:01:01 +0000","Sun, 12 Jun 2022 20:02:21 +0000","Tue, 21 Apr 2015 00:50:03 +0000",104305742,"I see it a lot in the heavy write scenario when major compaction occurs in the background, but to be realistic even when i see this method called 2000 times during a test of 5 hours, across a cluster of 10 RS (each RS log contains +/-200 calls of this method), i don't think this method present a performance problem. So for my point of view this is more readability issue before it becomes a performance problem. It is strange to me to see code asking each time in an iteration for object existence especially if creating the object is not heavy task.",-0.1471111111,-0.1471111111,neutral
hbase,5210,comment_3,"Any fix in getRandomFilename will just reduce the chance of file name collision. Since this a rare case, I think it may be better to just fail the task if failed to commit the files in the moveTaskOutputs(), without overwriting the existing files. In HDFS 0.23, rename() takes an option not to overwrite. With HADOOP 0.20, we can just do our best to check any conflicts before committing the files.",code_debt,low_quality_code,"Mon, 16 Jan 2012 19:03:42 +0000","Sun, 12 Jun 2022 20:10:10 +0000","Sat, 18 Jul 2015 10:15:59 +0000",110473937,"Any fix in getRandomFilename will just reduce the chance of file name collision. Since this a rare case, I think it may be better to just fail the task if failed to commit the files in the moveTaskOutputs(), without overwriting the existing files. In HDFS 0.23, rename() takes an option not to overwrite. With HADOOP 0.20, we can just do our best to check any conflicts before committing the files.",0.025,0.025,neutral
hbase,5282,comment_0,"When debugging, open region file was attempting to open either a truncated or 0 size hlogfile (which is throws IOException at out from getReader), and leaking a handle on every open attempt. Patch applies on 0.92 and trunk.",code_debt,low_quality_code,"Thu, 26 Jan 2012 13:05:35 +0000","Fri, 12 Oct 2012 05:35:00 +0000","Thu, 26 Jan 2012 23:50:18 +0000",38683,"When debugging, open region file was attempting to open either a truncated or 0 size hlogfile (which is throws IOException at out from getReader), and leaking a handle on every open attempt. Patch applies on 0.92 and trunk.",0.0,0.0,neutral
hbase,5329,comment_8,Thanks for activating this JIRA. Indentation is off for the above block. I prefer the old way of keeping long lockId so that the above parsing can be omitted.,code_debt,low_quality_code,"Fri, 3 Feb 2012 07:49:47 +0000","Mon, 23 Sep 2013 18:31:39 +0000","Fri, 28 Sep 2012 03:45:25 +0000",20548538,Thanks for activating this JIRA. Indentation is off for the above block. I prefer the old way of keeping long lockId so that the above parsing can be omitted.,0.06666666667,0.06666666667,neutral
hbase,5466,comment_1,Thanks for the finding. We use two spaces for indentation. Can you regenerate patch ? Refer to HBASE-3678.,code_debt,low_quality_code,"Thu, 23 Feb 2012 21:03:09 +0000","Fri, 12 Oct 2012 05:35:03 +0000","Fri, 24 Feb 2012 01:00:23 +0000",14234,Thanks for the finding. We use two spaces for indentation. Can you regenerate patch ? Refer to HBASE-3678.,0.1,0.1,neutral
hbase,5466,comment_5,+1 on patch (except for the spacing that is not like the rest of the file),code_debt,low_quality_code,"Thu, 23 Feb 2012 21:03:09 +0000","Fri, 12 Oct 2012 05:35:03 +0000","Fri, 24 Feb 2012 01:00:23 +0000",14234,+1 on patch (except for the spacing that is not like the rest of the file),-0.4,-0.4,neutral
hbase,5466,comment_6,"TestZooKeeper passed locally with patch v2. There should be a space between } and finally, finally and {, if and (, ) and { Overall, +1 on patch v2. Please fix formatting in v3.",code_debt,low_quality_code,"Thu, 23 Feb 2012 21:03:09 +0000","Fri, 12 Oct 2012 05:35:03 +0000","Fri, 24 Feb 2012 01:00:23 +0000",14234,"TestZooKeeper passed locally with patch v2. There should be a space between } and finally, finally and {, if and (, ) and { Overall, +1 on patch v2. Please fix formatting in v3.",0.06666666667,0.06666666667,neutral
hbase,5523,comment_3,"I remember the initial motivation for the +1 shift now. If somebody accidentally places a Delete at T it would not be possible to get at any Puts of T with normal Scan API. The +1 allow setting an interval that includes the Puts but not the Delete. I.e. setting the range to [0,T+1) would include the Puts and Deletes. (note that the lower bound inclusive and the upper bound is exclusive hence the [x,y) notation). With the +1 shift [0,T+1) would not contain the Delete, but [0,T+2) would. This is very confusing, since [0,T+1) doesn't actually mean that when it comes to deletes. To recover the above mentioned Puts one could use a raw scan, instead. I'm going to commit the attached patch; it makes these scenarios much clearer.",design_debt,non-optimal_design,"Mon, 5 Mar 2012 22:43:26 +0000","Tue, 26 Feb 2013 08:12:56 +0000","Tue, 6 Mar 2012 06:22:22 +0000",27536,"I remember the initial motivation for the +1 shift now. If somebody accidentally places a Delete at T it would not be possible to get at any Puts of T with normal Scan API. The +1 allow setting an interval that includes the Puts but not the Delete. I.e. setting the range to [0,T+1) would include the Puts and Deletes. (note that the lower bound inclusive and the upper bound is exclusive hence the [x,y) notation). With the +1 shift [0,T+1) would not contain the Delete, but [0,T+2) would. This is very confusing, since [0,T+1) doesn't actually mean that when it comes to deletes. To recover the above mentioned Puts one could use a raw scan, instead. I'm going to commit the attached patch; it makes these scenarios much clearer.",-0.03188888889,-0.03188888889,neutral
hbase,5591,comment_0,"sc requested code review of ""HBASE-5591 [jira] is identical to Bytes.getBytes()"". Reviewers: tedyu, dhruba, JIRA is identical to Bytes.getBytes() Remove the redundant method. Task ID: # Blame Rev: TEST PLAN Revert Plan: Tags: REVISION DETAIL AFFECTED FILES MANAGE HERALD DIFFERENTIAL RULES WHY DID I GET THIS EMAIL? Tip: use the X-Herald-Rules header to filter Herald messages in your client.",code_debt,duplicated_code,"Fri, 16 Mar 2012 02:11:50 +0000","Mon, 23 Sep 2013 18:44:58 +0000","Mon, 1 Oct 2012 18:44:12 +0000",17253142,"sc requested code review of ""HBASE-5591 [jira] ThiftServerRunner.HBaseHandler.toBytes() is identical to Bytes.getBytes()"". Reviewers: tedyu, dhruba, JIRA ThiftServerRunner.HBaseHandler.toBytes() is identical to Bytes.getBytes() Remove the redundant method. Task ID: # Blame Rev: TEST PLAN Revert Plan: Tags: REVISION DETAIL https://reviews.facebook.net/D2355 AFFECTED FILES src/main/java/org/apache/hadoop/hbase/regionserver/HRegionThriftServer.java src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java MANAGE HERALD DIFFERENTIAL RULES https://reviews.facebook.net/herald/view/differential/ WHY DID I GET THIS EMAIL? https://reviews.facebook.net/herald/transcript/5229/ Tip: use the X-Herald-Rules header to filter Herald messages in your client.",-0.0635,-0.03175,neutral
hbase,5591,comment_5,"sc has commented on the revision ""HBASE-5591 [jira] is identical to Bytes.getBytes()"". INLINE COMMENTS It was added by me actually. Because I checked bb) and found that it's different from this one. But this one is the same as bb). These names are really confusing. REVISION DETAIL BRANCH getbytes",code_debt,low_quality_code,"Fri, 16 Mar 2012 02:11:50 +0000","Mon, 23 Sep 2013 18:44:58 +0000","Mon, 1 Oct 2012 18:44:12 +0000",17253142,"sc has commented on the revision ""HBASE-5591 [jira] ThiftServerRunner.HBaseHandler.toBytes() is identical to Bytes.getBytes()"". INLINE COMMENTS src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java:611 It was added by me actually. Because I checked Bytes.toBytes(ByteBuffer bb) and found that it's different from this one. But this one is the same as Bytes.getBytes(ByteBuffer bb). These names are really confusing. REVISION DETAIL https://reviews.facebook.net/D2355 BRANCH getbytes",-0.06692857143,-0.03904166667,neutral
hbase,5636,comment_3,"I filed the new issue of the bug. By the way, the TestCase name is typo, isn't it? I will rename it to",documentation_debt,low_quality_documentation,"Mon, 26 Mar 2012 16:33:41 +0000","Tue, 26 Feb 2013 08:12:42 +0000","Mon, 2 Apr 2012 13:54:46 +0000",595265,"I filed the new issue of the bug. By the way, the TestCase name 'TestMulitthreadedTableMapper' is typo, isn't it? I will rename it to 'TestMultithreadedTableMapper'.",0.0,0.0,neutral
hbase,5958,comment_10,Makes me nervous to reach in and use the private constructor... do you have some benchmarks that show that there's a noticeable speedup by doing so?,code_debt,slow_algorithm,"Tue, 8 May 2012 17:34:38 +0000","Mon, 13 Jun 2022 16:35:55 +0000","Fri, 8 Jun 2012 16:28:02 +0000",2674404,Makes me nervous to reach in and use the private constructor... do you have some benchmarks that show that there's a noticeable speedup by doing so?,0.1,0.1,neutral
hbase,5958,comment_7,Protostuff is usually a little bit faster as well. I haven't personally run those benchmarks in a while and it looks like the most recent are not up yet. But still something to consider.,code_debt,slow_algorithm,"Tue, 8 May 2012 17:34:38 +0000","Mon, 13 Jun 2022 16:35:55 +0000","Fri, 8 Jun 2012 16:28:02 +0000",2674404,Protostuff is usually a little bit faster as well. https://github.com/eishay/jvm-serializers/wiki/Home/25fd014e66738268670adaf44ff5408ba2244d37 I haven't personally run those benchmarks in a while and it looks like the most recent are not up yet. But still something to consider.,0.304,0.304,neutral
hbase,5958,comment_9,"By the way, is it an option to use reflection to access the private constructor? If so, I can have a wrap method to use the private constructor, or the original copyFrom if the private constructor is not accessible. Reflection has overhead of course.",code_debt,low_quality_code,"Tue, 8 May 2012 17:34:38 +0000","Mon, 13 Jun 2022 16:35:55 +0000","Fri, 8 Jun 2012 16:28:02 +0000",2674404,"By the way, is it an option to use reflection to access the private constructor? If so, I can have a wrap method to use the private constructor, or the original copyFrom if the private constructor is not accessible. Reflection has overhead of course.",-0.1876666667,-0.1876666667,neutral
hbase,6009,comment_0,How do we define the markers ? Through a series of magic bytes ? Looks like total size field for ClusterStatus is better choice.,design_debt,non-optimal_design,"Wed, 16 May 2012 00:17:14 +0000","Mon, 13 Jun 2022 16:39:46 +0000","Mon, 25 Jun 2012 21:56:10 +0000",3533936,or we can have start and end markers How do we define the markers ? Through a series of magic bytes ? Looks like total size field for ClusterStatus is better choice.,0.1666666667,0.1666666667,neutral
hbase,6009,comment_1,"I looked at the total size field option for this, starting from the write case. To calculate total size written, you have to know how many bytes were written for each write() call on ClusterStatus, including any objects contained inside it. The DataOutput interface for Writables doesn't have a way to return how many bytes were written to the stream. This is not a problem for primitive types as we can figure that out trivially. Even for somewhat more complicated situations such as modified UTF-8s written with the writeUTF call, the number of written bytes for a String can at least be calculated based on the formula for modified UTF-8 conversion. However, for calls to Object's write functions (e.g. for HRegionLoad), this becomes somewhat more problematic as there is no obvious answer as to how many bytes were written. We could use reflection to grab the fields, but then there is no guarantee that all of the fields of the Object are actually written to the stream when write() is called. So you'd have to introduce some hardcoded way of knowing how much was written for each Object, which is Bad. I'm tempted to say that we shouldn't add any more fields to ClusterStatus or similar APIs until 0.96, when hopefully our wire compatibility efforts will kick in and we can do this in a compatible way without having to jump through hoops.",design_debt,non-optimal_design,"Wed, 16 May 2012 00:17:14 +0000","Mon, 13 Jun 2022 16:39:46 +0000","Mon, 25 Jun 2012 21:56:10 +0000",3533936,"I looked at the total size field option for this, starting from the write case. To calculate total size written, you have to know how many bytes were written for each write() call on ClusterStatus, including any objects contained inside it. The DataOutput interface for Writables doesn't have a way to return how many bytes were written to the stream. This is not a problem for primitive types as we can figure that out trivially. Even for somewhat more complicated situations such as modified UTF-8s written with the writeUTF call, the number of written bytes for a String can at least be calculated based on the formula for modified UTF-8 conversion. However, for calls to Object's write functions (e.g. for HRegionLoad), this becomes somewhat more problematic as there is no obvious answer as to how many bytes were written. We could use reflection to grab the fields, but then there is no guarantee that all of the fields of the Object are actually written to the stream when write() is called. So you'd have to introduce some hardcoded way of knowing how much was written for each Object, which is Bad. I'm tempted to say that we shouldn't add any more fields to ClusterStatus or similar APIs until 0.96, when hopefully our wire compatibility efforts will kick in and we can do this in a compatible way without having to jump through hoops.",0.03522222222,0.03522222222,neutral
hbase,6009,comment_4,"The immediate issue here is that HBASE-5209 was committed in 0.92.1, and that broke compatibility with 0.92.0. I suppose anyone who cares about 0.92 branch has moved to 0.92.1 so that there is no practical hit. You are right that adding a size would be another incompatible change, hence my later comment about ""let's just not make any more changes until 0.96"". :D Anyway in the absence of any changes, I can at least add a release note to 0.92.1 stating this incompatibility with 0.92.0. I'll use this JIRA to track that.",design_debt,non-optimal_design,"Wed, 16 May 2012 00:17:14 +0000","Mon, 13 Jun 2022 16:39:46 +0000","Mon, 25 Jun 2012 21:56:10 +0000",3533936,"The immediate issue here is that HBASE-5209 was committed in 0.92.1, and that broke compatibility with 0.92.0. I suppose anyone who cares about 0.92 branch has moved to 0.92.1 so that there is no practical hit. You are right that adding a size would be another incompatible change, hence my later comment about ""let's just not make any more changes until 0.96"". Anyway in the absence of any changes, I can at least add a release note to 0.92.1 stating this incompatibility with 0.92.0. I'll use this JIRA to track that.",0.2929,0.1729,neutral
hbase,6050,comment_8,Patch looks good. Minor: Please insert spaces around regionDir:,code_debt,low_quality_code,"Fri, 18 May 2012 15:41:10 +0000","Tue, 26 Feb 2013 08:16:18 +0000","Sun, 27 May 2012 16:38:55 +0000",781065,Patch looks good. Minor: Please insert spaces around regionDir:,0.488,0.488,positive
hbase,6184,comment_0,"This change will affect the look up in the META table? When searchRow is created with passing newformat=true, it will add the encoded name also at the end[<tableName In your issue you are getting the result but in that result the HRegionInfo seems coming as null only? Do this above change really fix your issue? Do u facing some other issues?",design_debt,non-optimal_design,"Thu, 7 Jun 2012 05:21:15 +0000","Mon, 13 Jun 2022 16:41:37 +0000","Thu, 25 Oct 2018 22:39:02 +0000",201460667,"This change will affect the look up in the META table? When searchRow is created with passing newformat=true, it will add the encoded name also at the end[<tableName>,<row>,<regionid>.<encodedname>.]. But the searchRow is used to do metaTable.getRowOrBefore(). Any way after the row we add HConstants.NINES using which we need to get correct row from META table. I mean adding this encodedname might not be needed for this lookup In your issue you are getting the result but in that result the HRegionInfo seems coming as null only? Do this above change really fix your issue? Do u facing some other issues?",0.14325,0.1448,neutral
hbase,6201,comment_6,"Sorry for chiming in late, but here's how I see it after quite a bit of internal discussions with some of the HBase and HDFS devs. First of all, lets not got caught up in terminology of what is a unit test, what is a functional test, etc. If we assume this stance than all the tests, essentially, fall under the 3 main categories: # tests that muck about with the internals of the particular single project (HDFS, HBase, etc) using things like private APIs (or sometimes even things like reflections, etc to really get into the guts of the system) # tests that concern themselves with a single project (HDFS, HBase, etc) but use only public APIs AND don't use # tests that concern themselves with multiple projects at the same time (imagine a test that submits an Oozie workflow that has some Pig and Hive actions actively manipulating data in HBase) but only using public APIs It is pretty clear that #3 definitely belongs to Bigtop while #1 definitely belongs to individual projects. For quite some time I was thinking that #2 belongs to Bigtop testbase as well, but I've changed my mind. I now believe that such tests should reside in individual projects and: # be clearly marked as such not to be confused with class #1 (test suites, test lists, naming convention work, etc) # be written/refactored in such a way that doesn't tie them to a particular deployment strategy. IOW they should assume the subsystem to be deployed. # be hooked up to the project build's system in such a way that takes care of deploying the least amount of a system to make them run (e.g. MiniDFS, MiniMR, etc.) Thus if HBase can follow these rules and have a subset of tests that can be executed in different envs. both HBase core devs and bigger Bigtop dev community win, since we can leverage each other's work. Makes sense? If it does I can help with re-factoring.",code_debt,low_quality_code,"Tue, 12 Jun 2012 02:33:19 +0000","Fri, 20 Nov 2015 11:53:53 +0000","Tue, 24 Sep 2013 21:21:05 +0000",40589266,"Sorry for chiming in late, but here's how I see it after quite a bit of internal discussions with some of the HBase and HDFS devs. First of all, lets not got caught up in terminology of what is a unit test, what is a functional test, etc. If we assume this stance than all the tests, essentially, fall under the 3 main categories: tests that muck about with the internals of the particular single project (HDFS, HBase, etc) using things like private APIs (or sometimes even things like reflections, etc to really get into the guts of the system) tests that concern themselves with a single project (HDFS, HBase, etc) but use only public APIs AND don't use tests that concern themselves with multiple projects at the same time (imagine a test that submits an Oozie workflow that has some Pig and Hive actions actively manipulating data in HBase) but only using public APIs It is pretty clear that #3 definitely belongs to Bigtop while #1 definitely belongs to individual projects. For quite some time I was thinking that #2 belongs to Bigtop testbase as well, but I've changed my mind. I now believe that such tests should reside in individual projects and: be clearly marked as such not to be confused with class #1 (test suites, test lists, naming convention work, etc) be written/refactored in such a way that doesn't tie them to a particular deployment strategy. IOW they should assume the subsystem to be deployed. be hooked up to the project build's system in such a way that takes care of deploying the least amount of a system to make them run (e.g. MiniDFS, MiniMR, etc.) Thus if HBase can follow these rules and have a subset of tests that can be executed in different envs. both HBase core devs and bigger Bigtop dev community win, since we can leverage each other's work. Makes sense? If it does I can help with re-factoring.",0.1753,0.1753,neutral
hbase,6201,comment_10,"Yes. At the current state, most of our unit tests, which are candidates to be upgraded to be system tests does start a mini-cluster of n-nodes, load some data, kill a few nodes, verify, etc. We are them to do the same things on the actual cluster. A particular test case, for example, starts 4 region servers, put some data, kills 1 RS, checks whether the regions are balanced, kills one more, checks agains, etc. Some basic functionality we can use from itest are: - Starting / stopping / sending a signal to daemons (start a region server on host1, kill master on host2, etc). For both HBase and Hadoop processes. - Basic cluster/node discovery (give me the nodes running hmaster) - Run this command on host3 (SSH)",design_debt,non-optimal_design,"Tue, 12 Jun 2012 02:33:19 +0000","Fri, 20 Nov 2015 11:53:53 +0000","Tue, 24 Sep 2013 21:21:05 +0000",40589266,"Are you saying that you would like the tests themeselves to get involved in the lifecycle of each service? Like bringing them up and down, etc? Yes. At the current state, most of our unit tests, which are candidates to be upgraded to be system tests does start a mini-cluster of n-nodes, load some data, kill a few nodes, verify, etc. We are converting/reimplementing them to do the same things on the actual cluster. A particular test case, for example, starts 4 region servers, put some data, kills 1 RS, checks whether the regions are balanced, kills one more, checks agains, etc. Some basic functionality we can use from itest are: Starting / stopping / sending a signal to daemons (start a region server on host1, kill master on host2, etc). For both HBase and Hadoop processes. Basic cluster/node discovery (give me the nodes running hmaster) Run this command on host3 (SSH)",-0.1138888889,-0.1083703704,neutral
hbase,6264,comment_0,Patch to correct documentation typos.,documentation_debt,low_quality_documentation,"Sat, 23 Jun 2012 20:01:33 +0000","Mon, 23 Sep 2013 18:31:39 +0000","Thu, 30 Aug 2012 23:10:43 +0000",5886550,Patch to correct documentation typos.,0.875,0.875,neutral
hbase,6282,comment_8,"Lets apply this and close this issue. W/ this applied, if you enable TRACE, you see this kinda of stuff: It is ridiculous detail but could be life saver debugging. Could later work on pb toStringing so it doesn't dump it all... just start of String and a length instead but this should be good for now. Remove Objects class since no longer used.",code_debt,dead_code,"Wed, 27 Jun 2012 19:07:57 +0000","Mon, 23 Sep 2013 18:30:39 +0000","Thu, 15 Nov 2012 03:56:24 +0000",12127707,"Lets apply this and close this issue. W/ this applied, if you enable TRACE, you see this kinda of stuff: It is ridiculous detail but could be life saver debugging. Could later work on pb toStringing so it doesn't dump it all... just start of String and a length instead but this should be good for now. Remove Objects class since no longer used.",0.1188333333,0.1188333333,neutral
hbase,6697,comment_0,"master & region server ports where set to ""random"" in the LocalHBaseCluster class; but not in the test hbase-site. So the pattern used in this test class was not working. Fixed, let's see if it breaks anything on hadoop qa...",code_debt,low_quality_code,"Thu, 30 Aug 2012 08:07:04 +0000","Mon, 23 Sep 2013 18:45:02 +0000","Sat, 1 Sep 2012 12:05:20 +0000",187096,"master & region server ports where set to ""random"" in the LocalHBaseCluster class; but not in the test hbase-site. So the pattern used in this test class was not working. Fixed, let's see if it breaks anything on hadoop qa...",-0.03333333333,-0.03333333333,negative
hbase,6806,comment_5,"python, c++ and java: worksforme I had no chance to test php, perl and ruby, they might even have syntactic errors in it...",test_debt,lack_of_tests,"Tue, 18 Sep 2012 13:55:58 +0000","Mon, 23 Sep 2013 18:30:49 +0000","Fri, 21 Sep 2012 04:08:14 +0000",223936,"python, c++ and java: worksforme I had no chance to test php, perl and ruby, they might even have syntactic errors in it...",-0.4,-0.4,negative
hbase,6835,comment_1,"Thanks for pointing to that, linking as related. HBASE-4198 seems like a good idea; for now, let's make the comment accurate.",documentation_debt,low_quality_documentation,"Wed, 19 Sep 2012 01:58:13 +0000","Mon, 23 Sep 2013 18:30:08 +0000","Wed, 19 Sep 2012 20:34:16 +0000",66963,"Thanks for pointing to that, linking as related. HBASE-4198 seems like a good idea; for now, let's make the comment accurate.",0.5815,0.5815,positive
hbase,6969,comment_2,"As written, at end of tests, won't your passed in zkcluster be shutdown? That is probably not what you want? Should there be a to answer your added Is baseZKCluster necessary? Why not just an internal flag which has whether or not HBaseTestingUtility started the zk cluster? If we didn't start it, we shouldn't stop it on the way out? Otherwise, looks like useful functionality to add. Thanks Micah.",design_debt,non-optimal_design,"Wed, 10 Oct 2012 02:34:56 +0000","Tue, 14 Jun 2022 22:00:52 +0000","Sat, 11 Apr 2015 00:23:59 +0000",78875343,"As written, at end of tests, won't your passed in zkcluster be shutdown? That is probably not what you want? Should there be a getMiniZookeeperCluster to answer your added setMiniZookeeperCluster? Is baseZKCluster necessary? Why not just an internal flag which has whether or not HBaseTestingUtility started the zk cluster? If we didn't start it, we shouldn't stop it on the way out? Otherwise, looks like useful functionality to add. Thanks Micah.",0.05714285714,0.05,negative
hbase,7212,comment_4,"Doc looks great. Do you have to write it yourself? Anything already available that you could use? Say we used the zk curator client, it has a few barriers implemented already: If we were using curator say, could you use these receipes as building blocks so you didn't have to write this yourself? (This feature has to be backportable to 0.94?) Reading the diagram, I""m not sure what receivedreached is. Or sendReached. sendReached is the coordinator saying all participants responded/are participating? On your barrier you say ""...but does not support ACID semantics"" and thats ok because the 'transactions' we'll be running over this mechanism do not require it? Because they can proceed and complete in any order and result will come off the same? You say ""....Does not recover on failures"" ... because the operation just FAILs. Right? Only one of these precedures can be ongoingn at any one time? Is that right? How do I read these set of slides? There is a 'Barrier Procedure Coordination' and then there is 'Procedure Coordination'? So, the PC makes use of a BPC? BPC is the skeleton you hang PC on? Why you say this 'If we arent doing proper 2PC do we need all this infrastructure?'? Are you making a case for our not needing 2PC given what is being implemented? Coordinator can be any client? Does not have to be master? What is Does this barrier acquistion have any relation to zk barrier receipe? What is 'class' in the zk node hierarchy? Class of procedure? Procedure looks good to me.",documentation_debt,low_quality_documentation,"Thu, 22 Nov 2012 19:08:26 +0000","Mon, 23 Sep 2013 18:30:35 +0000","Sat, 29 Dec 2012 06:18:52 +0000",3150626,"Doc looks great. Do you have to write it yourself? Anything already available that you could use? Say we used the zk curator client, it has a few barriers implemented already: https://github.com/Netflix/curator/wiki/Recipes If we were using curator say, could you use these receipes as building blocks so you didn't have to write this yourself? (This feature has to be backportable to 0.94?) Reading the diagram, I""m not sure what receivedreached is. Or sendReached. sendReached is the coordinator saying all participants responded/are participating? On your barrier you say ""...but does not support ACID semantics"" and thats ok because the 'transactions' we'll be running over this mechanism do not require it? Because they can proceed and complete in any order and result will come off the same? You say ""....Does not recover on failures"" ... because the operation just FAILs. Right? Only one of these precedures can be ongoingn at any one time? Is that right? How do I read these set of slides? There is a 'Barrier Procedure Coordination' and then there is 'Procedure Coordination'? So, the PC makes use of a BPC? BPC is the skeleton you hang PC on? Why you say this 'If we arent doing proper 2PC do we need all this infrastructure?'? Are you making a case for our not needing 2PC given what is being implemented? Coordinator can be any client? Does not have to be master? What is ProcedureCoordinateComms? Does this barrier acquistion have any relation to zk barrier receipe? http://zookeeper.apache.org/doc/trunk/recipes.html#sc_recipes_eventHandles What is 'class' in the zk node hierarchy? Class of procedure? Procedure looks good to me.",0.03497530864,-0.004202380952,neutral
hbase,7212,comment_5,"The online-snapshots is a 'class' (e.g. all online snapshots) while a procedure name is an actual name for a particular snapshotting request (snapshot121201, snapshot121202 etc). Off the top of my head I can't think of any other HBase processes that are ok with the procedure mechanism's semantics (other operations like enabling, disabling, schema change, splitting, merging probably want 2pc and its recovery requirements). I think this extra znode dir could probably get removed.",documentation_debt,low_quality_documentation,"Thu, 22 Nov 2012 19:08:26 +0000","Mon, 23 Sep 2013 18:30:35 +0000","Sat, 29 Dec 2012 06:18:52 +0000",3150626,"Thanks for taking a look. I'll get another rev out with cleaned up documentation in a day or two. Answers below. Do you have to write it yourself? Anything already available that you could use? Say we used the zk curator client, it has a few barriers implemented already: https://github.com/Netflix/curator/wiki/Recipes If we were using curator say, could you use these receipes as building blocks so you didn't have to write this yourself? (This feature has to be backportable to 0.94?) This is a simplified version of Jesse's patch. I just gave curator a quick it is similar to the double barrier (https://github.com/Netflix/curator/wiki/Double-barrier). If it is implemented as the recipe you pointed out, I think we'd still need to add in the ability for cancellation/abort to come from any of the members. Reading the diagram, I""m not sure what receivedreached is. Or sendReached. sendReached is the coordinator saying all participants responded/are participating? Yes  reached is sent when the coordinator figures out that it has ""reached"" the global barrier point because all members have taken their part of the global barrier. Basically, zk is being used for its async notifications and as the RPC mechanism. Arrows into the ZK column are calls writing to ZK, arrows out of ZK are callbacks being called at the target. So the red coordinator writes to zk via sendStart, zk node creation triggers a startNewOpearion callback on the the blue member1, and similarly on the the green member2. These names are short hand for the names in the review was posted  now sendStart -> sendBarrierStart, sendReached -> sendBarrierReached, startNewOperation -> Subprocedure's consturctor + acquireBarrier, receiveReached -> receiveReachedGlobalBarrier On your barrier you say ""...but does not support ACID semantics"" and thats ok because the 'transactions' we'll be running over this mechanism do not require it? Because they can proceed and complete in any order and result will come off the same? Previously, this code was called TwoPhaseCommit (2pc). While it had two phases, the code did not implement true two phase commit. The purpose of this explicit comparison is to make clear 2pc's purpose (distributed ACID guarantees), to point out that we don't have 2pc here, to point out that we don't need 2pc here, and to point out that we just need a global barrier. The online snapshot coordination does not need all of what 2pc provides. The first cut will have ""only on a sunny day"" semantics  e.g. it will only succeed if everything succeeds and if anything fails along the way whole attempt will be aborted. This is ok because the durable work that snapshots does goes into tmp dir (/hbase/.snapshots/.tmp/xxx) that is ""commited"" at the end atomically via HDFS dir rename, and that durable intermediate operation (e.g. new files from forcing a hlog roll or hlog flush) don't need to be undone to remain correct. You say ""....Does not recover on failures"" ... because the operation just FAILs. Right? Yup. Only one of these precedures can be ongoingn at any one time? Is that right? True for this first cut implementation, but not a fundamental limitation. This actually gets enforced at the snapshot manager level which may be visible in HBASE-7208 and definitely in HBASE-6866 when that gets posted. I believe as implemented if we picked a different class we could have multiple different kinds of procedure concurrently running on a different znode dir hierarchy. How do I read these set of slides? There is a 'Barrier Procedure Coordination' and then there is 'Procedure Coordination'? So, the PC makes use of a BPC? BPC is the skeleton you hang PC on? All those are synonymous  I've bee using procedure as a shorthand. The code implements one framework for a globally barriered procedure, and I've just tried to call it 'procedure' and 'subprocedure' everywhere (though from review I missed spots where it was called task, operation, or commit). This 'procedure' takes care of the global barrier coordination and cross process error propagation. Why you say this 'If we arent doing proper 2PC do we need all this infrastructure?'? Are you making a case for our not needing 2PC given what is being implemented? I could probably remove that line  I'm now convinced why we need what this code does. The main questions I had when I was initially understanding the previous implementation was ""Is this 2pc?"" and ""Do we need 2pc?"". The answers are: what we have implemented here has two phases but is not true two-phase commit. 2pc, as defined in the literature (http://www.cs.berkeley.edu/~brewer/cs262/Aries.pdf), requires that once the coordinator says something is committed, any failures at a member or coordinator must be recover by failing forward and completing it. The key point here is that while we will need a global barrier for one of the snapshot flavors (global), it don't need full 2PC because 1) the we don't need to undo work (like a log roll or flush) if some sub part of the first phase (our acquire/2pc's prepare) fails, and because 2) we don't need to recover failing forward if anything fails in the second phase (our release/2pc's commit). In the latter case we just fail and delete .snapshot/.tmp reminants in the fs, and carry on with extra flushed/rolled hlogs. Coordinator can be any client? Does not have to be master? It could be anywhere, but currently for snapshots the coordinator lives on the master. What is ProcedureCoordinateComms? This is actually a layer that separate the zk code (the rpc communications or comms code) from specific execution (snapshotting specific code). I could probably remove it, but the abstraction allows for testing the core pieces without zk. Does this barrier acquistion have any relation to zk barrier receipe? http://zookeeper.apache.org/doc/trunk/recipes.html#sc_recipes_eventHandles Yes. It is very similar to the double barrier. The main thing different here is this code allows for any member or coordinator to abort/cancel the whole shebang while the recipe doesn't seem to. From the recipe it seems that we could be a little bit more clever about how we use our znodes. (we might have one extra set). What is 'class' in the zk node hierarchy? Class of procedure? The online-snapshots is a 'class' (e.g. all online snapshots) while a procedure name is an actual name for a particular snapshotting request (snapshot121201, snapshot121202 etc). Off the top of my head I can't think of any other HBase processes that are ok with the procedure mechanism's semantics (other operations like enabling, disabling, schema change, splitting, merging probably want 2pc and its recovery requirements). I think this extra znode dir could probably get removed.",-0.065,0.008770502646,neutral
hbase,7212,comment_7,"On curator double-barrier, it would seem there is no 'abort' as you say. They do have timeouts on barrier enter and leave. Would that be enough See Rather than 'abort', you could just timeout? That might be simpler still? i.e. your ""Need to be able to force failure after a specified timeout elapses"" double-barrier does not seem to be enough though. There needs to be a means of telling cluster members to go for a particular snapshot barrier. To this end, I suppose all members need to be watching a snapshot dir and when a new snapshot appears, all try to 'enter' its barrier? Is it true that you do not want members to start 'snapshotting' until ALL participants have 'entered' the barrier? Does it matter if they start doing their work soon as they 'enter' the barrier (using curator/zk receipe terms). Reading on, it seems like its fine if members just go about their merry way....working on their part of snapshot. If not all members complete, the coordinator will clean up the incomplete. What do you think of the terms in the zk receipe: i.e. rather than 'reach' a barrier, 'enter' it? Some of the answers you give above should go into doc of this feature. They are quality. I buy your argument for going w/ the more basic barrier rather than 2pc function for snapshots (Yeah, 2pc would be useful for other distributed ops like table enable/disable w/ us 'failing forward' an interrupted table enable or disable) On 'Comms', it was just unclear to me what it was. Makes sense now.",documentation_debt,outdated_documentation,"Thu, 22 Nov 2012 19:08:26 +0000","Mon, 23 Sep 2013 18:30:35 +0000","Sat, 29 Dec 2012 06:18:52 +0000",3150626,"On curator double-barrier, it would seem there is no 'abort' as you say. They do have timeouts on barrier enter and leave. Would that be enough See http://www.jarvana.com/jarvana/view/com/netflix/curator/curator-recipes/0.6.4/curator-recipes-0.6.4-javadoc.jar!/com/netflix/curator/framework/recipes/barriers/DistributedDoubleBarrier.html#leave() Rather than 'abort', you could just timeout? That might be simpler still? i.e. your ""Need to be able to force failure after a specified timeout elapses"" double-barrier does not seem to be enough though. There needs to be a means of telling cluster members to go for a particular snapshot barrier. To this end, I suppose all members need to be watching a snapshot dir and when a new snapshot appears, all try to 'enter' its barrier? Yes  reached is sent when the coordinator figures out that it has ""reached"" the global barrier point because all members have taken their part of the global barrier. Is it true that you do not want members to start 'snapshotting' until ALL participants have 'entered' the barrier? Does it matter if they start doing their work soon as they 'enter' the barrier (using curator/zk receipe terms). Reading on, it seems like its fine if members just go about their merry way....working on their part of snapshot. If not all members complete, the coordinator will clean up the incomplete. What do you think of the terms in the zk receipe: i.e. rather than 'reach' a barrier, 'enter' it? Some of the answers you give above should go into doc of this feature. They are quality. I buy your argument for going w/ the more basic barrier rather than 2pc function for snapshots (Yeah, 2pc would be useful for other distributed ops like table enable/disable w/ us 'failing forward' an interrupted table enable or disable) On 'Comms', it was just unclear to me what it was. Makes sense now.",0.1078921569,0.09967592593,neutral
hbase,7212,comment_9,"I need to take a look at the source implementation of the curator double barrier and examples of its use to do a better job of comparing. Based on the api and the zk recipes, I'm going to make some assumptions here. As another analogy, it seems that our procedure mechanism is similar to a monitor (synchronized in java) that guarantees enter/acquire and leave/release of the barrier parts, while the curator one is lower level and leaves it to the implementer to enforce that invariant. So in this patch, the time-based abort trigger and a potential user-induced cancellation uses the same mechanism to notify all members (and the coordinator) that the procedure has aborted. I'm speculating but with think one assumption with this mechanism has vs the double barrier's is that we assume that the actions on the members may be slow (one implementation waits for a memstore flush per region) and may need to be interrupted before completion. The curator double barrier api doesn't have such a mechanism and we may have to wait for all operations to complete before we can abort them. I believe that would be the case if we used curator. I don't think we can't use it -- and the factoring out of the *Comms/*Rpcs would potentially allow us to move that in a future rev. At the end of the day, the full barrier is only required for the snapshot that completely blocks all writes to get a truly consistent snapshot. The weaker snapshots (either the timestamp based or log roll based) won't give those guarantees and doesn't actually need the full barrier. For the first cut however, I'm probably going to use it since it handles the error propagation and cross process cancellation. I'm fine with it -- I'll change the terms acquire - I'll do another rev of the docs to make it consistent with the changes being made.",documentation_debt,outdated_documentation,"Thu, 22 Nov 2012 19:08:26 +0000","Mon, 23 Sep 2013 18:30:35 +0000","Sat, 29 Dec 2012 06:18:52 +0000",3150626,"I need to take a look at the source implementation of the curator double barrier and examples of its use to do a better job of comparing. Based on the api and the zk recipes, I'm going to make some assumptions here. As another analogy, it seems that our procedure mechanism is similar to a monitor (synchronized in java) that guarantees enter/acquire and leave/release of the barrier parts, while the curator one is lower level and leaves it to the implementer to enforce that invariant. Rather than 'abort', you could just timeout? That might be simpler still? i.e. your ""Need to be able to force failure after a specified timeout elapses"" So in this patch, the time-based abort trigger and a potential user-induced cancellation uses the same mechanism to notify all members (and the coordinator) that the procedure has aborted. I'm speculating but with think one assumption with this mechanism has vs the double barrier's is that we assume that the actions on the members may be slow (one implementation waits for a memstore flush per region) and may need to be interrupted before completion. The curator double barrier api doesn't have such a mechanism and we may have to wait for all operations to complete before we can abort them. double-barrier does not seem to be enough though. There needs to be a means of telling cluster members to go for a particular snapshot barrier. To this end, I suppose all members need to be watching a snapshot dir and when a new snapshot appears, all try to 'enter' its barrier? I believe that would be the case if we used curator. I don't think we can't use it  and the factoring out of the *Comms/*Rpcs would potentially allow us to move that in a future rev. s it true that you do not want members to start 'snapshotting' until ALL participants have 'entered' the barrier? Does it matter if they start doing their work soon as they 'enter' the barrier (using curator/zk receipe terms). Reading on, it seems like its fine if members just go about their merry way....working on their part of snapshot. If not all members complete, the coordinator will clean up the incomplete. At the end of the day, the full barrier is only required for the snapshot that completely blocks all writes to get a truly consistent snapshot. The weaker snapshots (either the timestamp based or log roll based) won't give those guarantees and doesn't actually need the full barrier. For the first cut however, I'm probably going to use it since it handles the error propagation and cross process cancellation. What do you think of the terms in the zk receipe: i.e. rather than 'reach' a barrier, 'enter' it? I'm fine with it  I'll change the terms acquire -> enter, reached -> leave in the next rev I post. (in the v3 version I still need to clean up the nomenclature in the tests). I'll do another rev of the docs to make it consistent with the changes being made.",-0.1097222222,-0.01255333333,neutral
hbase,7933,comment_1,"I think the bug is a race condition for the parent znode for the table. deletes parent znode, so that we do not leak znodes for deleted tables.",code_debt,low_quality_code,"Mon, 25 Feb 2013 21:51:35 +0000","Mon, 23 Sep 2013 18:31:40 +0000","Wed, 27 Feb 2013 00:40:54 +0000",96559,"I think the bug is a race condition for the parent znode for the table. TableLockManager#tableDeleted() deletes parent znode, so that we do not leak znodes for deleted tables.",0.1,0.1,neutral
hbase,7933,comment_10,"I've run the tests with this once again. The ACL tests seem to be flaky. Master does not wait for _acl_ table to be created before accepting other create table statements. Will open another issue. I want to get this resolved sooner rather than later, since it affects the tests and pre-commit tests.",test_debt,flaky_test,"Mon, 25 Feb 2013 21:51:35 +0000","Mon, 23 Sep 2013 18:31:40 +0000","Wed, 27 Feb 2013 00:40:54 +0000",96559,"I've run the tests with this once again. The ACL tests seem to be flaky. Master does not wait for acl table to be created before accepting other create table statements. Will open another issue. I want to get this resolved sooner rather than later, since it affects the tests and pre-commit tests.",0.09,0.09,neutral
hbase,7933,comment_6,TestConstraint is a medium test. Meaning lock manager tests were not run.,test_debt,low_coverage,"Mon, 25 Feb 2013 21:51:35 +0000","Mon, 23 Sep 2013 18:31:40 +0000","Wed, 27 Feb 2013 00:40:54 +0000",96559,TestConstraint is a medium test. Meaning lock manager tests were not run.,0.3125,0.3125,neutral
hbase,7940,comment_1,Thanks for diggingin on this Ted. I changed the top-level pom but did not realize we had hard-coded versions in all submodules. How about this patch. It has us set version once in one place.,code_debt,low_quality_code,"Tue, 26 Feb 2013 17:41:38 +0000","Mon, 23 Sep 2013 18:31:32 +0000","Tue, 26 Feb 2013 18:29:20 +0000",2862,Thanks for diggingin on this Ted. I changed the top-level pom but did not realize we had hard-coded versions in all submodules. How about this patch. It has us set version once in one place.,0.175,0.175,neutral
hbase,8026,comment_3,Could you expand the help to include mention of how one uses RAW/VERSIONS? Also please add a test in should be straight forward.,test_debt,lack_of_tests,"Thu, 7 Mar 2013 17:36:28 +0000","Fri, 6 Apr 2018 17:55:55 +0000","Fri, 16 Jan 2015 17:47:09 +0000",58752641,Could you expand the help to include mention of how one uses RAW/VERSIONS? Also please add a test in hbase-shell/src/test/ruby/shell; should be straight forward.,0.25,0.25,neutral
hbase,8026,comment_4,"1) Usage : Its already present and can be seen once we do help scan on shell. Excerpt that include info on versions and raw : "" Also for experts, there is an advanced option -- RAW -- which instructs the scanner to return all cells (including delete markers and uncollected deleted cells). This option cannot be combined with requesting specific COLUMNS. Disabled by default. Example: hbase"" 2) Test - This jira is not about functionality of RAW and VERSIONS command. Its just the mention of these commands missing in the help. If you meant writing a new unit test for versions and raw command, then I will go through the shell tests to see if there is any corresponding test there and if not present, I can raise a new Jira and can work on it.",test_debt,lack_of_tests,"Thu, 7 Mar 2013 17:36:28 +0000","Fri, 6 Apr 2018 17:55:55 +0000","Fri, 16 Jan 2015 17:47:09 +0000",58752641,"1) Usage : Its already present and can be seen once we do help scan on shell. Excerpt that include info on versions and raw : "" Also for experts, there is an advanced option  RAW  which instructs the scanner to return all cells (including delete markers and uncollected deleted cells). This option cannot be combined with requesting specific COLUMNS. Disabled by default. Example: hbase> scan 't1', {RAW => true, VERSIONS => 10} "" 2) Test - This jira is not about functionality of RAW and VERSIONS command. Its just the mention of these commands missing in the help. If you meant writing a new unit test for versions and raw command, then I will go through the shell tests to see if there is any corresponding test there and if not present, I can raise a new Jira and can work on it.",0.09285714286,0.1747142857,neutral
hbase,8056,comment_6,"Moved code, added tests",architecture_debt,violation_of_modularity,"Sat, 9 Mar 2013 00:40:12 +0000","Thu, 16 Jun 2022 05:49:14 +0000","Wed, 20 Mar 2013 01:13:44 +0000",952412,"Moved code, added tests",0.0,0.0,neutral
hbase,8056,comment_4,Could make a new function to place the added code in StoreScanner and mark that it is used in stripe compactions. I think it would be clear for reader. The change is avaiable to me,design_debt,non-optimal_design,"Sat, 9 Mar 2013 00:40:12 +0000","Thu, 16 Jun 2022 05:49:14 +0000","Wed, 20 Mar 2013 01:13:44 +0000",952412,Could make a new function to place the added code in StoreScanner and mark that it is used in stripe compactions. I think it would be clear for reader. The change is avaiable to me,0.0,0.0,neutral
hbase,8056,comment_5,Can you store dropDeletesFromRow and dropDeletesToRow in ScanQueryMatcher and keep the logic local there?,design_debt,non-optimal_design,"Sat, 9 Mar 2013 00:40:12 +0000","Thu, 16 Jun 2022 05:49:14 +0000","Wed, 20 Mar 2013 01:13:44 +0000",952412,Can you store dropDeletesFromRow and dropDeletesToRow in ScanQueryMatcher and keep the logic local there?,0.0,0.0,neutral
hbase,8056,comment_1,"The comment in SQM should say ""set to false"", not ""set to true"", I will fix that.",documentation_debt,low_quality_documentation,"Sat, 9 Mar 2013 00:40:12 +0000","Thu, 16 Jun 2022 05:49:14 +0000","Wed, 20 Mar 2013 01:13:44 +0000",952412,"The comment in SQM should say ""set to false"", not ""set to true"", I will fix that.",-0.3865,-0.3865,neutral
hbase,8056,comment_0,A fairly simple patch with lots of comments. I am trying to see if there's a viable way to test the code in this area.,test_debt,low_coverage,"Sat, 9 Mar 2013 00:40:12 +0000","Thu, 16 Jun 2022 05:49:14 +0000","Wed, 20 Mar 2013 01:13:44 +0000",952412,A fairly simple patch with lots of comments. I am trying to see if there's a viable way to test the code in this area.,0.01575,0.01575,neutral
hbase,8089,comment_17,"Matt - the only other thing is a dependency on HBase's Bytes in a couple places. I think this can easily be removed. Part of the point of this effort is to have HBase ship a standard implementation that other Hadoop ecosystem components can rely on. If I just wanted something for my own application, I'd use Orderly and be done with it.",build_debt,over-declared_dependencies,"Wed, 13 Mar 2013 16:18:34 +0000","Thu, 16 Jun 2022 05:49:38 +0000","Sat, 11 Jun 2022 18:11:53 +0000",291779599,"Matt - the only other thing is a dependency on HBase's Bytes in a couple places. I think this can easily be removed. Part of the point of this effort is to have HBase ship a standard implementation that other Hadoop ecosystem components can rely on. If I just wanted something for my own application, I'd use Orderly and be done with it.",0.0,0.0,neutral
hbase,8089,comment_0,"I'm beginning to think variable-length encoding for anything but char,byte arrays is an unnecessary micro-optimization. Instead of helping a user pack data via encoding, we should encourage the use of compression.",design_debt,non-optimal_design,"Wed, 13 Mar 2013 16:18:34 +0000","Thu, 16 Jun 2022 05:49:38 +0000","Sat, 11 Jun 2022 18:11:53 +0000",291779599,"I'm beginning to think variable-length encoding for anything but char,byte arrays is an unnecessary micro-optimization. Instead of helping a user pack data via encoding, we should encourage the use of compression.",0.2145,0.2145,neutral
hbase,8089,comment_18,"Hey Nick, It might be worth updating this jira to reflect the latest state of the work. IIUC this work is about proving a client-side library that does the order-preserving serialization, that higher level projects (eg Phoenix & Kiji) can use for row keys and column qualifiers. Per the other jiras, cell serialization, defining types, and schema are out of scope. These are left to higher-level systems which may make different choices (eg in terms of how to create compound keys) and may have different type models, but at least will be able to share serialization. IMO it's worth considering creating a separate project for this as this is genuinely useful outside HBase (eg container formats) and would benefit from multiple language implementations (the serialization here is language agnostic right?) and so the HBase project may end up being a clunky place to maintain things. Thanks, Eli",design_debt,non-optimal_design,"Wed, 13 Mar 2013 16:18:34 +0000","Thu, 16 Jun 2022 05:49:38 +0000","Sat, 11 Jun 2022 18:11:53 +0000",291779599,"Hey Nick, It might be worth updating this jira to reflect the latest state of the work. IIUC this work is about proving a client-side library that does the order-preserving serialization, that higher level projects (eg Phoenix & Kiji) can use for row keys and column qualifiers. Per the other jiras, cell serialization, defining types, and schema are out of scope. These are left to higher-level systems which may make different choices (eg in terms of how to create compound keys) and may have different type models, but at least will be able to share serialization. IMO it's worth considering creating a separate project for this as this is genuinely useful outside HBase (eg container formats) and would benefit from multiple language implementations (the serialization here is language agnostic right?) and so the HBase project may end up being a clunky place to maintain things. Thanks, Eli",0.35325,0.35325,neutral
hbase,8193,comment_0,"A round-trip to ZK is expensive... Should we put this call as optionnal with a parameter? For long running clusters, thise case might never happend. But I also agree that this call might not be done so often so perfs impacts might be small...",code_debt,slow_algorithm,"Mon, 25 Mar 2013 10:25:48 +0000","Thu, 16 Jun 2022 05:54:51 +0000","Tue, 30 Dec 2014 04:36:14 +0000",55707026,"A round-trip to ZK is expensive... Should we put this call as optionnal with a parameter? For long running clusters, thise case might never happend. But I also agree that this call might not be done so often so perfs impacts might be small...",0.06666666667,0.06666666667,negative
hbase,8324,comment_1,"Talked with , and we found this in the MRAppMaster's logs: This is related to MAPREDUCE-4880 which is in turn fixed by MAPREDUCE-4607 (a race in speculative task execution, fixed in 2.0.3-alpha). Compiling and running against hadoop-2.0.3-alpha fails out even earlier so instead of going that route, I'm going to try an alternate workaround -- disabling mapper and reducer speculative execution.",design_debt,non-optimal_design,"Thu, 11 Apr 2013 05:16:56 +0000","Mon, 23 Sep 2013 19:08:23 +0000","Fri, 12 Apr 2013 14:59:26 +0000",121350,"Talked with sandyr, and we found this in the MRAppMaster's logs: This is related to MAPREDUCE-4880 which is in turn fixed by MAPREDUCE-4607 (a race in speculative task execution, fixed in 2.0.3-alpha). Compiling and running against hadoop-2.0.3-alpha fails out even earlier so instead of going that route, I'm going to try an alternate workaround  disabling mapper and reducer speculative execution.",-0.1833333333,-0.1833333333,negative
hbase,8324,comment_9,I don't think this is a one-off -- this would affect all of our MR tests that use more than one mapper/reducer. I don't think speculative execution should be on for our MR tests in general.,design_debt,non-optimal_design,"Thu, 11 Apr 2013 05:16:56 +0000","Mon, 23 Sep 2013 19:08:23 +0000","Fri, 12 Apr 2013 14:59:26 +0000",121350,I don't think this is a one-off  this would affect all of our MR tests that use more than one mapper/reducer. I don't think speculative execution should be on for our MR tests in general.,0.2,0.2,negative
hbase,8324,comment_5,v2 improves comments on patch.,documentation_debt,low_quality_documentation,"Thu, 11 Apr 2013 05:16:56 +0000","Mon, 23 Sep 2013 19:08:23 +0000","Fri, 12 Apr 2013 14:59:26 +0000",121350,v2 improves comments on patch.,0.4,0.4,negative
hbase,8324,comment_8,Please TODO in this comment or file a new ticket to remove this configuration tweak after the issue is resolved upstream. I don't want one-offs like this to become lost and forgotten.,requirement_debt,requirement_partially_implemented,"Thu, 11 Apr 2013 05:16:56 +0000","Mon, 23 Sep 2013 19:08:23 +0000","Fri, 12 Apr 2013 14:59:26 +0000",121350,Please TODO in this comment or file a new ticket to remove this configuration tweak after the issue is resolved upstream. I don't want one-offs like this to become lost and forgotten.,0.25,0.25,negative
hbase,8324,comment_10,"Fair enough. However, that means we are leaving HBase's interaction with this feature untested. Does that mean we will advise users against using it with HBase? If we're not going to test it, that means we're not taking responsibility for it; thus I conclude that we must ""officially drop support"" for the feature, however that's done.",test_debt,lack_of_tests,"Thu, 11 Apr 2013 05:16:56 +0000","Mon, 23 Sep 2013 19:08:23 +0000","Fri, 12 Apr 2013 14:59:26 +0000",121350,"Fair enough. However, that means we are leaving HBase's interaction with this feature untested. Does that mean we will advise users against using it with HBase? If we're not going to test it, that means we're not taking responsibility for it; thus I conclude that we must ""officially drop support"" for the feature, however that's done.",0.0875,0.0875,negative
hbase,8527,comment_5,lgtm. Minor nit: Should there be a comment on when the point of no return is?,code_debt,low_quality_code,"Fri, 10 May 2013 21:19:51 +0000","Thu, 16 Jun 2022 17:04:53 +0000","Thu, 23 May 2013 20:38:33 +0000",1120722,lgtm. Minor nit: Should there be a comment on when the point of no return is?,0.0,0.0,neutral
hbase,8665,comment_10,"Will submit some simple patch tomorrow, we are hitting this a lot under high load. As soon as I can make the test slightly less ugly.",code_debt,low_quality_code,"Thu, 30 May 2013 23:21:38 +0000","Thu, 5 May 2022 02:11:33 +0000","Wed, 19 Jun 2013 00:42:56 +0000",1646478,"Will submit some simple patch tomorrow, we are hitting this a lot under high load. As soon as I can make the test slightly less ugly.",-0.5,-0.5,neutral
hbase,8665,comment_5,"W.r.t. property of the store: 1) Current priority is derived entirely from store state. In fact it only depends on number of files and blocking limit, so there's no such thing as important store other than store. 2) It seems to be the obvious cause of this particular issue, see above. When selection is added to queue, its priority is low. As store fills up it's ""real"" priority changes, but we have no way to influence it. Bumping its priority when we queue another one is just a hack around that imho; if it even helps. If blocked store comes into a queue full of non-blocked stores why should we bump them? And how much. 3) Additionally if we pre-select in advance we simply end up with inefficient compactions, for example in this case we could have compacted 6 small files in one go, but would have ended up compacting 3 and 3 if it spent less time in the queue.",code_debt,low_quality_code,"Thu, 30 May 2013 23:21:38 +0000","Thu, 5 May 2022 02:11:33 +0000","Wed, 19 Jun 2013 00:42:56 +0000",1646478,"W.r.t. property of the store: 1) Current priority is derived entirely from store state. In fact it only depends on number of files and blocking limit, so there's no such thing as important store other than blocked/about-to-block store. 2) It seems to be the obvious cause of this particular issue, see above. When selection is added to queue, its priority is low. As store fills up it's ""real"" priority changes, but we have no way to influence it. Bumping its priority when we queue another one is just a hack around that imho; if it even helps. If blocked store comes into a queue full of non-blocked stores why should we bump them? And how much. 3) Additionally if we pre-select in advance we simply end up with inefficient compactions, for example in this case we could have compacted 6 small files in one go, but would have ended up compacting 3 and 3 if it spent less time in the queue.",0.01944444444,0.002777777778,neutral
hbase,8665,comment_11,"Here's the patch. It's rather simple, most complexity is in tests. It allows for both pre-selected compactions (from coprocessors and users) and non-pre-selected. When non-preselected compaction makes it to run() it is selected and executed. There are two special cases: 1) Store priority might have decreased since the compaction was queued; in that case it's re-queued with new priority to avoid inversion. 2) Without selecting, we don't know which pool to go to. We go to small pool by default, and if the compaction is large after selection, queue it to the large pool; it can bounce back if it becomes small while stuck in the large pool. Both of these cases can cause compaction to get continuously requeued, or bounced between pools, however it can only happen if store priority decreases (i.e. other compactions happen), or files are removed from store (same); or files are added in some very special pattern that causes policy to select continuously).",design_debt,non-optimal_design,"Thu, 30 May 2013 23:21:38 +0000","Thu, 5 May 2022 02:11:33 +0000","Wed, 19 Jun 2013 00:42:56 +0000",1646478,"Here's the patch. It's rather simple, most complexity is in tests. It allows for both pre-selected compactions (from coprocessors and users) and non-pre-selected. When non-preselected compaction makes it to run() it is selected and executed. There are two special cases: 1) Store priority might have decreased since the compaction was queued; in that case it's re-queued with new priority to avoid inversion. 2) Without selecting, we don't know which pool to go to. We go to small pool by default, and if the compaction is large after selection, queue it to the large pool; it can bounce back if it becomes small while stuck in the large pool. Both of these cases can cause compaction to get continuously requeued, or bounced between pools, however it can only happen if store priority decreases (i.e. other compactions happen), or files are removed from store (same); or files are added in some very special pattern that causes policy to select large-small-large-small-... continuously).",0.0406875,0.0406875,neutral
hbase,8665,comment_6,I was suggesting bumping the priority instead of queueing another. Bumping the compaction request to what would currently be computed seems reasonable. In this case pre-selecting doesn't seem to be a bad thing. Prioritizing a fast compaction over being a little more efficient seems like a trade off that most people would want. So since current implementation biases towards what I would expect users to want maybe we shouldn't change that until we can put those smarts into the compaction selection (plumbing reason the compaction was requested into selection).,design_debt,non-optimal_design,"Thu, 30 May 2013 23:21:38 +0000","Thu, 5 May 2022 02:11:33 +0000","Wed, 19 Jun 2013 00:42:56 +0000",1646478,"If blocked store comes into a queue full of non-blocked stores why should we bump them? And how much. I was suggesting bumping the priority instead of queueing another. Bumping the compaction request to what would currently be computed seems reasonable. 3) Additionally if we pre-select in advance we simply end up with inefficient compactions, for example in this case we could have compacted 6 small files in one go, but would have ended up compacting 3 and 3 if it spent less time in the queue. In this case pre-selecting doesn't seem to be a bad thing. Prioritizing a fast compaction over being a little more efficient seems like a trade off that most people would want. So since current implementation biases towards what I would expect users to want maybe we shouldn't change that until we can put those smarts into the compaction selection (plumbing reason the compaction was requested into selection).",0.3084,0.152125,neutral
hbase,8665,comment_7,"Well, the effect of getting a faster compaction in this case is a pure accident, if there was not a smaller one already queued, it would still compact 6 according to policy. Also, out of many possible faster compactions in this case, bad one (later files) is chosen, so it's not really what user would expect. Policy should make such decisions - if we prefer faster compactions for blocked store, we should have it in the policy, and so last-moment selection would still choose the best one. As for bumping the priority of current to what it would have been, it is actually equivalent to just sorting them by current store priority... I wonder if there's any fundamental reason to divorce selection from compaction? If we introduce compaction-based priority modifiers, not just store based, we could still apply them by doing selection in multiple stores and comparing priorities. Selecting is not that expensive, given how frequently we compact.",design_debt,non-optimal_design,"Thu, 30 May 2013 23:21:38 +0000","Thu, 5 May 2022 02:11:33 +0000","Wed, 19 Jun 2013 00:42:56 +0000",1646478,"Well, the effect of getting a faster compaction in this case is a pure accident, if there was not a smaller one already queued, it would still compact 6 according to policy. Also, out of many possible faster compactions in this case, bad one (later files) is chosen, so it's not really what user would expect. Policy should make such decisions - if we prefer faster compactions for blocked store, we should have it in the policy, and so last-moment selection would still choose the best one. As for bumping the priority of current to what it would have been, it is actually equivalent to just sorting them by current store priority... I wonder if there's any fundamental reason to divorce selection from compaction? If we introduce compaction-based priority modifiers, not just store based, we could still apply them by doing selection in multiple stores and comparing priorities. Selecting is not that expensive, given how frequently we compact.",0.1791777778,0.1791777778,neutral
hbase,8665,comment_12,Mind adding javadoc for the selectNow parameter ? Add debug log for the above case ? Please add license header for Can be private ?,documentation_debt,outdated_documentation,"Thu, 30 May 2013 23:21:38 +0000","Thu, 5 May 2022 02:11:33 +0000","Wed, 19 Jun 2013 00:42:56 +0000",1646478,Mind adding javadoc for the selectNow parameter ? Add debug log for the above case ? Please add license header for StatefulStoreMockMaker.java Can BlockingStoreMockMaker be private ?,0.06666666667,0.05,neutral
hbase,9131,comment_1,"Thanks. I think we are somewhere between too little detail and too much detail. First, can we add the config variables to hbase-default.xml (with full descriptions and with units). Now to the meat: The patch doesn't tell the admin why or when they'd want to consider using this. The link/pdf requires having to search for the bucket cache sections in the 2nd page and then goes on into too much design detail for an average admin. (It also lacks the config variables / instructions). My suggestion: Take let's take the high-level parts from section 3 of the pdf, polish it and add it to the official docs. Here's a stab at the sections that I think would be good for the ref guide with the prose improved a little bit: Let me know what you think, and feel free to update/correct the draft.",documentation_debt,low_quality_documentation,"Mon, 5 Aug 2013 23:28:26 +0000","Thu, 16 Jun 2022 17:50:38 +0000","Wed, 18 Jun 2014 03:37:06 +0000",27317320,"zjushch Thanks. I think we are somewhere between too little detail and too much detail. First, can we add the config variables to hbase-default.xml (with full descriptions and with units). Now to the meat: The patch doesn't tell the admin why or when they'd want to consider using this. The link/pdf requires having to search for the bucket cache sections in the 2nd page and then goes on into too much design detail for an average admin. (It also lacks the config variables / instructions). My suggestion: Take let's take the high-level parts from section 3 of the pdf, polish it and add it to the official docs. Here's a stab at the sections that I think would be good for the ref guide with the prose improved a little bit: Design and Motivation The Bucket Cache is an alternate block cache implementation that is designed to take advantage of large amounts of memory or low-latency storage. (something about how big would be useful). It is implemented as an off-the-jvm-heap and which has the secondary benefit of reducing JVM heap fragmentation that eventually causes stop-the-world JVM garbage collection operations. If one were to rely upon the standard JVM memory allocation and GC policies with large heaps (>16GB RAM) one would periodically incur instability in hbase due to long stop-the-world GC pauses (10's of secs to minutes) that can be misinterpreted as region server failures. The storage of cached blocks is is not constrained to in RAM-only use; one could cache blocks in memory and also use a high speed disk, such as SSD's, Fusion-IO devices, or ram-disks as massive secondary cache. (probably need something about the persistence properties not being required, but having the masssive capacity as a huge benefit. Internally, the bucket cache divided storage into many buckets, each of which contains blocks of a particular range of sizes. (this is a little fuzzy, needs some clarification). Insertions and evictions of blocks backed by physical storage just overwrites blocks on the device or reads data from the storage device. Managing these larger blocks prevents external fragmentation that causes GC pauses at the cost of some minor wasted space (internal fragmentation). Configuration and Usage To configure the bucket cache... (something along the line of what the current patch has).... Let me know what you think, and feel free to update/correct the draft.",-0.0292,0.01437719298,neutral
hbase,9158,comment_4,Cleaned up 0.94 patch.,code_debt,low_quality_code,"Thu, 8 Aug 2013 06:59:45 +0000","Wed, 21 Aug 2013 00:08:50 +0000","Thu, 8 Aug 2013 23:08:49 +0000",58144,Cleaned up 0.94 patch.,0.0,0.0,neutral
hbase,9315,comment_1,This patch alters the test to wait for the cache to stabilize before asserting on the number of evictions run. This may be overkill.,design_debt,non-optimal_design,"Fri, 23 Aug 2013 00:17:13 +0000","Fri, 20 Nov 2015 11:52:24 +0000","Tue, 27 Aug 2013 16:36:36 +0000",404363,This patch alters the test to wait for the cache to stabilize before asserting on the number of evictions run. This may be overkill.,0.0,0.0,negative
hbase,9340,comment_1,nit: no need for that extra newline.,code_debt,low_quality_code,"Mon, 26 Aug 2013 19:04:22 +0000","Fri, 20 Nov 2015 11:53:42 +0000","Mon, 26 Aug 2013 20:18:20 +0000",4438,nit: no need for that extra newline.,0.0,0.0,negative
hbase,9461,comment_0,"Here is a start. Adds start of a description of who is doing what to who. Removes thread local that made the RcpServer instance available -- not used (seemingly), ugly.",code_debt,low_quality_code,"Sat, 7 Sep 2013 17:30:25 +0000","Fri, 20 Nov 2015 11:55:25 +0000","Sun, 15 Sep 2013 02:24:06 +0000",636821,"Here is a start. Adds start of a description of who is doing what to who. Removes thread local that made the RcpServer instance available  not used (seemingly), ugly.",-0.3333333333,-0.3333333333,neutral
hbase,9461,comment_4,"More doc and untangling. More to do but this ok for now. RpcServer had internal CallRunner class. The scheduler was reaching over to use this inner class. Broke it out of RpcServer and made it Standalone. Made it take an RpcServerInterface instead of RpcServer. Undid CallRunner implementing Runnable (confusing -- how it is run is internal function of Scheduler implementation) Cleaned up RpcServerInterface explaining why a start and a startThreads and an openServer. Added a few methods to support CallRunner being outside of RpcServer. Added into CallRunner the managment of the call queue size. As it was, callQueueSize was incremented before we created a CallRunner but internal to CallRunner it was managing the decrement. Shutdown access on CallRunner constructor and CR#getStatus and other methods only for use in this package. Added javadoc all around to explain the cryptic. was like CallRunner, a class used by the Scheduler only it was an inner class of RpcServer. Moved it out. Removed unused thread local SERVER. Add simple test to demo being able to instantiate a CallRunner outside of RpcServer context.a",code_debt,low_quality_code,"Sat, 7 Sep 2013 17:30:25 +0000","Fri, 20 Nov 2015 11:55:25 +0000","Sun, 15 Sep 2013 02:24:06 +0000",636821,"More doc and untangling. More to do but this ok for now. RpcServer had internal CallRunner class. The scheduler was reaching over to use this inner class. Broke it out of RpcServer and made it Standalone. Made it take an RpcServerInterface instead of RpcServer. Undid CallRunner implementing Runnable (confusing  how it is run is internal function of Scheduler implementation) Cleaned up RpcServerInterface explaining why a start and a startThreads and an openServer. Added a few methods to support CallRunner being outside of RpcServer. Added into CallRunner the managment of the call queue size. As it was, callQueueSize was incremented before we created a CallRunner but internal to CallRunner it was managing the decrement. Shutdown access on CallRunner constructor and CR#getStatus and other methods only for use in this package. Added javadoc all around to explain the cryptic. RpcSchedulerContextImpl was like CallRunner, a class used by the Scheduler only it was an inner class of RpcServer. Moved it out. Removed unused thread local SERVER. Add simple test to demo being able to instantiate a CallRunner outside of RpcServer context.a",0.09417647059,0.09417647059,neutral
hbase,9461,comment_6,"Just asking, if you find the time. Some comments describing the intention (what this class is supposed to do) would be great. For example, the delayed calls, undelayed calls with delayed response are not trivial (are they used?). It's very difficult to understand what these classes are supposed to do and why. Or this low level, with a specific case with '==1' Basically, I wanted to try a Netty based implementation, but there are so many corner cases that even estimating the time needed for a hacked implementation is difficult....",code_debt,low_quality_code,"Sat, 7 Sep 2013 17:30:25 +0000","Fri, 20 Nov 2015 11:55:25 +0000","Sun, 15 Sep 2013 02:24:06 +0000",636821,"Just asking, if you find the time. Some comments describing the intention (what this class is supposed to do) would be great. For example, the delayed calls, undelayed calls with delayed response are not trivial (are they used?). It's very difficult to understand what these classes are supposed to do and why. Or this low level, with a specific case with '==1' Basically, I wanted to try a Netty based implementation, but there are so many corner cases that even estimating the time needed for a hacked implementation is difficult....",-1.11e-17,-1.11e-17,neutral
hbase,9461,comment_7,"Smile. I am in same boat as you. I want to untangle this rats nest so I can understand whats going on and try stuff (I want to try pool of bytebuffers -- direct bytebuffers even -- for incoming requests. Putting in netty too would be sweet). Unless you object, was going to commit this since it some progress. Was going to do more along this line soon but in other issues; I'm afraid the refactors will rot between getting time to work in here. The Delay stuff is unused I think. It was an experiment. Maybe I'll look at that next and purge it if I can.",code_debt,dead_code,"Sat, 7 Sep 2013 17:30:25 +0000","Fri, 20 Nov 2015 11:55:25 +0000","Sun, 15 Sep 2013 02:24:06 +0000",636821,"Smile. I am in same boat as you. I want to untangle this rats nest so I can understand whats going on and try stuff (I want to try pool of bytebuffers  direct bytebuffers even  for incoming requests. Putting in netty too would be sweet). Unless you object, was going to commit this since it some progress. Was going to do more along this line soon but in other issues; I'm afraid the refactors will rot between getting time to work in here. The Delay stuff is unused I think. It was an experiment. Maybe I'll look at that next and purge it if I can.",0.1312407407,0.1312407407,neutral
hbase,9479,comment_13,"Sorry about the pain. In 0.94, client and server were all bundled up in the one ball w/ client dependencies those of the servers so yeah, it is ugly. We've been trying to improve our story in 0.96. Some pruning/edit has been done to 'shield' downstreamers in 0.96 from the lorry-load of dependencies pulled in by our dependencies but for sure we could do better.",build_debt,build_others,"Mon, 9 Sep 2013 20:35:23 +0000","Thu, 16 Jun 2022 18:04:00 +0000","Mon, 9 Sep 2013 21:22:21 +0000",2818,"mobiusinversion Sorry about the pain. In 0.94, client and server were all bundled up in the one ball w/ client dependencies those of the servers so yeah, it is ugly. We've been trying to improve our story in 0.96. Some pruning/edit has been done to 'shield' downstreamers in 0.96 from the lorry-load of dependencies pulled in by our dependencies but for sure we could do better.",-0.284375,-0.284375,negative
hbase,9493,comment_0,Simple braindead patch. New name makes it clear that allocation is going on (and also clearly points out that we are doing a lot of small array allocations in tests).,code_debt,low_quality_code,"Tue, 10 Sep 2013 20:44:08 +0000","Fri, 20 Nov 2015 11:53:02 +0000","Tue, 10 Sep 2013 23:10:15 +0000",8767,Simple braindead patch. New name makes it clear that allocation is going on (and also clearly points out that we are doing a lot of small array allocations in tests).,0.0,0.0,negative
hbase,9545,comment_0,"Stack trace It looks like assumes its {{masterMonitor}} field is never null, but if there is no connection,that isn't true. The close() operation should be made a bit more robust, so as not to hide the underlying RPC failures I expect to see",code_debt,low_quality_code,"Mon, 16 Sep 2013 17:30:00 +0000","Thu, 16 Jun 2022 18:06:24 +0000","Tue, 17 Sep 2013 09:07:39 +0000",56259,"Stack trace It looks like MasterMonitorCallable.close() assumes its masterMonitor field is never null, but if there is no connection,that isn't true. The close() operation should be made a bit more robust, so as not to hide the underlying RPC failures I expect to see",-0.1198333333,-0.07988888889,negative
hbase,9671,comment_3,I would expect it to throw an exception like it currently is. The point is that chaos monkey actions shouldn't be trying to avoid exceptions. It should be issuing commands at will and not trying to protect the cluster at all. Anything else can hide bugs.,design_debt,non-optimal_design,"Fri, 27 Sep 2013 14:01:01 +0000","Thu, 16 Jun 2022 18:12:48 +0000","Wed, 27 Nov 2013 23:00:31 +0000",5302770,Can you clarify what would be the expected behavior ? I would expect it to throw an exception like it currently is. The point is that chaos monkey actions shouldn't be trying to avoid exceptions. It should be issuing commands at will and not trying to protect the cluster at all. Anything else can hide bugs.,0.0125,0.01,neutral
hbase,9683,comment_9,"Hi , I agree that the 0.94 RPC is ugly in terms of expansion and it needs much more carefulness and efforts than the PB based one. I believe that the 0.94 series will remain in production for a while (at least for us and for several other users that I know of). Having this back ported to 0.94 will definitely benefit a lot of existing users.",design_debt,non-optimal_design,"Sun, 29 Sep 2013 11:20:55 +0000","Thu, 16 Jun 2022 18:11:25 +0000","Wed, 22 Apr 2015 00:41:58 +0000",49209663,"Hi eclark, I agree that the 0.94 RPC is ugly in terms of expansion and it needs much more carefulness and efforts than the PB based one. I believe that the 0.94 series will remain in production for a while (at least for us and for several other users that I know of). Having this back ported to 0.94 will definitely benefit a lot of existing users.",0.1848888889,0.1848888889,neutral
hbase,9689,comment_7,"Looks good Correct the comment pls. Seems copy paste :) set_get_attributes, set_put_attributes seems same code repeating. We can make this one def?",code_debt,duplicated_code,"Tue, 1 Oct 2013 09:52:39 +0000","Fri, 20 Nov 2015 11:53:36 +0000","Thu, 17 Oct 2013 17:36:36 +0000",1410237,"Looks good Correct the comment pls. Seems copy paste set_scan_attributes, set_get_attributes, set_put_attributes seems same code repeating. We can make this one def?",0.339,0.2056666667,positive
hbase,9689,comment_9,"There is no way to have a backwards compatible put command that does not take an attributes hash, just a timestamp? It's fine if timestamp has to be specified as part of the attribute hash if one is present.",design_debt,non-optimal_design,"Tue, 1 Oct 2013 09:52:39 +0000","Fri, 20 Nov 2015 11:53:36 +0000","Thu, 17 Oct 2013 17:36:36 +0000",1410237,"So this change is a new behaviour for specifying puts. There is no way to have a backwards compatible put command that does not take an attributes hash, just a timestamp? It's fine if timestamp has to be specified as part of the attribute hash if one is present.",-0.1125,-0.075,positive
hbase,9689,comment_6,"+1 patch looks ok, would be good to have coverage in TestShell for this",test_debt,low_coverage,"Tue, 1 Oct 2013 09:52:39 +0000","Fri, 20 Nov 2015 11:53:36 +0000","Thu, 17 Oct 2013 17:36:36 +0000",1410237,"+1 patch looks ok, would be good to have coverage in TestShell for this",0.763,0.763,positive
hbase,9742,comment_1,"Some of the changes in HBASE-5732 aren't documented. In the configuration section, I read through the code to see what the changes are now.",documentation_debt,low_quality_documentation,"Thu, 10 Oct 2013 17:47:49 +0000","Fri, 20 Nov 2015 11:53:29 +0000","Fri, 11 Oct 2013 04:15:29 +0000",37660,"Some of the changes in HBASE-5732 aren't documented. In the configuration section, I read through the code to see what the changes are now.",0.0,0.0,neutral
hbase,9806,comment_0,"Here's an initial version, based heavily on HFilerPerfEval code. Would be nice to reduce duplication between the two. Sample output, for reference.",code_debt,duplicated_code,"Fri, 18 Oct 2013 22:13:17 +0000","Thu, 16 Jun 2022 18:22:32 +0000","Thu, 16 Jun 2022 18:22:30 +0000",273269353,"Here's an initial version, based heavily on HFilerPerfEval code. Would be nice to reduce duplication between the two. Sample output, for reference.",0.275,0.275,neutral
hbase,9806,comment_3,"Updated patch after running with a couple different configurations. Hopefully I'll get to run on a larger memory machine tomorrow. I have a couple more things to expose as configuration and I'd like to get it to the point where it can automatically determine the amount of data to write based on different cache:data ratios. I also tried running multiple concurrent the cache consumers, but saw warnings about double-storing blocks. Will investigate that further before adding.  how much RAM do your test machines have?",code_debt,low_quality_code,"Fri, 18 Oct 2013 22:13:17 +0000","Thu, 16 Jun 2022 18:22:32 +0000","Thu, 16 Jun 2022 18:22:30 +0000",273269353,"Updated patch after running with a couple different configurations. Hopefully I'll get to run on a larger memory machine tomorrow. I have a couple more things to expose as configuration and I'd like to get it to the point where it can automatically determine the amount of data to write based on different cache:data ratios. I also tried running multiple concurrent the cache consumers, but saw warnings about double-storing blocks. Will investigate that further before adding. jmspaggi how much RAM do your test machines have?",0.1125,0.1125,neutral
hbase,9806,comment_12,"Sorry for the delays,  -- Strata and all that. I wanted to add a kind of self-tuning to the patch, give it the ability to write records until it fills up the cache and no more. No luck as of yet, which means determining how many rows to write requires a little guess-work and checking the logs. I've run the test for 3g (-r 1700000) and 20g (-r 12000000) heaps using the attached configs and 100 iterations. Attached also are the charts I generated from the logs. I tried 1000 iterations to see if anything happens over a longer interval but nothing exciting. I'm also updating my log-parsing script to overly the GC events. More to follow. On your fancy rig, can you run for maybe 8g, 16g, 20g, and 28g? You'll have to apply one of these conf patches and then adjust the heap size yourself. You'll also need to play with the -r param to work out how many rows you can fit into the cache w.o eviction (please let me know what you end up using!) -i 100 should be a good starting point, but if you have time I'd take logs from 500 or 1000. Thank you much!",design_debt,non-optimal_design,"Fri, 18 Oct 2013 22:13:17 +0000","Thu, 16 Jun 2022 18:22:32 +0000","Thu, 16 Jun 2022 18:22:30 +0000",273269353,"Sorry for the delays, jmspaggi  Strata and all that. I wanted to add a kind of self-tuning to the patch, give it the ability to write records until it fills up the cache and no more. No luck as of yet, which means determining how many rows to write requires a little guess-work and checking the logs. I've run the test for 3g (-r 1700000) and 20g (-r 12000000) heaps using the attached configs and 100 iterations. Attached also are the charts I generated from the logs. I tried 1000 iterations to see if anything happens over a longer interval but nothing exciting. I'm also updating my log-parsing script to overly the GC events. More to follow. On your fancy rig, can you run for maybe 8g, 16g, 20g, and 28g? You'll have to apply one of these conf patches and then adjust the heap size yourself. You'll also need to play with the -r param to work out how many rows you can fit into the cache w.o eviction (please let me know what you end up using!) -i 100 should be a good starting point, but if you have time I'd take logs from 500 or 1000. Thank you much!",0.1285961538,0.1285961538,neutral
hbase,9806,comment_7,"Any chance one of you has the configs laying around from the benchmarks run on the slabcache [announcement I'm attempting to reproduce and slabcache looks pretty unstable. I wonder if I'm exercising some edge-case based on my configuration or if there's some bit-rot happened. Specifically, I'm looking for values for HBASE_HEAPSIZE, and anything else that was using a non-default value. Thanks.",design_debt,non-optimal_design,"Fri, 18 Oct 2013 22:13:17 +0000","Thu, 16 Jun 2022 18:22:32 +0000","Thu, 16 Jun 2022 18:22:30 +0000",273269353,"tlipcon li jdcryans Any chance one of you has the configs laying around from the benchmarks run on the slabcache announcement post? I'm attempting to reproduce and slabcache looks pretty unstable. I wonder if I'm exercising some edge-case based on my configuration or if there's some bit-rot happened. Specifically, I'm looking for values for HBASE_HEAPSIZE, -XX:MaxDirectMemorySize, hfile.block.cache.size, hbase.regionserver.global.memstore.upperLimit, hbase.regionserver.global.memstore.lowerLimit, hbase.offheapcache.percentage, hbase.offheapcache.slab.proportions, and anything else that was using a non-default value. Thanks.",0.4499166667,0.08728571429,neutral
hbase,9893,comment_5,"Excellent catch. Attached is a patch that also includes updated test cases. Please let me know if you have other value permutations you'd like to see tested. It'd be nice to have a more thorough test suite around this code, a la the suite Orderly has. From the commit message",test_debt,lack_of_tests,"Tue, 5 Nov 2013 08:28:54 +0000","Mon, 16 Dec 2013 18:46:46 +0000","Wed, 20 Nov 2013 02:55:44 +0000",1276010,"Excellent catch. Attached is a patch that also includes updated test cases. Please let me know if you have other value permutations you'd like to see tested. It'd be nice to have a more thorough test suite around this code, a la the suite Orderly has. From the commit message Correct an invalid assumption in remaining assertion code around OrderedBytes#decodeVarBlob. When an encoded value contains a 1-bit in its LSB position and the length of the encoded byte array is divisible by 7, the value remaining in variable t will be 0x80, resulting in the failed assertion coming out of the decoding loop. This patch preserves the assertion for the general case by resetting 't' at the conclusion of the 7-byte cycle.",0.4051,0.2804285714,positive
hbase,9901,comment_2,"The toString is ugly. Fix on commit: + return ""HTable{"" + ""connection="" + connection + "", tableName="" + tableName + '}'; Make it just: connection + "","" + tableName It will look like: Or Our logs are too profuse already -- they need paring. Let the above be the convention for tablename string. No need of having the '{' and the HTable preamble? If you do above, +1 on patch for branch and trunk",code_debt,low_quality_code,"Wed, 6 Nov 2013 08:50:52 +0000","Mon, 16 Dec 2013 18:46:55 +0000","Wed, 6 Nov 2013 18:32:18 +0000",34886,"The toString is ugly. Fix on commit: + return ""HTable {"" + ""connection="" + connection + "", tableName="" + tableName + '} '; Make it just: connection + "","" + tableName It will look like: hconnection-0x020234343,bigtable Or bigtable,hconnection-0x020234343 Our logs are too profuse already  they need paring. Let the above be the convention for tablename string. No need of having the '{' and the HTable preamble? If you do above, +1 on patch for branch and trunk",-0.13,-0.13,negative
hbase,9901,comment_1,"There is no javadoc in the patch, and the findbugs in hbase-client or hbase-servers seems unrelated...",documentation_debt,low_quality_documentation,"Wed, 6 Nov 2013 08:50:52 +0000","Mon, 16 Dec 2013 18:46:55 +0000","Wed, 6 Nov 2013 18:32:18 +0000",34886,"There is no javadoc in the patch, and the findbugs in hbase-client or hbase-servers seems unrelated...",0.0,0.0,negative
hbase,9950,comment_2,"not sure yet. Since there is no notion of row level data in hbase storage, I would have to create some special KVs that are stored for the row, which sounds very hacky.  Replication scope is defined at the CF level, so I don't think Ill be able to use it. I do need to plug in custom replication policy though if this is not a core feature. There are no observers for replication, are there?",design_debt,non-optimal_design,"Mon, 11 Nov 2013 22:26:36 +0000","Thu, 16 Jun 2022 18:32:26 +0000","Tue, 14 May 2019 00:38:34 +0000",173585518,"stack not sure yet. Since there is no notion of row level data in hbase storage, I would have to create some special KVs that are stored for the row, which sounds very hacky. apurtell Replication scope is defined at the CF level, so I don't think Ill be able to use it. I do need to plug in custom replication policy though if this is not a core feature. There are no observers for replication, are there?",0.03193333333,0.03193333333,negative
hbase,10074,comment_2,"I don't read docbook, so I cannot comment about the markup. However, the content is great! Here are some nits. Mind adding some JIRA references here? ""store file index to rise rising"" ? This sentence is confusing me. How about ""Hosting only 5 regions per RS will not be enough task splits for a mapreduce job, while 1000 regions will generate far too many map tasks."" In section {{<section +1",documentation_debt,low_quality_documentation,"Tue, 3 Dec 2013 22:47:13 +0000","Mon, 16 Dec 2013 18:46:36 +0000","Thu, 5 Dec 2013 23:22:28 +0000",174915,"I don't read docbook, so I cannot comment about the markup. However, the content is great! Here are some nits. <para>The master as is is allergic to tons of regions Mind adding some JIRA references here? tons of regions on a few RS can cause the store file index to rise raising heap usage and... ""store file index to rise rising"" ? This sentence is confusing me. Keeping 5 regions per RS would be too low for a job, whereas 1000 will generate too many maps. How about ""Hosting only 5 regions per RS will not be enough task splits for a mapreduce job, while 1000 regions will generate far too many map tasks."" In section <section xml:id=""ops.capacity.regions""><title>Determining region count and size</title> you suggest ""20-200 regions per RS"" but previously you said ""20-100"". +1",-0.023375,-0.01141666667,positive
hbase,10074,comment_4,+1 I see a few spelling issues and would maybe change some of the numbers and statements but can be done in another issue. This reorg is excellent. Thanks .,documentation_debt,low_quality_documentation,"Tue, 3 Dec 2013 22:47:13 +0000","Mon, 16 Dec 2013 18:46:36 +0000","Thu, 5 Dec 2013 23:22:28 +0000",174915,+1 I see a few spelling issues and would maybe change some of the numbers and statements but can be done in another issue. This reorg is excellent. Thanks sershe.,0.3833333333,0.3833333333,positive
hbase,10074,comment_6,"incorporated feedback, some spelling fixes and rephrases. I'd assume +1 stands, will commit in the afternoon",documentation_debt,low_quality_documentation,"Tue, 3 Dec 2013 22:47:13 +0000","Mon, 16 Dec 2013 18:46:36 +0000","Thu, 5 Dec 2013 23:22:28 +0000",174915,"incorporated feedback, some spelling fixes and rephrases. I'd assume +1 stands, will commit in the afternoon",0.1,0.1,positive
hbase,10115,comment_5,"Why not another coproc endpoint or improve the existing one? As long as it doesn't affect perf too much... we can already wrap (or replace) compaction scanner thru coproc. ScanQueryMatcher is what decides what KVs get skipped, so if a wrapping scanner could influence that (or even just be able to react to .match call), it should be almost sufficient. It has KV, it has the default verdict. The only problem is with seek to next something results, which can skip KVs. This will no longer be allowed if all deleted KVs are to be examined if I understand that code correctly, so coproc will have to also prevent matcher from doing that?",design_debt,non-optimal_design,"Mon, 9 Dec 2013 23:33:33 +0000","Fri, 17 Jun 2022 04:53:21 +0000","Tue, 29 Apr 2014 00:12:35 +0000",12098342,"Why not another coproc endpoint or improve the existing one? As long as it doesn't affect perf too much... we can already wrap (or replace) compaction scanner thru coproc. ScanQueryMatcher is what decides what KVs get skipped, so if a wrapping scanner could influence that (or even just be able to react to .match call), it should be almost sufficient. It has KV, it has the default verdict. The only problem is with seek to next something results, which can skip KVs. This will no longer be allowed if all deleted KVs are to be examined if I understand that code correctly, so coproc will have to also prevent matcher from doing that?",0.04471428571,0.04471428571,neutral
hbase,10213,comment_17,"the intention of this jira is good:-), but by examining the patch: the metric above only reflects the log read/parse rate, not the desired replicating data to peer cluster rate, since the read/parsed log files may contain many kvs from column-families with replication scope=0 which will be filtered out and removed from the entries list before the real replicating to peer cluster occurs... why not use currentSize, the size of all entries which will be really replicated to the peer cluster?",design_debt,non-optimal_design,"Fri, 20 Dec 2013 09:34:06 +0000","Sat, 21 Feb 2015 23:29:00 +0000","Tue, 7 Jan 2014 22:15:26 +0000",1600880,"However, it is not clear enough to know how many bytes replicating to peer cluster from these metrics. In production environment, it may be important to know the size of replicating data per second the intention of this jira is good, but by examining the patch: the metric above only reflects the log read/parse rate, not the desired replicating data to peer cluster rate, since the read/parsed log files may contain many kvs from column-families with replication scope=0 which will be filtered out and removed from the entries list before the real replicating to peer cluster occurs... why not use currentSize, the size of all entries which will be really replicated to the peer cluster?",0.2586666667,-0.1206666667,neutral
hbase,10213,comment_10,Thanks for the comment  and I should pay more attention on spelling.,documentation_debt,low_quality_documentation,"Fri, 20 Dec 2013 09:34:06 +0000","Sat, 21 Feb 2015 23:29:00 +0000","Tue, 7 Jan 2014 22:15:26 +0000",1600880,Thanks for the comment apurtell and yuzhihong@gmail.com. I should pay more attention on spelling.,0.1,0.1,neutral
hbase,10213,comment_8,"Committed to 0.98. Thanks Ted. I fixed a spelling error on commit. Attached is an addendum for trunk to match what went into 0.98. I committed this trivial change to trunk using CTR as r1554361.  and : We can close this after a decision on 0.96 and 0.94. Almost feel bad pinging you, should be no big deal.",documentation_debt,low_quality_documentation,"Fri, 20 Dec 2013 09:34:06 +0000","Sat, 21 Feb 2015 23:29:00 +0000","Tue, 7 Jan 2014 22:15:26 +0000",1600880,"Committed to 0.98. Thanks Ted. I fixed a spelling error on commit. Attached is an addendum for trunk to match what went into 0.98. I committed this trivial change to trunk using CTR as r1554361. stack and lhofhansl: We can close this after a decision on 0.96 and 0.94. Almost feel bad pinging you, should be no big deal.",0.1285714286,0.1285714286,neutral
hbase,10213,comment_9,"Thanks for the catch, Andy. It would be not so good if a public method has spelling mistake.",documentation_debt,low_quality_documentation,"Fri, 20 Dec 2013 09:34:06 +0000","Sat, 21 Feb 2015 23:29:00 +0000","Tue, 7 Jan 2014 22:15:26 +0000",1600880,"Thanks for the catch, Andy. It would be not so good if a public method has spelling mistake.",0.294,0.294,neutral
hbase,10702,comment_3,"You sure you want to use deleteColumn (vs. deleteColumns)? Delete.deleteColumn will only target the current latest version of that column for deletion (as determined by the time the delete arrives at the server). Not only is this very expensive (a Get on the server just to find the last ts), it also only targets the very latest version. (I mentioned before that the Delete API is confusing. This reminds me, maybe there's time to fix it before 1.0.)",design_debt,non-optimal_design,"Fri, 7 Mar 2014 22:16:14 +0000","Fri, 17 Jun 2022 05:10:24 +0000","Sat, 8 Mar 2014 02:15:01 +0000",14327,"You sure you want to use deleteColumn (vs. deleteColumns)? Delete.deleteColumn will only target the current latest version of that column for deletion (as determined by the time the delete arrives at the server). Not only is this very expensive (a Get on the server just to find the last ts), it also only targets the very latest version. (I mentioned before that the Delete API is confusing. This reminds me, maybe there's time to fix it before 1.0.)",0.02716666667,0.02716666667,neutral
hbase,10702,comment_8,"Now, why is it working in 0.96.1.1 with deleteColumn? I will try to write a testcase for that to validate that it's behaving as expected... Will close this one. Thanks!",test_debt,low_coverage,"Fri, 7 Mar 2014 22:16:14 +0000","Fri, 17 Jun 2022 05:10:24 +0000","Sat, 8 Mar 2014 02:15:01 +0000",14327,"Now, why is it working in 0.96.1.1 with deleteColumn? I will try to write a testcase for that to validate that it's behaving as expected... Will close this one. Thanks!",0.4,0.4,neutral
hbase,10864,comment_5,"Oh dear, i realized what the problem was, I plopped the wrong patch file in. The other one fixes something like 30 spelling mistakes. It may still be too minor",documentation_debt,low_quality_documentation,"Fri, 28 Mar 2014 17:42:56 +0000","Sat, 21 Feb 2015 23:38:18 +0000","Sat, 29 Mar 2014 03:40:27 +0000",35851,"Oh dear, i realized what the problem was, I plopped the wrong patch file in. The other one fixes something like 30 spelling mistakes. It may still be too minor",-0.05288888889,-0.05288888889,negative
hbase,10925,comment_7,"Looks good. This should be a configuration: + long maxRowSize = 10 * 1024 * 1024; On the below: + totalReadSize += kv.getRowLength() + + + + + if (totalReadSize + throw new size of row is :"" + maxRowSize + + "", but the row is bigger than that.""); + } ... it is a bit of a pain having to do the above. I suppose we should add a getSize to CellUtils. Could do that in another patch. Was also thinking what if the Cell is offheap but then above is probably fine still because while it is offheap in the server, in the client it may not be and a 10G rowsize is likely larger than it can swallow w/o OOME.",design_debt,non-optimal_design,"Mon, 7 Apr 2014 21:48:10 +0000","Tue, 21 Nov 2017 22:55:14 +0000","Wed, 30 Apr 2014 00:08:59 +0000",1909249,"Looks good. This should be a configuration: + long maxRowSize = 10 * 1024 * 1024; On the below: + totalReadSize += kv.getRowLength() + kv.getFamilyLength() + + kv.getQualifierLength() + kv.getValueLength(); + if (totalReadSize > maxRowSize) { + throw new RowTooBigException(""Max size of row is :"" + maxRowSize + + "", but the row is bigger than that.""); + } ... it is a bit of a pain having to do the above. I suppose we should add a getSize to CellUtils. Could do that in another patch. Was also thinking what if the Cell is offheap but then above is probably fine still because while it is offheap in the server, in the client it may not be and a 10G rowsize is likely larger than it can swallow w/o OOME.",0.05995238095,0.04196666667,positive
hbase,10968,comment_1,Thanks Matteo. Patch also removes an unused local variable.,code_debt,dead_code,"Sat, 12 Apr 2014 01:10:49 +0000","Sat, 21 Feb 2015 23:30:17 +0000","Sat, 12 Apr 2014 01:56:36 +0000",2747,Thanks Matteo. Patch also removes an unused local variable.,0.2,0.2,positive
hbase,11011,comment_3,"In this case is not bad, is an ""expected"" situation, where the RS died before moving the compacted file. Anyway, I think that all the loop can be removed, since is trying to lookup files in and if they are there are already loaded for sure.",code_debt,low_quality_code,"Thu, 17 Apr 2014 02:06:05 +0000","Sat, 21 Feb 2015 23:35:09 +0000","Fri, 18 Apr 2014 17:19:58 +0000",141233,"The fact that a file is missing seems pretty bad, yet it's at DEBUG. In this case is not bad, is an ""expected"" situation, where the RS died before moving the compacted file. Anyway, I think that all the ""for(compactionOutputs)"" loop can be removed, since is trying to lookup files in /table/region/family/ and if they are there are already loaded for sure.",-0.28125,-0.2763888889,neutral
hbase,11011,comment_4,"This might be, but the log message doesn't convey any of that. Taken out of context (so most users reading our logs), it just says a file is missing. I'm trusting you on this one :)",code_debt,low_quality_code,"Thu, 17 Apr 2014 02:06:05 +0000","Sat, 21 Feb 2015 23:35:09 +0000","Fri, 18 Apr 2014 17:19:58 +0000",141233,"In this case is not bad, is an ""expected"" situation, where the RS died before moving the compacted file. This might be, but the log message doesn't convey any of that. Taken out of context (so most users reading our logs), it just says a file is missing. since is trying to lookup files in /table/region/family/ and if they are there are already loaded for sure. I'm trusting you on this one",0.01666666667,-0.0925,neutral
hbase,11011,comment_6,"none of the above, the removed code was doing nothing (see the comment inside the function). The output files in the entry are simply not useful",code_debt,low_quality_code,"Thu, 17 Apr 2014 02:06:05 +0000","Sat, 21 Feb 2015 23:35:09 +0000","Fri, 18 Apr 2014 17:19:58 +0000",141233,"Does the changed code in completeCompactionMarker require a unit test? Or is there already one? none of the above, the removed code was doing nothing (see the comment inside the function). The output files in the entry are simply not useful",-0.25,-0.125,neutral
hbase,11053,comment_13,Javadoc fix. I missed this. Thanks Ted for the heads up.,documentation_debt,low_quality_documentation,"Wed, 23 Apr 2014 11:25:54 +0000","Sat, 21 Feb 2015 23:31:39 +0000","Sat, 26 Apr 2014 08:22:21 +0000",248187,Javadoc fix. I missed this. Thanks Ted for the heads up.,0.0,0.0,neutral
hbase,11129,comment_3,"Passing the scanner on the jobconf is a private API, now that the serialization details are private methods. This implementation detail should be isolated within a single job -- either it picks up the 0.X.Y hbase-server jar or it has 0.X.Z version, there's no mixing. We'd need to test it out, but I think making this change could be acceptable for a patch release. Looking at either is respected, or params are used. What I propose does away with the former. This way, these configs become part of the public API.",test_debt,lack_of_tests,"Wed, 7 May 2014 17:05:01 +0000","Fri, 17 Jun 2022 05:44:12 +0000","Mon, 23 Oct 2017 03:17:38 +0000",109246357,"Passing the scanner on the jobconf is a private API, now that the serialization details are private methods. This implementation detail should be isolated within a single job  either it picks up the 0.X.Y hbase-server jar or it has 0.X.Z version, there's no mixing. We'd need to test it out, but I think making this change could be acceptable for a patch release. Looking at TableInputFormat#setConf, either ""hbase.mapreduce.scan"" is respected, or ""hbase.mapreduce.scan.*"" params are used. What I propose does away with the former. This way, these configs become part of the public API.",0.166375,0.1023846154,neutral
hbase,11293,comment_10,"+1, would be good to have a test also",test_debt,lack_of_tests,"Tue, 3 Jun 2014 18:12:23 +0000","Fri, 17 Jun 2022 05:50:20 +0000","Sun, 12 Jun 2022 19:08:10 +0000",253241747,"+1, would be good to have a test also",0.776,0.776,positive
hbase,11352,comment_1,Expiration would be measured in days. Maybe use different unit for the above config ? There're long lines in the patch - line length should be 100 or shorter.,code_debt,low_quality_code,"Fri, 13 Jun 2014 22:35:40 +0000","Fri, 17 Jun 2022 05:58:29 +0000","Tue, 23 Feb 2016 17:38:39 +0000",53550179,Expiration would be measured in days. Maybe use different unit for the above config ? There're long lines in the patch - line length should be 100 or shorter.,0.0,0.0,neutral
hbase,11352,comment_2,Fixed the long log lines and updated the expiration time to hours.,code_debt,low_quality_code,"Fri, 13 Jun 2014 22:35:40 +0000","Fri, 17 Jun 2022 05:58:29 +0000","Tue, 23 Feb 2016 17:38:39 +0000",53550179,Fixed the long log lines and updated the expiration time to hours.,0.0,0.0,neutral
hbase,11511,comment_4,"Thanks Stack for taking a look. Good point. We writelock the updatesLock, then start the mvcc transaction, so it should be the case that sync() and sync(trx) should be the same thing I guess. But let me change it to be plain old sync() to be failure-proof. Indeed. The block above is a catch block for catching an ignoring any exception from writing the abort marker to WAL. The call to should still happen in the outer block, resulting in",design_debt,non-optimal_design,"Mon, 14 Jul 2014 18:14:43 +0000","Fri, 6 Apr 2018 17:51:19 +0000","Tue, 15 Jul 2014 21:52:15 +0000",99452,"Thanks Stack for taking a look. That is ok? What if an edit after the start flush edit was added? Good point. We writelock the updatesLock, then start the mvcc transaction, so it should be the case that sync() and sync(trx) should be the same thing I guess. But let me change it to be plain old sync() to be failure-proof. ... or are we still dealing with failure at this point? Indeed. The block above is a catch block for catching an ignoring any exception from writing the abort marker to WAL. The call to wal.abortCacheFlush() should still happen in the outer block, resulting in DroppedSnapshotException.",0.2618809524,0.2294242424,neutral
hbase,11575,comment_1,"Looked into it and attached a patch that fixed some documentation mismatches, changed the pseudo distributed mode setting a little so that we just start one instance. Additional regionservers can be started with the local regionserver sh script.",documentation_debt,low_quality_documentation,"Wed, 23 Jul 2014 00:18:13 +0000","Fri, 6 Apr 2018 17:50:52 +0000","Thu, 24 Jul 2014 17:12:20 +0000",147247,"Looked into it and attached a patch that fixed some documentation mismatches, changed the pseudo distributed mode setting a little so that we just start one instance. Additional regionservers can be started with the local regionserver sh script.",0.0,0.0,neutral
hbase,11621,comment_4,"Here was 0.98 test suite with patch - on the same host as the previous run: With patch, hbase-server module went from 66 min to 51 min.",code_debt,slow_algorithm,"Wed, 30 Jul 2014 22:36:24 +0000","Fri, 6 Apr 2018 17:51:13 +0000","Thu, 31 Jul 2014 16:46:45 +0000",65421,"Here was 0.98 test suite with patch - on the same host as the previous run: With patch, hbase-server module went from 66 min to 51 min.",0.0,0.0,neutral
hbase,11693,comment_2,"Right, I did only test this in threaded mode. I made several passes over the changes and believe all MR details are accounted for. I don't have access to a cluster but can test on a single node YARN setup. Will do that and report back.",test_debt,low_coverage,"Wed, 6 Aug 2014 22:37:16 +0000","Tue, 9 Sep 2014 04:19:26 +0000","Fri, 8 Aug 2014 00:52:35 +0000",94519,"Right, I did only test this in threaded mode. I made several passes over the changes and believe all MR details are accounted for. I don't have access to a cluster but can test on a single node YARN setup. Will do that and report back.",0.13175,0.13175,neutral
hbase,11935,comment_12,Turns out that this is not the full story. We're still leaking ZK Connection somewhere when the slave cluster's ZK ensemble is not up. Debugging...,code_debt,low_quality_code,"Wed, 10 Sep 2014 18:47:37 +0000","Fri, 17 Jun 2022 17:13:44 +0000","Fri, 13 Feb 2015 01:59:31 +0000",13417914,Turns out that this is not the full story. We're still leaking ZK Connection somewhere when the slave cluster's ZK ensemble is not up. Debugging...,-0.05316666667,-0.05316666667,neutral
hbase,11935,comment_3,Let's add some logging too... The queue failover if very quiet.,code_debt,low_quality_code,"Wed, 10 Sep 2014 18:47:37 +0000","Fri, 17 Jun 2022 17:13:44 +0000","Fri, 13 Feb 2015 01:59:31 +0000",13417914,Let's add some logging too... The queue failover if very quiet.,0.438,0.438,neutral
hbase,11935,comment_13,This one has held up to initial testing. Will post a trunk patch soon and report back with more actual cluster testing tomorrow. The core of the change is that we do schedule every single queue on a thread (there might be 1000's or even 100's of 1000's). Instead we schedule a thread per failed RS and handle all that RSs queue inside the same thread (by calling ReplicationSource's run() inline). While we looked at the code we all felt it is time to rethink the architecture and rewrite the replication source side from scratch. It has reached the point where incremental changes are no longer appropriate... But that is for another jira.,design_debt,non-optimal_design,"Wed, 10 Sep 2014 18:47:37 +0000","Fri, 17 Jun 2022 17:13:44 +0000","Fri, 13 Feb 2015 01:59:31 +0000",13417914,This one has held up to initial testing. Will post a trunk patch soon and report back with more actual cluster testing tomorrow. The core of the change is that we do schedule every single queue on a thread (there might be 1000's or even 100's of 1000's). Instead we schedule a thread per failed RS and handle all that RSs queue inside the same thread (by calling ReplicationSource's run() inline). While we looked at the code we all felt it is time to rethink the architecture and rewrite the replication source side from scratch. It has reached the point where incremental changes are no longer appropriate... But that is for another jira.,-0.1125,-0.1125,neutral
hbase,12030,comment_3,"looks ok to me, it will be nice having a test to cover the path when zk throws an exception.. maybe the easiest way there is mocking zk?",test_debt,low_coverage,"Fri, 19 Sep 2014 18:22:46 +0000","Fri, 17 Jun 2022 17:17:14 +0000","Thu, 25 Sep 2014 04:19:34 +0000",467808,"looks ok to me, it will be nice having a test to cover the path when zk throws an exception.. maybe the easiest way there is mocking zk?",0.3916666667,0.3916666667,positive
hbase,12115,comment_0,Removed unnecessary changes from the commit.,code_debt,dead_code,"Mon, 29 Sep 2014 22:36:24 +0000","Fri, 6 Apr 2018 17:54:29 +0000","Tue, 30 Sep 2014 20:35:08 +0000",79124,Removed unnecessary changes from the commit.,0.2,0.2,neutral
hbase,12238,comment_0,In standalone mode this is a little disorientating... it shows up frequently:,design_debt,non-optimal_design,"Mon, 13 Oct 2014 02:23:59 +0000","Fri, 6 Apr 2018 17:55:53 +0000","Thu, 30 Oct 2014 04:54:55 +0000",1477856,In standalone mode this is a little disorientating... it shows up frequently:,0.0,0.0,negative
hbase,12271,comment_6,"ExportSnapshot will skip files that are the same (either via checksum or via name+length comparison). We use this to cut down on transfer time. First we snapshot the table, then export that snapshot to the remote DFS. Next run, we instead start by copying the previously exported snapshot into the current snapshot's destination (all on the remote DFS). We then do the snapshot and export dance again, and it dutifully copies only the changed files. Our (fb) HDFS has hard links (last I heard upstream does not) so we can copy backups around pretty cheaply. In the future we may just throw everything into one directory and let the manifest be the decider of what files are involved, but in the mean time it's copy and differential.",design_debt,non-optimal_design,"Wed, 15 Oct 2014 16:59:00 +0000","Fri, 6 Apr 2018 17:54:40 +0000","Wed, 15 Oct 2014 17:46:45 +0000",2865,"ExportSnapshot will skip files that are the same (either via checksum or via name+length comparison). We use this to cut down on transfer time. First we snapshot the table, then export that snapshot to the remote DFS. Next run, we instead start by copying the previously exported snapshot into the current snapshot's destination (all on the remote DFS). We then do the snapshot and export dance again, and it dutifully copies only the changed files. Our (fb) HDFS has hard links (last I heard upstream does not) so we can copy backups around pretty cheaply. In the future we may just throw everything into one directory and let the manifest be the decider of what files are involved, but in the mean time it's copy and differential.",-0.03514285714,-0.03514285714,neutral
hbase,12293,comment_1,"tests should be at info level at the minimum, as in production: if not we will discover in test that we log too much (or worse triggers NPE or stuff like this). For the same reason, I prefer to use the debug level in tests, to be sure that I won't have surprises (NPE) if I try to use them. What I did in the past is reusing the info from the apache build (run time and logs), and looked at the both the log size and the log rate per test to prioritize the tests I was looking at. Then I was just improving the logs around these area.",design_debt,non-optimal_design,"Sun, 19 Oct 2014 19:53:26 +0000","Fri, 17 Jun 2022 17:48:21 +0000","Wed, 29 Oct 2014 18:05:26 +0000",857520,"tests should be at info level at the minimum, as in production: if not we will discover in production/integration test that we log too much (or worse triggers NPE or stuff like this). For the same reason, I prefer to use the debug level in tests, to be sure that I won't have surprises (NPE) if I try to use them. What I did in the past is reusing the info from the apache build (run time and logs), and looked at the both the log size and the log rate per test to prioritize the tests I was looking at. Then I was just improving the logs around these area.",0.2625,0.2625,neutral
hbase,12400,comment_1,"Thank you for taking this on . In this patch find See how it changes our example code to do the 'new' way. I was thinking of changing places where we have bits of code so there is a 1.0 version, the more prominent, and then the old way of doing it as it is now. I can do this one, np... might take you a good while more time than I since I've had my head in this a while now. Otherwise, ask more questions if not clear. When HBASE-12404 goes in, there will be more examples to pull from. Thanks.",documentation_debt,low_quality_documentation,"Fri, 31 Oct 2014 21:19:58 +0000","Fri, 6 Apr 2018 17:54:34 +0000","Wed, 26 Nov 2014 17:31:28 +0000",2232690,"Thank you for taking this on syuanjiang. In this patch https://issues.apache.org/jira/secure/attachment/12683500/12404v20.txt, find client/package-info.java See how it changes our example code to do the 'new' way. I was thinking of changing places where we have bits of code so there is a 1.0 version, the more prominent, and then the old way of doing it as it is now. I can do this one, np... might take you a good while more time than I since I've had my head in this a while now. Otherwise, ask more questions if not clear. When HBASE-12404 goes in, there will be more examples to pull from. Thanks.",0.3125714286,0.2735,positive
hbase,12673,comment_6,Thanks for the pointer in HMobStore @ 312. The retry there should address the situation I was concerned about. Looking at that code again one other concern comes up -- do you know if line 319 in there will throw another exception if we got the FNFE on line 313? (we'd have an open file instance that got moved  not sure what would happen on close on line 319). Can we change it so that we capture the other exceptions that could be caught there [1] and then try the next location? HFileLink essentially makes this file redirection mechanism transparent and would potentially make the code easier to follow. I'd prefer it if we could use that code so if we find other cases we can just fix it in one centralized place. [1],design_debt,non-optimal_design,"Thu, 11 Dec 2014 03:41:47 +0000","Fri, 17 Jun 2022 05:56:42 +0000","Tue, 3 Mar 2015 02:37:17 +0000",7080930,Thanks for the pointer in HMobStore @ 312. The retry there should address the situation I was concerned about. Looking at that code again one other concern comes up  do you know if line 319 in there will throw another exception if we got the FNFE on line 313? (we'd have an open file instance that got moved  not sure what would happen on close on line 319). Can we change it so that we capture the other exceptions that could be caught there [1] and then try the next location? HFileLink essentially makes this file redirection mechanism transparent and would potentially make the code easier to follow. I'd prefer it if we could use that code so if we find other cases we can just fix it in one centralized place. [1] https://github.com/apache/hbase/blob/hbase-11339/hbase-server/src/main/java/org/apache/hadoop/hbase/io/FileLink.java#L124,0.0,0.0,neutral
hbase,12673,comment_9,"Ok, my main ask is to use the wrapping that the FileLink/HFileLink provides when a reader to the file is opened. You don't need to use the funny encoded name sine we aren't in a situation where we have a placeholder file present. We are in a place where we open a file and it could move to the archive. I spent a little bit of time in there and between the StoreFile, StoreFileInfo, Reference and HFileLink it is a bit messy. I'm planning to spend a little bit of time to refactor/clean things up in there so we have only have one mechanism to use. I think the short term solution is to add the other exception checks -- e.g. handle more than just the FNFE and catch these two other exception cases.",design_debt,non-optimal_design,"Thu, 11 Dec 2014 03:41:47 +0000","Fri, 17 Jun 2022 05:56:42 +0000","Tue, 3 Mar 2015 02:37:17 +0000",7080930,"Ok, my main ask is to use the wrapping that the FileLink/HFileLink provides when a reader to the file is opened. You don't need to use the funny encoded name sine we aren't in a situation where we have a placeholder file present. We are in a place where we open a file and it could move to the archive. I spent a little bit of time in there and between the StoreFile, StoreFileInfo, Reference and HFileLink it is a bit messy. I'm planning to spend a little bit of time to refactor/clean things up in there so we have only have one mechanism to use. I think the short term solution is to add the other exception checks  e.g. handle more than just the FNFE and catch these two other exception cases.",0.008333333333,0.008333333333,neutral
hbase,12673,comment_4,"Hi , I don't think this unit test covers the case I'm concerned about. This unit test covers a fairly coarse grained happening -- a read is happening and inbetween read operations a table delete happens. What I'm concerned about is finer grained -- Let's say I'm reading a from a snapshot mob file which is pointing to the mob in the original dir. While this happens (while still in the middle of the read operation) a table deletion on the original table happens which move the original mob file to the archive. The read operation may fail (can't find more data from the file). With the HFilelink, we'd intercept that exception and then point the read to the moved location if the file ends up in the correct place. With the current code, I believe we get an IO exception and fail to return the proper data, or return an error. I'm in the process of crafting a rig that will constantly exercise these concurrently and hopefully will be able to produce a stack trace when this fails in a day or two.",test_debt,low_coverage,"Thu, 11 Dec 2014 03:41:47 +0000","Fri, 17 Jun 2022 05:56:42 +0000","Tue, 3 Mar 2015 02:37:17 +0000",7080930,"Hi jiajia, I don't think this unit test covers the case I'm concerned about. This unit test covers a fairly coarse grained happening  a read is happening and inbetween read operations a table delete happens. What I'm concerned about is finer grained  Let's say I'm reading a from a snapshot mob file which is pointing to the mob in the original dir. While this happens (while still in the middle of the read operation) a table deletion on the original table happens which move the original mob file to the archive. The read operation may fail (can't find more data from the file). With the HFilelink, we'd intercept that exception and then point the read to the moved location if the file ends up in the correct place. With the current code, I believe we get an IO exception and fail to return the proper data, or return an error. I'm in the process of crafting a rig that will constantly exercise these concurrently and hopefully will be able to produce a stack trace when this fails in a day or two.",0.05429166667,0.05429166667,neutral
hbase,12729,comment_1,"Hey , in the trunk patch, in RsRpcServices, this hunk: This is applying the mutations twice. I think you meant to remove the second invocation of mutateRows() right?",code_debt,duplicated_code,"Fri, 19 Dec 2014 19:49:36 +0000","Fri, 20 Nov 2015 11:54:27 +0000","Tue, 20 Jan 2015 01:07:09 +0000",2697453,"Hey jesse_yates, in the trunk patch, in RsRpcServices, this hunk: This is applying the mutations twice. I think you meant to remove the second invocation of mutateRows() right?",0.2635,0.2635,neutral
hbase,12729,comment_0,This is mildly complicated by the fact we can't change the Public annotated interface HConnection in 0.98. Hacking around with alternatives to find something reasonable.,design_debt,non-optimal_design,"Fri, 19 Dec 2014 19:49:36 +0000","Fri, 20 Nov 2015 11:54:27 +0000","Tue, 20 Jan 2015 01:07:09 +0000",2697453,This is mildly complicated by the fact we can't change the Public annotated interface HConnection in 0.98. Hacking around with alternatives to find something reasonable.,0.271,0.271,neutral
hbase,12729,comment_9,"Did a 10 minute looksee, lgtm. Spelling Nit: Is the synchronized needed here: Could get the ServerStatitics, if null create one and then use putIfAbsent. In the race case we'd create the object in vain, but we'd save the synchronized. Is this even on the hot path?",documentation_debt,low_quality_documentation,"Fri, 19 Dec 2014 19:49:36 +0000","Fri, 20 Nov 2015 11:54:27 +0000","Tue, 20 Jan 2015 01:07:09 +0000",2697453,"Did a 10 minute looksee, lgtm. Spelling Nit: Is the synchronized needed here: Could get the ServerStatitics, if null create one and then use putIfAbsent. In the race case we'd create the object in vain, but we'd save the synchronized. Is this even on the hot path?",-0.02025,-0.02025,neutral
hbase,12749,comment_7,"check out HBASE-12332. We don't have a FileStatus to attach to in that particular case initially. In the snapshot case we have the pattern file and get a fileStatus, and in the replicas case we have a file status as well[1] The fs was needed for (and and Probably can just as easily as of a FS in Didn't tackle those pieces yet (they also seem more tightly coupled than ideal) [1]",architecture_debt,violation_of_modularity,"Tue, 23 Dec 2014 03:01:48 +0000","Wed, 3 Jun 2015 15:29:07 +0000","Wed, 24 Dec 2014 13:09:30 +0000",122862,"the only other concern is the removed FileStatus from the StoreInfo and the addition of the FileSystem, but I still have to figure out why is that necessary. check out HBASE-12332. We don't have a FileStatus to attach to in that particular case initially. In the snapshot case we have the pattern file and get a fileStatus, and in the replicas case we have a file status as well[1] The fs was needed for StoreFileInfo#getFileStatus (and getModificaitonTime) and StoreFile.Reader#open. Probably can just as easily as of a FS in StoreFileInfo#getFileStatus. Didn't tackle those pieces yet (they also seem more tightly coupled than ideal) [1] https://github.com/apache/hbase/blob/master/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java#L186",0.2103333333,0.0745,neutral
hbase,12749,comment_12,"v3 tackles the javadoc, fingbugs, checkstyle and line length issues.",code_debt,low_quality_code,"Tue, 23 Dec 2014 03:01:48 +0000","Wed, 3 Jun 2015 15:29:07 +0000","Wed, 24 Dec 2014 13:09:30 +0000",122862,"v3 tackles the javadoc, fingbugs, checkstyle and line length issues.",0.0,0.0,neutral
hbase,12833,comment_13,"On closer inspection, it looks like SecurityAdmin and need updated with this new style as well. ReplicationAdmin appears to manage it's own connection by design, though the rational eludes me.",code_debt,low_quality_code,"Fri, 9 Jan 2015 20:40:05 +0000","Fri, 6 Apr 2018 17:55:48 +0000","Fri, 16 Jan 2015 19:55:32 +0000",602127,"On closer inspection, it looks like SecurityAdmin and VisibilityLabelsAdmin need updated with this new style as well. ReplicationAdmin appears to manage it's own connection by design, though the rational eludes me.",0.6125,0.6125,neutral
hbase,12833,comment_11,"A couple of comments on this: 1) the interface changes and the managed/unmanaged connection issues are separate concerns. 2) On the mailing list you said: ""Yes, the concept of connection caching, aka, managed connections are going away."" If managed connections are going away, then let's make them go away across the board. If they're staying, then let's keep them and rework the interfaces to support managed connections as a first class citizen. I only see disadvantages for inconsistency in this aspect. That said, this can totally be something that I'm missing. Do you see any advantages to keeping a mix of managed and unmanaged connections around?",design_debt,non-optimal_design,"Fri, 9 Jan 2015 20:40:05 +0000","Fri, 6 Apr 2018 17:55:48 +0000","Fri, 16 Jan 2015 19:55:32 +0000",602127,"A couple of comments on this: 1) the interface changes and the managed/unmanaged connection issues are separate concerns. 2) On the mailing list you said: ""Yes, the concept of connection caching, aka, managed connections are going away."" If managed connections are going away, then let's make them go away across the board. If they're staying, then let's keep them and rework the interfaces to support managed connections as a first class citizen. I only see disadvantages for inconsistency in this aspect. That said, this can totally be something that I'm missing. Do you see any advantages to keeping a mix of managed and unmanaged connections around?",0.1821428571,0.1821428571,neutral
hbase,12833,comment_16,"I guess it could be, but won't connections still be leaked when these classes are used? All this shows is our test usage of this part of the client is not as through as others. Whatever you say .",test_debt,low_coverage,"Fri, 9 Jan 2015 20:40:05 +0000","Fri, 6 Apr 2018 17:55:48 +0000","Fri, 16 Jan 2015 19:55:32 +0000",602127,"On closer inspection, it looks like SecurityAdmin and VisibilityLabelsAdmin need updated with this new style as well. That is a different jira right? I guess it could be, but won't connections still be leaked when these classes are used? All this shows is our test usage of this part of the client is not as through as others. Whatever you say enis.",0.0135,0.2397,neutral
hbase,12888,comment_2,"For consistency sake, it should also print this number of regions for meta. Right now that's fixed at 1.",code_debt,low_quality_code,"Tue, 20 Jan 2015 21:25:58 +0000","Fri, 17 Jun 2022 18:57:04 +0000","Fri, 2 Mar 2018 02:26:23 +0000",98168425,"For consistency sake, it should also print this number of regions for meta. Right now that's fixed at 1.",0.2635,0.2635,neutral
hbase,13184,comment_1,"HBASE-10513 added user documentation for Phase 1, but the book still does not contain changes reflecting Phase 2. I have written up something already, but did not get around to get them into asciidoc yet. I'll create a subtask in HBASE-10070 for that. Do you have any suggestions for how to organize it better ?",documentation_debt,outdated_documentation,"Tue, 10 Mar 2015 01:42:28 +0000","Thu, 23 Jun 2022 20:31:23 +0000","Wed, 29 Apr 2015 14:34:05 +0000",4366297,"I think the whole region replica area could use a rewrite and reorganization. HBASE-10513 added user documentation for Phase 1, but the book still does not contain changes reflecting Phase 2. I have written up something already, but did not get around to get them into asciidoc yet. I'll create a subtask in HBASE-10070 for that. Do you have any suggestions for how to organize it better ?",0.19525,0.1562,negative
hbase,13341,comment_4,"Oops, disregard the above. (Wrong JIRA). Only two things are nits: 1. Put a period at the end of the usage message addition to be consistent with the other lines. 2. Change the dereference from {{""$ALL"" != ""true""}} to {{""$\{ALL}"" != ""true}}, again, just to be consistent with the rest of the script. +1!",code_debt,low_quality_code,"Thu, 26 Mar 2015 17:06:45 +0000","Fri, 24 Jun 2022 17:41:45 +0000","Thu, 2 Apr 2015 04:59:37 +0000",561172,"Oops, disregard the above. (Wrong JIRA). Only two things are nits: 1. Put a period at the end of the usage message addition to be consistent with the other lines. 2. Change the dereference from ""$ALL"" != ""true"" to ""${ALL}"" != ""true, again, just to be consistent with the rest of the script. +1!",0.03222222222,0.03222222222,neutral
hbase,13395,comment_5,It is deprecated in 1.0 I think. So we can remove it.,code_debt,dead_code,"Fri, 3 Apr 2015 09:37:09 +0000","Thu, 16 Jun 2022 18:18:32 +0000","Sat, 25 Mar 2017 16:09:04 +0000",62404315,It is deprecated in 1.0 I think. So we can remove it.,0.0,0.0,neutral
hbase,13395,comment_19,please add a release note that lets downstream folks know what has happened and what they should do to account for it. This will be particularly important for folks coming from 0.98.,documentation_debt,outdated_documentation,"Fri, 3 Apr 2015 09:37:09 +0000","Thu, 16 Jun 2022 18:18:32 +0000","Sat, 25 Mar 2017 16:09:04 +0000",62404315,please add a release note that lets downstream folks know what has happened and what they should do to account for it. This will be particularly important for folks coming from 0.98.,0.3,0.3,neutral
hbase,13528,comment_0,"I think this line is also redundant? The if selectNow is false, then we will not execute throttleCompaction then 'size' is useless Thanks.",code_debt,low_quality_code,"Wed, 22 Apr 2015 01:19:47 +0000","Sat, 4 Jul 2015 12:30:36 +0000","Fri, 24 Apr 2015 09:24:37 +0000",201890,"I think this line is also redundant? The if selectNow is false, then we will not execute throttleCompaction then 'size' is useless Thanks.",-0.15,-0.03333333333,negative
hbase,13528,comment_1,"Yes, it's redundant, just like this is OK?",code_debt,low_quality_code,"Wed, 22 Apr 2015 01:19:47 +0000","Sat, 4 Jul 2015 12:30:36 +0000","Fri, 24 Apr 2015 09:24:37 +0000",201890,"Yes, it's redundant, just like this is OK?",0.475,0.475,negative
hbase,13582,comment_0,I think I remember seeing some references to the old package (org.htrace) instead of the one for the ASF project (org.apache.htrace) in the book which could be updated as well.,documentation_debt,outdated_documentation,"Tue, 28 Apr 2015 15:39:49 +0000","Fri, 24 Jun 2022 18:23:50 +0000","Tue, 19 May 2015 23:06:06 +0000",1841177,I think I remember seeing some references to the old package (org.htrace) instead of the one for the ASF project (org.apache.htrace) in the book which could be updated as well.,0.15775,0.15775,neutral
hbase,13582,comment_1,This patch fixes all the issues I could find with the documentation. It also includes a few Asciidoc things and typos etc. so I changed the JIRA title slightly,documentation_debt,low_quality_documentation,"Tue, 28 Apr 2015 15:39:49 +0000","Fri, 24 Jun 2022 18:23:50 +0000","Tue, 19 May 2015 23:06:06 +0000",1841177,This patch fixes all the issues I could find with the documentation. It also includes a few Asciidoc things and typos etc. so I changed the JIRA title slightly,0.0,0.0,neutral
hbase,13629,comment_3,"Currently in the description of this issue the before and after examples are the same text. :-) More typo-ing, whee. On the parent I remarked we should figure out how to test the bin scripts. Or convert them all to Java utilities and test those. Let's not make that a requirement to get this fix in though.",test_debt,low_coverage,"Wed, 6 May 2015 05:19:16 +0000","Fri, 24 Jun 2022 18:45:02 +0000","Wed, 6 May 2015 05:33:09 +0000",833,"Currently in the description of this issue the before and after examples are the same text. More typo-ing, whee. On the parent I remarked we should figure out how to test the bin scripts. Or convert them all to Java utilities and test those. Let's not make that a requirement to get this fix in though.",0.08,0.0,neutral
hbase,13776,comment_8,lgtm : Is it possible to add a unit test so that there is no regression in the future ?,test_debt,lack_of_tests,"Tue, 26 May 2015 04:05:42 +0000","Fri, 18 Dec 2015 05:41:33 +0000","Fri, 29 May 2015 18:38:15 +0000",311553,lgtm byh0831: Is it possible to add a unit test so that there is no regression in the future ?,0.0,0.0,neutral
hbase,13776,comment_9,"Can we add the wrong values of min and max versions configured, in the message? Yes pls add a test to cover this scenario. Thanks.",test_debt,lack_of_tests,"Tue, 26 May 2015 04:05:42 +0000","Fri, 18 Dec 2015 05:41:33 +0000","Fri, 29 May 2015 18:38:15 +0000",311553,"Can we add the wrong values of min and max versions configured, in the message? Yes pls add a test to cover this scenario. Thanks.",0.1166666667,0.1166666667,neutral
hbase,13799,comment_0,Fixup of javadoc on Scan class.,documentation_debt,low_quality_documentation,"Thu, 28 May 2015 23:17:02 +0000","Mon, 31 Aug 2015 22:39:33 +0000","Fri, 29 May 2015 18:54:02 +0000",70620,Fixup of javadoc on Scan class.,0.0,0.0,neutral
hbase,13871,comment_3,"Do we have to have a class named FirstOnRowFakeCell at top level of our class hierarchy? Is it only used in CellUtil? Could it be an inner class of it? Any reason for this change? 5252 this.isScan = scan.isGetScan() ? -1 : 0; 5252 this.isScan = scan.isGetScan() ? 1 : 0; Otherwise, seems good.",architecture_debt,violation_of_modularity,"Tue, 9 Jun 2015 06:25:08 +0000","Fri, 17 Jun 2022 05:25:11 +0000","Wed, 10 Jun 2015 13:12:06 +0000",110818,"Do we have to have a class named FirstOnRowFakeCell at top level of our class hierarchy? Is it only used in CellUtil? Could it be an inner class of it? Any reason for this change? 5252 this.isScan = scan.isGetScan() ? -1 : 0; 5252 this.isScan = scan.isGetScan() ? 1 : 0; Otherwise, seems good.",0.1069090909,0.1069090909,neutral
hbase,13871,comment_4,Yes this class need not be top level.. Let me see how we can make it inner. Yes this change is intended. Pls see the compare in isStopROw is changed. So we need this change.,architecture_debt,violation_of_modularity,"Tue, 9 Jun 2015 06:25:08 +0000","Fri, 17 Jun 2022 05:25:11 +0000","Wed, 10 Jun 2015 13:12:06 +0000",110818,Yes this class need not be top level.. Let me see how we can make it inner. 5252 this.isScan = scan.isGetScan() ? -1 : 0; 5252 this.isScan = scan.isGetScan() ? 1 : 0; Yes this change is intended. Pls see the compare in isStopROw is changed. So we need this change.,0.1125,0.045,neutral
hbase,13871,comment_6,"I will change it in next patch. BTW , we might add some more similar classes in future. This is byte[] backed fake Cell impl and we will have a BB backed one also. Also LastOnRowFakeCell will come in.. JFYI. I will keep this class(es) private audience only even if CellUtil is public exposed. Thanks Stack.",architecture_debt,violation_of_modularity,"Tue, 9 Jun 2015 06:25:08 +0000","Fri, 17 Jun 2022 05:25:11 +0000","Wed, 10 Jun 2015 13:12:06 +0000",110818,"Do we have to have a class named FirstOnRowFakeCell at top level of our class hierarchy? Is it only used in CellUtil? Could it be an inner class of it? I will change it in next patch. BTW , we might add some more similar classes in future. This is byte[] backed fake Cell impl and we will have a BB backed one also. Also LastOnRowFakeCell will come in.. JFYI. I will keep this class(es) private audience only even if CellUtil is public exposed. Thanks Stack.",0.02544444444,0.06140740741,neutral
hbase,13871,comment_1,"LGTM. nit, createFirstOnRow can have a small doc.",documentation_debt,outdated_documentation,"Tue, 9 Jun 2015 06:25:08 +0000","Fri, 17 Jun 2022 05:25:11 +0000","Wed, 10 Jun 2015 13:12:06 +0000",110818,"LGTM. nit, createFirstOnRow can have a small doc.",0.0,0.0,neutral
hbase,13905,comment_9,"Test is still in the flakey category, occasionally timing out with this stack",test_debt,flaky_test,"Mon, 15 Jun 2015 17:09:18 +0000","Mon, 31 Aug 2015 22:39:39 +0000","Tue, 16 Jun 2015 00:42:25 +0000",27187,"Test is still in the flakey category, occasionally timing out with this stack",0.281,0.281,negative
hbase,13924,comment_0,Should we fix the doc to remove coprocessors? Or should we fix the code to use this config value to load jars for coprocessors?,documentation_debt,low_quality_documentation,"Wed, 17 Jun 2015 12:58:31 +0000","Fri, 24 Jun 2022 19:24:29 +0000","Mon, 10 Aug 2015 04:08:38 +0000",4633807,Should we fix the doc to remove coprocessors? Or should we fix the code to use this config value to load jars for coprocessors?,0.0,0.0,neutral
hbase,13924,comment_3,HBASE-13867 also targets to improve coprocessor documentation.,documentation_debt,low_quality_documentation,"Wed, 17 Jun 2015 12:58:31 +0000","Fri, 24 Jun 2022 19:24:29 +0000","Mon, 10 Aug 2015 04:08:38 +0000",4633807,HBASE-13867 also targets to improve coprocessor documentation.,0.4,0.4,neutral
hbase,13928,comment_1,"is right, the added key is wrong, and never used. It is missing the {{.bucket.}} as mentioned and should be reading Only then an operator can set the sizes. The wrong property is misleading at most, but needs fixing anyways.",code_debt,low_quality_code,"Wed, 17 Jun 2015 23:08:35 +0000","Fri, 24 Jun 2022 19:24:13 +0000","Mon, 11 Jul 2016 15:08:08 +0000",33667173,"gsbiju is right, the added hbase-default.xml key is wrong, and never used. It is missing the .bucket. as mentioned and should be reading hbase.bucketcache.bucket.sizes. Only then an operator can set the sizes. The wrong property is misleading at most, but needs fixing anyways.",-0.1282,-0.07615,negative
hbase,13942,comment_0,It is becoming increasingly difficult with having 256 threads/cluster. Is it not possible to reduce this? We are afraid the same issue might prop up in case of drastically reducing the threads.,design_debt,non-optimal_design,"Mon, 22 Jun 2015 05:29:08 +0000","Fri, 24 Jun 2022 19:23:12 +0000","Mon, 13 Feb 2017 19:25:27 +0000",52062979,It is becoming increasingly difficult with having 256 threads/cluster. Is it not possible to reduce this? We are afraid the same issue might prop up in case of drastically reducing the threads.,-0.0435,-0.0435,negative
hbase,13973,comment_2,Some nits: 1. You should talk about the fact the storefile refresher is not needed when Async WAL replication is ON. Basically throw some color on this to make it clear as to when to use what. 2. There is a copy-paste issue in the section on configuration - the description of talks about meta replication which it shouldn't..,documentation_debt,low_quality_documentation,"Thu, 25 Jun 2015 22:54:28 +0000","Fri, 17 Jun 2022 04:50:43 +0000","Fri, 26 Jun 2015 22:31:54 +0000",85046,Some nits: 1. You should talk about the fact the storefile refresher is not needed when Async WAL replication is ON. Basically throw some color on this to make it clear as to when to use what. 2. There is a copy-paste issue in the section on configuration - the description of hbase.regionserver.storefile.refresh.period talks about meta replication which it shouldn't..,0.1,0.05555555556,neutral
hbase,14161,comment_4,"Ok. The tests in hbase-spark are run as regular unit tests now after HBASE-17574. We may need to write new integration tests for the spark module. But until that is available, there is real IT yet.",test_debt,lack_of_tests,"Tue, 28 Jul 2015 17:23:42 +0000","Sat, 28 Sep 2024 02:54:23 +0000","Mon, 17 Jul 2017 15:28:34 +0000",62201092,"Ok. The tests in hbase-spark are run as regular unit tests now after HBASE-17574. We may need to write new integration tests for the spark module. But until that is available, there is real IT yet.",0.2875,0.2875,neutral
hbase,14162,comment_5,Using the variable thrift.version instead of hard coding version 0.9.2 in v3.,code_debt,low_quality_code,"Tue, 28 Jul 2015 17:39:19 +0000","Fri, 1 Jul 2022 20:45:51 +0000","Fri, 31 Jul 2015 22:06:36 +0000",275237,Using the variable thrift.version instead of hard coding version 0.9.2 in v3.,-0.1,-0.1,neutral
hbase,14162,comment_7,-1 on v3. the point of the check is to make sure the thrift.version variable doesn't get updated without a review of compatibility. also it's a regular expression so using the variable will be unnecessarily permissive.,code_debt,low_quality_code,"Tue, 28 Jul 2015 17:39:19 +0000","Fri, 1 Jul 2022 20:45:51 +0000","Fri, 31 Jul 2015 22:06:36 +0000",275237,-1 on v3. the point of the check is to make sure the thrift.version variable doesn't get updated without a review of compatibility. also it's a regular expression so using the variable will be unnecessarily permissive.,0.0,0.0,neutral
hbase,14494,comment_0,Simple patch which adds some commas to the help message for the follow shell commands: * * grant.rb * * revoke.rb,code_debt,low_quality_code,"Fri, 25 Sep 2015 21:44:37 +0000","Tue, 16 Jan 2018 12:31:51 +0000","Thu, 1 Oct 2015 19:07:03 +0000",508946,Simple patch which adds some commas to the help message for the follow shell commands: delete_table_snapshots.rb grant.rb list_table_snapshots.rb revoke.rb,-0.1083333333,-0.005,neutral
hbase,14604,comment_4,The classes are in the same package. Please make the new methods package private. Patch for 0.98 branch should contain 0.98 in the filename. Extension should be either .txt or .patch. Please attach patch for master branch.,code_debt,low_quality_code,"Wed, 14 Oct 2015 09:54:05 +0000","Tue, 20 Oct 2015 15:20:36 +0000","Tue, 20 Oct 2015 09:40:16 +0000",517571,The classes are in the same package. Please make the new methods package private. Patch for 0.98 branch should contain 0.98 in the filename. Extension should be either .txt or .patch. Please attach patch for master branch.,0.07857142857,0.07857142857,neutral
hbase,14604,comment_1,Any chance of a unit test ?,test_debt,lack_of_tests,"Wed, 14 Oct 2015 09:54:05 +0000","Tue, 20 Oct 2015 15:20:36 +0000","Tue, 20 Oct 2015 09:40:16 +0000",517571,Any chance of a unit test ?,0.4,0.4,neutral
hbase,14622,comment_13,"zkless is the future.. It will be default in 2.0, or at least a version of it -- one w/ state preserved in procedure store rather than up in meta. zkless in 1.0 is not well tested and I thought no one was using it (Only case I know of someone using it, is on a branch made of 0.98) I removed the tests here because it was an 'unsupported' feature test running on top of read replicas, a feature that is not on by default throwing an NPE. Rather than try and figure what was up in the test, and not expecting that someone would be jumping at the opportunity to fix issues in here, I just removed these zkless tests from branch-1 so they can't fail again. I can put them back if you'd like , just say.",test_debt,lack_of_tests,"Thu, 15 Oct 2015 19:56:14 +0000","Sat, 2 Nov 2019 23:55:29 +0000","Thu, 15 Oct 2015 20:04:00 +0000",466,"zkless is the future.. It will be default in 2.0, or at least a version of it  one w/ state preserved in procedure store rather than up in meta. zkless in 1.0 is not well tested and I thought no one was using it (Only case I know of someone using it, is on a branch made of 0.98) I removed the tests here because it was an 'unsupported' feature test running on top of read replicas, a feature that is not on by default throwing an NPE. Rather than try and figure what was up in the test, and not expecting that someone would be jumping at the opportunity to fix issues in here, I just removed these zkless tests from branch-1 so they can't fail again. I can put them back if you'd like eclark, just say.",-0.1597,-0.1597,neutral
hbase,14753,comment_1,"Ok, sorry for the spam then. I was trying to write a shell test, and was confused. Do we need to keep this open for tracking re-enabling or HBASE-14678 covers that?",test_debt,lack_of_tests,"Tue, 3 Nov 2015 22:53:34 +0000","Thu, 9 Nov 2017 19:11:41 +0000","Wed, 17 Aug 2016 00:03:31 +0000",24800997,"Ok, sorry for the spam then. I was trying to write a shell test, and was confused. Do we need to keep this open for tracking re-enabling or HBASE-14678 covers that?",-0.1666666667,-0.1666666667,neutral
hbase,14753,comment_2,I was thinking HBASE-14678 would do. I'd file issues for stuff I don't want to bring back in because flakey still... tests on shell are pretty important.,test_debt,flaky_test,"Tue, 3 Nov 2015 22:53:34 +0000","Thu, 9 Nov 2017 19:11:41 +0000","Wed, 17 Aug 2016 00:03:31 +0000",24800997,I was thinking HBASE-14678 would do. I'd file issues for stuff I don't want to bring back in because flakey still... tests on shell are pretty important.,0.335125,0.335125,neutral
hbase,14941,comment_5,Can we use begin ensure block to ensure we do not leak it ?,design_debt,non-optimal_design,"Mon, 7 Dec 2015 17:44:41 +0000","Fri, 11 Dec 2015 03:53:34 +0000","Thu, 10 Dec 2015 21:35:48 +0000",273067,Can we use begin ensure block to ensure we do not leak it ?,0.1,0.1,neutral
hbase,14956,comment_0,We cannot bump JLine to a newer version since the version of JRuby we ship is an old one (see HBASE-13338). Also this shouldn't cause any issue to use zkcli since it only disables the auto complete functionality. Another option is to use a version of ZK that doesn't have ZOOKEEPER-1718.,architecture_debt,using_obsolete_technology,"Wed, 9 Dec 2015 07:30:32 +0000","Sat, 19 Dec 2015 00:18:27 +0000","Sat, 19 Dec 2015 00:18:27 +0000",838075,We cannot bump JLine to a newer version since the version of JRuby we ship is an old one (see HBASE-13338). Also this shouldn't cause any issue to use zkcli since it only disables the auto complete functionality. Another option is to use a version of ZK that doesn't have ZOOKEEPER-1718.,0.0,0.0,neutral
hbase,14984,comment_1,+1. I can't figure out what the javadoc warning is supposed to be based on looking at the precommit artifacts and the patch.,documentation_debt,low_quality_documentation,"Tue, 15 Dec 2015 21:04:15 +0000","Thu, 17 Dec 2015 23:59:36 +0000","Wed, 16 Dec 2015 18:59:38 +0000",78923,+1. I can't figure out what the javadoc warning is supposed to be based on looking at the precommit artifacts and the patch.,0.3,0.3,negative
hbase,15287,comment_6,Can you modify the following tests to cover your fix ?,test_debt,lack_of_tests,"Wed, 17 Feb 2016 22:01:45 +0000","Thu, 9 Nov 2017 00:42:02 +0000","Sun, 17 Apr 2016 16:30:17 +0000",5164112,Can you modify the following tests to cover your fix ? hbase-server/src/test//java/org/apache/hadoop/hbase/mapreduce/TestImportExport.java hbase-server/src/test//java/org/apache/hadoop/hbase/mapreduce/TestCopyTable.java hbase-server/src/test//java/org/apache/hadoop/hbase/mapreduce/TestCellCounter.java,0.0,0.0,neutral
hbase,15293,comment_9,"Please insert space around 'catch' There're 7 tabs in the patch. Please remove them. Once these are addressed, it should be good.",code_debt,low_quality_code,"Fri, 19 Feb 2016 13:41:34 +0000","Thu, 15 Sep 2016 19:31:40 +0000","Fri, 1 Apr 2016 14:01:45 +0000",3630011,"Please insert space around 'catch' There're 7 tabs in the patch. Please remove them. Once these are addressed, it should be good.",0.346,0.346,neutral
hbase,15397,comment_3,It is not used any more.,code_debt,dead_code,"Fri, 4 Mar 2016 10:39:15 +0000","Fri, 17 Jun 2022 18:42:53 +0000","Fri, 4 Mar 2016 21:50:19 +0000",40264,Why is the Boolean removed ? It is not used any more.,0.0,0.0,neutral
hbase,15617,comment_1,"The new depends on some internal details likely to change. We should treat ZK state as a black box and avoid relying on it. This is a layering violation, really. Why not use the supported Admin API's to get the live server list to enumerate? That unit test failure appears unrelated.",design_debt,non-optimal_design,"Fri, 8 Apr 2016 16:55:56 +0000","Thu, 19 May 2016 17:52:21 +0000","Thu, 19 May 2016 02:21:41 +0000",3489945,"The new getDrainingServers() depends on some internal details likely to change. We should treat ZK state as a black box and avoid relying on it. This is a layering violation, really. Why not use the supported Admin API's Admin#getClusterStatus to get the live server list to enumerate? That unit test failure appears unrelated.",-0.28,-0.28,negative
hbase,15640,comment_0,"Here is small patch which ups our limit to 1M blocks from 100k so less likely we'll hit limit and then if we do, puts in place a message in red with how to address the limit (I couldn't figure what the previous message was trying to say... didn't seem to make sense). This could be better.. but will do for now.",design_debt,non-optimal_design,"Tue, 12 Apr 2016 23:28:31 +0000","Thu, 9 Nov 2017 00:43:41 +0000","Wed, 20 Apr 2016 21:28:00 +0000",683969,"Here is small patch which ups our limit to 1M blocks from 100k so less likely we'll hit limit and then if we do, puts in place a message in red with how to address the limit (I couldn't figure what the previous message was trying to say... didn't seem to make sense). This could be better.. but will do for now.",0.325,0.325,neutral
hbase,15835,comment_7,"Hey , can you remove instances of setting the port to -1 in existing tests? I did a quick grep and there looks to be only a handful. Can you add some class-level javadoc about the minicluster ports going random instead of default. Do you think it might be worth a debug-level logging message too? I could image someone setting a value and in one special case having it fail.",code_debt,low_quality_code,"Mon, 16 May 2016 08:54:16 +0000","Fri, 1 Jul 2022 20:27:59 +0000","Wed, 9 May 2018 14:58:37 +0000",62489061,"Hey daniel_vimont, can you remove instances of setting the port to -1 in existing tests? I did a quick grep and there looks to be only a handful. Can you add some class-level javadoc about the minicluster ports going random instead of default. Do you think it might be worth a debug-level logging message too? I could image someone setting a value and in one special case having it fail.",-0.1,-0.1,neutral
hbase,15835,comment_5,"Curious: It appears that TestFSHLogProvider is (inadvertently, I imagine) instantiating an HBaseTestingUtility with a *null* Configuration (this within the context of kicking off a thread). This might be worthy of further investigation (since it's probably not intended to be this way), but for now I will simply precede my newly inserted logic with a check for a null Configuration, and thus avoid the that my original patch encountered.",design_debt,non-optimal_design,"Mon, 16 May 2016 08:54:16 +0000","Fri, 1 Jul 2022 20:27:59 +0000","Wed, 9 May 2018 14:58:37 +0000",62489061,"Curious: It appears that TestFSHLogProvider is (inadvertently, I imagine) instantiating an HBaseTestingUtility with a null Configuration (this within the context of kicking off a WALPerformanceEvaluation thread). This might be worthy of further investigation (since it's probably not intended to be this way), but for now I will simply precede my newly inserted logic with a check for a null Configuration, and thus avoid the null-pointer-exception that my original patch encountered.",0.096875,0.096875,neutral
hbase,15835,comment_9,"Submitting a revised patch which includes all of the following... Subtask 1: Remove instances of setting the ports to -1 in existing tests. The following modules were modified to remove their (now apparently extraneous) setting of master-info-port and region-server-port: Subtask 2: Add some class-level javadoc. The following was added to the HBaseTestingUtility class-level javadoc comment: Subtask 3: Add a debug-level logging message for when port values are overridden to ""-1"". The following code now appears at the end of the main constructor for Note the ""debug"" logging that has been added: Subtask 4: Add new method to for testing port overrides. The following new method assures that port override is taking place when it should, and is NOT taking place when it should NOT:",documentation_debt,low_quality_documentation,"Mon, 16 May 2016 08:54:16 +0000","Fri, 1 Jul 2022 20:27:59 +0000","Wed, 9 May 2018 14:58:37 +0000",62489061,"Submitting a revised patch which includes all of the following... Subtask 1: Remove instances of setting the ports to -1 in existing tests. The following modules were modified to remove their (now apparently extraneous) setting of master-info-port and region-server-port: Subtask 2: Add some class-level javadoc. The following was added to the HBaseTestingUtility class-level javadoc comment: Subtask 3: Add a debug-level logging message for when port values are overridden to ""-1"". The following code now appears at the end of the main constructor for HBaseTestingUtility. Note the ""debug"" logging that has been added: Subtask 4: Add new method to TestHBaseTestingUtility for testing port overrides. The following new method assures that port override is taking place when it should, and is NOT taking place when it should NOT:",0.1,0.08333333333,neutral
hbase,15835,comment_11,"Okay, the above-listed QA-failure prompted me to read in more detail those emails regarding the problem of ""flaky|flakey"" tests. I see that is indeed on the list, and also that testing of HBASE-15876 ran into exactly the same (apparently ""flaky|flakey"") failure. In this case, an awkward aspect is that the changes I made could conceivably have caused this kind of a failure!! So, just for good measure, I did a fresh clone/install of the Master branch and ran the same test: yep, it failed in exactly the same way.",test_debt,flaky_test,"Mon, 16 May 2016 08:54:16 +0000","Fri, 1 Jul 2022 20:27:59 +0000","Wed, 9 May 2018 14:58:37 +0000",62489061,"Okay, the above-listed QA-failure prompted me to read in more detail those emails regarding the problem of ""flaky|flakey"" tests. I see that TestRegionServerMetrics is indeed on the list, and also that testing of HBASE-15876 ran into exactly the same (apparently ""flaky|flakey"") failure. In this case, an awkward aspect is that the changes I made could conceivably have caused this kind of a failure!! So, just for good measure, I did a fresh clone/install of the Master branch and ran the same test: yep, it failed in exactly the same way.",-0.151625,-0.151625,neutral
hbase,15892,comment_6,Pushed to master branch. Lets try it.  helped me w/ test. I needed to do this: Run this first : pip install gitpython && pip install rbtools Excellent work  Bit of doc please in refguide when you get a chance so we can easily point folks this route.,documentation_debt,outdated_documentation,"Thu, 26 May 2016 12:17:12 +0000","Tue, 31 May 2016 02:27:03 +0000","Thu, 26 May 2016 16:20:02 +0000",14570,Pushed to master branch. Lets try it. appy helped me w/ test. I needed to do this: Run this first : pip install gitpython && pip install rbtools Excellent work appy Bit of doc please in refguide when you get a chance so we can easily point folks this route.,0.246875,0.246875,positive
hbase,16157,comment_6,lgtm. Is the test stable enough? Looks like it might end up being a flaky test.,test_debt,flaky_test,"Thu, 30 Jun 2016 18:21:41 +0000","Wed, 6 Jul 2016 02:00:24 +0000","Tue, 5 Jul 2016 20:57:21 +0000",441340,lgtm. Is the test stable enough? Looks like it might end up being a flaky test.,0.15,0.15,negative
hbase,16789,comment_0,"Removed directory layout reference from CompactionTool and moved it to The new tool named CompactionTool is added with changed interface. The new CT takes table, regions, column families as an input command line arguments. Both the legacy and new CT use APIs provided by MasterStorage/ RegionStorage classes. Map Reduce functionality of old CT is not yet implemented in the new CT as there are on-going discussions about it. Manually tested old CT and the new CT.",architecture_debt,violation_of_modularity,"Fri, 7 Oct 2016 00:44:02 +0000","Thu, 23 Jun 2022 20:30:29 +0000","Thu, 20 Oct 2016 20:50:08 +0000",1195566,"Removed directory layout reference from CompactionTool and moved it to LagacyCompactionTool. The new tool named CompactionTool is added with changed interface. The new CT takes table, regions, column families as an input command line arguments. Both the legacy and new CT use APIs provided by MasterStorage/ RegionStorage classes. Map Reduce functionality of old CT is not yet implemented in the new CT as there are on-going discussions about it. Manually tested old CT and the new CT.",0.1126,0.09383333333,neutral
hbase,16789,comment_2,", here are a few points that are discussed: * This is an offline Compaction Tool (CT). Without MR option, CT will compact files for input table/ region/ column family on local node where CT is run. * Current CT, decides on node to run MR jobs based on location of first block of a first file in an input directory. * This can be improved to consider nodes based on last know region assignments with fallback on location of first block of first file in a table/ region/ column family. This will provide better locality. * Even with the improved logic, locality cannot be guaranteed. * So, whether to run with MR and MR job node selection can be determined by code outside of CT or a User. CT will be just responsible for compaction of files for input table/ region/ cf without deciding on MR or node selection for MR. * CT may query/ consider local regions and only compact files belonging to local regions. Workaround with -force option can be provided for the default behavior.",design_debt,non-optimal_design,"Fri, 7 Oct 2016 00:44:02 +0000","Thu, 23 Jun 2022 20:30:29 +0000","Thu, 20 Oct 2016 20:50:08 +0000",1195566,"busbey, here are a few points that are discussed: This is an offline Compaction Tool (CT). Without MR option, CT will compact files for input table/ region/ column family on local node where CT is run. Current CT, decides on node to run MR jobs based on location of first block of a first file in an input directory. This can be improved to consider nodes based on last know region assignments with fallback on location of first block of first file in a table/ region/ column family. This will provide better locality. Even with the improved logic, locality cannot be guaranteed. So, whether to run with MR and MR job node selection can be determined by code outside of CT or a User. CT will be just responsible for compaction of files for input table/ region/ cf without deciding on MR or node selection for MR. CT may query/ consider local regions and only compact files belonging to local regions. Workaround with -force option can be provided for the default behavior.",0.05,0.05,neutral
hbase,16817,comment_1,It might be odd to pass this boolean.. Even withTags we plan to remove. Intentionally it was removed that the oswrite within Cell impl write the length of the cell. Whether the length to be written or no is up to the caller. In case of HFIle write we dont need to write the length. In case of KVCodec we need it. Some other codec may come tomorrow which want to write the length not as an int but as a varint. So IMO it is better to leave it and not club the length write part within write method in Cell. Got ur point of one extra calc but it is ok.,design_debt,non-optimal_design,"Wed, 12 Oct 2016 14:38:08 +0000","Thu, 13 Oct 2016 12:26:13 +0000","Thu, 13 Oct 2016 12:26:13 +0000",78485,"int write(OutputStream out, boolean withTags, boolean withLengthHeader) It might be odd to pass this boolean.. Even withTags we plan to remove. Intentionally it was removed that the oswrite within Cell impl write the length of the cell. Whether the length to be written or no is up to the caller. In case of HFIle write we dont need to write the length. In case of KVCodec we need it. Some other codec may come tomorrow which want to write the length not as an int but as a varint. So IMO it is better to leave it and not club the length write part within write method in Cell. Got ur point of one extra calc but it is ok.",0.0875,0.0875,neutral
hbase,16872,comment_0,"Ah there is a typo in the comment. ""We need to the MultiRequest"" = Will fix on commit.",documentation_debt,low_quality_documentation,"Tue, 18 Oct 2016 14:48:06 +0000","Thu, 20 Oct 2016 09:12:21 +0000","Thu, 20 Oct 2016 01:41:18 +0000",125592,"Ah there is a typo in the comment. ""We need to the MultiRequest"" => ""We need the MultiRequest"" Will fix on commit.",0.1,0.1,negative
hbase,16872,comment_2,Please fix below javadoc warnings as well as the comment mentioned above: Other parts lgtm. +1,documentation_debt,low_quality_documentation,"Tue, 18 Oct 2016 14:48:06 +0000","Thu, 20 Oct 2016 09:12:21 +0000","Thu, 20 Oct 2016 01:41:18 +0000",125592,Please fix below javadoc warnings as well as the comment mentioned above: Other parts lgtm. +1,0.0385,0.0385,negative
hbase,16872,comment_3,Fix javadoc issues.,documentation_debt,low_quality_documentation,"Tue, 18 Oct 2016 14:48:06 +0000","Thu, 20 Oct 2016 09:12:21 +0000","Thu, 20 Oct 2016 01:41:18 +0000",125592,Fix javadoc issues.,0.0,0.0,negative
hbase,16998,comment_6,".004 latest from rb. Missing InterfaceAudience annotations, constructor cleanup in QuotaObserverChore.",code_debt,low_quality_code,"Wed, 2 Nov 2016 19:26:32 +0000","Fri, 6 Apr 2018 04:00:26 +0000","Mon, 30 Jan 2017 17:12:40 +0000",7681568,".004 latest from rb. Missing InterfaceAudience annotations, constructor cleanup in QuotaObserverChore.",-0.1333333333,-0.1333333333,negative
hbase,17025,comment_7,Is it possible to add some .rb test ? Thanks,test_debt,lack_of_tests,"Fri, 4 Nov 2016 18:42:27 +0000","Fri, 6 Apr 2018 04:00:24 +0000","Fri, 17 Feb 2017 16:51:07 +0000",9065320,Is it possible to add some .rb test ? Thanks,0.1333333333,0.1333333333,neutral
hbase,17101,comment_0,"including a rough patch, will cleanup and upload.",code_debt,low_quality_code,"Tue, 15 Nov 2016 18:35:53 +0000","Thu, 23 Jun 2022 19:12:57 +0000","Tue, 31 Jan 2017 18:59:41 +0000",6654228,"including a rough patch, will cleanup and upload.",0.0,0.0,neutral
hbase,17192,comment_6,+1. Thanks for debug steps (ugly).,code_debt,low_quality_code,"Tue, 29 Nov 2016 09:50:31 +0000","Fri, 24 Jun 2022 19:34:43 +0000","Tue, 29 Nov 2016 14:52:25 +0000",18114,+1. Thanks for debug steps (ugly).,-0.15,-0.15,positive
hbase,17338,comment_19,+1 on commit. Lets get up on this new basis. Nice cleanup  (Thanks too for doc edit).,code_debt,low_quality_code,"Mon, 19 Dec 2016 13:18:49 +0000","Fri, 1 Jul 2022 21:33:56 +0000","Fri, 10 Mar 2017 05:43:36 +0000",6971087,+1 on commit. Lets get up on this new basis. Nice cleanup anoop.hbase (Thanks too for doc edit).,0.4375,0.48125,neutral
hbase,17338,comment_8,+1 Add more comment on commit to MemstoreSize about diff between heap and data size.... (can be a version of comment that is later in class...) Glad of the simplification. Good stuff @anoop sam john,code_debt,complex_code,"Mon, 19 Dec 2016 13:18:49 +0000","Fri, 1 Jul 2022 21:33:56 +0000","Fri, 10 Mar 2017 05:43:36 +0000",6971087,+1 Add more comment on commit to MemstoreSize about diff between heap and data size.... (can be a version of comment that is later in RegionServerAccounting class...) Glad of the simplification. Good stuff @anoop sam john,0.5753333333,0.5753333333,neutral
hbase,17338,comment_10,"Thanks for the reviews Stack & Ram.. I have to fix the failed test as those were having assert on heapOverhead size after cell addition to Memstore. Now we track heapSize not overhead. Will add more comments as Stack asked Regarding Ram's comment. I have different opinion. I dont think we need change this way. We track dataSize (irrespective of cell data in on heap or off heap area).. This dataSize been used at Segment level for in memory flush decision, Region level for on disk flushes and globally to force flush some regions. At the 1st 2 levels, it is not doubt that we have to track all the cell data size together. Now the point Ram says is when we have off heap configured and max off heap global size is say 12 GB, once the data size globally reaches this level, we will force flush some regions. So his point is for this tracking, we have to consider only off heap Cells and on heap Cell's data size should not get accounted in the data size but only in the heapSize. (At global level. But at region and segment level it has to get applied). 2 reasons why I am not in favor of this 1. This makes the impl so complex. We need to add isOffheap check down the layers. Also at 2 layers we have to consider these on heap cell data size and one level not. 2. When off heap is enabled, (We have the MSLAB pool off heap in place), we will end up in having on heap Cells when the pool is out of BBs. We will create on demand LABs on heap. If we dont consider those cell's data size at global level, we may reach forced flush level a bit late. That is the only gain. But here the on demand LAB creation is a sign that the write load is so high and delaying the flush will add more pressure to the MSLAB and more and more on demand BBs (2 MB sized) need to get created. One aim of the off heap work is to reduce the max heap space need for the servers. So lets consider the cell data size globally also (how we do now) and make global flushes. Now even if MSLAB is used, the append/increment use cases wont use MSLAB. The cells will be on heap then. But for such a use case, enabling MSLAB (and pool) is totally waste. That is a mis configuration. More and more on demand BB creation, when MSLAB pool is a bad sign. We have a WARN log for just one time as of now.. May be we should repeat this log at certain intervals. (Like for every 100th pool miss or so.) We should be able to turn MSLAB usage ON/OFF per table also. Now this is possible? Am not sure. These 2 things need to be checked and done in another jiras IMO.",design_debt,non-optimal_design,"Mon, 19 Dec 2016 13:18:49 +0000","Fri, 1 Jul 2022 21:33:56 +0000","Fri, 10 Mar 2017 05:43:36 +0000",6971087,"Thanks for the reviews Stack & Ram.. I have to fix the failed test as those were having assert on heapOverhead size after cell addition to Memstore. Now we track heapSize not overhead. Will add more comments as Stack asked Regarding Ram's comment. I have different opinion. I dont think we need change this way. We track dataSize (irrespective of cell data in on heap or off heap area).. This dataSize been used at Segment level for in memory flush decision, Region level for on disk flushes and globally to force flush some regions. At the 1st 2 levels, it is not doubt that we have to track all the cell data size together. Now the point Ram says is when we have off heap configured and max off heap global size is say 12 GB, once the data size globally reaches this level, we will force flush some regions. So his point is for this tracking, we have to consider only off heap Cells and on heap Cell's data size should not get accounted in the data size but only in the heapSize. (At global level. But at region and segment level it has to get applied). 2 reasons why I am not in favor of this 1. This makes the impl so complex. We need to add isOffheap check down the layers. Also at 2 layers we have to consider these on heap cell data size and one level not. 2. When off heap is enabled, (We have the MSLAB pool off heap in place), we will end up in having on heap Cells when the pool is out of BBs. We will create on demand LABs on heap. If we dont consider those cell's data size at global level, we may reach forced flush level a bit late. That is the only gain. But here the on demand LAB creation is a sign that the write load is so high and delaying the flush will add more pressure to the MSLAB and more and more on demand BBs (2 MB sized) need to get created. One aim of the off heap work is to reduce the max heap space need for the servers. So lets consider the cell data size globally also (how we do now) and make global flushes. Now even if MSLAB is used, the append/increment use cases wont use MSLAB. The cells will be on heap then. But for such a use case, enabling MSLAB (and pool) is totally waste. That is a mis configuration. More and more on demand BB creation, when MSLAB pool is a bad sign. We have a WARN log for just one time as of now.. May be we should repeat this log at certain intervals. (Like for every 100th pool miss or so.) We should be able to turn MSLAB usage ON/OFF per table also. Now this is possible? Am not sure. These 2 things need to be checked and done in another jiras IMO.",-0.0004852941176,-0.0004852941176,neutral
hbase,17338,comment_3,"In case of on heap MSLAB, all the size calc doing global memstore data size + heap overhead based. That is why was not adding the cell data size in case of on heap. But I think I can do it bit diff way.. So instead of tracking the data size and heap overhead, we will track cell data size and heap size. Pls see it is not heap overhead alone.. This is the total heap space occupied by memstore(s). I can change the calc accordingly. For on heap MSLAB , this heap space count will be the same as what we have before off heap write path work (ie. It will include all cell data size and overhead part). The new accounting ie. cellDataSize will include only the cell data bytes size part. This will be a subset of the former one then. Will do all necessary changes.. This will be a bigger patch then as I will rename all the related area from heapOverhead to heapSize or so. I believe that way will look cleaner. Thoughts? Append/Increment is not adding cells into MSLAB area.. This is to avoid MSLAB wastage. Say same cell is getting incremented 1000 times and cell key+ value size is 100 bytes. If every increment (add to memstore) cell was added to MSLAB, we will overall take ~100KB MSLAB space whereas only 100 bytes is valid at any point.. All the former cells are getting deleted by the addition of a new cell.. The Cell POJO as such is removed from CSLM. But we can not free up that space in MSLAB.. MSLAB is not designed to do this way. That the chunk allocation and offset allocation within a chunk is serially incremented way happening. We can not mark some in btw space as free and reuse.. That will make things very complex for us.. So to avoid these, the simplest way was to NOT use MSLAB for upsert.",design_debt,non-optimal_design,"Mon, 19 Dec 2016 13:18:49 +0000","Fri, 1 Jul 2022 21:33:56 +0000","Fri, 10 Mar 2017 05:43:36 +0000",6971087,"In case of on heap MSLAB, all the size calc doing global memstore data size + heap overhead based. That is why was not adding the cell data size in case of on heap. But I think I can do it bit diff way.. So instead of tracking the data size and heap overhead, we will track cell data size and heap size. Pls see it is not heap overhead alone.. This is the total heap space occupied by memstore(s). I can change the calc accordingly. For on heap MSLAB , this heap space count will be the same as what we have before off heap write path work (ie. It will include all cell data size and overhead part). The new accounting ie. cellDataSize will include only the cell data bytes size part. This will be a subset of the former one then. Will do all necessary changes.. This will be a bigger patch then as I will rename all the related area from heapOverhead to heapSize or so. I believe that way will look cleaner. Thoughts? need to read on why Append/Increment can't be out in offheap. Append/Increment is not adding cells into MSLAB area.. This is to avoid MSLAB wastage. Say same cell is getting incremented 1000 times and cell key+ value size is 100 bytes. If every increment (add to memstore) cell was added to MSLAB, we will overall take ~100KB MSLAB space whereas only 100 bytes is valid at any point.. All the former cells are getting deleted by the addition of a new cell.. The Cell POJO as such is removed from CSLM. But we can not free up that space in MSLAB.. MSLAB is not designed to do this way. That the chunk allocation and offset allocation within a chunk is serially incremented way happening. We can not mark some in btw space as free and reuse.. That will make things very complex for us.. So to avoid these, the simplest way was to NOT use MSLAB for upsert.",0.1058823529,0.1,neutral
hbase,17338,comment_9,Rest is all good. But as discussed internally that isOffheap() addition to MSLAB is better so that we can really avoid adding the dataSize to the MemstoreSize when the memstore is offheap but still the entire Cell is onheap. Now currently we just account for the dataSize also. We can do in another JIRA. +1 otherwise.,design_debt,non-optimal_design,"Mon, 19 Dec 2016 13:18:49 +0000","Fri, 1 Jul 2022 21:33:56 +0000","Fri, 10 Mar 2017 05:43:36 +0000",6971087,Rest is all good. But as discussed internally that isOffheap() addition to MSLAB is better so that we can really avoid adding the dataSize to the MemstoreSize when the memstore is offheap but still the entire Cell is onheap. Now currently we just account for the dataSize also. We can do in another JIRA. +1 otherwise.,0.2939333333,0.2939333333,neutral
hbase,17338,comment_18,Yes Stack. I have done edit to the doc so as to change the solution detailing part as per the discussion in this jira. Thanks for the remind. Are you ok with this patch as such. I may have to rebase it. Let me check. Tks,documentation_debt,low_quality_documentation,"Mon, 19 Dec 2016 13:18:49 +0000","Fri, 1 Jul 2022 21:33:56 +0000","Fri, 10 Mar 2017 05:43:36 +0000",6971087,Yes Stack. I have done edit to the doc so as to change the solution detailing part as per the discussion in this jira. Thanks for the remind. Are you ok with this patch as such. I may have to rebase it. Let me check. Tks,0.2214285714,0.2214285714,neutral
hbase,17383,comment_3,No longer this case as some other cleanup corrected the log. Just closing as can not reproduce,code_debt,low_quality_code,"Wed, 28 Dec 2016 10:14:44 +0000","Thu, 22 Mar 2018 07:28:21 +0000","Thu, 22 Mar 2018 07:28:21 +0000",38783617,No longer this case as some other cleanup corrected the log. Just closing as can not reproduce,0.0,0.0,negative
hbase,17480,comment_2,+1 That is a nice lump of code removed. You remove Do we have an explicit test of the splitting path any more post this removal?,code_debt,dead_code,"Tue, 17 Jan 2017 23:08:57 +0000","Fri, 17 Jun 2022 18:05:45 +0000","Thu, 19 Jan 2017 17:05:56 +0000",151019,+1 That is a nice lump of code removed. You remove TestSplitTransaction. Do we have an explicit test of the splitting path any more post this removal? syuanjiang,0.6625,0.33125,positive
hbase,17500,comment_2,Fix the javadoc warning.,documentation_debt,low_quality_documentation,"Fri, 20 Jan 2017 07:35:51 +0000","Wed, 25 Jan 2017 08:12:45 +0000","Wed, 25 Jan 2017 03:35:08 +0000",417557,Fix the javadoc warning.,-0.6,-0.6,negative
hbase,17808,comment_1,"I have done a little performance test. I used YSCB's workloadc with 1 client 100 threads against one regionserver. But surprisedly I noticed performance regression with 'fastpath', no matter with I implemented or the original Is it something wrong with fastpath or am I doing something wrong? Ping , since Stack is the original author of fastpath. YSCB workloadc with 100 threads |32982| issue)|32287| fastpath)|34563|",code_debt,slow_algorithm,"Mon, 20 Mar 2017 09:13:13 +0000","Wed, 15 Dec 2021 02:48:27 +0000","Wed, 15 Dec 2021 02:48:27 +0000",149535314,"I have done a little performance test. I used YSCB's workloadc with 1 client 100 threads against one regionserver. But surprisedly I noticed performance regression with 'fastpath', no matter with FastPathRWQueueRpcExecutor I implemented or the original FastPathBalancedQueueRpcExecutor. Is it something wrong with fastpath or am I doing something wrong? Ping stack, since Stack is the original author of fastpath. YSCB workloadc with 100 threads",0.02,-0.075,neutral
hbase,18085,comment_13,"Sorry, don't quite catch you sir, mind clarify? While writing the JMH case, I felt that using AtomicBoolean or volatile boolean we're actually re-implementing the tryLock logic. And I won't be surprised if JIT has any optimization for its native implementation (smile).",code_debt,duplicated_code,"Fri, 19 May 2017 19:04:08 +0000","Wed, 1 Aug 2018 06:24:22 +0000","Wed, 24 May 2017 07:57:01 +0000",391973,"Can we use the return value of purgeLock.tryLock() passed to BlackHole or so? Sorry, don't quite catch you sir, mind clarify? While writing the JMH case, I felt that using AtomicBoolean or volatile boolean we're actually re-implementing the tryLock logic. And I won't be surprised if JIT has any optimization for its native implementation (smile).",-0.4873333333,-0.2924,neutral
hbase,18085,comment_14,"In ur test method for tryLock, there is no logic other than just try lock and release. If that is been removed as dead code by compiler? One way to avoid that is using the return value. See BlackHole in JMH and its usage.. The diff in numbers that reported by JMH benchmark is so huge! The impl of tryLock is having a volatile read and all. So this huge diff in numbers looks strange no? That was my doubt.",code_debt,dead_code,"Fri, 19 May 2017 19:04:08 +0000","Wed, 1 Aug 2018 06:24:22 +0000","Wed, 24 May 2017 07:57:01 +0000",391973,"In ur test method for tryLock, there is no logic other than just try lock and release. If that is been removed as dead code by compiler? One way to avoid that is using the return value. See BlackHole in JMH and its usage.. The diff in numbers that reported by JMH benchmark is so huge! The impl of tryLock is having a volatile read and all. So this huge diff in numbers looks strange no? That was my doubt.",-0.1302857143,-0.1302857143,neutral
hbase,18085,comment_9,"Reading the code in I can see it uses a state variable up in the layer in which is declared volatile. So we ourselves using a boolean volatile will be better any way right? Pls check once whether I misread some code path,",code_debt,low_quality_code,"Fri, 19 May 2017 19:04:08 +0000","Wed, 1 Aug 2018 06:24:22 +0000","Wed, 24 May 2017 07:57:01 +0000",391973,"Reading the code in ReentrantLock#tryLock(), I can see it uses a state variable up in the layer in AbstractQueuedSynchronizer which is declared volatile. So we ourselves using a boolean volatile will be better any way right? Pls check once whether I misread some code path,",-0.1356666667,-0.1356666667,neutral
hbase,18085,comment_6,Any chance for a micro benchmark tests Yu? Appreciate ur effort to consider all possible options :-),test_debt,lack_of_tests,"Fri, 19 May 2017 19:04:08 +0000","Wed, 1 Aug 2018 06:24:22 +0000","Wed, 24 May 2017 07:57:01 +0000",391973,Any chance for a micro benchmark tests Yu? Appreciate ur effort to consider all possible options,0.4,0.4,neutral
hbase,18092,comment_13,"Just a nitpick, I think you merged the incorrect patch for master. I was looking at my latest master checkout and see that patch for master that went in is dated May 25, not yesterday. It's not a big deal in this case because the two patches have no functional difference, only that the newer patch got rid of a redundant piece of code.",code_debt,dead_code,"Mon, 22 May 2017 18:47:28 +0000","Wed, 1 Aug 2018 06:22:22 +0000","Fri, 9 Jun 2017 18:15:45 +0000",1553297,"yuzhihong@gmail.com Just a nitpick, I think you merged the incorrect patch for master. I was looking at my latest master checkout and see that patch for master that went in is dated May 25, not yesterday. It's not a big deal in this case because the two patches have no functional difference, only that the newer patch got rid of a redundant piece of code.",0.2666666667,0.2666666667,neutral
hbase,18092,comment_4,"Attaching another patch for master. I found a redundant piece of code related to the patch. The only difference between the previous and this patch is {Code} @@ -528,9 +542,7 @@ public class implements ReplicationListener { */ public void src) { LOG.info(""Done with the recovered queue "" + - if (src instanceof ReplicationSource) { - - } + {Code} Also attaching patches for branch-1 and branch-1.3 as they both have diverged a little bit.",code_debt,low_quality_code,"Mon, 22 May 2017 18:47:28 +0000","Wed, 1 Aug 2018 06:22:22 +0000","Fri, 9 Jun 2017 18:15:45 +0000",1553297,Attaching another patch for master. I found a redundant piece of code related to the patch. The only difference between the previous and this patch is Also attaching patches for branch-1 and branch-1.3 as they both have diverged a little bit.,0.1,0.125,neutral
hbase,18180,comment_1,Have a look at TableOutputFormat inside Connection leak problem has already been addressed in this package. Code sample for reference.,code_debt,low_quality_code,"Wed, 7 Jun 2017 02:54:21 +0000","Wed, 1 Aug 2018 06:21:19 +0000","Mon, 19 Jun 2017 13:40:24 +0000",1075563,pankaj2461 Have a look at TableOutputFormat inside org.apache.hadoop.hbase.mapreduce. Connection leak problem has already been addressed in this package. Code sample for reference.,-0.01666666667,-0.004761904762,neutral
hbase,18180,comment_8,You can attach branch-1 patch alone where TestLockProcedure is not flaky.,test_debt,flaky_test,"Wed, 7 Jun 2017 02:54:21 +0000","Wed, 1 Aug 2018 06:21:19 +0000","Mon, 19 Jun 2017 13:40:24 +0000",1075563,You can attach branch-1 patch alone where TestLockProcedure is not flaky.,-0.4,-0.4,neutral
hbase,18549,comment_3,"Yes i believe that as well. The patch mostly lgtm. The word region is misspelled in some exception text. That exception could provide more information for debugging. You have: Add the region name, the znode path, and the stacktrace to the log line.",documentation_debt,low_quality_documentation,"Wed, 9 Aug 2017 20:58:19 +0000","Fri, 1 Feb 2019 19:55:54 +0000","Tue, 2 Oct 2018 01:51:52 +0000",36132813,"my takeaway from this JIRA's description is to providing metrics regarding failed to recover replication queue. Yes i believe that as well. The patch mostly lgtm. The word region is misspelled in some exception text. That exception could provide more information for debugging. You have: Add the region name, the znode path, and the stacktrace to the log line.",0.0831,0.002583333333,neutral
hbase,18549,comment_7,+1 modulo minor spelling nit: What do you think about a patch for branch-1 too?,documentation_debt,low_quality_documentation,"Wed, 9 Aug 2017 20:58:19 +0000","Fri, 1 Feb 2019 19:55:54 +0000","Tue, 2 Oct 2018 01:51:52 +0000",36132813,+1 modulo minor spelling nit: What do you think about a patch for branch-1 too? xucang,0.0,0.0,neutral
hbase,18549,comment_8,added branch-1 patch. added master branch patch to address typo issue Andrew mentioned.,documentation_debt,low_quality_documentation,"Wed, 9 Aug 2017 20:58:19 +0000","Fri, 1 Feb 2019 19:55:54 +0000","Tue, 2 Oct 2018 01:51:52 +0000",36132813,added branch-1 patch. added master branch patch to address typo issue Andrew mentioned.,0.25,0.25,neutral
hbase,18909,comment_14,Does this still reference to deprecated API?,code_debt,low_quality_code,"Sat, 30 Sep 2017 03:40:48 +0000","Wed, 21 Mar 2018 22:21:06 +0000","Sat, 7 Oct 2017 13:30:27 +0000",640179,Does this still reference to deprecated API?,0.281,0.281,neutral
hbase,18909,comment_6,"Attach a patch to fix the too long line report. But why ruby-lint report ""undefined method java_import""?  Any ideas?",code_debt,low_quality_code,"Sat, 30 Sep 2017 03:40:48 +0000","Wed, 21 Mar 2018 22:21:06 +0000","Sat, 7 Oct 2017 13:30:27 +0000",640179,"Attach a patch to fix the too long line report. But why ruby-lint report ""undefined method java_import""? mdrob Any ideas?",0.0,0.0,neutral
hbase,18909,comment_0,"The API you are suggesting to use is marked deprecated below in the patch, So we should suggest to use in the first place also. also There is reference to this method in java doc of other public APIs to use this one, I suggest to remove their also and point it to actual API we would like user to use. General practice what I follow is when ever I mark a API as deprecated I replace all of its reference from the code with the one we are suggesting to use in the java doc. If you also want to do this please also take care of the references in ruby scripts...",documentation_debt,outdated_documentation,"Sat, 30 Sep 2017 03:40:48 +0000","Wed, 21 Mar 2018 22:21:06 +0000","Sat, 7 Oct 2017 13:30:27 +0000",640179,"The API you are suggesting to use is marked deprecated below in the patch, So we should suggest to use listTableDescriptors(Pattern) in the first place also. also There is reference to this method in java doc of other public APIs to use this one, I suggest to remove their also and point it to actual API we would like user to use. General practice what I follow is when ever I mark a API as deprecated I replace all of its reference from the code with the one we are suggesting to use in the java doc. If you also want to do this please also take care of the references in ruby scripts...",0.06666666667,0.06666666667,neutral
hbase,18909,comment_11,Attach a 005 patch to fix the whitespace and javadoc warnings.,documentation_debt,low_quality_documentation,"Sat, 30 Sep 2017 03:40:48 +0000","Wed, 21 Mar 2018 22:21:06 +0000","Sat, 7 Oct 2017 13:30:27 +0000",640179,Attach a 005 patch to fix the whitespace and javadoc warnings.,-0.6,-0.6,neutral
hbase,19031,comment_6,The deprecation in RemoteHTable was added by this which shipped in version 0.99 tree parent author Enis Soztutar Enis Soztutar <enis@apache.org HBASE-11797 Create Table interface to replace HTableInterface (Carter) ... so the removal is fine. +1 from me on patch.,code_debt,dead_code,"Tue, 17 Oct 2017 20:28:32 +0000","Wed, 21 Mar 2018 22:20:10 +0000","Sat, 28 Oct 2017 23:24:46 +0000",960974,The deprecation in RemoteHTable was added by this which shipped in version 0.99 tree 9ea2b68183049c62f4216ebd09fee17868ab4983 parent 310ac4f71d0c8f28aa8dd8aa2b144fcf206dc83f author Enis Soztutar <enis@apache.org> Tue Sep 2 13:07:02 2014 -0700 committer Enis Soztutar <enis@apache.org> Tue Sep 2 13:07:02 2014 -0700 HBASE-11797 Create Table interface to replace HTableInterface (Carter) ... so the removal is fine. +1 from me on patch.,-0.025,-0.025,neutral
hbase,19031,comment_1,s change on HBASE-19043 helps with {{HTableWrapper}}. In {{RemoteHTable}} the {{public Boolean[] exists(List<Get> gets)}} method is marked as deprecated but it does not have any documentation when it will be removed. Is it possible to remove that method before alpha4?,documentation_debt,outdated_documentation,"Tue, 17 Oct 2017 20:28:32 +0000","Wed, 21 Mar 2018 22:20:10 +0000","Sat, 28 Oct 2017 23:24:46 +0000",960974,stack's change on HBASE-19043 helps with HTableWrapper. In RemoteHTable the public Boolean[] exists(List<Get> gets) method is marked as deprecated but it does not have any documentation when it will be removed. Is it possible to remove that method before alpha4?,0.1333333333,0.1333333333,neutral
hbase,19031,comment_3,Fixed a leftover TODO in version 2.,requirement_debt,requirement_partially_implemented,"Tue, 17 Oct 2017 20:28:32 +0000","Wed, 21 Mar 2018 22:20:10 +0000","Sat, 28 Oct 2017 23:24:46 +0000",960974,Fixed a leftover TODO in version 2.,0.0,0.0,neutral
hbase,19073,comment_2,"So the only test passing in QA is But it's weird, this is second time am seeing it failing in a precommit, and the last patch was completely unrelated. It's not even in the flaky list. Let me dig.",test_debt,flaky_test,"Mon, 23 Oct 2017 22:47:02 +0000","Wed, 1 Aug 2018 06:22:22 +0000","Wed, 25 Oct 2017 03:05:27 +0000",101905,"So the only test passing in QA is TestDistributedLogSplitting. But it's weird, this is second time am seeing it failing in a precommit, and the last patch was completely unrelated. It's not even in the flaky list. Let me dig.",-0.1333333333,-0.1,negative
hbase,19187,comment_8,Correction of test failures. Removed the on heap BC tests. Fixed one javadoc warn.,code_debt,low_quality_code,"Mon, 6 Nov 2017 12:23:41 +0000","Tue, 29 Dec 2020 06:18:44 +0000","Sat, 11 Nov 2017 07:11:53 +0000",413292,Correction of test failures. Removed the on heap BC tests. Fixed one javadoc warn.,-0.2666666667,-0.2666666667,neutral
hbase,19241,comment_5,"+1 after fix the checkstyle warnings. Meanwhile, please add one line change change to trigger hbase-server test. Thanks.",code_debt,low_quality_code,"Sat, 11 Nov 2017 12:43:01 +0000","Wed, 21 Mar 2018 22:21:08 +0000","Mon, 13 Nov 2017 09:02:52 +0000",159591,"+1 after fix the checkstyle warnings. Meanwhile, please add one line change change to trigger hbase-server test. Thanks.",1.85e-17,1.85e-17,neutral
hbase,19241,comment_6,Fix checkstyle and javadoc warnings. Add one line change in hbase-server module to trigger the UTs.,code_debt,low_quality_code,"Sat, 11 Nov 2017 12:43:01 +0000","Wed, 21 Mar 2018 22:21:08 +0000","Mon, 13 Nov 2017 09:02:52 +0000",159591,Fix checkstyle and javadoc warnings. Add one line change in hbase-server module to trigger the UTs.,-0.3,-0.3,neutral
hbase,19241,comment_0,Add more javadoc for AsyncAdmin. Also cleanup the warnings of RawAsyncHBaseAdmin.,documentation_debt,low_quality_documentation,"Sat, 11 Nov 2017 12:43:01 +0000","Wed, 21 Mar 2018 22:21:08 +0000","Mon, 13 Nov 2017 09:02:52 +0000",159591,Add more javadoc for AsyncAdmin. Also cleanup the warnings of RawAsyncHBaseAdmin.,-0.3,-0.3,neutral
hbase,19241,comment_3,+1 Fix javadoc complaint on commit.,documentation_debt,low_quality_documentation,"Sat, 11 Nov 2017 12:43:01 +0000","Wed, 21 Mar 2018 22:21:08 +0000","Mon, 13 Nov 2017 09:02:52 +0000",159591,+1 Fix javadoc complaint on commit.,-0.1,-0.1,neutral
hbase,19300,comment_2,"As far as I can tell, synchronizing on outer (the context) is correct: I don't know why error-prone flagged {{synchronized (outer)}}",code_debt,low_quality_code,"Sun, 19 Nov 2017 03:33:56 +0000","Wed, 1 Aug 2018 06:22:30 +0000","Mon, 27 Nov 2017 19:27:38 +0000",748422,"As far as I can tell, synchronizing on outer (the context) is correct: I don't know why error-prone flagged synchronized (outer)",0.6375,0.6375,neutral
hbase,19384,comment_1,"any chance you could share the specifics on how the coprocessors are set up in which you see this? Or, if you really want to go the extra mile, distill it down to a generic test case? Assuming there is a bug anyways, having a clear example of what is going wrong would be good both for identifying the problem as well as preventing a regression later :)",test_debt,lack_of_tests,"Thu, 30 Nov 2017 04:43:15 +0000","Wed, 1 Aug 2018 06:20:54 +0000","Tue, 5 Dec 2017 16:08:25 +0000",473110,"rajeshbabu any chance you could share the specifics on how the coprocessors are set up in which you see this? Or, if you really want to go the extra mile, distill it down to a generic test case? Assuming there is a bug anyways, having a clear example of what is going wrong would be good both for identifying the problem as well as preventing a regression later",0.1432857143,0.1207222222,neutral
hbase,19384,comment_3,Sure will write test case for this.  Correct. Yes it's because of removal of complete so we are not able to skip running subsequent coprocessors which do not have any implementation for preAppend or preIncrement hooks.,test_debt,lack_of_tests,"Thu, 30 Nov 2017 04:43:15 +0000","Wed, 1 Aug 2018 06:20:54 +0000","Tue, 5 Dec 2017 16:08:25 +0000",473110,"elserj Rajeshbabu Chintaguntla any chance you could share the specifics on how the coprocessors are set up in which you see this? Or, if you really want to go the extra mile, distill it down to a generic test case? Sure will write test case for this. stack So you have multiple coprocessors stacked on a region. One intercepts preAppend (or preIncrement) to return its own result instead. Are you saying that this result is overwritten by the null that subsequent coprocessors return? Correct. Is the problem our removal of 'complete'? i.e. HBASE-19123 Purge 'complete' support from Coprocesor Observers ? Thanks. Yes it's because of removal of complete so we are not able to skip running subsequent coprocessors which do not have any implementation for preAppend or preIncrement hooks.",0.2103333333,0.1333636364,neutral
hbase,19478,comment_1,"It seems 'Future' is not needed in above sentence. For WALFiles, 'are' should be used - w.r.t. meaning of values in its returned Map, since you have this in the caller: please modify the javadoc to match the actual meaning.",documentation_debt,low_quality_documentation,"Sun, 10 Dec 2017 17:10:15 +0000","Mon, 1 Jan 2018 17:45:29 +0000","Mon, 1 Jan 2018 14:55:59 +0000",1892744,"It seems 'Future' is not needed in above sentence. For WALFiles, 'are' should be used - areWALFilesDeletable(). w.r.t. meaning of values in its returned Map, since you have this in the caller: please modify the javadoc to match the actual meaning.",0.2416666667,0.1611111111,neutral
hbase,19570,comment_6,Looks great  Fix this misspelling on commit. +1 PROJET_PERSONALITY,documentation_debt,low_quality_documentation,"Wed, 20 Dec 2017 21:04:59 +0000","Fri, 1 Feb 2019 20:20:24 +0000","Fri, 22 Dec 2017 01:47:48 +0000",103369,Looks great appy Fix this misspelling on commit. +1 PROJET_PERSONALITY,0.05,0.05,positive
hbase,19570,comment_7,Fixed spelling. Thanks for the review stack. Pushed all the way back till branch-1.1.,documentation_debt,low_quality_documentation,"Wed, 20 Dec 2017 21:04:59 +0000","Fri, 1 Feb 2019 20:20:24 +0000","Fri, 22 Dec 2017 01:47:48 +0000",103369,Fixed spelling. Thanks for the review stack. Pushed all the way back till branch-1.1.,0.1,0.1,positive
hbase,19775,comment_4,"yep, @josh is right about the oneliner style - it's much more ruby-esque. {{cause = cause.getCause if cause.is_a? can we add a test in",test_debt,lack_of_tests,"Thu, 11 Jan 2018 21:27:45 +0000","Wed, 1 Aug 2018 06:23:45 +0000","Fri, 12 Jan 2018 18:02:33 +0000",74088,"yep, @josh is right about the oneliner style - it's much more ruby-esque. cause = cause.getCause if cause.is_a? java.io.UncheckedIOException can we add a test in hbase-shell/src/test/ruby/hbase/table_test.rb?",0.0727,0.0454375,positive
hbase,19862,comment_2,Will cleanup checkstyles on commit. Ping  since you reviewed the related change too.,code_debt,low_quality_code,"Thu, 25 Jan 2018 22:01:27 +0000","Wed, 21 Mar 2018 22:23:45 +0000","Fri, 26 Jan 2018 08:27:33 +0000",37566,Will cleanup checkstyles on commit. Ping zghaobac since you reviewed the related change too.,0.1,0.1,neutral
hbase,19939,comment_1,"The flakey-finder fingered the above commit as breaking the split test. Here is when the split test went bad... Unstable Build #1372 (Feb 5, 2018 3:53:40 PM) add description Build Artifacts Changes HBASE-19703 Functionality added as part of HBASE-12583 is not working (detail) Started by timer This run spent: 3 min 39 sec waiting in the queue; 27 min building on an executor; 31 min total from scheduled to completion. Revision: Test Result (6 failures / +3) See how the commit is ""HBASE-19703 Functionality added as part of HBASE-12583 is not working (detail)""",test_debt,flaky_test,"Mon, 5 Feb 2018 20:07:16 +0000","Wed, 1 Aug 2018 06:22:34 +0000","Tue, 6 Feb 2018 04:53:36 +0000",31580,"The flakey-finder fingered the above commit as breaking the split test. Here is when the split test went bad... Unstable Build #1372 (Feb 5, 2018 3:53:40 PM) add description Build Artifacts Changes HBASE-19703 Functionality added as part of HBASE-12583 is not working (detail) Started by timer This run spent: 3 min 39 sec waiting in the queue; 27 min building on an executor; 31 min total from scheduled to completion. Revision: f0a5f12d97784f609ccd15e1228d424bcab59c41 refs/remotes/origin/branch-2 Test Result (6 failures / +3) org.apache.hadoop.hbase.client.TestAvoidCellReferencesIntoShippedBlocks.org.apache.hadoop.hbase.client.TestAvoidCellReferencesIntoShippedBlocks org.apache.hadoop.hbase.client.TestAvoidCellReferencesIntoShippedBlocks.org.apache.hadoop.hbase.client.TestAvoidCellReferencesIntoShippedBlocks org.apache.hadoop.hbase.client.TestMetaWithReplicas.org.apache.hadoop.hbase.client.TestMetaWithReplicas org.apache.hadoop.hbase.client.TestMetaWithReplicas.org.apache.hadoop.hbase.client.TestMetaWithReplicas org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure.testSplitWithoutPONR org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure.testRecoveryAndDoubleExecution See how the commit is ""HBASE-19703 Functionality added as part of HBASE-12583 is not working (detail)""",-0.1138888889,0.005327868852,negative
hbase,19939,comment_3,"is already in flaky list so the QA didn't run it for HBASE-19703.- -Ok, It is rather than I misunderstood the test name due to the topic...Let me correct the test name for the topic. The correct test name is rather than",test_debt,flaky_test,"Mon, 5 Feb 2018 20:07:16 +0000","Wed, 1 Aug 2018 06:22:34 +0000","Tue, 6 Feb 2018 04:53:36 +0000",31580,"TestSplitTableRegionProcedure is already in flaky list so the QA didn'trun it for HBASE-19703. Ok,It isTestSplitTableRegion rather thanTestSplitTableRegionProcedure I misunderstood the test name due to the topic...Let me correct the test name for the topic. The correct test name isTestSplitTableRegionProcedure rather thanTestSplitTableRegion.",0.6416666667,0.48125,negative
hbase,19939,comment_4,The latest QA in HBASE-19703 is shown below. The is already in flaky.  Do you intent to fix the test totally? I'm +1 to your patch even if the is still flaky.,test_debt,flaky_test,"Mon, 5 Feb 2018 20:07:16 +0000","Wed, 1 Aug 2018 06:22:34 +0000","Tue, 6 Feb 2018 04:53:36 +0000",31580,The flakey-finder fingered the above commit as breaking the split test. The latest QA in HBASE-19703 is shown below. TheTestSplitTableRegionProcedure is already in flaky. uagashe Do you intent to fix the test totally? I'm +1to your patch even if theTestSplitTableRegionProcedure is still flaky.,0.2265,0.2212,negative
hbase,19939,comment_5,Thanks for correcting the name ! Let me fix the commit description as well and resubmit the patch. After this fix if the test is still flaky then I will take another look at it.,test_debt,flaky_test,"Mon, 5 Feb 2018 20:07:16 +0000","Wed, 1 Aug 2018 06:22:34 +0000","Tue, 6 Feb 2018 04:53:36 +0000",31580,Thanks for correcting the name chia7712! Let me fix the commit description as well and resubmit the patch. After this fix if the test is still flaky then I will take another look at it.,0.3655,0.3655,negative
hbase,19969,comment_21,Josh: See HBASE-20123. Looks like we would need hadoop 3.1.0+ (or 3.0.2+) to make backup tests fully working.,architecture_debt,using_obsolete_technology,"Fri, 9 Feb 2018 22:57:42 +0000","Wed, 28 Mar 2018 02:50:38 +0000","Sat, 10 Mar 2018 17:49:55 +0000",2487133,Josh: See HBASE-20123. Looks like we would need hadoop 3.1.0+ (or 3.0.2+) to make backup tests fully working.,0.3,0.3,neutral
hbase,19969,comment_14,Vlad: Can you address checkstyle warnings ?,code_debt,low_quality_code,"Fri, 9 Feb 2018 22:57:42 +0000","Wed, 28 Mar 2018 02:50:38 +0000","Sat, 10 Mar 2018 17:49:55 +0000",2487133,Vlad: Can you address checkstyle warnings ?,-0.6,-0.6,neutral
hbase,19969,comment_2,"Add backup after 'Get' I don't think the above passes checkstyle Do you want to implement in this JIRA ? I don't think the above is right - we use org.slf4j Is the change to public for testing ? Drop commented out code. If we get into the if block, the rename() call below would fail, right ? Drop commented out code. Please address checkstyle, findbugs warnings. Will continue reviewing.",code_debt,low_quality_code,"Fri, 9 Feb 2018 22:57:42 +0000","Wed, 28 Mar 2018 02:50:38 +0000","Sat, 10 Mar 2018 17:49:55 +0000",2487133,"Add backup after 'Get' I don't think the above passes checkstyle Do you want to implement in this JIRA ? I don't think the above is right - we use org.slf4j Is the change to public for testing ? Drop commented out code. If we get into the if block, the rename() call below would fail, right ? Drop commented out code. Please address checkstyle, findbugs warnings. Will continue reviewing.",-0.1689166667,-0.1689166667,neutral
hbase,19969,comment_5,One more comment about the new test : You can store the return value from in a variable. The count would not change in between the log and the assertion. I looped the following tests locally which passed:,design_debt,non-optimal_design,"Fri, 9 Feb 2018 22:57:42 +0000","Wed, 28 Mar 2018 02:50:38 +0000","Sat, 10 Mar 2018 17:49:55 +0000",2487133,One more comment about the new test : You can store the return value from TEST_UTIL.countRows() in a variable. The count would not change in between the log and the assertion. I looped the following tests locally which passed:,0.0,0.0,neutral
hbase,19969,comment_3,"I looked at the calls to where return value is wrapped by Path. It would be cleaner if returns Path. Can you add javadoc to the above method ? It is easier to understand what the two Paths are for. The method throws IOException. If newPath doesn't exist, throwing IOException is better. Unused variable.",documentation_debt,low_quality_documentation,"Fri, 9 Feb 2018 22:57:42 +0000","Wed, 28 Mar 2018 02:50:38 +0000","Sat, 10 Mar 2018 17:49:55 +0000",2487133,"I looked at the calls to HBackupFileSystem.getBackupTmpDirForBackupId where return value is wrapped by Path. It would be cleaner if HBackupFileSystem.getBackupTmpDirForBackupId() returns Path. Can you add javadoc to the above method ? It is easier to understand what the two Paths are for. The method throws IOException. If newPath doesn't exist, throwing IOException is better. Unused variable.",0.1285714286,0.1,neutral
hbase,19969,comment_4,Would be nice to see some javadoc on the methods added to Feels like this should be its own utility method. I think Ted covered all of the other stuff I noticed.,documentation_debt,low_quality_documentation,"Fri, 9 Feb 2018 22:57:42 +0000","Wed, 28 Mar 2018 02:50:38 +0000","Sat, 10 Mar 2018 17:49:55 +0000",2487133,Would be nice to see some javadoc on the methods added to MapReduceBackupMergeJob Feels like this should be its own utility method. I think Ted covered all of the other stuff I noticed.,0.4125,0.4125,neutral
hbase,19969,comment_20,", looks like you might have some flaky tests on Hadoop3. Would be good to take a quick look to rule out test issues (the cnxn refused sounds like it might be just be the node itself). Logs are at",test_debt,flaky_test,"Fri, 9 Feb 2018 22:57:42 +0000","Wed, 28 Mar 2018 02:50:38 +0000","Sat, 10 Mar 2018 17:49:55 +0000",2487133,"vrodionov, looks like you might have some flaky tests on Hadoop3. Would be good to take a quick look to rule out test issues (the cnxn refused sounds like it might be just be the node itself). Logs are at https://builds.apache.org/job/HBase%20Nightly/job/master/259/artifact/output-jdk8-hadoop3/",0.196,0.196,neutral
hbase,19977,comment_1,Yes . Using AtomicInteger will solve the problem. Will test this in a cluster.,test_debt,lack_of_tests,"Mon, 12 Feb 2018 09:08:13 +0000","Wed, 1 Aug 2018 06:21:18 +0000","Tue, 13 Feb 2018 10:22:15 +0000",90842,Yes Apache9. Using AtomicInteger will solve the problem. Will test this in a cluster.,0.03333333333,0.03333333333,positive
hbase,19991,comment_1,"careful debugging revealed a few more places where the jersey deps were leaking in. I also need to add a comment somewhere that upgrading to jersey 2.26 (from our current 2.25.1) will likely need an upgrade to jetty 9.4, so should be done with great care, as that was one of the things i tried here and it didn't work as well as I thought it would",code_debt,low_quality_code,"Tue, 13 Feb 2018 18:22:17 +0000","Wed, 1 Aug 2018 06:23:06 +0000","Wed, 21 Feb 2018 16:52:43 +0000",685826,"careful debugging revealed a few more places where the jersey deps were leaking in. I also need to add a comment somewhere that upgrading to jersey 2.26 (from our current 2.25.1) will likely need an upgrade to jetty 9.4, so should be done with great care, as that was one of the things i tried here and it didn't work as well as I thought it would",0.3556,0.3556,neutral
hbase,19998,comment_4,Could we add some commit message to remind readers that the commit is for debug? It can help readers to realize that the flaky hasn't been fixed.,test_debt,flaky_test,"Wed, 14 Feb 2018 06:29:48 +0000","Tue, 7 May 2019 16:08:44 +0000","Fri, 16 Feb 2018 21:00:02 +0000",225014,Could we add some commit message to remind readers that the commit is for debug? It can help readers to realize that the flaky hasn't been fixed.,0.3,0.3,neutral
hbase,19998,comment_7,"The test failed in flakies a few times last night, lets see how it does.",test_debt,flaky_test,"Wed, 14 Feb 2018 06:29:48 +0000","Tue, 7 May 2019 16:08:44 +0000","Fri, 16 Feb 2018 21:00:02 +0000",225014,"The test failed in flakies a few times last night, https://builds.apache.org/job/HBASE-Flaky-Tests-branch2.0/2048/artifact/hbase-server/target/surefire-reports/org.apache.hadoop.hbase.security.visibility.TestVisibilityLabelsWithCustomVisLabService-output.txt, lets see how it does.",-0.4,-0.4,neutral
hbase,20100,comment_4,Linked to a follow-on issue. The AssignProcedure is too coarsely grained (feedback from  that makes sense). HBASE-20103 is about breaking it up to do finer steps. Will do in follow-on.,design_debt,non-optimal_design,"Tue, 27 Feb 2018 19:59:19 +0000","Tue, 7 May 2019 16:08:43 +0000","Thu, 1 Mar 2018 16:55:27 +0000",161768,Linked to a follow-on issue. The AssignProcedure is too coarsely grained (feedback from Apache9 that makes sense). HBASE-20103 is about breaking it up to do finer steps. Will do in follow-on.,-0.40625,-0.40625,neutral
hbase,20108,comment_5,"looks like jline shows up in the modules now. the rest of the approach looks really good, though! I assume this works both in dev tree and out of a bin tarball?",design_debt,non-optimal_design,"Wed, 28 Feb 2018 22:01:52 +0000","Wed, 1 Aug 2018 06:23:17 +0000","Wed, 7 Mar 2018 21:01:46 +0000",601194,"looks like jline shows up in the shaded-client/mapreduce modules now. the rest of the approach looks really good, though! I assume this works both in dev tree and out of a bin tarball?",0.4586666667,0.4586666667,positive
hbase,20595,comment_7,Updated master patch removes the unused imports that checkstyle complained about.,code_debt,low_quality_code,"Wed, 16 May 2018 21:37:08 +0000","Mon, 22 Apr 2019 16:10:14 +0000","Wed, 23 May 2018 18:58:44 +0000",595296,Updated master patch removes the unused imports that checkstyle complained about.,0.05,0.05,neutral
hbase,20595,comment_0,"The issue I was thinking of was HBASE-20500 which maintains one server in the 'default' group, but that is not the full scope of what we should have. We should guarantee the placement, specifically, of ""special tables"" into a rsgroup that must always have a nonzero number of servers, and not assume that will be the 'default' group. In fact I think we should have two default rsgroups, very similar to how we do namespacing: a ""default"" group into which goes all user level stuff not otherwise specified; and a system group into which goes system/special tables (in namespace terms, akin to the 'hbase' namespace). Special tables should not be allowed to move into rsgroups for user tables.",design_debt,non-optimal_design,"Wed, 16 May 2018 21:37:08 +0000","Mon, 22 Apr 2019 16:10:14 +0000","Wed, 23 May 2018 18:58:44 +0000",595296,"The issue I was thinking of was HBASE-20500 which maintains one server in the 'default' group, but that is not the full scope of what we should have. We should guarantee the placement, specifically, of ""special tables"" into a rsgroup that must always have a nonzero number of servers, and not assume that will be the 'default' group. In fact I think we should have two default rsgroups, very similar to how we do namespacing: a ""default"" group into which goes all user level stuff not otherwise specified; and a system group into which goes system/special tables (in namespace terms, akin to the 'hbase' namespace). Special tables should not be allowed to move into rsgroups for user tables.",-0.01875,-0.01875,neutral
hbase,20595,comment_1,"that makes sense. is the ""system group"" treated like a normal rsgroup in terms of exclusivity? If so, how do we handle it and the user group needing to be distinct? if we just delegate it to manual operator config that means we can't have the rsgroup on by default ever right? we'll probably need to document ""can't turn on rsgroup feature"" as a limitation of single-node deployments.",documentation_debt,outdated_documentation,"Wed, 16 May 2018 21:37:08 +0000","Mon, 22 Apr 2019 16:10:14 +0000","Wed, 23 May 2018 18:58:44 +0000",595296,"that makes sense. is the ""system group"" treated like a normal rsgroup in terms of exclusivity? If so, how do we handle it and the user group needing to be distinct? if we just delegate it to manual operator config that means we can't have the rsgroup on by default ever right? we'll probably need to document ""can't turn on rsgroup feature"" as a limitation of single-node deployments.",-0.0427,-0.0427,neutral
hbase,20693,comment_1,"This patch takes care of point 1. Point 2 is optional. I think we should do it as its always a good idea to break up jsp s into header/footers. Also as the rest/thrift UIs grow more complex, we will have to do it someday. Keeping it open for now, will do it if asked for.",design_debt,non-optimal_design,"Wed, 6 Jun 2018 20:20:51 +0000","Wed, 8 Jan 2025 12:30:17 +0000","Fri, 27 Sep 2024 12:55:43 +0000",199125292,"This patch takes care of point 1. Point 2 is optional. I think we should do it as its always a good idea to break up jsp s into header/footers. Also as the rest/thrift UIs grow more complex, we will have to do it someday. Keeping it open for now, will do it if asked for.",0.2352,0.2352,neutral
hbase,20693,comment_4,"Attached which refactors {{rest.jsp}} and {{thrift.jsp}} and extracts header and footer out of them. Also, fixed following typo in thrift.jsp. Please review. Ping , , .",documentation_debt,low_quality_documentation,"Wed, 6 Jun 2018 20:20:51 +0000","Wed, 8 Jan 2025 12:30:17 +0000","Fri, 27 Sep 2024 12:55:43 +0000",199125292,"AttachedHBASE-20693.master.001.patchwhich refactors rest.jsp and thrift.jsp and extracts header and footer out of them. Also, fixed following typo in thrift.jsp. Please review. Ping tianjingyun, zghaobac, stack.",0.02857142857,0.07,neutral
hbase,20787,comment_0,I will also remove the various commits/reverts of the initial patch to simplify things.,code_debt,low_quality_code,"Tue, 26 Jun 2018 00:11:57 +0000","Tue, 10 Jul 2018 04:55:21 +0000","Tue, 10 Jul 2018 04:55:21 +0000",1226604,I will also remove the various commits/reverts of the initial patch to simplify things.,0.2,0.2,neutral
hbase,20975,comment_0,"+1 on removing it for now. We have optimized too much before getting things correct... Let's keep the logic simple first. Also, there are some bad style issues. At least let's remove the space between 'stackTail' and '--', it looks like '-- And do not do assignment in the condition block of if. Let's change to",code_debt,low_quality_code,"Mon, 30 Jul 2018 07:56:06 +0000","Tue, 14 Aug 2018 11:44:47 +0000","Mon, 13 Aug 2018 12:24:30 +0000",1225704,"+1 on removing it for now. We have optimized too much before getting things correct... Let's keep the logic simple first. Also, there are some bad style issues. At least let's remove the space between 'stackTail' and '-', it looks like '->' is the operator... And do not do assignment in the condition block of if. Let's change to",-0.0425,-0.0425,negative
hbase,20975,comment_8,"The failed UTs are related, upload a patch to fix it. My fix reveals a bug which exists all the time... the acquire and release lock logic in procedure is a bit of mess here... For different procedures, we have holdLock() set to true of false, for procedures which holdLock() == true, we need to override hasLock() method to test whether we have the lock... when executing procedure, we acquire the lock and release it immediately. And when the procedure is success, we release the lock again in It works, since for procedure with holdLock = true, we call releaseLock() with force = false the first time, the lock won't release here, but in where we call releaseLock() with force = true, the lock will be released there. For procedure with holdLock = false, it also works since when the first time we call releaseLock() with force = false, the lock will release, and the second time we call relseaseLock in we only release it lock if procedure with holdLock=true. So releasing lock won't called twice But when rolling back, we acquire the lock but only release the lock for procedure with holdLock = true... *Rolling back for procedures with holdLock=false(e.g. most of the Table Procedures) will never release the lock...*",design_debt,non-optimal_design,"Mon, 30 Jul 2018 07:56:06 +0000","Tue, 14 Aug 2018 11:44:47 +0000","Mon, 13 Aug 2018 12:24:30 +0000",1225704,"The failed UTs are related, upload a patch to fix it. My fix reveals a bug which exists all the time... the acquire and release lock logic in procedure is a bit of mess here... For different procedures, we have holdLock() set to true of false, for procedures which holdLock() == true, we need to override hasLock() method to test whether we have the lock... when executing procedure, we acquire the lock and release it immediately. And when the procedure is success, we release the lock again in execCompletionCleanup()... It works, since for procedure with holdLock = true, we call releaseLock() with force = false the first time, the lock won't release here, but in execCompletionCleanup, where we call releaseLock() with force = true, the lock will be released there. For procedure with holdLock = false, it also works since when the first time we call releaseLock() with force = false, the lock will release, and the second time we call relseaseLock in execCompletionCleanup(), we only release it lock if procedure with holdLock=true. So releasing lock won't called twice But when rolling back, we acquire the lock but only release the lock for procedure with holdLock = true... Rolling back for procedures with holdLock=false(e.g. most of the Table Procedures) will never release the lock...",0.08738611111,0.1048633333,negative
hbase,20975,comment_4,Please include a small change in hbase-server module so we can run more tests?,test_debt,low_coverage,"Mon, 30 Jul 2018 07:56:06 +0000","Tue, 14 Aug 2018 11:44:47 +0000","Mon, 13 Aug 2018 12:24:30 +0000",1225704,Please include a small change in hbase-server module so we can run more tests?,0.2,0.2,negative
hbase,20990,comment_1,"Then you need to record the exceptions in the memory and send them back to master when reporting. The sync RPC call become a async one, what if the RS restarts before sending this info. The procedure in master even don't know whether the open/close procedure is executing, whether a RPC retry is needed.",design_debt,non-optimal_design,"Wed, 1 Aug 2018 03:40:40 +0000","Thu, 21 Nov 2019 13:59:28 +0000","Thu, 21 Nov 2019 13:59:28 +0000",41249928,"I prefer not returning anything when calling executeProcedure, instead, using reportRegionTransition and reportProcedureResult to send back the response... Then you need to record the exceptions in the memory and send them back to master when reporting. The sync RPC call become a async one, what if the RS restarts before sending this info. The procedure in master even don't know whether the open/close procedure is executing, whether a RPC retry is needed.",0.5,0.5,neutral
hbase,21160,comment_1,"Since the catch block currently re-throws IOException, that means the catch block is no longer needed. Please run with the change locally before attaching patch. Thanks",design_debt,non-optimal_design,"Thu, 6 Sep 2018 10:56:25 +0000","Tue, 18 Sep 2018 13:34:47 +0000","Mon, 17 Sep 2018 15:25:42 +0000",966557,"Since the catch block currently re-throws IOException, that means the catch block is no longer needed. Please run with the change locally before attaching patch. Thanks",0.1333333333,0.1333333333,neutral
hbase,21160,comment_2,Hi I found so many re-throws blocks in the file of . Should we resolve it all?,design_debt,non-optimal_design,"Thu, 6 Sep 2018 10:56:25 +0000","Tue, 18 Sep 2018 13:34:47 +0000","Mon, 17 Sep 2018 15:25:42 +0000",966557,Hi yuzhihong@gmail.com I found so many re-throws blocks in the file of TestVisibilityLabelsWithDeletes.java . Should we resolve it all?,0.1,0.06666666667,neutral
hbase,21173,comment_1,I think the intention of HBASE-21138 is to let do the cleanup. Can you remove the duplicate region.close() call in these subtests ? Thanks,code_debt,duplicated_code,"Sat, 8 Sep 2018 14:28:39 +0000","Fri, 1 Feb 2019 20:12:31 +0000","Tue, 11 Sep 2018 09:52:38 +0000",242639,I think the intention of HBASE-21138 is to let HBaseTestingUtility.closeRegionAndWAL do the cleanup. Can you remove the duplicate region.close() call in these subtests ? Thanks,0.1,0.08,neutral
hbase,21173,comment_4,"Attach 002 patch as  suggestions.Thanks {{testSequenceId}} - In line 269, Replace region.close() with - In line 285, we need to verify that the value of is consistent before and after region.close(), so we keep region.close() and replace it with - In line 315, replace region.close() with - In line 317, remove duplicate and set this.region to null - In line 578, replace region.close() with and set this.region to null - In line 951, replace region.close() with - In line 1083, replace region.close() with - In line 1281, set this.region to null - In line 1281, set this.region to null - In line 4175, keep region.close() and set region to null as said by Mingliang Liu - In line 6234, remove region.close() Other places where set this.region null value after will be fine as said by .",code_debt,duplicated_code,"Sat, 8 Sep 2018 14:28:39 +0000","Fri, 1 Feb 2019 20:12:31 +0000","Tue, 11 Sep 2018 09:52:38 +0000",242639,"Attach 002 patch as yuzhihong@gmail.com liuml07 suggestions.Thanks testSequenceId In line 269, Replace region.close() with HBaseTestingUtility.closeRegionAndWAL(region) In line 285, we need to verify that the value of region.getMaxFlushedSeqId() is consistent before and after region.close(), so we keep region.close() and replace it with HBaseTestingUtility.closeRegionAndWAL(region). testCloseCarryingSnapshot In line 315, replace region.close() with HBaseTestingUtility.closeRegionAndWAL(region) In line 317, remove duplicate HBaseTestingUtility.closeRegionAndWAL(region) and set this.region to null testCloseWithFailingFlush In line 578, replace region.close() with HBaseTestingUtility.closeRegionAndWAL(region) and set this.region to null testRecoveredEditsReplayCompaction In line 951, replace region.close() with HBaseTestingUtility.closeRegionAndWAL(region) testFlushMarkers In line 1083, replace region.close() with HBaseTestingUtility.closeRegionAndWAL(region) testGetWhileRegionClose In line 1281, set this.region to null testBatchPut_whileMultipleRowLocksHeld In line 1281, set this.region to null testRegionInfoFileCreation In line 4175, keep region.close() and set region to null as said by Mingliang Liu testBulkLoadReplicationEnabled In line 6234, remove region.close() Other places to set this.region null value after HTU.closeRegionAndWAL() is good to explicitly make the HTU.closeRegionAndWAL() in tearDown a no-op. Other places where set this.region null value after HTU.closeRegionAndWAL(), will be fine as said by liuml07.",0.05,0.05253333333,neutral
hbase,21173,comment_2,"Thanks for working on this JIRA, . In previous discussion, I thought calling on closed/null region is no harm, while deleting the duplicate close may make some tests unhappy. So we were not very strict to make region close only once. Good discussion to revisit this. - The last one in was added after HBASE-21138 and can be removed here. - The in was followed by the assertion to verify that the .regioninfo file is still there. I saw the close-and-assertion happens in the same test method multiple times so I was not sure we could remove the close here. - In {{testSequenceId}} and the pattern in this patch, i.e. ""{{region.close() && region = null}}"", is not correct. The reason is that, it makes the in {{teardown()}} a no-op, leaving WAL not closed. One fix is to not set the null value and leave the test as-is; a better one I think is as suggested, we can replace the {{region.close()}} with and set {{this.region}} null value. - Other places to set {{this.region}} null value after is good to explicitly make the in {{tearDown}} a no-op.",design_debt,non-optimal_design,"Sat, 8 Sep 2018 14:28:39 +0000","Fri, 1 Feb 2019 20:12:31 +0000","Tue, 11 Sep 2018 09:52:38 +0000",242639,"Thanks for working on this JIRA, andrewcheng. In previous discussion, I thought calling HTU.closeRegionAndWAL() on closed/null region is no harm, while deleting the duplicate close may make some tests unhappy. So we were not very strict to make region close only once. Good discussion to revisit this. The last one in testBulkLoadReplicationEnabled() was added after HBASE-21138 and can be removed here. The HTU.closeRegionAndWAL() in testRegionInfoFileCreation() was followed by the assertion to verify that the .regioninfo file is still there. I saw the close-and-assertion happens in the same test method multiple times so I was not sure we could remove the close here. In testSequenceId and testCloseCarryingSnapshot, the pattern in this patch, i.e. ""region.close() && region = null"", is not correct. The reason is that, it makes the HTU.closeRegionAndWAL() in teardown() a no-op, leaving WAL not closed. One fix is to not set the null value and leave the test as-is; a better one I think is as yuzhihong@gmail.com suggested, we can replace the region.close() with HTU.closeRegionAndWAL(), and set this.region null value. Other places to set this.region null value after HTU.closeRegionAndWAL() is good to explicitly make the HTU.closeRegionAndWAL() in tearDown a no-op.",0.1465,0.07990909091,neutral
hbase,21589,comment_2,"It is very strange, it never failed in my environment. , can you upload an output or something, I can't find the failing test in jenkins or Flaky test board.",test_debt,flaky_test,"Wed, 12 Dec 2018 19:12:09 +0000","Mon, 4 Mar 2019 08:58:21 +0000","Mon, 17 Dec 2018 17:32:49 +0000",426040,"It is very strange, it never failed in my environment. stack, can you upload an output or something, I can't find the failing test in jenkins PreCommit-HBASE-Build or Flaky test board.",0.2,0.25,negative
hbase,21599,comment_3,Thanks Ankit! Let me rebase and fix the checkstyle warnings.,code_debt,low_quality_code,"Thu, 13 Dec 2018 22:05:35 +0000","Thu, 3 Jan 2019 18:09:05 +0000","Thu, 3 Jan 2019 18:09:05 +0000",1800210,Thanks Ankit! Let me rebase and fix the checkstyle warnings.,-0.1,-0.1,positive
hbase,21678,comment_1,Test failures are unrelated. The checkstyle and whitespace issues reported are related. I didn't change formatting during the backport but can do so to make these tools happier. Let me do that,code_debt,low_quality_code,"Fri, 4 Jan 2019 21:19:11 +0000","Tue, 22 Sep 2020 07:36:25 +0000","Thu, 24 Jan 2019 17:32:06 +0000",1714375,Test failures are unrelated. The checkstyle and whitespace issues reported are related. I didn't change formatting during the backport but can do so to make these tools happier. Let me do that,0.025,0.025,negative
hbase,21678,comment_2,Adjusted some formatting in StoreFile. Let's see if that is better,code_debt,low_quality_code,"Fri, 4 Jan 2019 21:19:11 +0000","Tue, 22 Sep 2020 07:36:25 +0000","Thu, 24 Jan 2019 17:32:06 +0000",1714375,Adjusted some formatting in StoreFile. Let's see if that is better,0.25,0.25,negative
hbase,21678,comment_5,Updated patch address checkstyle warnings and unit test failure,code_debt,low_quality_code,"Fri, 4 Jan 2019 21:19:11 +0000","Tue, 22 Sep 2020 07:36:25 +0000","Thu, 24 Jan 2019 17:32:06 +0000",1714375,Updated patch address checkstyle warnings and unit test failure,-0.5,-0.5,negative
hbase,21678,comment_7,"Sigh, new checkstyle nits now that previous attempt is touching more files just to clean up checkstyle nits",code_debt,low_quality_code,"Fri, 4 Jan 2019 21:19:11 +0000","Tue, 22 Sep 2020 07:36:25 +0000","Thu, 24 Jan 2019 17:32:06 +0000",1714375,"Sigh, new checkstyle nits now that previous attempt is touching more files just to clean up checkstyle nits",0.0,0.0,negative
hbase,21678,comment_8,Otherwise change looks good. I can fix the remaining checkstyle warns on commit. Would be good to get a +1 if you have any spare time to make a quick check   or original author,code_debt,low_quality_code,"Fri, 4 Jan 2019 21:19:11 +0000","Tue, 22 Sep 2020 07:36:25 +0000","Thu, 24 Jan 2019 17:32:06 +0000",1714375,Otherwise change looks good. I can fix the remaining checkstyle warns on commit. Would be good to get a +1 if you have any spare time to make a quick check lhofhansl abhishek.chouhan or original author andrewcheng,0.484,0.363,negative
hbase,22017,comment_7,Removing the whitespace,code_debt,low_quality_code,"Fri, 8 Mar 2019 12:41:07 +0000","Wed, 2 Oct 2019 17:13:51 +0000","Wed, 13 Mar 2019 07:04:24 +0000",411797,Removing thewhitespace,0.0,0.0,neutral
hbase,22034,comment_3,"Great, I munged some files so now I own their checkstyle and javadoc problems. :-/",code_debt,low_quality_code,"Mon, 11 Mar 2019 19:15:33 +0000","Fri, 6 Sep 2019 00:32:11 +0000","Fri, 22 Mar 2019 00:28:40 +0000",882787,"Great, I munged some files so now I own their checkstyle and javadoc problems. :-/",-0.15,-0.15,negative
hbase,22034,comment_4,Updated patch for checkstyle and javadoc warnings,code_debt,low_quality_code,"Mon, 11 Mar 2019 19:15:33 +0000","Fri, 6 Sep 2019 00:32:11 +0000","Fri, 22 Mar 2019 00:28:40 +0000",882787,Updated patch for checkstyle and javadoc warnings,-0.6,-0.6,negative
hbase,22034,comment_6,Unit test failure is not related as far as I can tell. Does not reproduce locally. It looks like a flake in the precommit env. I can fix the one remaining checkstyle nit upon commit. (Indentation at,test_debt,flaky_test,"Mon, 11 Mar 2019 19:15:33 +0000","Fri, 6 Sep 2019 00:32:11 +0000","Fri, 22 Mar 2019 00:28:40 +0000",882787,Unit test failure is not related as far as I can tell. Does not reproduce locally. It looks like a flake in the precommit env. I can fix the one remaining checkstyle nit upon commit. (Indentation at TestKeyValue.java:573),-0.04,-0.03333333333,negative
hbase,22193,comment_1,"Talked with  offline, the problem here is not the retry number, but the retry interval. When a region is failed open, we will try to reassign it ASAP, the intention here is to make the region online soon. But sometimes, the region can not online on any RS because of config error or some other problems, then it is not a good idea to retry immediately as it will lead to so many proc wals... So the first thing is to detect this problem and increase the retry interval... And for a long term solution, I think we need to find out a way to better deal with config error. For now, the will hang there forever and the only way is to use HBCK2 to bypass the procedure and fix the table state, which is a bit difficult. For hbase version before 2.0, I think there is a straight forward way to fix this is to disable the table, fix the schema, and enable it again...",design_debt,non-optimal_design,"Tue, 9 Apr 2019 03:21:48 +0000","Tue, 30 Apr 2019 13:15:19 +0000","Sat, 13 Apr 2019 03:17:46 +0000",345358,"Talked with zghaobac offline, the problem here is not the retry number, but the retry interval. When a region is failed open, we will try to reassign it ASAP, the intention here is to make the region online soon. But sometimes, the region can not online on any RS because of config error or some other problems, then it is not a good idea to retry immediately as it will lead to so many proc wals... So the first thing is to detect this problem and increase the retry interval... And for a long term solution, I think we need to find out a way to better deal with config error. For now, the ModifyTableProcedure will hang there forever and the only way is to use HBCK2 to bypass the procedure and fix the table state, which is a bit difficult. For hbase version before 2.0, I think there is a straight forward way to fix this is to disable the table, fix the schema, and enable it again...",-0.2019,-0.2019,neutral
hbase,22225,comment_1,"After checking our there're some prerequisites to use this profiler servlet. However, not all users would check the book before clicking the button, so maybe a more comprehensive error message plus a link to the refguide is a better idea? Anyway I don't think this should be marked as a blocker any more.",documentation_debt,low_quality_documentation,"Fri, 12 Apr 2019 13:05:16 +0000","Mon, 7 Oct 2019 18:39:35 +0000","Mon, 29 Apr 2019 20:42:55 +0000",1496259,"After checking our refguide, there're some prerequisites to use this profiler servlet. However, not all users would check the book before clicking the button, so maybe a more comprehensive error message plus a link to the refguide is a better idea? Anyway I don't think this should be marked as a blocker any more.",0.1073333333,0.1073333333,negative
hbase,22378,comment_14,This commit landed on all branches but Fix versions were set to None. I checked the branches and release dates and set it accordingly. The release notes for the affected releases are most probably incorrect.,documentation_debt,outdated_documentation,"Wed, 8 May 2019 00:33:51 +0000","Thu, 23 Jan 2020 14:10:35 +0000","Mon, 13 May 2019 19:47:02 +0000",501191,This commit landed on all branches but Fix versions were set to None. I checked the branches and release dates and set it accordingly. The release notes for the affected releases are most probably incorrect.,0.0,0.0,negative
hbase,22400,comment_1,Please ignore my RR. I was just fed up with the ugly adapter code and can't wait to remove them. :) Please continue.,design_debt,non-optimal_design,"Sun, 12 May 2019 07:16:29 +0000","Wed, 2 Oct 2019 17:12:32 +0000","Sat, 18 May 2019 06:14:38 +0000",514689,Please ignore my RR. I was just fed up with the ugly adapter code and can't wait to remove them. Please continue.,-0.1083333333,-0.1416666667,negative
hbase,22424,comment_1,"nice findings. Just curious, have you ran the flaky tests multiple times (such as 10) and all are passing?",test_debt,flaky_test,"Wed, 15 May 2019 08:47:37 +0000","Fri, 17 May 2019 19:59:26 +0000","Fri, 17 May 2019 01:18:34 +0000",145857,"nice findings. Just curious, have you ran the flaky tests multiple times (such as 10) and all are passing?",0.5125,0.5125,positive
hbase,22656,comment_1,+1 (non-binding) Nice catch. The two method and are never used.,code_debt,dead_code,"Thu, 4 Jul 2019 03:44:46 +0000","Wed, 14 Aug 2019 02:28:55 +0000","Sun, 7 Jul 2019 15:35:11 +0000",301825,+1 (non-binding) Nice catch. The two method RegionServerTableMetrics::updatePutBatch() and RegionServerTableMetrics::updateDeleteBatch() are never used.,0.4125,0.4125,positive
hbase,22707,comment_0,"Interesting. We need something like this. I like that you hook it into assigns. I'm wary though of re-use of joinCluster. The messaging in logs will look strange. Will say stuff like 'Joining cluster...' and 'Waiting for RegionServers to join;....'. Then we re-add chores, do the unnecessary wait on RS. Should we just add a method that gets the new row in hbase:meta and then does what call does?",design_debt,non-optimal_design,"Wed, 17 Jul 2019 17:04:59 +0000","Tue, 7 Apr 2020 22:45:30 +0000","Fri, 2 Aug 2019 10:47:20 +0000",1359741,"Interesting. We need something like this. I like that you hook it into assigns. I'm wary though of re-use of joinCluster. The messaging in logs will look strange. Will say stuff like 'Joining cluster...' and 'Waiting for RegionServers to join;....'. Then we re-add chores, do the unnecessary wait on RS. Should we just add a method that gets the new row in hbase:meta and then does what loadMeta#visitRegionState call does?",0.1055555556,0.1055555556,neutral
hbase,22707,comment_1,"Yeah, using _joinCluster_ was a bit too much of an attempt of reusing existing code without changing it as much as possible, but I agree with all the side effects you had pointed out, . Had done some refactoring to re-use some of the adding a with the changes. Let me know what you think, if you feel this PR approach is fine, I will work on some UTs for it.",design_debt,non-optimal_design,"Wed, 17 Jul 2019 17:04:59 +0000","Tue, 7 Apr 2020 22:45:30 +0000","Fri, 2 Aug 2019 10:47:20 +0000",1359741,"Yeah, usingjoinClusterwas a bit too much of an attempt of reusing existing code without changing it as much as possible, but I agree with all the side effects you had pointed out, stack. Had done some refactoring to re-use some of the loadMeta#visitRegionState, adding aPR with the changes. Let me know what you think, if you feel this PR approach is fine, I will work on some UTs for it.",0.1166666667,0.1166666667,neutral
hbase,23651,comment_3,Please update the Release Note. Thanks.,documentation_debt,outdated_documentation,"Mon, 6 Jan 2020 07:51:00 +0000","Fri, 10 Jan 2020 09:21:20 +0000","Wed, 8 Jan 2020 11:08:04 +0000",184624,binlijinPlease update the Release Note. Thanks.,0.3,0.3,neutral
hbase,23752,comment_2,"Mind taking a look at this? This is the last (hopefully) patch that fixes all the test failures from the full nightly run. I verified that the two failed tests are flakes, ran them locally.",test_debt,flaky_test,"Tue, 28 Jan 2020 20:59:07 +0000","Wed, 8 Apr 2020 22:08:15 +0000","Mon, 3 Feb 2020 18:18:29 +0000",508762,"stack/andrew.purtell@gmail.com Mind taking a look at this? This is the last (hopefully) patch that fixes all the test failures from the full nightly run. I verified that the two failed tests are flakes, ran them locally.",0.35,0.35,neutral
hbase,23792,comment_0,"I can't repro this locally or find a source for the filesystem implementation getting flipped to a distributed fs. However, the only place in Hadoop code where I see this ""Wrong FS"" message thrown as an is in Looking closer at the xml report, I see that the test failed once with the above. Surefire tried to re-run it, but it failed the rerun with which implies to me that when surefire reruns a test method, it does not run the BeforeClass business. I also notice that the test method runs the same code twice, but both times it's using I think one of the invocations is supposed to be calling So. # Survive flakey rerunning by converting the static {{BeforeClass}} stuff into instance-level {{Before}}. # Break the test method into two, one for running over each of the snapshot manifest versions.",test_debt,flaky_test,"Tue, 4 Feb 2020 16:51:38 +0000","Fri, 31 Dec 2021 14:06:28 +0000","Fri, 31 Dec 2021 14:06:07 +0000",60124469,"I can't repro this locally or find a source for the filesystem implementation getting flipped to a distributed fs. However, the only place in Hadoop code where I see this ""Wrong FS"" message thrown as an IllegalArgumentException is in FileSystem#checkPath. Looking closer at the xml report, I see that the test failed once with the above. Surefire tried to re-run it, but it failed the rerun with which implies to me that when surefire reruns a test method, it does not run the BeforeClass business. I also notice that the test method runs the same code twice, but both times it's using createSnapshotV2... I think one of the invocations is supposed to be calling createSnapshotV1. So. Survive flakey rerunning by converting the static BeforeClass stuff into instance-level Before. Break the test method into two, one for running over each of the snapshot manifest versions.",-0.2666666667,-0.2138888889,negative
hbase,23825,comment_5,Ah... We considered BoundedByteString case there but did not do a check whether that is subclass of LiteralByteString BTW why no 1.5.1 fix version when it says affected at 1.5.0?,test_debt,low_coverage,"Mon, 10 Feb 2020 22:14:22 +0000","Wed, 12 Feb 2020 23:53:24 +0000","Wed, 12 Feb 2020 01:18:51 +0000",97469,Ah... We considered BoundedByteString case there but did not do a check whether that is subclass of LiteralByteString BTW why no 1.5.1 fix version when it says affected at 1.5.0?,0.35,0.35,negative
impala,72,comment_0,Hm - we have a test that tests almost exactly this case: We had better check that the test is running as expected.,test_debt,lack_of_tests,"Tue, 19 Feb 2013 22:18:38 +0000","Mon, 4 Mar 2013 17:27:09 +0000","Mon, 4 Mar 2013 17:27:09 +0000",1105711,Hm - we have a test that tests almost exactly this case: We had better check that the test is running as expected.,0.5,0.5,neutral
impala,74,comment_0,"Just to let everyone know what happened here: -nn_port and -nn are still around, but if -nn is not specified (and it defaults to blank), Impala will try to read the value of {{fs.defaultFS}} (and if that doesn't exist, from the frontend which has loaded a Hadoop configuration. We will consider removing -nn and -nn_port if this proves to be robust enough. This change will be in Impala 0.6.",design_debt,non-optimal_design,"Wed, 20 Feb 2013 00:16:40 +0000","Sun, 20 Dec 2015 00:04:57 +0000","Wed, 20 Feb 2013 02:18:11 +0000",7291,"Just to let everyone know what happened here: -nn_port and -nn are still around, but if -nn is not specified (and it defaults to blank), Impala will try to read the value of fs.defaultFS (and if that doesn't exist, fs.default.name) from the frontend which has loaded a Hadoop configuration. We will consider removing -nn and -nn_port if this proves to be robust enough. This change will be in Impala 0.6.",0.18275,0.0385,neutral
impala,187,comment_0,The way joins are performed right now in Impala (broadcast of right-hand side input) is a limitation that will be addressed by adding more join strategies (and having the planner chose between them). It's not a good idea to address that via additional syntax that otherwise doesn't really make sense.,design_debt,non-optimal_design,"Wed, 3 Apr 2013 00:06:53 +0000","Wed, 3 Apr 2013 00:13:59 +0000","Wed, 3 Apr 2013 00:13:59 +0000",426,The way joins are performed right now in Impala (broadcast of right-hand side input) is a limitation that will be addressed by adding more join strategies (and having the planner chose between them). It's not a good idea to address that via additional syntax that otherwise doesn't really make sense.,-0.2326,-0.2326,negative
impala,211,comment_2,"I think it is useful to log it once, but logging it for each impalad every 1/2 second seems excessive.",design_debt,non-optimal_design,"Tue, 9 Apr 2013 16:16:36 +0000","Thu, 11 Apr 2013 06:36:45 +0000","Thu, 11 Apr 2013 06:36:45 +0000",138009,"I think it is useful to log it once, but logging it for each impalad every 1/2 second seems excessive.",0.5,0.5,neutral
impala,231,comment_0,"We have to mark local var using ""DeleteLocalRef"" when the local var is done. This will help reduce mem pressure.",code_debt,low_quality_code,"Thu, 11 Apr 2013 20:43:39 +0000","Wed, 17 Apr 2013 20:28:22 +0000","Wed, 17 Apr 2013 20:28:12 +0000",517473,"We have to mark local var using ""DeleteLocalRef"" when the local var is done. This will help reduce mem pressure.",0.05,0.05,neutral
impala,596,comment_0,I actually do not think the leak is related to the HDFS close error now that I look at the log timestamps.,code_debt,low_quality_code,"Thu, 19 Sep 2013 22:07:11 +0000","Thu, 12 Feb 2015 09:21:39 +0000","Thu, 19 Sep 2013 22:23:33 +0000",982,I actually do not think the leak is related to the HDFS close error now that I look at the log timestamps.,0.3,0.3,negative
impala,605,comment_0,"0 means default and default value is 1k. Impala already adjust the hbase_caching according to the estimated stats. But if there are a few outliner that are HUGE, then the hbase_caching value will still be too big.",design_debt,non-optimal_design,"Thu, 26 Sep 2013 00:20:37 +0000","Wed, 9 May 2018 18:29:49 +0000","Wed, 9 May 2018 18:29:49 +0000",145735752,"0 means default and default value is 1k. Impala already adjust the hbase_caching according to the estimated stats. But if there are a few outliner that are HUGE, then the hbase_caching value will still be too big.",0.08016666667,0.08016666667,neutral
impala,636,comment_0,"When we process topic deletion, we just remove an entry from the backend list without removing the entry from backend_map_. We should remove the entry from backend_map_ if the list is empty.",code_debt,low_quality_code,"Thu, 24 Oct 2013 02:56:18 +0000","Sun, 20 Dec 2015 00:05:08 +0000","Fri, 15 Nov 2013 23:12:07 +0000",1973749,"When we process topic deletion, we just remove an entry from the backend list without removing the entry from backend_map_. We should remove the entry from backend_map_ if the list is empty.",-0.1,-0.1,neutral
impala,946,comment_0,"I think this is a duplicate of IMPALA-428. The time is being spent loading the table metadata from the Hive Metastore. I believe most of the time is spent loading column stats, but I need to re-run experiments to confirm.",code_debt,slow_algorithm,"Fri, 11 Apr 2014 22:34:38 +0000","Fri, 11 Apr 2014 23:43:29 +0000","Fri, 11 Apr 2014 22:55:24 +0000",1246,"I think this is a duplicate of IMPALA-428. The time is being spent loading the table metadata from the Hive Metastore. I believe most of the time is spent loading column stats, but I need to re-run experiments to confirm.",0.0,0.0,neutral
impala,991,comment_2,"It means I will have to state an exception in the docs though. You can cast BOOLEAN to other numeric types but not DECIMAL: [localhost:21000] | cast(true as int) | | 1 | [localhost:21000] | cast(true as float) | | 1 | [localhost:21000] ERROR: AnalysisException: Invalid type cast of TRUE from BOOLEAN to DECIMAL(9,0)",documentation_debt,low_quality_documentation,"Fri, 9 May 2014 18:10:23 +0000","Tue, 2 Sep 2014 20:51:33 +0000","Wed, 30 Jul 2014 21:21:18 +0000",7096255,"It means I will have to state an exception in the docs though. You can cast BOOLEAN to other numeric types but not DECIMAL: [localhost:21000] > select cast(true as int); ------------------- ------------------- ------------------- [localhost:21000] > select cast(true as float); --------------------- --------------------- --------------------- [localhost:21000] > select cast(true as decimal); ERROR: AnalysisException: Invalid type cast of TRUE from BOOLEAN to DECIMAL(9,0)",0.021625,0.089625,neutral
impala,991,comment_3,I am using this opportunity to fill in some holes in the doc discussion of casting to/from BOOLEAN. BOOLEAN -BOOLEAN <-> TIMESTAMP treats false as the epoch date (1970-01-01 00:00:00) and true as 1 second later (1970-01-01 00:00:01).,documentation_debt,low_quality_documentation,"Fri, 9 May 2014 18:10:23 +0000","Tue, 2 Sep 2014 20:51:33 +0000","Wed, 30 Jul 2014 21:21:18 +0000",7096255,I am using this opportunity to fill in some holes in the doc discussion of casting to/from BOOLEAN. BOOLEAN -> numeric gives 1 or 0 for true or false. BOOLEAN <-> TIMESTAMP treats false as the epoch date (1970-01-01 00:00:00) and true as 1 second later (1970-01-01 00:00:01).,0.29325,0.2576666667,neutral
impala,1082,comment_2,We are running CDH 4.4 and would need to upgrade to a later build. This appears to be occurring on only 1 or 2 of our nodes on the cluster and thus I believe is data centric. Are they any workarounds or ways to identify/cleanup the data?,code_debt,low_quality_code,"Fri, 11 Jul 2014 15:14:05 +0000","Mon, 14 Jul 2014 17:57:10 +0000","Fri, 11 Jul 2014 15:26:15 +0000",730,We are running CDH 4.4 and would need to upgrade to a later build. This appears to be occurring on only 1 or 2 of our nodes on the cluster and thus I believe is data centric. Are they any workarounds or ways to identify/cleanup the data?,0.175,0.175,neutral
impala,1118,comment_2,Fixing this would make query gen testing easier.,design_debt,non-optimal_design,"Mon, 28 Jul 2014 21:00:54 +0000","Thu, 14 May 2015 22:33:54 +0000","Wed, 24 Sep 2014 15:49:46 +0000",4992532,Fixing this would make query gen testing easier.,0.0,0.0,neutral
impala,1493,comment_1,"It looks like almost all boost timestamp functions can throw. We can either try catch each of those or do it at a higher level up, e.g. scalar fn call. Putting it higher up is a bit tricky since we codegen scalar fn call.",design_debt,non-optimal_design,"Wed, 19 Nov 2014 02:25:09 +0000","Fri, 21 Nov 2014 19:27:26 +0000","Fri, 21 Nov 2014 19:27:26 +0000",234137,"It looks like almost all boost timestamp functions can throw. We can either try catch each of those or do it at a higher level up, e.g. scalar fn call. Putting it higher up is a bit tricky since we codegen scalar fn call.",0.2333333333,0.2333333333,neutral
impala,1577,comment_1,Thanks for the feedback. I just looked at the code a little and didn't see any easy improvements. The main perf problem is most likely due to the function not being codegen'd. There's a note in the code about some LLVM limitations. Our LLVM version is about 2 years old so an upgrade might help. An upgrade is on our todo list but I couldn't say when that would be done. Also the function heavily relies on boost so that could be an issue.,architecture_debt,using_obsolete_technology,"Fri, 5 Dec 2014 01:16:08 +0000","Mon, 9 Jul 2018 16:45:30 +0000","Mon, 9 Jul 2018 16:45:30 +0000",113412562,Thanks for the feedback. I just looked at the code a little and didn't see any easy improvements. The main perf problem is most likely due to the function not being codegen'd. There's a note in the code about some LLVM limitations. Our LLVM version is about 2 years old so an upgrade might help. An upgrade is on our todo list but I couldn't say when that would be done. Also the function heavily relies on boost so that could be an issue.,0.1279285714,0.1279285714,negative
impala,1577,comment_0,"I've just signed on to the Impala issue tracker and was about to report the exact same problem as described above. We had to add timezone conversions to our queries lately and the query performance decreased massively when we started using the from_utc_timestamp function. Like you said it's not only slow, but the performance is inconsistent, it seems to get worse the more rows are processed. Unfortunately (at least for us ;-)) this issue doesn't seem to get much attention. It would be great if you could let me know whether this will be addressed sometime soon or if you have any other news on this issue. Thanks in advance! P.S. We're using Impala 2.2 with CDH5.4.2.",code_debt,slow_algorithm,"Fri, 5 Dec 2014 01:16:08 +0000","Mon, 9 Jul 2018 16:45:30 +0000","Mon, 9 Jul 2018 16:45:30 +0000",113412562,"I've just signed on to the Impala issue tracker and was about to report the exact same problem as described above. We had to add timezone conversions to our queries lately and the query performance decreased massively when we started using the from_utc_timestamp function. Like you said it's not only slow, but the performance is inconsistent, it seems to get worse the more rows are processed. Unfortunately (at least for us ) this issue doesn't seem to get much attention. It would be great if you could let me know whether this will be addressed sometime soon or if you have any other news on this issue. Thanks in advance! P.S. We're using Impala 2.2 with CDH5.4.2.",-0.02185,-0.02185,negative
impala,1577,comment_2,"IMPALA-3307 replaced the timezone implementation, which became generally faster and should be more consistent. The old implementation looked up timezone aliases much slower than canonical timezone names, while there shouldn't be any difference in the new one (many aliases were removed, while some were added to a map for fast lookups).",code_debt,slow_algorithm,"Fri, 5 Dec 2014 01:16:08 +0000","Mon, 9 Jul 2018 16:45:30 +0000","Mon, 9 Jul 2018 16:45:30 +0000",113412562,"IMPALA-3307 replaced the timezone implementation, which became generally faster and should be more consistent. The old implementation looked up timezone aliases much slower than canonical timezone names, while there shouldn't be any difference in the new one (many aliases were removed, while some were added to a map for fast lookups).",-0.25,-0.25,negative
impala,1584,comment_0,"You don't have to do it this time, but next time (and in general) I think we should add the source code snippet of the failed DCHECK along with the relevant backtrace to bugs. That will make it easier to both review the fix (since it'll be easier to understand what went wrong), and also easier to identify duplicates (for all of dev, cce, and support).",code_debt,low_quality_code,"Mon, 8 Dec 2014 20:45:26 +0000","Sat, 21 Mar 2015 22:51:10 +0000","Wed, 24 Dec 2014 23:59:12 +0000",1394026,"You don't have to do it this time, but next time (and in general) I think we should add the source code snippet of the failed DCHECK along with the relevant backtrace to bugs. That will make it easier to both review the fix (since it'll be easier to understand what went wrong), and also easier to identify duplicates (for all of dev, cce, and support).",-0.1625,-0.1625,neutral
impala,1587,comment_1,The WITH REPLICATION syntax will be in the next doc refresh for the appropriate release(s).,documentation_debt,outdated_documentation,"Tue, 9 Dec 2014 22:07:22 +0000","Thu, 25 Jun 2015 23:35:52 +0000","Tue, 27 Jan 2015 04:32:33 +0000",4170311,The WITH REPLICATION syntax will be in the next doc refresh for the appropriate release(s).,0.75,0.75,neutral
impala,1618,comment_0,"Probably the best fix here would be to add additional buffering in PlanRootSink so that the hand-off between the fetching thread and the producing thread was not synchronous. We would need to think carefully about the design - e.g. how much buffering, how should the buffered rows be stored, etc.",code_debt,multi-thread_correctness,"Wed, 17 Dec 2014 21:44:47 +0000","Thu, 14 May 2020 17:44:56 +0000","Thu, 29 Aug 2019 20:19:54 +0000",148257307,"Probably the best fix here would be to add additional buffering in PlanRootSink so that the hand-off between the fetching thread and the producing thread was not synchronous. We would need to think carefully about the design - e.g. how much buffering, how should the buffered rows be stored, etc.",0.66875,0.66875,neutral
impala,1618,comment_1,"I'm going to close this, this has been implemented in IMPALA-8819. The fix is specific to result spooling = true), but I think that is okay.",design_debt,non-optimal_design,"Wed, 17 Dec 2014 21:44:47 +0000","Thu, 14 May 2020 17:44:56 +0000","Thu, 29 Aug 2019 20:19:54 +0000",148257307,"I'm going to close this, this has been implemented inIMPALA-8819. The fix is specific to result spooling (spool_query_results = true), but I think that is okay.",0.34625,0.34625,neutral
impala,1691,comment_1,We should get rid of the cached {{Partition}} object inside {{HdfsPartition}}. I'm pretty sure we can reconstruct it from the partition itself when needed (i.e. when sending updates to the HMS).,design_debt,non-optimal_design,"Fri, 23 Jan 2015 21:27:05 +0000","Tue, 3 Mar 2015 17:42:45 +0000","Tue, 3 Mar 2015 17:42:45 +0000",3356140,We should get rid of the cached Partition object inside HdfsPartition. I'm pretty sure we can reconstruct it from the partition itself when needed (i.e. when sending updates to the HMS).,-0.181,-0.181,neutral
impala,1691,comment_2,I just looked at a catalogd instance with a large table loaded in a profiler. This seems to bear out the idea that the arrays of {{FieldSchema}} in the take a lot of memory. take up about 1.1GB of heap even with a relatively small catalog.,design_debt,non-optimal_design,"Fri, 23 Jan 2015 21:27:05 +0000","Tue, 3 Mar 2015 17:42:45 +0000","Tue, 3 Mar 2015 17:42:45 +0000",3356140,I just looked at a catalogd instance with a large table loaded in a profiler. This seems to bear out the idea that the arrays of FieldSchema in the StorageDescriptor take a lot of memory. StorageDescriptors take up about 1.1GB of heap even with a relatively small catalog.,0.0,0.0,neutral
impala,1697,comment_0,"With our current metadata/loading infrastructure this seemingly simple change is actually tricky to implement. Since the initial catalog update only contains the names of all databases and tables, but not their types implementing SHOW TABLES would force us to load all the table metadata from the catalogd for those tables. Since table metadata loading is rather expensive, the SHOW TABLES command could have a very high latency which is probably incomprehensible to users. As an alternative, we could display the type as ""UNKNOWN"" for tables that have not yet been loaded, but again, this seems incomprehensible to users. Needless to say, we should consider this use case when making architectural changes to the catalog/metadata infra.",design_debt,non-optimal_design,"Mon, 26 Jan 2015 16:45:21 +0000","Fri, 11 Aug 2017 21:55:58 +0000","Fri, 11 Aug 2017 21:55:58 +0000",80197837,"With our current metadata/loading infrastructure this seemingly simple change is actually tricky to implement. Since the initial catalog update only contains the names of all databases and tables, but not their types implementing SHOW TABLES would force us to load all the table metadata from the catalogd for those tables. Since table metadata loading is rather expensive, the SHOW TABLES command could have a very high latency which is probably incomprehensible to users. As an alternative, we could display the type as ""UNKNOWN"" for tables that have not yet been loaded, but again, this seems incomprehensible to users. Needless to say, we should consider this use case when making architectural changes to the catalog/metadata infra.",-0.38,-0.38,negative
impala,1774,comment_2,"This is fixed in however, I'm waiting for us to rebase on the latest Hive bits in order to commit a test for it",test_debt,lack_of_tests,"Sat, 14 Feb 2015 06:25:02 +0000","Wed, 8 Apr 2015 09:04:31 +0000","Wed, 8 Apr 2015 09:04:31 +0000",4588769,"This is fixed in https://github.com/cloudera/Impala/commit/b7d41e57ba380a31429a5d0fdda4ee8723385349, however, I'm waiting for us to rebase on the latest Hive bits in order to commit a test for it (http://gerrit.cloudera.org:8080/#/c/151/).",0.4,0.4,neutral
impala,1907,comment_0,"The receiver case just needs a simple fix -- cancel the receiver before closing it. The sender case is caused by accessing invalid memory. The consumption value is a counter from the runtime profile. The profile has already been destroyed by the time ~MemTracker is called. Since nothing else in ~MemTracker accesses the runtime profile, for 2.2 the DCHECK should just be removed. Here is the log with additional info added",code_debt,low_quality_code,"Sun, 22 Mar 2015 17:29:08 +0000","Sun, 20 Dec 2015 00:05:28 +0000","Wed, 19 Aug 2015 00:53:28 +0000",12900260,"The receiver case just needs a simple fix  cancel the receiver before closing it. The sender case is caused by accessing invalid memory. The consumption value is a counter from the runtime profile. The profile has already been destroyed by the time ~MemTracker is called. Since nothing else in ~MemTracker accesses the runtime profile, for 2.2 the DCHECK should just be removed. Here is the log with additional info added",-0.1333333333,-0.1333333333,negative
impala,1963,comment_2,"For my current project, I only really need it to handle the Z properly. Anything else (other timezones, full support) would be a bonus. In terms of Tableau/Impala usability - this problem will come up for anyone with a format that doesn't convert automatically, so the more coverage we get the better. However, in terms of release schedules, I would love to have time zones or at least Zulu time working sooner.",design_debt,non-optimal_design,"Wed, 22 Apr 2015 15:03:34 +0000","Fri, 5 Jun 2015 04:47:07 +0000","Fri, 5 Jun 2015 04:47:07 +0000",3764613,"For my current project, I only really need it to handle the Z properly. Anything else (other timezones, full support) would be a bonus. In terms of Tableau/Impala usability - this problem will come up for anyone with a format that doesn't convert automatically, so the more coverage we get the better. However, in terms of release schedules, I would love to have time zones or at least Zulu time working sooner.",0.27125,0.27125,neutral
impala,2076,comment_0,The DataStreamSender's total time includes a lot of things that don't necessarily contribute to network bottlenecks: # Time taken to serialize batches # Time taken waiting for concurrent RPCs to finish (in random case) # Time taken blocked waiting for RPC response when receiver hasn't yet arrived (after IMPALA-1599) What precisely is it that the EXCHANGE node's time should measure? Should it just be processing time + time spent in {{read()}} calls in the underlying socket? Is this what you mean by 'active' time? I think we could add instrumentation to Thrift to measure the latter (by subclassing {{TSocket}}).,documentation_debt,low_quality_documentation,"Thu, 18 Jun 2015 02:08:59 +0000","Thu, 28 Apr 2016 23:57:23 +0000","Thu, 21 Apr 2016 18:24:19 +0000",26669720,The DataStreamSender's total time includes a lot of things that don't necessarily contribute to network bottlenecks: Time taken to serialize batches Time taken waiting for concurrent RPCs to finish (in random case) Time taken blocked waiting for RPC response when receiver hasn't yet arrived (after IMPALA-1599) What precisely is it that the EXCHANGE node's time should measure? Should it just be processing time + time spent in read() calls in the underlying socket? Is this what you mean by 'active' time? I think we could add instrumentation to Thrift to measure the latter (by subclassing TSocket).,0.03333333333,0.03333333333,neutral
impala,2099,comment_0,"You may not want to spend too much time debugging this at the moment as this class basically needs a full rewrite anyway. Also, maybe a dup of IMPALA-2072? I don't know what the stack trace in that case looked like though.",code_debt,low_quality_code,"Wed, 24 Jun 2015 23:07:23 +0000","Sun, 20 Dec 2015 00:05:31 +0000","Thu, 27 Aug 2015 00:24:46 +0000",5447843,"You may not want to spend too much time debugging this at the moment as this class basically needs a full rewrite anyway. Also, maybe a dup of IMPALA-2072? I don't know what the stack trace in that case looked like though.",-0.05,-0.05,negative
impala,2178,comment_1,- that patch doesn't have query tests. Is there a simple repro query that we can add?,test_debt,lack_of_tests,"Wed, 5 Aug 2015 20:27:04 +0000","Fri, 9 Oct 2015 22:03:38 +0000","Fri, 28 Aug 2015 18:24:58 +0000",1979874,skye - that patch doesn't have query tests. Is there a simple repro query that we can add?,0.0,0.0,neutral
impala,2208,comment_1,"Up until now Impala does not use the min/max metadata of Parquet so we don't need to backport it. But it is in our roadmap to exploit them, should that happen we need to be very careful and consider PARQUET-251.",design_debt,non-optimal_design,"Fri, 14 Aug 2015 06:04:56 +0000","Sun, 20 Dec 2015 00:05:33 +0000","Fri, 14 Aug 2015 16:19:11 +0000",36855,"Up until now Impala does not use the min/max metadata of Parquet so we don't need to backport it. But it is in our roadmap to exploit them, should that happen we need to be very careful and consider PARQUET-251.",0.0,0.0,neutral
impala,2386,comment_0,"Isilon stress was run manually. Separate issue, IMPALA-2387 to document what was done. Future Isilon test coverage is to be tracked in a test plan.",test_debt,lack_of_tests,"Wed, 23 Sep 2015 00:55:37 +0000","Fri, 7 Oct 2016 17:17:06 +0000","Fri, 7 Oct 2016 17:17:06 +0000",32890889,"Isilon stress was run manually. Separate issue, IMPALA-2387 to document what was done. Future Isilon test coverage is to be tracked in a test plan.",-0.06666666667,-0.06666666667,neutral
impala,2657,comment_2,"Turns out CM already computes this based on the sampling counters already in the profile. The ""MemUsage"" profile timeseries counter keeps a max of 64 samples, downsampling after 64 are collected to make room for more. While this isn't really a perfect calculation of memory accrual, it's probably more than sufficient. We can reopen this if we find we need something more sophisticated.",design_debt,non-optimal_design,"Tue, 10 Nov 2015 05:11:10 +0000","Wed, 4 Jan 2017 23:58:13 +0000","Fri, 5 Feb 2016 20:18:59 +0000",7571269,"Turns out CM already computes this based on the sampling counters already in the profile. The ""MemUsage"" profile timeseries counter keeps a max of 64 samples, downsampling after 64 are collected to make room for more. While this isn't really a perfect calculation of memory accrual, it's probably more than sufficient. We can reopen this if we find we need something more sophisticated.",0.151125,0.151125,neutral
impala,2659,comment_1,"I did some further digging. It seems to be okay to relocate the driver as long as the companion files and {{cacerts.pem}} are relocated to the same directory as well. If is missing, you get curious error messages and you need to use the {{LD_PRELOAD}} hack. A better error message in this case ({{File SimbaImpalaODBC.did could not be found}}) would have helped a lot.",design_debt,non-optimal_design,"Tue, 10 Nov 2015 13:37:00 +0000","Mon, 12 Jun 2017 20:55:09 +0000","Mon, 12 Jun 2017 20:55:09 +0000",50138289,"I did some further digging. It seems to be okay to relocate the driver as long as the companion files SimbaImpalaODBC.did and cacerts.pem are relocated to the same directory as well. If SimbaImpalaODBC.did is missing, you get curious error messages and you need to use the LD_PRELOAD hack. A better error message in this case (File SimbaImpalaODBC.did could not be found) would have helped a lot.",0.3613333333,0.271,neutral
impala,2724,comment_0,"Dan, you seem to be the last person who touched this test. It appears the flakiness of the test had to do with the general non-determinism of impala memory usage. I believe that's the same case for IMPALA-2728.",test_debt,flaky_test,"Sat, 28 Nov 2015 21:29:22 +0000","Tue, 15 Dec 2015 21:39:21 +0000","Tue, 15 Dec 2015 21:39:21 +0000",1469399,"Dan, you seem to be the last person who touched this test. It appears the flakiness of the test had to do with the general non-determinism of impala memory usage. I believe that's the same case for IMPALA-2728.",0.0,0.0,neutral
impala,3099,comment_0,I think we should hold off on this one - the {{rpc_pool}} is going to be mostly superseded by async rpcs when we switch to KRPC.,defect_debt,uncorrected_known_defects,"Sun, 28 Feb 2016 22:35:19 +0000","Tue, 30 Apr 2019 20:01:22 +0000","Thu, 12 Jan 2017 20:33:47 +0000",27554308,I think we should hold off on this one - the rpc_pool is going to be mostly superseded by async rpcs when we switch to KRPC.,0.0,0.0,neutral
impala,3103,comment_1,The following fix improves serialisation efficiency by 20x (!),code_debt,slow_algorithm,"Tue, 1 Mar 2016 00:12:14 +0000","Tue, 1 Mar 2016 15:35:11 +0000","Tue, 1 Mar 2016 15:35:11 +0000",55377,The following fix improves serialisation efficiency by 20x https://github.com/cloudera/Impala/commit/055bd7e4088de8286c3abd35a88ec70b15be4be9,0.4,0.4,positive
impala,3252,comment_1,", thanks a lot for nailing this; it has been a nuisance for ages.",code_debt,low_quality_code,"Mon, 28 Mar 2016 17:55:33 +0000","Mon, 11 Jun 2018 20:57:57 +0000","Mon, 11 Jun 2018 20:42:56 +0000",69562043,"tarmstrong, thanks a lot for nailing this; it has been a nuisance for ages.",0.4,0.4,negative
impala,3352,comment_0,"I've seen this before where ""hosts=2"" leads to a different plan. Maybe the tests should check that the number of hosts is as expected.",test_debt,low_coverage,"Thu, 14 Apr 2016 03:42:37 +0000","Mon, 1 May 2017 21:49:34 +0000","Mon, 1 May 2017 21:49:34 +0000",33070017,"I've seen this before where ""hosts=2"" leads to a different plan. Maybe the tests should check that the number of hosts is as expected.",0.0,0.0,neutral
impala,3652,comment_1,"The required fix is for Reset() to take a RowBatch as an argument, to which memory referenced by previously returned rows can be returned. The logic for Reset() attaching memory needs to be similar to the handling of ReachedLimit(), e.g. this example in NLJNode::GetNext():",design_debt,non-optimal_design,"Tue, 31 May 2016 20:46:57 +0000","Mon, 19 Nov 2018 10:45:14 +0000","Wed, 7 Nov 2018 23:06:04 +0000",76904347,"The required fix is for Reset() to take a RowBatch as an argument, to which memory referenced by previously returned rows can be returned. The logic for Reset() attaching memory needs to be similar to the handling of ReachedLimit(), e.g. this example in NLJNode::GetNext():",-0.2,-0.2,neutral
impala,3671,comment_1,- Docs work required.,documentation_debt,outdated_documentation,"Fri, 3 Jun 2016 21:31:45 +0000","Thu, 15 Dec 2016 00:36:24 +0000","Sat, 24 Sep 2016 04:53:45 +0000",9703320,jrussell - Docs work required.,0.6,0.6,neutral
impala,4027,comment_2,"Despite this is pretty rare, I mean that variable ""probe_expr_ctxs_"" is added twice while variable ""filter_expr_ctxs_"" is not called by function AddExprCtxsToFree",code_debt,low_quality_code,"Fri, 26 Aug 2016 02:54:27 +0000","Tue, 30 Aug 2016 16:18:12 +0000","Tue, 30 Aug 2016 16:18:12 +0000",393825,"Despite this is pretty rare, I mean that variable ""probe_expr_ctxs_"" is added twice while variable ""filter_expr_ctxs_"" is not called by function AddExprCtxsToFree",-0.275,-0.275,neutral
impala,4145,comment_0,This is leftover from a previous test plan. We still want to do this kind of testing but need to rescope it.,test_debt,lack_of_tests,"Sat, 17 Sep 2016 03:15:12 +0000","Wed, 11 Jul 2018 16:54:44 +0000","Wed, 11 Jul 2018 16:54:44 +0000",57245972,This is leftover from a previous test plan. We still want to do this kind of testing but need to rescope it.,0.12025,0.12025,neutral
impala,4173,comment_0,"CHAR doesn't currently work properly, and it's very likely that in the future CHAR will be implemented as VARCHAR/STRING under the covers (with the appropriate blank padding). In that case, we still want to know the actual average length, excluding trailing spaces (because those won't be materialized during join and agg processing).",design_debt,non-optimal_design,"Wed, 21 Sep 2016 05:33:26 +0000","Thu, 19 Jan 2017 22:51:32 +0000","Thu, 19 Jan 2017 22:51:05 +0000",10430259,"CHAR doesn't currently work properly, and it's very likely that in the future CHAR will be implemented as VARCHAR/STRING under the covers (with the appropriate blank padding). In that case, we still want to know the actual average length, excluding trailing spaces (because those won't be materialized during join and agg processing).",0.08433333333,0.08433333333,negative
impala,4231,comment_2,"There are a few things that changed with the codegen'd code in the patch that may be relevant. * A couple of hash functions that were previously shared between the build and probe codegen are generated separately. The final IR should be the same since they're copied during inlining, but there may be some redundant optimisation done before inlining happens. * The function signature for AppendRow() changed so that it returned a status",design_debt,non-optimal_design,"Fri, 30 Sep 2016 18:19:07 +0000","Mon, 17 Oct 2016 00:30:56 +0000","Mon, 17 Oct 2016 00:30:56 +0000",1404709,"There are a few things that changed with the codegen'd code in the patch that may be relevant. A couple of hash functions that were previously shared between the build and probe codegen are generated separately. The final IR should be the same since they're copied during inlining, but there may be some redundant optimisation done before inlining happens. The function signature for AppendRow() changed so that it returned a status",0.05,0.05,neutral
impala,4245,comment_0,The links do not exist in the Apache Impala docs. Will address where applicable.,documentation_debt,outdated_documentation,"Tue, 4 Oct 2016 10:10:27 +0000","Fri, 25 May 2018 20:55:56 +0000","Fri, 25 May 2018 20:55:56 +0000",51705929,The links do not exist in the Apache Impala docs. Will address where applicable.,0.25,0.25,neutral
impala,4267,comment_2,", I think some scripts are in the ASF repo. If they are not, some should be - it will ease new contributor onboarding to have well-maintained Docker images available.",build_debt,build_others,"Mon, 10 Oct 2016 18:16:29 +0000","Thu, 20 Oct 2016 18:14:30 +0000","Thu, 20 Oct 2016 18:14:30 +0000",863881,"henryr, I think some scripts are in the ASF repo. If they are not, some should be - it will ease new contributor onboarding to have well-maintained Docker images available.",0.2406666667,0.2406666667,neutral
impala,4291,comment_0,"IMPALA-4291: Reduce LLVM module's preparation time Previously, when creating a LlvmCodeGen object, we run an O(mn) algorithm to map the IRFunction::Type to the actual LLVM::Function object in the module. m is the size of IRFunction::Type enum and n is the total number of functions in the module. This is a waste of time if we only use few functions from the module. This change reduces the preparation time of a simple query from 23ms to 10ms. select count(*) from where l_orderkey > 20;",code_debt,slow_algorithm,"Thu, 13 Oct 2016 18:37:43 +0000","Thu, 30 Aug 2018 18:49:49 +0000","Thu, 20 Oct 2016 05:58:34 +0000",559251,"https://github.com/apache/incubator-impala/commit/47b8aa3a9e7682ebb182696901916900d3323039 IMPALA-4291: Reduce LLVM module's preparation time Previously, when creating a LlvmCodeGen object, we run an O(mn) algorithm to map the IRFunction::Type to the actual LLVM::Function object in the module. m is the size of IRFunction::Type enum and n is the total number of functions in the module. This is a waste of time if we only use few functions from the module. This change reduces the preparation time of a simple query from 23ms to 10ms. select count from tpch100_parquet.lineitem where l_orderkey > 20;",-0.2185,-0.1748,neutral
impala,4801,comment_1,"Yeah I figured out what this is. RuntimeProfile does some cleanup in it's destructor, which includes unregistering some callbacks that periodically touch MemTrackers. So if the MemTracker gets destroyed before the profile, we have a problem. Ideally we should move that out of the RuntimeProfile destructor, but we can fix the immediate problem by fixing the lifetime of the MemTracker.",design_debt,non-optimal_design,"Mon, 23 Jan 2017 18:58:36 +0000","Thu, 26 Jan 2017 01:33:15 +0000","Thu, 26 Jan 2017 01:33:15 +0000",196479,"Yeah I figured out what this is. RuntimeProfile does some cleanup in it's destructor, which includes unregistering some callbacks that periodically touch MemTrackers. So if the MemTracker gets destroyed before the profile, we have a problem. Ideally we should move that out of the RuntimeProfile destructor, but we can fix the immediate problem by fixing the lifetime of the MemTracker.",-0.175,-0.175,neutral
impala,4833,comment_0,There's some relevant code in the Scheduler and Coordinator. The Scheduler does a pass over all fragment instances to determine the set of hosts here: The coordinator also does something similar: It would maybe be helpful if the scheduler produced backend_params_map_ itself to make it easier to compute the per-host aggregates.,design_debt,non-optimal_design,"Fri, 27 Jan 2017 17:33:58 +0000","Mon, 14 Aug 2017 16:14:41 +0000","Mon, 14 Aug 2017 16:14:41 +0000",17188843,There's some relevant code in the Scheduler and Coordinator. The Scheduler does a pass over all fragment instances to determine the set of hosts here: The coordinator also does something similar: It would maybe be helpful if the scheduler produced backend_params_map_ itself to make it easier to compute the per-host aggregates.,0.3125,0.3125,neutral
impala,4967,comment_0,"For refresh or query after invalidate most of the time is usually spent fetching block metadata and partitions from HMS, around 10-15% of the overall time is in serializing the table. The bigger benefit from partition wise refresh is that memory overhead is reduced.",design_debt,non-optimal_design,"Wed, 22 Feb 2017 20:41:44 +0000","Fri, 11 Aug 2017 21:59:54 +0000","Fri, 11 Aug 2017 21:59:54 +0000",14692690,"For refresh or query after invalidate most of the time is usually spent fetching block metadata and partitions from HMS, around 10-15% of the overall time is in serializing the table. The bigger benefit from partition wise refresh is that memory overhead is reduced.",0.497,0.497,neutral
impala,4967,comment_1,"Thanks for that info, I'm less concerned about serialization costs and more about network. On larger clusters the Catalog Service will need to ship large amounts of metadata to every node on the cluster, which is a single point of congestion. If you have less nodes or fast network you wouldn't run into that as a problem in your testing. The CS becomes a single hot spot especially on busy large clusters.",design_debt,non-optimal_design,"Wed, 22 Feb 2017 20:41:44 +0000","Fri, 11 Aug 2017 21:59:54 +0000","Fri, 11 Aug 2017 21:59:54 +0000",14692690,"Thanks for that info, I'm less concerned about serialization costs and more about network. On larger clusters the Catalog Service will need to ship large amounts of metadata to every node on the cluster, which is a single point of congestion. If you have less nodes or fast network you wouldn't run into that as a problem in your testing. The CS becomes a single hot spot especially on busy large clusters.",-0.1,-0.1,neutral
impala,5084,comment_0,"As a temporary solution, it may be reasonable to just increase the default buffer size in the sorter to match the maximum row size, given that the sorter's buffer requirements are more modest than the joins and aggs, and there are typically fewer sorts in a plan.",design_debt,non-optimal_design,"Thu, 16 Mar 2017 22:49:31 +0000","Sat, 15 Apr 2017 00:12:16 +0000","Sat, 15 Apr 2017 00:12:16 +0000",2510565,"As a temporary solution, it may be reasonable to just increase the default buffer size in the sorter to match the maximum row size, given that the sorter's buffer requirements are more modest than the joins and aggs, and there are typically fewer sorts in a plan.",0.2134,0.2134,neutral
impala,5084,comment_1,This wouldn't offer enough benefit to offset the complexity. We currently only need 6 regular-sized buffers to execute a spilling sort with var-len data. To support rows larger than the regular page size we'd still need 4 max-sized buffers and 2 regular-sized buffers (since we need to keep a read and write buffer in memory at the same time). A bigger memory reduction could be achieved by different means. E.g. packing fixed and variable-length data in merged runs into the same buffers.,design_debt,non-optimal_design,"Thu, 16 Mar 2017 22:49:31 +0000","Sat, 15 Apr 2017 00:12:16 +0000","Sat, 15 Apr 2017 00:12:16 +0000",2510565,This wouldn't offer enough benefit to offset the complexity. We currently only need 6 regular-sized buffers to execute a spilling sort with var-len data. To support rows larger than the regular page size we'd still need 4 max-sized buffers and 2 regular-sized buffers (since we need to keep a read and write buffer in memory at the same time). A bigger memory reduction could be achieved by different means. E.g. packing fixed and variable-length data in merged runs into the same buffers.,0.03056666667,0.03056666667,neutral
impala,5618,comment_1,yeah I *think* that could defer construction of the boost::function until the non-templated slow path function is called. I'm thinking though that the performance is very hard to reason about and it may be better as a matter of policy to avoid lambdas with captured variables in perf-critical code.,design_debt,non-optimal_design,"Thu, 6 Jul 2017 00:51:03 +0000","Fri, 7 Jul 2017 18:02:02 +0000","Fri, 7 Jul 2017 18:02:02 +0000",148259,jbapple yeah I think that could defer construction of the boost::function until the non-templated slow path function is called. I'm thinking though that the performance is very hard to reason about and it may be better as a matter of policy to avoid lambdas with captured variables in perf-critical code.,0.1133333333,0.1133333333,neutral
impala,5636,comment_0,"I think just replacing 2 occurrences of with Encoding::RLE makes sense. I've manually tested with parquet-tools and it seems to work. Since parquet-tools is not in our distribution, I'm not sure how to write a test. Any suggestion?",test_debt,lack_of_tests,"Sun, 9 Jul 2017 21:08:28 +0000","Thu, 17 Aug 2017 00:23:43 +0000","Mon, 31 Jul 2017 17:37:57 +0000",1888169,"I think just replacing 2 occurrences of Encoding::BIT_PACKED with Encoding::RLE makes sense. I've manually tested with parquet-tools and it seems to work. Since parquet-tools is not in our distribution, I'm not sure how to write a test. Any suggestion?",0.145375,0.145375,neutral
impala,5732,comment_1,"This Jira is old, has little information, and there have been several recent JIRAs filed for similar flakiness in this test, so there's not really any value to keeping this around.",test_debt,flaky_test,"Thu, 27 Jul 2017 15:16:15 +0000","Tue, 22 May 2018 18:28:25 +0000","Thu, 25 Jan 2018 23:23:45 +0000",15754050,"This Jira is old, has little information, and there have been several recent JIRAs filed for similar flakiness in this test, so there's not really any value to keeping this around.",0.0,0.0,negative
impala,5923,comment_0,"As far as I understand, you want the unexpected information(i.e. opid) will not be printed. If so, the line 132 is removed and then refine the logging message. I think it's too simple. Please leave more detailed description if there is any missing.",design_debt,non-optimal_design,"Tue, 12 Sep 2017 18:59:28 +0000","Mon, 16 Oct 2017 16:32:39 +0000","Mon, 16 Oct 2017 16:32:09 +0000",2928761,"lv As far as I understand, you want the unexpected information(i.e. opid) will not be printed. If so, the line 132 is removed and then refine the logging message. I think it's too simple. Please leave more detailed description if there is any missing.",0.1140625,0.1140625,neutral
impala,6026,comment_2,"Thanks, Juan. Do you have the steps to repro this (for an older release)? I recently came across a setup that could repro this on 2.9.0 and we narrowed down the problem to be with the files in the partition mapping to the table root directory. For example '/tmp/foo' is the table root directory and one of the partition directories mapped to '/tmp/foo' (by mistake). We have some weird logic with adding a ""default partition"" with an immutable partition-key list and I'm guessing that the above state has caused a mixup and we tried appending to the Immutable list causing this behavior. In the above setup, we fixed the partition structure to unblock the table operations, fwiw. Interestingly I'm not able to reproduce it locally either. I'll keep this open for a while incase someone else runs into this and we have a more reliable repro.",code_debt,complex_code,"Sat, 7 Oct 2017 00:18:31 +0000","Fri, 19 Oct 2018 17:21:18 +0000","Fri, 19 Oct 2018 17:21:18 +0000",32634167,"Thanks, Juan. Do you have the steps to repro this (for an older release)? I recently came across a setup that could repro this on 2.9.0 and we narrowed down the problem to be with the files in the partition mapping to the table root directory. For example '/tmp/foo' is the table root directory and one of the partition directories mapped to '/tmp/foo' (by mistake). We have some weird logic with adding a ""default partition"" with an immutable partition-key list and I'm guessing that the above state has caused a mixup and we tried appending to the Immutable list causing this behavior. In the above setup, we fixed the partition structure to unblock the table operations, fwiw. Interestingly I'm not able to reproduce it locally either. I'll keep this open for a while incase someone else runs into this and we have a more reliable repro.",0.01683333333,0.01683333333,neutral
impala,6048,comment_3,"So, as explained, this could be triggered by heavy spilling under high concurrency so some nodes are slow to consume row batches, leading to long RPC wait time. This seems to be very similar to what's being tracked in IMPALA-6294.",code_debt,slow_algorithm,"Thu, 12 Oct 2017 23:54:47 +0000","Tue, 14 May 2019 01:53:18 +0000","Tue, 14 May 2019 01:53:18 +0000",49946311,"So, as explained, this could be triggered by heavy spilling under high concurrency so some nodes are slow to consume row batches, leading to long RPC wait time. This seems to be very similar to what's being tracked in IMPALA-6294.",0.2,0.2,neutral
impala,6080,comment_1,"I don't think anyone has picked that up due to a lack of time - I agree it would be good to get in but I think parts of that patch require deeper understanding than this subset, which is fairly mechanical cleanup. Separating this out would shrink that patch and I think make it easier to focus on the meat of it.",code_debt,low_quality_code,"Wed, 18 Oct 2017 23:53:05 +0000","Thu, 16 Nov 2017 22:12:16 +0000","Thu, 16 Nov 2017 22:12:16 +0000",2499551,"I don't think anyone has picked that up due to a lack of time - I agree it would be good to get in but I think parts of that patch require deeper understanding than this subset, which is fairly mechanical cleanup. Separating this out would shrink that patch and I think make it easier to focus on the meat of it.",-0.096,-0.096,neutral
impala,6408,comment_0,"I have found another minor insert hint related issue in ""To use a hint to influence the join order, put the hint keyword /* +SHUFFLE */ or /* +NOSHUFFLE */ (including the square brackets) after the PARTITION clause, immediately before the SELECT keyword."" I do not think that any join order is influenced by SHUFFLE in inserts, and there is also an inconsistency in hint style: "" /* +NOSHUFFLE */ (including the square brackets)""",documentation_debt,low_quality_documentation,"Wed, 17 Jan 2018 15:48:26 +0000","Wed, 13 Jun 2018 15:48:46 +0000","Tue, 12 Jun 2018 15:39:57 +0000",12613891,"I have found anotherminor insert hint related issue in topics/impala_parquet.html#parquet_etl: ""To use a hint to influence the join order, put the hint keyword /* +SHUFFLE / or / +NOSHUFFLE */ (including the square brackets) after the PARTITION clause, immediately before the SELECT keyword."" I do not think that any join order is influenced by SHUFFLE in inserts, and there is also an inconsistency in hint style: "" /* +NOSHUFFLE */ (including the square brackets)""",0.122,0.08133333333,negative
impala,6623,comment_0,The existing documentation: should be changed to: And for rtrim:,documentation_debt,low_quality_documentation,"Wed, 7 Mar 2018 23:04:59 +0000","Fri, 13 Apr 2018 22:59:54 +0000","Wed, 11 Apr 2018 22:08:43 +0000",3020624,The existing documentation: should be changed to: And for rtrim:,0.0,0.0,neutral
impala,6817,comment_1,Is there any user-facing feature change with this story? That needs to be documented?,documentation_debt,outdated_documentation,"Fri, 6 Apr 2018 03:14:31 +0000","Tue, 10 Apr 2018 17:48:45 +0000","Tue, 10 Apr 2018 14:02:53 +0000",384502,fredyw Is there any user-facing feature change with this story? That needs to be documented?,0.0,0.0,neutral
impala,6817,comment_2,Not for this. This story is just for code clean-up. It doesn't affect the behavior. We do need a documentation for this: As soon as we're done with we need to start documenting it.,documentation_debt,outdated_documentation,"Fri, 6 Apr 2018 03:14:31 +0000","Tue, 10 Apr 2018 17:48:45 +0000","Tue, 10 Apr 2018 14:02:53 +0000",384502,"Not for this. Thisstory is just for code clean-up. It doesn't affect the behavior. We do need a documentation for this: https://issues.apache.org/jira/browse/IMPALA-6648.As soon as we're done with https://issues.apache.org/jira/browse/IMPALA-6804,we need to start documenting it.",0.1,0.08,neutral
impala,6937,comment_2,As expected a subsequent build passed so I'm marking this one as flaky.,test_debt,flaky_test,"Thu, 26 Apr 2018 01:03:00 +0000","Mon, 4 Jun 2018 16:00:07 +0000","Mon, 4 Jun 2018 16:00:06 +0000",3423426,As expected a subsequent build passed so I'm marking this one as flaky.,0.0,0.0,neutral
impala,7161,comment_0,"Additional issue: Suppose a user sets JAVA_HOME in their environment like they are supposed to. Suppose they also have JAVA_HOME in like writes. The two can get out of sync. The JAVA would be from the environment JAVA_HOME, but after setting JAVA, value for JAVA_HOME would overwrite the environment variable and JAVA_HOME would be that value. These could point to two different places.",design_debt,non-optimal_design,"Mon, 11 Jun 2018 23:50:00 +0000","Fri, 22 Feb 2019 03:42:37 +0000","Fri, 22 Jun 2018 18:46:27 +0000",932187,"Additional issue: Suppose a user sets JAVA_HOME in their environment like they are supposed to. Suppose they also have JAVA_HOME in bin/impala-config-local.sh like bin/bootstrap_system.sh writes. The two can get out of sync. The JAVA would be from the environment JAVA_HOME, but after setting JAVA, bin/impala-config-local.sh's value for JAVA_HOME would overwrite the environment variable and JAVA_HOME would be that value. These could point to two different places.",0.1,0.0625,neutral
impala,7234,comment_0,"Should this function actually take into account the total byte sizes or counts of ranges or files? In recently looking at this code I couldn't quite make sense of the logic. For example, if we have 10 partitions that are text, each containing one file, and one partition which is Parquet, containing 100 files, maybe it makes more sense to estimate scan range memory usage based on Parquet instead of text?",design_debt,non-optimal_design,"Mon, 2 Jul 2018 20:22:07 +0000","Wed, 1 Aug 2018 17:30:06 +0000","Wed, 1 Aug 2018 17:30:06 +0000",2581679,"Should this function actually take into account the total byte sizes or counts of ranges or files? In recently looking at this code I couldn't quite make sense of the logic. For example, if we have 10 partitions that are text, each containing one file, and one partition which is Parquet, containing 100 files, maybe it makes more sense to estimate scan range memory usage based on Parquet instead of text?",0.0,0.0,neutral
impala,7987,comment_0,"A major obstacle here that I'm running into is that ""localhost"" appears in many places in configuration files. Finding those and replacing them with the gateway IP of the docker network isn't sufficient to solve the problem without reloading data, because ""localhost"" makes its way into table definitions and other places (probably sentry?).",design_debt,non-optimal_design,"Fri, 14 Dec 2018 23:21:15 +0000","Wed, 23 Jan 2019 18:53:45 +0000","Wed, 23 Jan 2019 18:53:45 +0000",3439950,"A major obstacle here that I'm running into is that ""localhost"" appears in many places in configuration files. Finding those and replacing them with the gateway IP of the docker network isn't sufficient to solve the problem without reloading data, because ""localhost"" makes its way into table definitions and other places (probably sentry?).",-0.2708333333,-0.2708333333,negative
impala,8146,comment_3,"I think we should stage this differently - first make it possible to do everything without make_impala.sh, have an interim period where the script is still present so any downstream users have a chance to migrate their tools, then remove the old scripts.",code_debt,low_quality_code,"Wed, 30 Jan 2019 10:44:43 +0000","Tue, 5 Feb 2019 10:00:46 +0000","Tue, 5 Feb 2019 10:00:46 +0000",515763,"I think we should stage this differently - first make it possible to do everything without make_impala.sh, have an interim period where the script is still present so any downstream users have a chance to migrate their tools, then remove the old scripts.",0.17025,0.17025,neutral
impala,8473,comment_0,"Can you define ""ImpalaPostExecHook "" as an interface instead an abstract class? I don't see benefits of defining it as abstract class. On the other hand, defining it as interface allows the implementation to extends from another base class. Thanks! Instead of Can you define",design_debt,non-optimal_design,"Tue, 30 Apr 2019 18:58:16 +0000","Wed, 29 May 2019 21:54:40 +0000","Wed, 29 May 2019 21:54:40 +0000",2516184,"radford-nguyen Can you define ""ImpalaPostExecHook "" as an interface instead an abstract class? I don't see benefits of defining it as abstract class. On the other hand, defining it as interface allows the implementation to extends from another base class. Thanks! Instead of Can you define",0.04,0.04,negative
impala,8534,comment_0,"I'm not going to get to this right away, although I'd like to circle back. If you want to turn it on to get some additional testing, feel free to pick it up - I can provide any pointers if you need them.",test_debt,low_coverage,"Fri, 10 May 2019 15:39:22 +0000","Tue, 6 Aug 2019 20:45:17 +0000","Tue, 6 Aug 2019 20:45:17 +0000",7621555,"kwho I'm not going to get to this right away, although I'd like to circle back. If you want to turn it on to get some additional testing, feel free to pick it up - I can provide any pointers if you need them.",-0.1635,-0.1635,neutral
impala,8912,comment_0,"The second call is required since the limit_ may be updated, which may cut the cardinality. I think what we should avoid is sampling twice on hbase.",design_debt,non-optimal_design,"Fri, 30 Aug 2019 12:39:19 +0000","Thu, 5 Sep 2019 00:59:26 +0000","Thu, 5 Sep 2019 00:59:26 +0000",476407,"The second call is required since the limit_ may be updated, which may cut the cardinality. I think what we should avoid is sampling twice on hbase.",-0.2,-0.2,neutral
impala,8935,comment_2,"So the use-case you're talking about, accessing the minicluster webui in a dev environment from another machine, I think is difficult to make work in all cases. Rather than using ip addresses, we could specify hostnames for these links, but that's not guaranteed to always work - I think its common for people to develop on machines that don't have DNS-resolvable hostnames (eg see IMPALA-8917). And its always going to be the case that the machine's hostname resolves to 127.0.0.1 locally due to I don't think this is too big of a deal - it should work in any sort of real, non-development environment, and of course its possible in a dev environment to access the other webuis by manually specifying the right host:port instead of clicking the link, as its always been. One option if you really want this to work in a dev environment is to specify the flag to some public IP on minicluster startup. This flag is currently broken, but I have a patch out to get it working:",design_debt,non-optimal_design,"Tue, 10 Sep 2019 18:56:04 +0000","Thu, 19 Sep 2019 20:17:48 +0000","Thu, 19 Sep 2019 20:17:48 +0000",782504,"So the use-case you're talking about, accessing the minicluster webui in a dev environment from another machine, I think is difficult to make work in all cases. Rather than using ip addresses, we could specify hostnames for these links, but that's not guaranteed to always work - I think its common for people to develop on machines that don't have DNS-resolvable hostnames (eg see IMPALA-8917). And its always going to be the case that the machine's hostname resolves to 127.0.0.1 locally due to https://github.com/apache/impala/blob/master/bin/bootstrap_system.sh#L362 I don't think this is too big of a deal - it should work in any sort of real, non-development environment, and of course its possible in a dev environment to access the other webuis by manually specifying the right host:port instead of clicking the link, as its always been. One option if you really want this to work in a dev environment is to specify the --webserver_interface flag to some public IP on minicluster startup. This flag is currently broken, but I have a patch out to get it working: https://gerrit.cloudera.org/#/c/14266/",0.06908,0.06908,neutral
impala,9373,comment_0,"Notes so far: * In many cases it recommends including internal headers instead of the public-facing header * It gets confused by ""using"" statements in headers, e.g. it thinks is needed for references to ""string"" * The recommendations are mostly pretty good, but there were various small misfires, e.g. recommendations that didn't work or match our coding standards",code_debt,low_quality_code,"Tue, 11 Feb 2020 17:04:39 +0000","Wed, 25 Mar 2020 16:36:22 +0000","Wed, 25 Mar 2020 16:36:22 +0000",3713503,"Notes so far: In many cases it recommends including internal headers instead of the public-facing header It gets confused by ""using"" statements in headers, e.g. it thinks gutil/strings/substitute.h is needed for references to ""string"" The recommendations are mostly pretty good, but there were various small misfires, e.g. recommendations that didn't work or match our coding standards",0.0752,0.06266666667,neutral
thrift,39,comment_0,"I've looked this over pretty thoroughly, and I can't seem to figure out what the cause is. Only some of my structs' indentation is being dropped, while others are completely fine. There's no apparent connection between the ones that work and the ones that don't. For the moment, I'm stumped. If anyone else can think of a reason as to why indent() calls in the generator would be getting outright ignored, I'm all ears.",code_debt,low_quality_code,"Wed, 18 Jun 2008 23:50:20 +0000","Tue, 1 Nov 2011 02:54:21 +0000","Fri, 21 Nov 2008 20:25:27 +0000",13466107,"I've looked this over pretty thoroughly, and I can't seem to figure out what the cause is. Only some of my structs' indentation is being dropped, while others are completely fine. There's no apparent connection between the ones that work and the ones that don't. For the moment, I'm stumped. If anyone else can think of a reason as to why indent() calls in the generator would be getting outright ignored, I'm all ears.",-0.24,-0.24,negative
thrift,48,comment_0,"Line 74 of the patch, please edit the comment to say ""We need remove the old unix socket if the file exists and nobody is listening on it."" Line 95 of the patch, I personally prefer leaving the parens here. Line 103 of the patch, please use ""is not None"" instead of ""!="" Other than those nitpicks, I'm fine with this. bmaurer, you wrote the original Python Unix-domain code. What do you think of it?",code_debt,low_quality_code,"Mon, 23 Jun 2008 12:56:07 +0000","Tue, 1 Nov 2011 02:54:21 +0000","Thu, 31 Jul 2008 20:15:25 +0000",3309558,"Line 74 of the patch, please edit the comment to say ""We need remove the old unix socket if the file exists and nobody is listening on it."" Line 95 of the patch, I personally prefer leaving the parens here. Line 103 of the patch, please use ""is not None"" instead of ""!="" Other than those nitpicks, I'm fine with this. bmaurer, you wrote the original Python Unix-domain code. What do you think of it?",0.1333333333,0.1333333333,neutral
thrift,62,comment_3,"In talking with Kevin Clark, the benchmark this patch provides demonstrates a significant performance penalty between the old code and the new (new meaning my recent work). We've narrowed it down to two areas: MemoryBuffer's new implementation, and calling #dup on default values. The first issue is addressed by THRIFT-63. The second is still a problem, and I don't know if there's any way we can fix it, because calling #dup is fairly important to protect against destructive modification of default values.",code_debt,slow_algorithm,"Thu, 26 Jun 2008 00:32:20 +0000","Tue, 8 Jul 2008 00:48:12 +0000","Tue, 8 Jul 2008 00:48:12 +0000",1037752,"In talking with Kevin Clark, the benchmark this patch provides demonstrates a significant performance penalty between the old code and the new (new meaning my recent work). We've narrowed it down to two areas: MemoryBuffer's new implementation, and Thrift::Struct#initialize calling #dup on default values. The first issue is addressed by THRIFT-63. The second is still a problem, and I don't know if there's any way we can fix it, because calling #dup is fairly important to protect against destructive modification of default values.",0.05802083333,0.0576875,neutral
thrift,62,comment_4,"We definitely need the dup effect. If we really want to try and improve the performance of that code, we could try generating the default value stuff inline rather than storing it in a class-variable map which requires dups.",code_debt,slow_algorithm,"Thu, 26 Jun 2008 00:32:20 +0000","Tue, 8 Jul 2008 00:48:12 +0000","Tue, 8 Jul 2008 00:48:12 +0000",1037752,"We definitely need the dup effect. If we really want to try and improve the performance of that code, we could try generating the default value stuff inline rather than storing it in a class-variable map which requires dups.",0.01666666667,0.01666666667,neutral
thrift,62,comment_1,"We need setup.rb back for the automake stuff to work, and more generally until the RubyGems work is completed. I'm going to apply this to the tree shortly, but until then you'll need to apply this yourself for testing.",test_debt,lack_of_tests,"Thu, 26 Jun 2008 00:32:20 +0000","Tue, 8 Jul 2008 00:48:12 +0000","Tue, 8 Jul 2008 00:48:12 +0000",1037752,"We need setup.rb back for the automake stuff to work, and more generally until the RubyGems work is completed. I'm going to apply this to the tree shortly, but until then you'll need to apply this yourself for testing.",0.2,0.2,neutral
thrift,137,comment_6,"Fourth iteration of the GWT patch. Now included is a TGWTProtocol JSON implementation that uses the native GWT JSON parser (compatible with the standard JSONProtocol implementation), and a TGWTProxyGenerator that provides deferred binding support for Thrift cilents. This means that it's now as easy to use Thrift from GWT as using the standard GWT RPC framework. Also, I made some build changes since the last patch which means that a single JAR can be used for both server and client code, but also that I did not have to modify or move any of the existing classes (almost). I suppose the patch is more or less committable, as it is now feature complete, but I'm sure there's a bunch of bugs in the GWT part of it and could definitely use a pair of extra eyes.....",design_debt,non-optimal_design,"Tue, 16 Sep 2008 09:01:30 +0000","Thu, 12 Apr 2012 04:04:58 +0000","Mon, 9 Apr 2012 18:25:15 +0000",112440225,"Fourth iteration of the GWT patch. Now included is a TGWTProtocol JSON implementation that uses the native GWT JSON parser (compatible with the standard JSONProtocol implementation), and a TGWTProxyGenerator that provides deferred binding support for Thrift cilents. This means that it's now as easy to use Thrift from GWT as using the standard GWT RPC framework. Also, I made some build changes since the last patch which means that a single JAR can be used for both server and client code, but also that I did not have to modify or move any of the existing classes (almost). I suppose the patch is more or less committable, as it is now feature complete, but I'm sure there's a bunch of bugs in the GWT part of it and could definitely use a pair of extra eyes.....",0.1609,0.1609,neutral
thrift,137,comment_5,"New version of the modifications; now including the client library side of it that wasn't submitted the last time. Compile the Thrift Java library with 'ant compile-gwt dist' to enable the GWT changes. This should now provide fully working Thrift/GWT integration. What remains to be done is rewriting the TGWTJSONProtocol, which is currently a butchered version of the standard TJSONProtocol.",requirement_debt,requirement_partially_implemented,"Tue, 16 Sep 2008 09:01:30 +0000","Thu, 12 Apr 2012 04:04:58 +0000","Mon, 9 Apr 2012 18:25:15 +0000",112440225,"New version of the modifications; now including the client library side of it that wasn't submitted the last time. Compile the Thrift Java library with 'ant compile-gwt dist' to enable the GWT changes. This should now provide fully working Thrift/GWT integration. What remains to be done is rewriting the TGWTJSONProtocol, which is currently a butchered version of the standard TJSONProtocol.",0.275,0.275,neutral
thrift,163,comment_2,"This is actually way more primitive than I remembered. If I recall properly, it successfully builds the generators as shared objects. Since this was written, we've modified most of the generators to use a dynamic registry, which was one of the big hurdles to getting this to work. I think the big chunks that remain to be done are first modifying the compiler to load the appropriate shared objects with libltdl and investigating whether it is possible to compile some of the objects directly into the compiler so that we don't have to worry about library search paths.",design_debt,non-optimal_design,"Wed, 8 Oct 2008 18:46:10 +0000","Thu, 12 Apr 2012 04:04:50 +0000","Mon, 9 Apr 2012 18:47:28 +0000",110505678,"This is actually way more primitive than I remembered. If I recall properly, it successfully builds the generators as shared objects. Since this was written, we've modified most of the generators to use a dynamic registry, which was one of the big hurdles to getting this to work. I think the big chunks that remain to be done are first modifying the compiler to load the appropriate shared objects with libltdl and investigating whether it is possible to compile some of the objects directly into the compiler so that we don't have to worry about library search paths.",0.2945625,0.2945625,neutral
thrift,191,comment_4,"I like the map declaration, but I think that the MetaData class should live outside of the generated code. It's not actually a class that will vary by structure, so it should just live in c.f.thrift.",architecture_debt,violation_of_modularity,"Tue, 4 Nov 2008 03:21:09 +0000","Tue, 1 Nov 2011 02:54:08 +0000","Mon, 5 Jan 2009 21:16:54 +0000",5421345,"I like the map declaration, but I think that the MetaData class should live outside of the generated code. It's not actually a class that will vary by structure, so it should just live in c.f.thrift.",0.2,0.2,neutral
thrift,191,comment_3,"Here is a first patch. For the time being the metadata structure only contains the field name and is an inner class just like Isset, but unlike Isset this one will be the same for all classes, so may be it should be placed somewhere else. Let me know your thoughts.",code_debt,low_quality_code,"Tue, 4 Nov 2008 03:21:09 +0000","Tue, 1 Nov 2011 02:54:08 +0000","Mon, 5 Jan 2009 21:16:54 +0000",5421345,"Here is a first patch. For the time being the metadata structure only contains the field name and is an inner class just like Isset, but unlike Isset this one will be the same for all classes, so may be it should be placed somewhere else. Let me know your thoughts.",-0.1666666667,-0.1666666667,neutral
thrift,195,comment_0,"The docs about an apache/php endpoint are sorta facebook specific. Other than that, LGTM.",documentation_debt,low_quality_documentation,"Sat, 8 Nov 2008 00:59:00 +0000","Tue, 1 Nov 2011 02:51:54 +0000","Sat, 31 Jan 2009 22:13:12 +0000",7334052,"The docs about an apache/php endpoint are sorta facebook specific. Other than that, LGTM.",0.0,0.0,neutral
thrift,211,comment_3,"Replaced tabs with spaces (oops). Fixed #1. Is this unified enough to you? It eliminates the double search. It is not super-unified, but unifying them more would be kind of gross. For 2, I did consider it, but who would forge a 'DOWN' message? Verifying it just requires more state. I think converting to eunit would be great.",code_debt,low_quality_code,"Thu, 20 Nov 2008 22:30:30 +0000","Tue, 1 Nov 2011 02:52:21 +0000","Thu, 4 Jun 2009 02:01:37 +0000",16860667,"Replaced tabs with spaces (oops). Fixed #1. Is this unified enough to you? It eliminates the double search. It is not super-unified, but unifying them more would be kind of gross. For 2, I did consider it, but who would forge a 'DOWN' message? Verifying it just requires more state. I think converting to eunit would be great.",0.076,0.076,neutral
thrift,211,comment_6,"OK. This is probably good enough to check in. I don't remember if erlang:monitor has a monopoly on DOWN messages, so hence there's some reason to at least add a guard clause (is_reference() on the monitor_ref) to make sure that it's in the right format.",code_debt,low_quality_code,"Thu, 20 Nov 2008 22:30:30 +0000","Tue, 1 Nov 2011 02:52:21 +0000","Thu, 4 Jun 2009 02:01:37 +0000",16860667,"OK. This is probably good enough to check in. I don't remember if erlang:monitor has a monopoly on DOWN messages, so hence there's some reason to at least add a guard clause (is_reference() on the monitor_ref) to make sure that it's in the right format.",0.7198333333,0.7198333333,neutral
thrift,211,comment_2,"1/ Unify the setup of {Starter, Opts} so that you don't have to do the monitor keysearch twice. 2/ Consider checking the source of the DOWN message ... having an unverified DOWN message could lead to some very hard-to-trace bugs. Lg otherwise. Also, what do you think about converting all the Erl unit tests to use eunit at some point (now that it's standard as of R12B) ?",design_debt,non-optimal_design,"Thu, 20 Nov 2008 22:30:30 +0000","Tue, 1 Nov 2011 02:52:21 +0000","Thu, 4 Jun 2009 02:01:37 +0000",16860667,"1/ Unify the setup of {Starter, Opts} so that you don't have to do the monitor keysearch twice. 2/ Consider checking the source of the DOWN message ... having an unverified DOWN message could lead to some very hard-to-trace bugs. Lg otherwise. Also, what do you think about converting all the Erl unit tests to use eunit at some point (now that it's standard as of R12B) ?",-0.0880625,-0.0880625,neutral
thrift,241,comment_0,Sorry about the messed up formatting. Clean patch attached.,code_debt,low_quality_code,"Wed, 24 Dec 2008 16:53:54 +0000","Thu, 15 Jan 2009 22:15:34 +0000","Mon, 5 Jan 2009 20:19:05 +0000",1049111,Sorry about the messed up formatting. Clean patch attached.,-0.025,-0.025,negative
thrift,255,comment_2,1) const std::string& for path 2) does specifying default params in just the .cpp file really work? seems like the compiler would have trouble understanding that. 3) Document using O_APPEND. Maybe allow overwriting as another option,code_debt,low_quality_code,"Sat, 10 Jan 2009 01:07:06 +0000","Tue, 1 Nov 2011 02:52:03 +0000","Thu, 26 Mar 2009 06:23:50 +0000",6499004,1) const std::string& for path 2) does specifying default params in just the .cpp file really work? seems like the compiler would have trouble understanding that. 3) Document using O_APPEND. Maybe allow overwriting as another option,-0.0524,-0.0524,neutral
thrift,275,comment_1,"This patch removes all the deprecation stuff and the t*.rb classes that were only placeholders. In addition, I've changed the implementations of some ""abstract"" methods to throw NotImplementedError instead of returning nil, and fixed the test accordingly. Finally, I removed the no longer required borrow and consume methods from all the transport implementations that had them. (Borrow and consume have been supplanted by the thrift_native package.) All the specs and unit test pass, so I think the job is done. It seems like the only documentation that needs to be made is to indicate that when you see you just change it to and your problems more or less go away. (You will have to replace the T with Thrift:: on some declarations, but that's pretty easy too.) I'd love to have this reviewed and get it committed.",documentation_debt,outdated_documentation,"Sat, 17 Jan 2009 00:44:02 +0000","Tue, 1 Nov 2011 02:54:12 +0000","Tue, 24 Mar 2009 05:31:09 +0000",5719627,"This patch removes all the deprecation stuff and the t*.rb classes that were only placeholders. In addition, I've changed the implementations of some ""abstract"" methods to throw NotImplementedError instead of returning nil, and fixed the test accordingly. Finally, I removed the no longer required borrow and consume methods from all the transport implementations that had them. (Borrow and consume have been supplanted by the thrift_native package.) All the specs and unit test pass, so I think the job is done. It seems like the only documentation that needs to be made is to indicate that when you see you just change it to and your problems more or less go away. (You will have to replace the T with Thrift:: on some declarations, but that's pretty easy too.) I'd love to have this reviewed and get it committed.",0.02566666667,0.02566666667,neutral
thrift,278,comment_2,"Does this work? Maybe I'm reading it wrong, but: {{+ indent(out) << ""throw new field '"" << field- Looks like it just prints the field name as the value. Got a test?",test_debt,lack_of_tests,"Tue, 20 Jan 2009 00:52:35 +0000","Tue, 1 Nov 2011 02:54:22 +0000","Wed, 18 Mar 2009 01:51:07 +0000",4928312,"Does this work? Maybe I'm reading it wrong, but: + indent(out) << ""throw new TProtocolException(\""The field '"" << field->get_name() << ""' has been assigned the invalid value \"" + "" << field->get_name() << "");"" << endl; Looks like it just prints the field name as the value. Got a test?",0.1166666667,0.1166666667,neutral
thrift,298,comment_3,"Well, if you can believe it, I don't get what's going on here, either :). I think this code is a little too complicated. For now, I think I'll just apply your patch, and hopefully one day it won't be a problem, as I refactor my way through the Ruby libraries.",code_debt,complex_code,"Sat, 31 Jan 2009 09:36:34 +0000","Tue, 3 Feb 2009 01:17:25 +0000","Tue, 3 Feb 2009 00:32:30 +0000",226556,"Well, if you can believe it, I don't get what's going on here, either . I think this code is a little too complicated. For now, I think I'll just apply your patch, and hopefully one day it won't be a problem, as I refactor my way through the Ruby libraries.",0.3051666667,0.3436666667,negative
thrift,298,comment_2,"I guessed just enough to fix the problem but I don't understand the code enough to write a correct spec for this. I think the original author of the feature (Kevin Ballard (committed by Kevin Clark), I believe, according to the log) and whoever is refactoring ruby protocols (in this case, you, Bryan) should write specs for all the features of thrift.",documentation_debt,outdated_documentation,"Sat, 31 Jan 2009 09:36:34 +0000","Tue, 3 Feb 2009 01:17:25 +0000","Tue, 3 Feb 2009 00:32:30 +0000",226556,"I guessed just enough to fix the problem but I don't understand the code enough to write a correct spec for this. I think the original author of the feature (Kevin Ballard (committed by Kevin Clark), I believe, according to the log) and whoever is refactoring ruby protocols (in this case, you, Bryan) should write specs for all the features of thrift.",0.090625,0.090625,negative
thrift,353,comment_3,here is the correct version. mispelled function name.,documentation_debt,low_quality_documentation,"Wed, 4 Mar 2009 19:14:17 +0000","Thu, 5 Mar 2009 00:42:52 +0000","Wed, 4 Mar 2009 21:34:23 +0000",8406,here is the correct version. mispelled function name.,0.4375,0.4375,neutral
thrift,372,comment_0,"How's this? I cleaned up some missed dead code related to deprecation, as well.",code_debt,dead_code,"Fri, 13 Mar 2009 23:44:08 +0000","Thu, 26 Mar 2009 18:42:05 +0000","Thu, 26 Mar 2009 18:42:05 +0000",1105077,"How's this? I cleaned up some missed dead code related to deprecation, as well.",-0.108625,-0.108625,neutral
thrift,427,comment_1,"The patch looks good, and everything compiles and the tests pass still. It'd be nice if this code was exercised through at least a test .thrift file though.",test_debt,lack_of_tests,"Fri, 3 Apr 2009 22:37:56 +0000","Tue, 1 Nov 2011 02:51:46 +0000","Tue, 7 Apr 2009 20:51:53 +0000",339237,"The patch looks good, and everything compiles and the tests pass still. It'd be nice if this code was exercised through at least a test .thrift file though.",0.4511666667,0.4511666667,positive
thrift,447,comment_0,"This patch restructures the code generator to rely on some abstract base classes for clients, processors, and process functions. It makes the generated code marginally smaller and a lot cleaner.",code_debt,complex_code,"Thu, 9 Apr 2009 17:38:54 +0000","Tue, 8 Feb 2011 17:26:55 +0000","Tue, 8 Feb 2011 17:26:55 +0000",57887281,"This patch restructures the code generator to rely on some abstract base classes for clients, processors, and process functions. It makes the generated code marginally smaller and a lot cleaner.",0.2,0.2,positive
thrift,452,comment_2,"That's because you didn't make your foo field optional. If you make it optional, the isset will be checked. Personally, I think the ""default"" requiredness is confusing - this is the problem you end up with.",design_debt,non-optimal_design,"Sun, 12 Apr 2009 04:16:47 +0000","Tue, 1 Nov 2011 02:52:17 +0000","Fri, 2 Oct 2009 00:58:39 +0000",14935312,"That's because you didn't make your foo field optional. If you make it optional, the isset will be checked. Personally, I think the ""default"" requiredness is confusing - this is the problem you end up with.",-0.1485555556,-0.1485555556,negative
thrift,452,comment_4,"I agree that it is weird that we initialize the enum to an invalid value. Maybe the default value for enums should default to the first declared value, rather than 0?",design_debt,non-optimal_design,"Sun, 12 Apr 2009 04:16:47 +0000","Tue, 1 Nov 2011 02:52:17 +0000","Fri, 2 Oct 2009 00:58:39 +0000",14935312,"I agree that it is weird that we initialize the enum to an invalid value. Maybe the default value for enums should default to the first declared value, rather than 0?",-0.3,-0.3,negative
thrift,471,comment_0,"yeah, the python docs say that __repr__ should be used if there is no __str__ and every other case I have seen follows that rule but the exception printing apparently does not. +1",code_debt,low_quality_code,"Wed, 29 Apr 2009 21:12:16 +0000","Tue, 1 Nov 2011 02:52:11 +0000","Wed, 29 Apr 2009 23:35:22 +0000",8586,"yeah, the python docs say that _repr_ should be used if there is no _str_ and every other case I have seen follows that rule but the exception printing apparently does not. +1",0.1,0.1,neutral
thrift,544,comment_5,"I think the patch is correct, but the argument for correctness is very complicated because constants_[name] is a mutating operation that creates an entry if the ""name"" key is not present. I'd much prefer the condition be != constants.end()""",code_debt,complex_code,"Tue, 21 Jul 2009 15:00:39 +0000","Fri, 10 Sep 2010 17:04:55 +0000","Thu, 5 Aug 2010 23:24:18 +0000",32862219,"I think the patch is correct, but the argument for correctness is very complicated because constants_[name] is a mutating operation that creates an entry if the ""name"" key is not present. I'd much prefer the condition be ""constants_.find(name) != constants.end()""",0.1615,0.1292,negative
thrift,595,comment_0,I made some slight cosmetic changes to your v6 patch to get this.,design_debt,non-optimal_design,"Thu, 1 Oct 2009 20:51:14 +0000","Tue, 1 Nov 2011 02:54:08 +0000","Thu, 1 Oct 2009 20:54:29 +0000",195,I made some slight cosmetic changes to your v6 patch to get this.,0.0,0.0,neutral
thrift,597,comment_1,"One query: I want to use THttpServer in my python project but from initial performance review for a single client single request (No ThreadingMixIn required at this point), it shows that HttpServer is always 2 to 4 times slower than NonBlocking server in tutorial Calculator client server app and same was observed in my product testing as well. Is there a way to make the performance equal for THTTPServer as compared to NonBlocking? Heres the code for NonBlocking client and server in python and its performance on ubuntu. single machine.  In my project I observed that on client side, its the generated code send_<api Will really appreciate if someone can shed some light on how to improve performance for THttpServer in thrift 0.8.0.",code_debt,slow_algorithm,"Fri, 2 Oct 2009 17:12:03 +0000","Sun, 14 Oct 2012 06:49:13 +0000","Thu, 2 Sep 2010 15:15:44 +0000",28937021,"One query: I want to use THttpServer in my python project but from initial performance review for a single client single request (No ThreadingMixIn required at this point), it shows that HttpServer is always 2 to 4 times slower than NonBlocking server in tutorial Calculator client server app and same was observed in my product testing as well. Is there a way to make the performance equal for THTTPServer as compared to NonBlocking? Heres the code for NonBlocking client and server in python and its performance on ubuntu. single machine. ------------------ client: def non_blocking_server_client(): try: Make socket transport = TSocket.TSocket('localhost', 9090) Buffering is critical. Raw sockets are very slow transport = TTransport.TFramedTransport(transport) Wrap in a protocol protocol = TBinaryProtocol.TBinaryProtocol(transport) Create a client to use the protocol encoder client = Calculator.Client(protocol) Connect! transport.open() perform_ops(client) Close! transport.close() except Thrift.TException, tx: print '%s' % (tx.message) server: def non_blocking_server(): handler = CalculatorHandler() processor = Calculator.Processor(handler) transport = TSocket.TServerSocket(port=9090) server = TNonblockingServer.TNonblockingServer(processor, transport) print 'Starting the server...' server.serve() print 'done.' performance timings: $ ./PythonClient.py ping took 0.633 ms <================== ping() add took 0.395 ms 1+1=2 InvalidOperation: InvalidOperation(what=4, why='Cannot divide by 0') calculate took 0.399 ms 15-10=5 getStruct took 0.361 ms Check log: 5 $ ./PythonClient.py ping took 0.536 ms <================== ping() add took 0.362 ms 1+1=2 InvalidOperation: InvalidOperation(what=4, why='Cannot divide by 0') calculate took 0.403 ms 15-10=5 getStruct took 0.364 ms Check log: 5 ------------------ Heres the code for HttpServer client and server in python and its performance ------------------ client: def http_server_client(): try: path = ""http://%s:%s/"" % ('127.0.0.1', 9090) transport = THttpClient.THttpClient(uri_or_host=path) Wrap in a protocol protocol = TBinaryProtocol.TBinaryProtocol(transport) Create a client to use the protocol encoder client = Calculator.Client(protocol) Connect! transport.open() perform_ops(client) Close! transport.close() except Thrift.TException, tx: print '%s' % (tx.message) server: def http_server(): handler = CalculatorHandler() processor = Calculator.Processor(handler) pfactory = TBinaryProtocol.TBinaryProtocolFactory() server = THttpServer.THttpServer(processor, ('127.0.0.1', 9090), pfactory) print 'Starting the server...' server.serve() print 'done.' performance timings: $ ./PythonClient.py ping took 1.535 ms <================== ping() add took 0.972 ms 1+1=2 InvalidOperation: InvalidOperation(what=4, why='Cannot divide by 0') calculate took 0.929 ms 15-10=5 getStruct took 0.943 ms Check log: 5 $ ./PythonClient.py ping took 1.243 ms <================== ping() add took 0.944 ms 1+1=2 InvalidOperation: InvalidOperation(what=4, why='Cannot divide by 0') calculate took 0.930 ms 15-10=5 getStruct took 0.925 ms Check log: 5 ------------------ server side timings are pretty low. In my project I observed that on client side, its the generated code send_<api> takes most of the time. which internally calls httplib's getresponse which probably is waiting for something to come on wire? Will really appreciate if someone can shed some light on how to improve performance for THttpServer in thrift 0.8.0.",0.1631,0.00240625,neutral
thrift,636,comment_2,"I think this would be somewhat clunky and inconsistent to use. If you'd like to submit a patch, feel free to reopen.",design_debt,non-optimal_design,"Fri, 20 Nov 2009 21:50:05 +0000","Thu, 2 May 2013 02:29:23 +0000","Fri, 26 Mar 2010 23:16:22 +0000",10891577,"I think this would be somewhat clunky and inconsistent to use. If you'd like to submit a patch, feel free to reopen.",0.1,0.1,negative
thrift,673,comment_0,Updated patch which catches one more case of trailing whitespace.,code_debt,low_quality_code,"Wed, 13 Jan 2010 23:59:26 +0000","Thu, 2 Sep 2010 14:21:53 +0000","Thu, 2 Sep 2010 14:21:53 +0000",20010147,Updated patch which catches one more case of trailing whitespace.,0.0,0.0,neutral
thrift,685,comment_0,"+1. The template code that Chad and I worked on for C\+\+ (almost ready for trunk) gets the complier to do something similar for us, and we saw a really significant boost.",design_debt,non-optimal_design,"Thu, 21 Jan 2010 23:07:10 +0000","Thu, 18 Feb 2010 18:28:23 +0000","Thu, 18 Feb 2010 18:28:23 +0000",2402473,"+1. The template code that Chad and I worked on for C++ (almost ready for trunk) gets the complier to do something similar for us, and we saw a really significant boost.",0.1541666667,0.1541666667,positive
thrift,685,comment_1,"Here's my initial effort. This produces a pretty noticeable speedup in deserialization time when using the compact protocol and TDeserializer. This patch notably does not yet address the binary protocol implementation. I'd love some people to review it and give me feedback. Ideally, we'd commit what I have here as a starting point and then continue to improve other parts.",design_debt,non-optimal_design,"Thu, 21 Jan 2010 23:07:10 +0000","Thu, 18 Feb 2010 18:28:23 +0000","Thu, 18 Feb 2010 18:28:23 +0000",2402473,"Here's my initial effort. This produces a pretty noticeable speedup in deserialization time when using the compact protocol and TDeserializer. This patch notably does not yet address the binary protocol implementation. I'd love some people to review it and give me feedback. Ideally, we'd commit what I have here as a starting point and then continue to improve other parts.",0.204,0.204,positive
thrift,695,comment_3,"Here's a simpler to use, more pythonic version.",code_debt,complex_code,"Tue, 2 Feb 2010 00:04:30 +0000","Tue, 1 Nov 2011 02:53:48 +0000","Fri, 26 Feb 2010 00:56:40 +0000",2076730,"Here's a simpler to use, more pythonic version.",0.0,0.0,neutral
thrift,695,comment_1,"This seems like a very faithful translation of the Java version, which has benefits and drawbacks. I'm okay committing it, but the patch seems corrupt (""%ld"" where there should be a line number).",design_debt,non-optimal_design,"Tue, 2 Feb 2010 00:04:30 +0000","Tue, 1 Nov 2011 02:53:48 +0000","Fri, 26 Feb 2010 00:56:40 +0000",2076730,"This seems like a very faithful translation of the Java version, which has benefits and drawbacks. I'm okay committing it, but the patch seems corrupt (""%ld"" where there should be a line number).",0.3354166667,0.3354166667,neutral
thrift,710,comment_0,"This patch makes TBinaryProtocol use direct buffer access in the relevant methods. My performance testing was somewhat rudimentary, but I think it may have as much as doubled performance. Obviously your performance boost will be really dependent on the contents of your struct, but this seems pretty great. As a side effect of this issue, I refactored the TCompactProtocol test so that we could exact TBinaryProtocol to the same bevy of test cases.",code_debt,slow_algorithm,"Thu, 18 Feb 2010 18:29:55 +0000","Tue, 2 Mar 2010 18:49:10 +0000","Tue, 2 Mar 2010 18:49:10 +0000",1037955,"This patch makes TBinaryProtocol use direct buffer access in the relevant methods. My performance testing was somewhat rudimentary, but I think it may have as much as doubled performance. Obviously your performance boost will be really dependent on the contents of your struct, but this seems pretty great. As a side effect of this issue, I refactored the TCompactProtocol test so that we could exact TBinaryProtocol to the same bevy of test cases.",0.09120833333,0.09120833333,positive
thrift,714,comment_1,"After reading up on it some more, I think this change is what we actually want. Up until the min number of threads, new ""core"" pool threads will be created. Once we pass this limit, new threads will be created up to the max number of threads, at which point the server will start rejecting executions. After (default) 60 seconds of idleness, the pool will start to kill threads, but go no lower than the min number of threads. This patch is probably incomplete, as if we get a rejected execution exception when trying to queue an invocation, we should respond to the client with an error immediately.",code_debt,low_quality_code,"Wed, 24 Feb 2010 16:20:58 +0000","Wed, 28 Jul 2010 21:32:18 +0000","Wed, 28 Jul 2010 21:32:18 +0000",13324280,"After reading up on it some more, I think this change is what we actually want. Up until the min number of threads, new ""core"" pool threads will be created. Once we pass this limit, new threads will be created up to the max number of threads, at which point the server will start rejecting executions. After (default) 60 seconds of idleness, the pool will start to kill threads, but go no lower than the min number of threads. This patch is probably incomplete, as if we get a rejected execution exception when trying to queue an invocation, we should respond to the client with an error immediately.",-0.04,-0.04,neutral
thrift,714,comment_3,I committed a patch to remove maxWorkerThreads and rename minWorkerThreads to workerThreads. This will keep the behavior the same and remove the ineffectual parameter.,code_debt,dead_code,"Wed, 24 Feb 2010 16:20:58 +0000","Wed, 28 Jul 2010 21:32:18 +0000","Wed, 28 Jul 2010 21:32:18 +0000",13324280,I committed a patch to remove maxWorkerThreads and rename minWorkerThreads to workerThreads. This will keep the behavior the same and remove the ineffectual parameter.,0.05,0.05,neutral
thrift,717,comment_3,"Well, I can't accept this patch because it doesn't update every use of THRIFT_ROOT. However, I'm not convinced that this would be a good approach even if the patch were complete. I think the security argument is completely pointless, since any attacker capable of injecting PHP code into your execution would not need to mess with Thrift to take over your server. The performance argument is also not compelling, since the cost of a single global lookup is tiny compared to the cost of including a library file (even with APC (or HipHop)). Finally, I think that leaving the value in a global gives users more freedom when setting up their environment. It is easy to determine if it is already set and easy to fix after-the-fact if you need to hack around something in your sandbox.",design_debt,non-optimal_design,"Thu, 25 Feb 2010 16:40:07 +0000","Tue, 1 Nov 2011 02:54:07 +0000","Mon, 11 Apr 2011 16:13:52 +0000",35422425,"Well, I can't accept this patch because it doesn't update every use of THRIFT_ROOT. However, I'm not convinced that this would be a good approach even if the patch were complete. I think the security argument is completely pointless, since any attacker capable of injecting PHP code into your execution would not need to mess with Thrift to take over your server. The performance argument is also not compelling, since the cost of a single global lookup is tiny compared to the cost of including a library file (even with APC (or HipHop)). Finally, I think that leaving the value in a global gives users more freedom when setting up their environment. It is easy to determine if it is already set and easy to fix after-the-fact if you need to hack around something in your sandbox.",0.1018611111,0.1018611111,negative
thrift,717,comment_4,I agree with David on all points above. the globals lookup is meaningless for performance next to the multi-millisecond RPC you're about to send out :),design_debt,non-optimal_design,"Thu, 25 Feb 2010 16:40:07 +0000","Tue, 1 Nov 2011 02:54:07 +0000","Mon, 11 Apr 2011 16:13:52 +0000",35422425,I agree with David on all points above. the globals lookup is meaningless for performance next to the multi-millisecond RPC you're about to send out,0.1,-0.1,negative
thrift,717,comment_6,"Ok, sure, the performance and security issues are fairly pointless. The biggest problem here is pollution of the global namespace, which is a bit of a show-stopper when integrating in to larger frameworks or libraries. Constants work because they don't pollute the $GLOBALS super-global and are immutable, so other system components can't accidentally modify them (i.e. through naming clashes) without the developer being alerted to it. I thought I'd got all references to I take it some reside outside of lib/php/src/ ?",design_debt,non-optimal_design,"Thu, 25 Feb 2010 16:40:07 +0000","Tue, 1 Nov 2011 02:54:07 +0000","Mon, 11 Apr 2011 16:13:52 +0000",35422425,"Ok, sure, the performance and security issues are fairly pointless. The biggest problem here is pollution of the global namespace, which is a bit of a show-stopper when integrating in to larger frameworks or libraries. Constants work because they don't pollute the $GLOBALS super-global and are immutable, so other system components can't accidentally modify them (i.e. through naming clashes) without the developer being alerted to it. I thought I'd got all references to $GLOBALS['THRIFT_ROOT'], I take it some reside outside of lib/php/src/ ?",0.1354166667,0.1354166667,negative
thrift,717,comment_7,"Constants pollute a different global namespace, so I don't see the win. Plus, the likelihood of a collision with ""THRIFT_ROOT"" is small. Yes, immutability protects you from another library messing you up by running later, but exposes you to another library messing you up by running sooner, so I don't see the win. Plus, it reduces your flexibility with how you choose to set THRIFT_ROOT. Yes, there are references in the generated code, created by the compiler.",design_debt,non-optimal_design,"Thu, 25 Feb 2010 16:40:07 +0000","Tue, 1 Nov 2011 02:54:07 +0000","Mon, 11 Apr 2011 16:13:52 +0000",35422425,"Constants pollute a different global namespace, so I don't see the win. Plus, the likelihood of a collision with ""THRIFT_ROOT"" is small. Yes, immutability protects you from another library messing you up by running later, but exposes you to another library messing you up by running sooner, so I don't see the win. Plus, it reduces your flexibility with how you choose to set THRIFT_ROOT. Yes, there are references in the generated code, created by the compiler.",0.0374,0.0374,negative
thrift,750,comment_3,"Okay. This seems acceptable. There is a little bit of a performance hit, but it is not major. Can you attach a patch?",code_debt,slow_algorithm,"Fri, 2 Apr 2010 04:31:57 +0000","Thu, 14 Dec 2017 13:54:39 +0000","Thu, 26 Jan 2017 01:55:31 +0000",215213014,"Okay. This seems acceptable. There is a little bit of a performance hit, but it is not major. Can you attach a patch?",0.367,0.367,neutral
thrift,788,comment_0,"Refining my notes bellow according to the comment of original creator of this issue: Mine is happening to SuperColumnFamily with 9 or more column per row. Moving discoveries from CASSANDRA-1199 to here: I am comparing the following * Reading of 100 SuperColumns in 1 SCF row using multiget_slice() with 1 key and 1 column name in 100 loop iterations * Reading of 100 SuperColumns in 1 SCF row using multiget_slice() with 1 key and 100 column names in a single call I always get a consistent result and that is the single call takes more time then 100 calls. After some investigation, it seamed that the time it took to execute multiget_slice with 100 columns is always close to the TSocket- This only happens if is used. I have attached my code to reproduce this issue. You can set the timeouts to see how it affects the read call in multiget_slice. Please investigate. This a timing example for the above scenario with TSocket's default timeouts: 100 Sequential Writes took: 0.4047749042511 seconds; 100 Sequential Reads took: 0.16357207298279 seconds; 100 Batch Read took: 0.77017998695374 seconds;",documentation_debt,low_quality_documentation,"Mon, 24 May 2010 16:02:00 +0000","Wed, 10 Aug 2011 18:27:51 +0000","Mon, 11 Apr 2011 15:46:52 +0000",27819892,"Refining my notes bellow according to the comment of original creator of this issue: Mine is happening to SuperColumnFamily with 9 or more column per row. Moving discoveries from CASSANDRA-1199 to here: I am comparing the following Reading of 100 SuperColumns in 1 SCF row using multiget_slice() with 1 key and 1 column name in 100 loop iterations Reading of 100 SuperColumns in 1 SCF row using multiget_slice() with 1 key and 100 column names in a single call I always get a consistent result and that is the single call takes more time then 100 calls. After some investigation, it seamed that the time it took to execute multiget_slice with 100 columns is always close to the TSocket->recvTimeout, Increasing the recvTimeout results that call to take that much time before retuning. After digged into TSocket->read (TSocket.php line 261) and looking at some of the meta data of fread, it seams that none of the buffer chunks get the eof flag=1. And the stream waits till timeout has reached. This only happens if TBinaryProtocolAccelerated (thrift_protocol.so) is used. I have attached my code to reproduce this issue. You can set the timeouts to see how it affects the read call in multiget_slice. Please investigate. This a timing example for the above scenario with TSocket's default timeouts: 100 Sequential Writes took: 0.4047749042511 seconds; 100 Sequential Reads took: 0.16357207298279 seconds; 100 Batch Read took: 0.77017998695374 seconds;",0.05238095238,0.04722222222,neutral
thrift,873,comment_1,"I like all of this patch except for the separate JVMs. I know that's its probably a more responsible way to do things, but it makes the test run slower. Are you certain that this is what is necessary in order to make the test pass? Wouldn't it be possible for us just to clean up after ourselves in the test for",test_debt,expensive_tests,"Fri, 27 Aug 2010 05:50:41 +0000","Tue, 1 Nov 2011 02:52:01 +0000","Fri, 27 Aug 2010 06:18:02 +0000",1641,"I like all of this patch except for the separate JVMs. I know that's its probably a more responsible way to do things, but it makes the test run slower. Are you certain that this is what is necessary in order to make the test pass? Wouldn't it be possible for us just to clean up after ourselves in the test for TAsyncClientManager?",0.1166666667,0.1166666667,neutral
thrift,873,comment_2,"On my box it goes a bit slower, but still pretty fast. With forkmode=""once"" I did indeed see failures that went away with the separate JVMs. It also made it harder to figure out which test was actually at fault, since the problem with AsyncClientManager caused a bunch of later tests to fail too.",test_debt,expensive_tests,"Fri, 27 Aug 2010 05:50:41 +0000","Tue, 1 Nov 2011 02:52:01 +0000","Fri, 27 Aug 2010 06:18:02 +0000",1641,"On my box it goes a bit slower, but still pretty fast. With forkmode=""once"" I did indeed see failures that went away with the separate JVMs. It also made it harder to figure out which test was actually at fault, since the problem with AsyncClientManager caused a bunch of later tests to fail too.",-0.1865,-0.1865,neutral
thrift,897,comment_6,"LGTM. In the main.cc part, you could possibly check the part before the dot matches the name of the enum (possibly with the scope qualifier). Not critical.",code_debt,low_quality_code,"Fri, 10 Sep 2010 18:59:08 +0000","Sun, 12 Sep 2010 14:39:15 +0000","Sun, 12 Sep 2010 14:39:15 +0000",157207,"LGTM. In the main.cc part, you could possibly check the part before the dot matches the name of the enum (possibly with the scope qualifier). Not critical.",0.0,0.0,neutral
thrift,897,comment_4,"I think the smart resolution thing is only worthwhile if we really want to keep backwards compatibility. I'm not crazy about breaking people's IDL, but I think that's preferable in the long term to supporting a more convoluted syntax. If we were starting from scratch today, I think that we'd want to use the fully-qualified approach, and the cost of fixing the breakage *should* be reasonably low, especially because you're getting a consistency improvement at the same time. This is certainly my preference.",design_debt,non-optimal_design,"Fri, 10 Sep 2010 18:59:08 +0000","Sun, 12 Sep 2010 14:39:15 +0000","Sun, 12 Sep 2010 14:39:15 +0000",157207,"I think the smart resolution thing is only worthwhile if we really want to keep backwards compatibility. I'm not crazy about breaking people's IDL, but I think that's preferable in the long term to supporting a more convoluted syntax. If we were starting from scratch today, I think that we'd want to use the fully-qualified approach, and the cost of fixing the breakage should be reasonably low, especially because you're getting a consistency improvement at the same time. This is certainly my preference.",0.4149375,0.4149375,neutral
thrift,962,comment_5,Brilliant! I was thinking on it but I thought it would be really hard to get done. It'd be awesome to get rid of this redundancy! We would basically be able to compile the web site ;),code_debt,complex_code,"Mon, 18 Oct 2010 23:53:50 +0000","Thu, 10 Jul 2014 13:42:34 +0000","Thu, 10 Jul 2014 13:42:34 +0000",117553724,Brilliant! I was thinking on it but I thought it would be really hard to get done. It'd be awesome to get rid of this redundancy! We would basically be able to compile the web site,0.46,0.496,positive
thrift,962,comment_0,"fully agree Bryan! It is really important to have a consistent tutorial across all Languages using the same tutorial.thrift file and possible interaction between these tutorials might be great! Another place is the wiki, some ThriftUsage pages do not use the tutorial example, they have their own",documentation_debt,low_quality_documentation,"Mon, 18 Oct 2010 23:53:50 +0000","Thu, 10 Jul 2014 13:42:34 +0000","Thu, 10 Jul 2014 13:42:34 +0000",117553724,"fully agree Bryan! It is really important to have a consistent tutorial across all Languages using the same tutorial.thrift file and possible interaction between these tutorials might be great! Another place is the wiki, some ThriftUsage pages do not use the tutorial example, they have their own http://wiki.apache.org/thrift/ThriftUsage",0.3,0.3,positive
thrift,962,comment_1,just did the nodejs tutorial an had a look on other missing ones... managing tutorial for the website as copy of existing tutorial source is a maintenance nightmare. I think we should move the website to the git repo within a www folder and reference markdown files within source repo directly. source =let's go for markdown: rename all files such as README to README.md we can still publish the web site via svn. any thoughts? ;-r,documentation_debt,low_quality_documentation,"Mon, 18 Oct 2010 23:53:50 +0000","Thu, 10 Jul 2014 13:42:34 +0000","Thu, 10 Jul 2014 13:42:34 +0000",117553724,just did the nodejs tutorial an had a look on other missing ones... managing tutorial for the website as copy of existing tutorial source is a maintenance nightmare. I think we should move the website to the git repo within a www folder and reference markdown files within source repo directly. source => web site let's go for markdown: rename all files such as README to README.md we can still publish the web site via svn. any thoughts? ;-r,0.0635,0.0635,positive
thrift,962,comment_2,"Anthing that helps towards getting better documentation is fine with me. There are some old tutorials in the old wiki and I really would like to see them on the web site. Not sure if git will help with this ;-), but I don't have any problems with it either, as long as it produces less overhead, instead of more.",documentation_debt,low_quality_documentation,"Mon, 18 Oct 2010 23:53:50 +0000","Thu, 10 Jul 2014 13:42:34 +0000","Thu, 10 Jul 2014 13:42:34 +0000",117553724,"Anthing that helps towards getting better documentation is fine with me. There are some old tutorials in the old wiki and I really would like to see them on the web site. Not sure if git will help with this , but I don't have any problems with it either, as long as it produces less overhead, instead of more.",0.1888888889,0.1444444444,positive
thrift,962,comment_4,"I would love to have the capability to include source code directly on the web site especially for tutorial, test, IDL's. see here: = People do probably not know how to contribute documentation to our project, having the web site within the source tree can simplify this. What about patches including code and web site fixes?",documentation_debt,low_quality_documentation,"Mon, 18 Oct 2010 23:53:50 +0000","Thu, 10 Jul 2014 13:42:34 +0000","Thu, 10 Jul 2014 13:42:34 +0000",117553724,"I would love to have the capability to include source code directly on the web site especially for tutorial, test, IDL's. see here: http://octopress.org/docs/plugins/include-code/ => simple maintenance, no duplicates People do probably not know how to contribute documentation to our project, having the web site within the source tree can simplify this. What about patches including code and web site fixes?",0.095,0.095,positive
thrift,962,comment_7,"tutorials are now incorporating code from repo /tutorials, this will always be a work in progress to make better and improve what we can provide to the community. closing as we now have something available",documentation_debt,low_quality_documentation,"Mon, 18 Oct 2010 23:53:50 +0000","Thu, 10 Jul 2014 13:42:34 +0000","Thu, 10 Jul 2014 13:42:34 +0000",117553724,"tutorials are now incorporating code from repo /tutorials, this will always be a work in progress to make better and improve what we can provide to the community. closing as we now have something available",0.2375,0.2375,positive
thrift,1062,comment_4,THRIFT-1735 integrates Python Tutorials into regular build please create a test case for the test suite: - test/test.sh (interoperability) - test/py/ - test/py.twisted,test_debt,lack_of_tests,"Wed, 16 Feb 2011 11:38:17 +0000","Sat, 8 Jun 2013 03:15:38 +0000","Sat, 8 Jun 2013 03:15:38 +0000",72805041,THRIFT-1735 integrates Python Tutorials into regular build please create a test case for the test suite: test/test.sh (interoperability) test/py/ test/py.twisted,0.06666666667,0.06666666667,neutral
thrift,1065,comment_1,remove some misplaced code (probably copied from another implementation?),code_debt,dead_code,"Thu, 17 Feb 2011 18:30:31 +0000","Wed, 9 Mar 2011 18:17:15 +0000","Tue, 22 Feb 2011 21:04:05 +0000",441214,remove some misplaced code (probably copied from another implementation?),-0.4,-0.4,neutral
thrift,1065,comment_3,"apparently the java server is not encoding undeclared exceptions ""properly"" (it could be my impl., please see patch), but it works fine on C++ Incoming content: [cpp] Outgoing content: is a [java] Outgoing content: Error:This is a",design_debt,non-optimal_design,"Thu, 17 Feb 2011 18:30:31 +0000","Wed, 9 Mar 2011 18:17:15 +0000","Tue, 22 Feb 2011 21:04:05 +0000",441214,"apparently the java server is not encoding undeclared exceptions ""properly"" (it could be my impl., please see patch), but it works fine on C++ Incoming content: [1,""testException"",1,0,{""1"":{""str"":""TApplicationException""}}] [cpp] Outgoing content: [1,""testException"",3,0,{""1"": {""str"":""This is a TApplicationException""} ,""2"":{""i32"":0}}] [java] Outgoing content: Error:This is a TApplicationException",0.3285714286,0.1666666667,neutral
thrift,1121,comment_1,Bryan can you revert THRIFT-959? We noticed the same slowdown,code_debt,slow_algorithm,"Mon, 28 Mar 2011 18:06:04 +0000","Thu, 3 Jan 2019 04:32:08 +0000","Tue, 27 Sep 2011 17:08:01 +0000",15807717,Bryan can you revert THRIFT-959? We noticed the same slowdown,0.0,0.0,neutral
thrift,1121,comment_3,"I don't have any test case - the regression showed up in large scale performance tests of a distributed system. Just forwarding along some results for some folks who, at the time, were not permitted to participate in the JIRA.",code_debt,slow_algorithm,"Mon, 28 Mar 2011 18:06:04 +0000","Thu, 3 Jan 2019 04:32:08 +0000","Tue, 27 Sep 2011 17:08:01 +0000",15807717,"I don't have any test case - the regression showed up in large scale performance tests of a distributed system. Just forwarding along some results for some folks who, at the time, were not permitted to participate in the JIRA.",0.0,0.0,neutral
thrift,1121,comment_4,"Although this issue is closed and considered fixed, I would like to add my two cents on the combination of TFramedTransport and TSocket. I have worked with Hector, a Cassandra client, which is using Thrift v0.6.1, which is suffering from a large deal of overhead, at least in my analysis. I came to the conclusion that the performance regression is caused - at least in my setup - by overhead on the TCP layer. In my setup I use the binary protocol over the framed transport over the Thrift socket (without buffering in v0.6.1). I discovered, that two TCP segments are being sent for every frame sent. One for the length of the frame and one for the frame itself (or more if the frame is larger than the maximum a TCP segment can hold). With small messages, the overhead is substantial: 56 extra bytes of headers in case of Ethernet + IP + TCP. Also considering that in my setup the PSH flag was raised, causing the 4 bytes length of the frame to be pushed from the TCP stack to the application ... while the data itself is on its way. Now this issue is alleviated by using a buffered output stream in TSocket. However, I do think that this solution causes unnecessary memory overhead in cases where framed transport is used. Then the data is in memory twice. And the data is first written into the framed transport buffer, then written in the buffered output stream of the socket and then written to the TCP stack. The above goes for the writing side of things. As for the reading: it's a system call extra (first read the length and then in a separte call read the frame from the socket). I do not estimate this to account for the big difference in throughput. Of course in cases where the framed transport isn't used, buffered in and output streams are very useful.",design_debt,non-optimal_design,"Mon, 28 Mar 2011 18:06:04 +0000","Thu, 3 Jan 2019 04:32:08 +0000","Tue, 27 Sep 2011 17:08:01 +0000",15807717,"Although this issue is closed and considered fixed, I would like to add my two cents on the combination of TFramedTransport and TSocket. I have worked with Hector, a Cassandra client, which is using Thrift v0.6.1, which is suffering from a large deal of overhead, at least in my analysis. I came to the conclusion that the performance regression is caused - at least in my setup - by overhead on the TCP layer. In my setup I use the binary protocol over the framed transport over the Thrift socket (without buffering in v0.6.1). I discovered, that two TCP segments are being sent for every frame sent. One for the length of the frame and one for the frame itself (or more if the frame is larger than the maximum a TCP segment can hold). With small messages, the overhead is substantial: 56 extra bytes of headers in case of Ethernet + IP + TCP. Also considering that in my setup the PSH flag was raised, causing the 4 bytes length of the frame to be pushed from the TCP stack to the application ... while the data itself is on its way. Now this issue is alleviated by using a buffered output stream in TSocket. However, I do think that this solution causes unnecessary memory overhead in cases where framed transport is used. Then the data is in memory twice. And the data is first written into the framed transport buffer, then written in the buffered output stream of the socket and then written to the TCP stack. The above goes for the writing side of things. As for the reading: it's a system call extra (first read the length and then in a separte call read the frame from the socket). I do not estimate this to account for the big difference in throughput. Of course in cases where the framed transport isn't used, buffered in and output streams are very useful.",0.0561,0.0561,neutral
thrift,1130,comment_0,"Not sure if this is the best fix, but seems to be rather minimalistic By the way - this is my first attempt at contribution here, please let me know if there is some other procedure should be followed.",design_debt,non-optimal_design,"Tue, 5 Apr 2011 23:47:24 +0000","Thu, 13 Oct 2011 21:49:25 +0000","Thu, 13 Oct 2011 21:34:48 +0000",16494444,"Not sure if this is the best fix, but seems to be rather minimalistic By the way - this is my first attempt at contribution here, please let me know if there is some other procedure should be followed.",-0.3375,-0.3375,negative
thrift,1130,comment_2,"I haven't tested this patch yet, but my concern is that by adding these tokens, you may possibly have broken string literals or identifiers named ""true"" and ""false"". Can you add some examples to ThriftTest.thrift that exercises these corner cases?",test_debt,lack_of_tests,"Tue, 5 Apr 2011 23:47:24 +0000","Thu, 13 Oct 2011 21:49:25 +0000","Thu, 13 Oct 2011 21:34:48 +0000",16494444,"I haven't tested this patch yet, but my concern is that by adding these tokens, you may possibly have broken string literals or identifiers named ""true"" and ""false"". Can you add some examples to ThriftTest.thrift that exercises these corner cases?",-0.0325,-0.0325,negative
thrift,1130,comment_3,"This was my concern too, and my threshold for passage was to ensure that it will provide an error, obviously its not the same error as it originally did. At the end of the day, this seems to be pretty much like a c-pre-processor macros: #define true 1 #define false 0 The the items NOT in my negative tests that WOULD fail are attempts to use true/false as field numbers (for example) What you are asking for is a negative test (which I performed locally), I only saw one but it is commented out (so its not in any automated test) Is this how you would like to see it? If not can you point me at an example of a negative test in ThriftTest.thrift? This is what I tried and what I had in mind ...",test_debt,lack_of_tests,"Tue, 5 Apr 2011 23:47:24 +0000","Thu, 13 Oct 2011 21:49:25 +0000","Thu, 13 Oct 2011 21:34:48 +0000",16494444,"This was my concern too, and my threshold for passage was to ensure that it will provide an error, obviously its not the same error as it originally did. At the end of the day, this seems to be pretty much like a c-pre-processor macros: #define true 1 #define false 0 The the items NOT in my negative tests that WOULD fail are attempts to use true/false as field numbers (for example) What you are asking for is a negative test (which I performed locally), I only saw one but it is commented out (so its not in any automated test) Is this how you would like to see it? If not can you point me at an example of a negative test in ThriftTest.thrift? This is what I tried and what I had in mind ... struct boolDefValTst { // positive tests: 1: optional bool myBool_1 = true, 2: optional bool myBool_2 = false, 3: optional bool myBool_3 = 1, 4: optional bool myBool_4 = 0, 5: optional bool myBool_5, 6: optional i32 myI32_1 = true, // negative tests: // (uncomment each, one at a time to test) // - Test that we get an error using boolean constants as field names //7: optional i32 true, // [ERROR:...] (last token was 'true') syntax error //8: optional i32 false, // [ERROR:...] (last token was 'false') syntax error // - Test that we get an error using boolean constants type names //9: optional true myI32_2, // [ERROR:...] (last token was 'true') syntax error //10: optional false myI32_3, // [ERROR:...] (last token was 'false') syntax error // - Test that we get an error using boolean constants as values for incompatible types //11: optional string myStr1 = true, // [FAILURE:...] type error: const ""myStr"" was declared as string } Thanks",0.1083333333,0.07961568627,negative
thrift,1135,comment_4,From what I can tell Pierre put together a pretty complete calculator tutorial in another patch which is now committed. I added some minor clean up in the attached patch: The predates the above work and should not be applied. I think 1135 can be safely closed.,code_debt,low_quality_code,"Fri, 8 Apr 2011 21:20:38 +0000","Wed, 12 Mar 2014 02:36:59 +0000","Wed, 12 Mar 2014 02:36:59 +0000",92294181,From what I can tell Pierre put together a pretty complete calculator tutorial in another patch which is now committed. I added some minor clean up in the attached patch: 0001-node.js-tutorial-cleanup.patch. The thrift-1135-nodejs-tutorial_1.patch predates the above work and should not be applied. I think 1135 can be safely closed.,0.35,0.2214285714,neutral
thrift,1141,comment_0,"patch did not apply cleanly.. fixed that, the wrong libthrift.jar file location and updated the changelog. => committed",architecture_debt,violation_of_modularity,"Tue, 12 Apr 2011 10:41:32 +0000","Tue, 1 Nov 2011 02:53:46 +0000","Tue, 12 Apr 2011 20:37:07 +0000",35735,"patch did not apply cleanly.. fixed that, the wrong libthrift.jar file location and updated the changelog. => committed",0.08333333333,0.08333333333,negative
thrift,1174,comment_2,got this all working and published to the staging repo with no issues. Generates the pom and swc files based on the current java version. Artifacts uploaded where I'm cleaning up the and as3 build files to share common components and reduce code duplications. Will check the updates into trunk shortly,code_debt,duplicated_code,"Thu, 19 May 2011 19:12:52 +0000","Fri, 20 May 2011 19:37:34 +0000","Fri, 20 May 2011 19:05:14 +0000",85942,got this all working and published to the staging repo with no issues. Generates the pom and swc files based on the current java version. Artifacts uploaded where libthrift-as3-0.7.0-snapshot.pom libthrift-as3-0.7.0-snapshot.swc I'm cleaning up the java/fb303/javascript and as3 build files to share common components and reduce code duplications. Will check the updates into trunk shortly,0.2,0.08,positive
thrift,1241,comment_0,"Thanks Darius, initial work looks great. Will want to change from namespace53 to just namespace since php5.3+ will have this option available and php5.4 is in rc right now so no sense in defining a specific version in the option. Can you attach this as a patch with asf inclusion please.",design_debt,non-optimal_design,"Wed, 20 Jul 2011 15:24:17 +0000","Wed, 10 Aug 2011 18:27:52 +0000","Thu, 4 Aug 2011 23:00:01 +0000",1323344,"Thanks Darius, initial work looks great. Will want to change from namespace53 to just namespace since php5.3+ will have this option available and php5.4 is in rc right now so no sense in defining a specific version in the option. Can you attach this as a patch with asf inclusion please.",0.2920666667,0.2920666667,positive
thrift,1243,comment_4,"Included is a patch on 0.8 that adds more error handling to the libevent based server. It also fixes 2 minor bugs (handle failure of evhttp_bind_socket and prevent exceptions to reach libevent in As far as I can tell, the question of @diwaker is still valid, there is no good way currently to have the callback of to be passed the exact failure. However I don't think returning a bool helps, because libevent is the one asynchronously calling, so the error will have to be handled by the client *within* the callback. This callback will invariably throw a when a disconnect or a 404 happens. I added to this patch the ability to log such failures to stderr, but it would take more changes (compiler?) to actually pass the error to the client, so that a call will throw the correct error (and *not* EOF). Finally I removed all abort calls, trying to replace them by appropriate exceptions instead. Thanks!",code_debt,low_quality_code,"Sun, 24 Jul 2011 10:03:04 +0000","Tue, 1 Nov 2011 00:57:57 +0000","Sun, 11 Sep 2011 07:29:12 +0000",4224368,"Included is a patch on 0.8 that adds more error handling to the libevent based server. It also fixes 2 minor bugs (handle failure of evhttp_bind_socket and prevent exceptions to reach libevent in TEvhttpClientChannel::response). As far as I can tell, the question of @diwaker is still valid, there is no good way currently to have the callback of TEvhttpClientChannel::finish, to be passed the exact failure. However I don't think returning a bool helps, because libevent is the one asynchronously calling, so the error will have to be handled by the client within the callback. This callback will invariably throw a TTransportException::END_OF_FILE, when a disconnect or a 404 happens. I added to this patch the ability to log such failures to stderr, but it would take more changes (compiler?) to actually pass the error to the client, so that a call will throw the correct error (and not EOF). Finally I removed all abort calls, trying to replace them by appropriate exceptions instead. Thanks!",-0.04545833333,-0.04096296296,neutral
thrift,1243,comment_2,"Saw this a bit late but I'm concerned about this change. With this change, there is no way for implementors of to signal to the callers that something unexpected happened or any other error conditions. Just because the current implementation ignores the return values doesn't mean they're not useful. Is the recommendation now to throw a TException instead?",design_debt,non-optimal_design,"Sun, 24 Jul 2011 10:03:04 +0000","Tue, 1 Nov 2011 00:57:57 +0000","Sun, 11 Sep 2011 07:29:12 +0000",4224368,"Saw this a bit late but I'm concerned about this change. With this change, there is no way for implementors of sendMessage/recvMessage to signal to the callers that something unexpected happened or any other error conditions. Just because the current implementation ignores the return values doesn't mean they're not useful. Is the recommendation now to throw a TException instead?",-0.2,-0.2,neutral
thrift,1243,comment_3,"@diwaker: Thanks for your comments, I share your concerns and am working currently on a patch for 0.8 that will use exception handling, and more generally handle/propagate libevent errors, log errors... For example I'll propose to remove 'abort' calls in TEvhttpServer.cpp, and replace them by exceptions. Also one significant issue I plan to cover, is to avoid propagating exceptions to libevent (which is C code): see THRIFT-1222 for example.",design_debt,non-optimal_design,"Sun, 24 Jul 2011 10:03:04 +0000","Tue, 1 Nov 2011 00:57:57 +0000","Sun, 11 Sep 2011 07:29:12 +0000",4224368,"@diwaker: Thanks for your comments, I share your concerns and am working currently on a patch for 0.8 that will use exception handling, and more generally handle/propagate libevent errors, log errors... For example I'll propose to remove 'abort' calls in TEvhttpClientChannel.cpp TEvhttpServer.cpp, and replace them by exceptions. Also one significant issue I plan to cover, is to avoid propagating exceptions to libevent (which is C code): see THRIFT-1222 for example.",0.02666666667,0.02,neutral
thrift,1290,comment_0,"Hm, this patch doesn't apply cleanly for me.",code_debt,low_quality_code,"Wed, 24 Aug 2011 16:52:01 +0000","Thu, 25 Aug 2011 17:47:08 +0000","Thu, 25 Aug 2011 17:29:00 +0000",88619,"Hm, this patch doesn't apply cleanly for me.",0.0,0.0,negative
thrift,1431,comment_1,committed. Could you please create the patch from thrift source root directory? This makes it much easier to handle. If you have some spare time... we need a test suite for node.js THRIFT-1134 ;-),test_debt,low_coverage,"Sat, 19 Nov 2011 11:39:17 +0000","Sat, 26 Nov 2011 18:39:16 +0000","Wed, 23 Nov 2011 20:46:30 +0000",378433,committed. Could you please create the patch from thrift source root directory? This makes it much easier to handle. If you have some spare time... we need a test suite for node.js THRIFT-1134,0.22,0.14,positive
thrift,1440,comment_0,"Thanks for Reporting that, we like to apply your patches to improve packaging and build procedure in general. The Build Job for Debian Packages is here: currently we only hav a amd64 at Apache Build Infrastructure... Do some simply tricks or cross compile approaches exist to build more architectures on that build server?",design_debt,non-optimal_design,"Mon, 28 Nov 2011 22:14:06 +0000","Thu, 20 Jun 2013 16:12:38 +0000","Thu, 20 Jun 2013 16:12:38 +0000",49226312,"Thanks for Reporting that, we like to apply your patches to improve packaging and build procedure in general. The Build Job for Debian Packages is here: https://builds.apache.org/view/S-Z/view/Thrift/job/Thrift-Debian-Packages/ currently we only hav a amd64 at Apache Build Infrastructure... Do some simply tricks or cross compile approaches exist to build more architectures on that build server?",0.2,0.2,positive
thrift,1452,comment_0,"Line 1514 of t_cpp_generator.cc in the function has an unused variable ttype, any reason for this or can it be removed? t_type *ttype =",code_debt,dead_code,"Wed, 7 Dec 2011 22:17:56 +0000","Fri, 9 Dec 2011 21:27:30 +0000","Thu, 8 Dec 2011 21:16:16 +0000",82700,"Line 1514 of t_cpp_generator.cc in the generate_struct_swap function has an unused variable ttype, any reason for this or can it be removed? t_type *ttype = get_true_type(tfield->get_type());",0.0,0.191,negative
thrift,1452,comment_1,Nope no reason it looks unused in our codebase as well.,code_debt,dead_code,"Wed, 7 Dec 2011 22:17:56 +0000","Fri, 9 Dec 2011 21:27:30 +0000","Thu, 8 Dec 2011 21:16:16 +0000",82700,Nope no reason it looks unused in our codebase as well.,0.631,0.631,negative
thrift,1466,comment_0,"Currently there are no tutorials available for this lib, also not all generators create a skeleton as you have seen. Your best starting point is the test cases included with the lib lib/c_glib/test/. We are working on trying to create better documentation as we redo the website, but right now your best bet is to email the dev list with any questions you have",documentation_debt,low_quality_documentation,"Thu, 15 Dec 2011 15:35:32 +0000","Fri, 27 Jan 2012 02:34:29 +0000","Fri, 27 Jan 2012 02:34:28 +0000",3668336,"Currently there are no tutorials available for this lib, also not all generators create a skeleton as you have seen. Your best starting point is the test cases included with the lib lib/c_glib/test/. We are working on trying to create better documentation as we redo the website, but right now your best bet is to email the dev list with any questions you have",0.4251333333,0.4251333333,neutral
thrift,1503,comment_1,"Is there any progress on this? It would be great to have FramedTransport support in cocoa, as it's being used more and more.",requirement_debt,requirement_partially_implemented,"Tue, 24 Jan 2012 15:10:45 +0000","Wed, 1 Oct 2014 19:41:18 +0000","Wed, 1 Oct 2014 19:41:18 +0000",84774633,"Is there any progress on this? It would be great to have FramedTransport support in cocoa, as it's being used more and more.",0.45,0.45,positive
thrift,1624,comment_2,"Here's a theory: maybe t_struct::is_union_ is just an uninitialized variable? The t_struct constructor doesn't initialize it. thrifty.yy line 724 will initialize it for struct/union declarations, but I'm guessing that codepath is skipped for service declarations.",code_debt,low_quality_code,"Thu, 7 Jun 2012 17:29:11 +0000","Tue, 26 Jun 2012 16:45:58 +0000","Tue, 26 Jun 2012 16:45:58 +0000",1639007,"Here's a theory: maybe t_struct::is_union_ is just an uninitialized variable? The t_struct constructor doesn't initialize it. thrifty.yy line 724 will initialize it for struct/union declarations, but I'm guessing that codepath is skipped for service declarations.",0.14075,0.14075,neutral
thrift,1624,comment_4,"Hey Bryan, would you consider merging this patch? Nathaniel never replied to confirm that it fixed his issue, but fixing an uninitialized variable couldn't hurt.",code_debt,low_quality_code,"Thu, 7 Jun 2012 17:29:11 +0000","Tue, 26 Jun 2012 16:45:58 +0000","Tue, 26 Jun 2012 16:45:58 +0000",1639007,"Hey Bryan, would you consider merging this patch? Nathaniel never replied to confirm that it fixed his issue, but fixing an uninitialized variable couldn't hurt.",0.2855,0.2855,neutral
thrift,1745,comment_2,"Yes there is a typo in TJSONProtocol.py line 49, it should be ""rec"" instead of ""rect"" sorry... Also i run the test again and it seems there are other problems, coming from somewhere else and affect all protocols",documentation_debt,low_quality_documentation,"Tue, 30 Oct 2012 15:31:10 +0000","Sat, 20 Feb 2021 15:27:14 +0000","Tue, 20 Nov 2012 22:12:50 +0000",1838500,"Yes there is a typo in TJSONProtocol.py line 49, it should be ""rec"" instead of ""rect"" sorry... Also i run the test again and it seems there are other problems, coming from somewhere else and affect all protocols: Apache Thrift - integration test suite Fri Nov 2 09:31:33 CET 2012 ====================================================== client-server: protocol: transport: result: cpp-cpp binary buffered-ip success cpp-cpp binary buffered-domain success cpp-cpp binary framed-ip success cpp-cpp binary framed-domain success cpp-cpp binary http-ip success cpp-cpp binary http-domain success cpp-cpp json buffered-ip success cpp-cpp json buffered-domain success cpp-cpp json framed-ip success cpp-cpp json framed-domain success cpp-cpp json http-ip success cpp-cpp json http-domain success py-py binary buffered-ip success py-py json buffered-ip success py-cpp binary buffered-ip success py-cpp json buffered-ip success cpp-py binary buffered-ip failure =================== server message =================== Traceback (most recent call last): File ""py/../../lib/py/build/lib.linux-x86_64-2.7/thrift/server/TServer.py"", line 84, in serve self.processor.process(iprot, oprot) File ""py/gen-py/ThriftTest/ThriftTest.py"", line 1006, in process self._processMap[name](self, seqid, iprot, oprot) File ""py/gen-py/ThriftTest/ThriftTest.py"", line 1170, in process_testMapMap result.write(oprot) File ""py/gen-py/ThriftTest/ThriftTest.py"", line 3054, in write oprot.writeMapBegin(TType.I32, TType.MAP, len(self.success)) TypeError: object of type 'int' has no len() =================== client message =================== testMap( {0 => -10, 1 => -9, 2 => -8, 3 => -7, 4 => -6}) = {0 => -10, 1 => -9, 2 => -8, 3 => -7, 4 => -6} testSet({-2, -1, 0, 1, 2}) = {-2, -1, 0, 1, 2} testList({-2, -1, 0, 1, 2}) = {-2, -1, 0, 1, 2} testEnum(ONE) = 1 testEnum(TWO) = 2 testEnum(THREE) = 3 testEnum(FIVE) = 5 testEnum(EIGHT) = 8 testTypedef(309858235082523) = 309858235082523 testMapMap(1)Aborted ====================================================== client-server: protocol: transport: result: cpp-py json buffered-ip failure =================== server message =================== Traceback (most recent call last): File ""py/../../lib/py/build/lib.linux-x86_64-2.7/thrift/server/TServer.py"", line 84, in serve self.processor.process(iprot, oprot) File ""py/gen-py/ThriftTest/ThriftTest.py"", line 1006, in process self._processMap[name](self, seqid, iprot, oprot) File ""py/gen-py/ThriftTest/ThriftTest.py"", line 1170, in process_testMapMap result.write(oprot) File ""py/gen-py/ThriftTest/ThriftTest.py"", line 3054, in write oprot.writeMapBegin(TType.I32, TType.MAP, len(self.success)) TypeError: object of type 'int' has no len() =================== client message =================== testMap( {0 => -10, 1 => -9, 2 => -8, 3 => -7, 4 => -6}) = {0 => -10, 1 => -9, 2 => -8, 3 => -7, 4 => -6} testSet({-2, -1, 0, 1, 2}) = {-2, -1, 0, 1, 2} testList({-2, -1, 0, 1, 2}) = {-2, -1, 0, 1, 2} testEnum(ONE) = 1 testEnum(TWO) = 2 testEnum(THREE) = 3 testEnum(FIVE) = 5 testEnum(EIGHT) = 8 testTypedef(309858235082523) = 309858235082523 testMapMap(1)Aborted ======================================================",-0.125,0.00561622807,neutral
thrift,1745,comment_3,"Thanks! committed the typo fix. Yes, there are some other issues we need to fix with the cross language test suite: THRIFT-847 Test Framework harmonization across all languages any help is welcome!",documentation_debt,low_quality_documentation,"Tue, 30 Oct 2012 15:31:10 +0000","Sat, 20 Feb 2021 15:27:14 +0000","Tue, 20 Nov 2012 22:12:50 +0000",1838500,"Thanks! committed the typo fix. Yes, there are some other issues we need to fix with the cross language test suite: THRIFT-847 Test Framework harmonization across all languages any help is welcome!",0.45,0.45,neutral
thrift,1745,comment_0,"Frederic, Thank you for your work on this. Would you please attach the patch for this to the ticket. We can not accept gist or pull requests from github due to ASF licensing requirements. Once the test cases are added i'll gladly review and commit this patch.",test_debt,lack_of_tests,"Tue, 30 Oct 2012 15:31:10 +0000","Sat, 20 Feb 2021 15:27:14 +0000","Tue, 20 Nov 2012 22:12:50 +0000",1838500,"Frederic, Thank you for your work on this. Would you please attach the patch for this to the ticket. We can not accept gist or pull requests from github due to ASF licensing requirements. Once the test cases are added i'll gladly review and commit this patch.",0.175,0.175,neutral
thrift,1799,comment_2,"Cleaned up patch, no other changes.",code_debt,low_quality_code,"Fri, 21 Dec 2012 20:16:32 +0000","Sat, 8 Jun 2013 03:15:51 +0000","Sat, 8 Jun 2013 03:15:50 +0000",14540358,"Cleaned up patch, no other changes.",0.0,0.0,neutral
thrift,1800,comment_1,"Can you change test/DocTest.thrift to demonstrate the change that you made? I'm not able to get it to properly escape the characters (instead, it is adding additional bogus characters).",code_debt,low_quality_code,"Fri, 21 Dec 2012 21:45:26 +0000","Sat, 8 Jun 2013 03:15:43 +0000","Sat, 8 Jun 2013 03:15:43 +0000",14535017,"Can you change test/DocTest.thrift to demonstrate the change that you made? I'm not able to get it to properly escape the characters (instead, it is adding additional bogus characters).",-0.1653333333,-0.1653333333,negative
thrift,1810,comment_2,initial patch without cross language test,test_debt,lack_of_tests,"Thu, 27 Dec 2012 00:19:05 +0000","Sun, 23 Feb 2014 20:56:22 +0000","Sun, 23 Feb 2014 20:56:14 +0000",36621429,initial patch without cross language test,0.0,0.0,neutral
thrift,1813,comment_3,"I rebased Arvind's patch, removed the duplication between autogen_comment and autogen_summary, and added a date as I previously suggested. GitHub pull request also available at:",code_debt,duplicated_code,"Fri, 28 Dec 2012 15:28:08 +0000","Wed, 17 Jun 2015 08:01:22 +0000","Sat, 9 Nov 2013 18:43:06 +0000",27314098,"roger.meier I rebased Arvind's patch, removed the duplication between autogen_comment and autogen_summary, and added a date as I previously suggested. GitHub pull request also available at: https://github.com/apache/thrift/pull/63",0.0,0.0,neutral
thrift,1853,comment_0,"Hi Nick, Do you have any updates on this? If you have any unit-test for this please also attach it here. Thank you, Henrique",test_debt,lack_of_tests,"Mon, 11 Feb 2013 05:09:57 +0000","Sun, 12 May 2013 23:49:35 +0000","Sun, 12 May 2013 23:49:35 +0000",7843178,"Hi Nick, Do you have any updates on this? If you have any unit-test for this please also attach it here. Thank you, Henrique",0.2,0.2,neutral
thrift,1883,comment_2,I'm withdrawing the request to fix this since I'm working on a full rewrite of this library (see,code_debt,low_quality_code,"Thu, 14 Mar 2013 07:41:50 +0000","Sat, 8 Jun 2013 18:35:53 +0000","Sat, 8 Jun 2013 18:35:53 +0000",7469643,I'm withdrawing the request to fix this since I'm working on a full rewrite of this library (see https://mail-archives.apache.org/mod_mbox/thrift-dev/201303.mbox/browser).,0.6,0.6,neutral
thrift,1932,comment_1,"Hi Roger, here is the patch you requested. It does, however, only suppress the type-punned pointer warning. Since I'm new to Thrift, I'm quite unsure if the sequence of bytes read from file handle 'fd_' is already in host byte order. If this is not the case the patch is still wrong",design_debt,non-optimal_design,"Thu, 18 Apr 2013 16:10:01 +0000","Mon, 20 May 2013 03:13:13 +0000","Sun, 5 May 2013 21:38:30 +0000",1488509,"Hi Roger, here is the patch you requested. It does, however, only suppress the type-punned pointer warning. Since I'm new to Thrift, I'm quite unsure if the sequence of bytes read from file handle 'fd_' is already in host byte order. If this is not the case the patch is still wrong",-0.255375,-0.255375,negative
thrift,1953,comment_3,"Overall, looks good. A few more comments: - Tests have moved from ./test/csharp/ to ./lib/csharp/test - The C# generator has a warning about an unused variable. Can you please delete that variable? - I don't think that we should be building the MVC projects by default. For example, Mono does not have support for MVC3, so I couldn't compile those elements. Overall, looks good though.",architecture_debt,violation_of_modularity,"Wed, 1 May 2013 16:23:43 +0000","Tue, 28 May 2013 02:25:27 +0000","Sat, 18 May 2013 13:35:51 +0000",1458728,"Overall, looks good. A few more comments: Tests have moved from ./test/csharp/ to ./lib/csharp/test The C# generator has a warning about an unused variable. Can you please delete that variable? I don't think that we should be building the MVC projects by default. For example, Mono does not have support for MVC3, so I couldn't compile those elements. Overall, looks good though.",0.2205625,0.2205625,neutral
thrift,1953,comment_4,"hi, Carl Yeksigian 1. Tests for MVC have moved to ./lib/csharp/test 2. I don't find any unused variable in my patch, could you please show me where it is? 3. I agree with your point, by default, we should not build the MVC projects. so I create a solution file ""thrift.mvc.sln"", but this will increase the maintenance effort. please check the attachment patch file.",architecture_debt,violation_of_modularity,"Wed, 1 May 2013 16:23:43 +0000","Tue, 28 May 2013 02:25:27 +0000","Sat, 18 May 2013 13:35:51 +0000",1458728,"hi, Carl Yeksigian 1. Tests for MVC have moved to ./lib/csharp/test 2. I don't find any unused variable in my patch, could you please show me where it is? 3. I agree with your point, by default, we should not build the MVC projects. so I create a solution file ""thrift.mvc.sln"", but this will increase the maintenance effort. please check the attachment patch file.",0.065,0.065,neutral
thrift,1953,comment_1,Thanks for the Patch! Great idea to integrate Apache Thrift with ASP.NET MVC 3. What do you think about renaming according to the same naming as System.Web.Mvc? - Thrift.Server.MVC - - Could you merge into to help reducing maintenance effort? And please add the mvc option to the macro within thanks! ;-r,code_debt,low_quality_code,"Wed, 1 May 2013 16:23:43 +0000","Tue, 28 May 2013 02:25:27 +0000","Sat, 18 May 2013 13:35:51 +0000",1458728,Thanks for the Patch! Great idea to integrate Apache Thrift with ASP.NET MVC 3. What do you think about renaming according to the same naming as System.Web.Mvc? Thrift.Server.MVC > Thrift.Server.Mvc lib/csharp/src/Server/MVC/ > lib/csharp/src/Server/Mvc/ lib/csharp/src/Thrift.MVC3.csproj > lib/csharp/src/Thrift.Mvc.csproj Could you merge lib/csharp/src/Thrift.MVC3.sln into lib/csharp/src/Thrift.sln to help reducing maintenance effort? And please add the mvc option to the THRIFT_REGISTER_GENERATOR macro within compiler/cpp/src/generate/t_csharp_generator.cc thanks! ;-r,0.2060606061,0.1261904762,neutral
thrift,1953,comment_2,"hi, Roger Meier, i've changed the code style as you wish, please check the attachment patch, thanks",code_debt,low_quality_code,"Wed, 1 May 2013 16:23:43 +0000","Tue, 28 May 2013 02:25:27 +0000","Sat, 18 May 2013 13:35:51 +0000",1458728,"hi, Roger Meier, i've changed the code style as you wish, please check the attachment patch, thanks",0.2666666667,0.2666666667,neutral
thrift,1953,comment_5,"Thanks! Here is the error that I got when making the compiler: I've attached a patch to remove the variable, rather than having more back and forth about that. I'd rather have them in a single solution, where we just don't build all of them.  : Do either of you have an opinion on this?",code_debt,dead_code,"Wed, 1 May 2013 16:23:43 +0000","Tue, 28 May 2013 02:25:27 +0000","Sat, 18 May 2013 13:35:51 +0000",1458728,"Thanks! Here is the error that I got when making the compiler: I've attached a patch to remove the variable, rather than having more back and forth about that. I'd rather have them in a single solution, where we just don't build all of them. jensg roger.meier: Do either of you have an opinion on this?",0.05,0.04,neutral
thrift,1953,comment_6,"thanks, Carl Yeksigian I find the variable now. I upload a new patch to remove it.",code_debt,low_quality_code,"Wed, 1 May 2013 16:23:43 +0000","Tue, 28 May 2013 02:25:27 +0000","Sat, 18 May 2013 13:35:51 +0000",1458728,"thanks, Carl Yeksigian I find the variable now. I upload a new patch to remove it.",0.2,0.2,neutral
thrift,1990,comment_1,"Ah ha, turns out Ubuntu's thrift package is v 0.8, which is quite outdated and doesn't include exception support.",architecture_debt,using_obsolete_technology,"Mon, 3 Jun 2013 05:20:56 +0000","Sat, 20 Feb 2021 15:28:26 +0000","Fri, 7 Jun 2013 05:23:52 +0000",345776,"Ah ha, turns out Ubuntu's thrift package is v 0.8, which is quite outdated and doesn't include exception support.",0.0,0.0,negative
thrift,2006,comment_1,There should be an upper limit on the size of the string that is read for the RPC method name in the TBinaryProtocol message header. Does anybody know what the maximum length of a call name is? Does the compiler have an upper limit on string length for the name of a RPC method?,code_debt,low_quality_code,"Sat, 8 Jun 2013 08:18:10 +0000","Wed, 30 Sep 2015 20:18:37 +0000","Mon, 28 Sep 2015 13:34:36 +0000",72767786,There should be an upper limit on the size of the string that is read for the RPC method name in the TBinaryProtocol message header. Does anybody know what the maximum length of a call name is? Does the compiler have an upper limit on string length for the name of a RPC method?,0.2083333333,0.2083333333,neutral
thrift,2006,comment_3,"The code path is inside backwards compatibility (pre-versioned) header reads. By setting a string size limit in BinaryProtocol before serving, this will prevent a core. Use setStringSizeLimit in BinaryProtocol to set an upper limit on string reads. We could be more strict here, for example if someone was able to tell me what the maximum allowed size of a method call name was, we could hardcode the limit and prevent this without ""optional"" behavior. Nobody could provide this information, so I provided a way to achieve the desired behavior without any code changes.",code_debt,low_quality_code,"Sat, 8 Jun 2013 08:18:10 +0000","Wed, 30 Sep 2015 20:18:37 +0000","Mon, 28 Sep 2015 13:34:36 +0000",72767786,"The code path is inside backwards compatibility (pre-versioned) header reads. By setting a string size limit in BinaryProtocol before serving, this will prevent a core. Use setStringSizeLimit in BinaryProtocol to set an upper limit on string reads. We could be more strict here, for example if someone was able to tell me what the maximum allowed size of a method call name was, we could hardcode the limit and prevent this without ""optional"" behavior. Nobody could provide this information, so I provided a way to achieve the desired behavior without any code changes.",0.1142,0.1142,neutral
thrift,2021,comment_2,"+1 Applies after fixing EOL style (win=>*nix), looks good to me.",code_debt,low_quality_code,"Thu, 13 Jun 2013 13:59:34 +0000","Fri, 12 Aug 2016 01:29:52 +0000","Fri, 8 Apr 2016 05:36:08 +0000",88961794,"+1 Applies after fixing EOL style (win=>*nix), looks good to me.",0.788,0.788,positive
thrift,2028,comment_4,"Another suggestion would be to eliminate the project-specific abstraction and require either boost or C++11, assuming either of them have all the primitives. This would simplify the code base quite a bit and remove the need to maintain this abstraction layer.",code_debt,complex_code,"Thu, 13 Jun 2013 14:42:57 +0000","Wed, 6 Jan 2016 02:06:04 +0000","Sat, 10 Oct 2015 23:22:11 +0000",73384754,"Another suggestion would be to eliminate the project-specific abstraction and require either boost or C++11, assuming either of them have all the primitives. This would simplify the code base quite a bit and remove the need to maintain this abstraction layer.",-0.3,-0.3,neutral
thrift,2031,comment_0,"Ack... doesn't apply cleanly, because of other unsubmitted patches. I'll clean this up shortly.",code_debt,low_quality_code,"Fri, 14 Jun 2013 20:57:16 +0000","Fri, 12 Aug 2016 01:29:46 +0000","Fri, 8 Apr 2016 05:36:10 +0000",88850334,"Ack... doesn't apply cleanly, because of other unsubmitted patches. I'll clean this up shortly.",0.2,0.2,negative
thrift,2031,comment_1,"Seems outdated, the -1 has been replaced with in the meantime.",code_debt,low_quality_code,"Fri, 14 Jun 2013 20:57:16 +0000","Fri, 12 Aug 2016 01:29:46 +0000","Fri, 8 Apr 2016 05:36:10 +0000",88850334,"Seems outdated, the -1 has been replaced with THRIFT_INVALID_SOCKET in the meantime.",0.0,0.0,negative
thrift,2097,comment_1,please help fix and test this THRIFT-2229 first thanks! -roger,test_debt,lack_of_tests,"Sun, 21 Jul 2013 01:58:26 +0000","Thu, 10 Oct 2019 23:00:14 +0000","Mon, 14 Jan 2019 15:03:22 +0000",173106296,please help fix and test this THRIFT-2229 first thanks! -roger,0.1666666667,0.1666666667,neutral
thrift,2116,comment_0,"If there is anyone out there spending the time to review this and THRIFT-2144, I'd be happy to commit it. Any comments on whether or not the stuff from the Wiki is still useful for the new tutorial pages or is outdated would be highly appreciated too.",documentation_debt,outdated_documentation,"Sun, 11 Aug 2013 14:49:54 +0000","Thu, 10 Oct 2019 23:00:05 +0000","Mon, 14 Jan 2019 15:03:33 +0000",171245619,"If there is anyone out there spending the time to review this and THRIFT-2144, I'd be happy to commit it. Any comments on whether or not the stuff from the Wiki is still useful for the new tutorial pages or is outdated would be highly appreciated too.",0.4343333333,0.4343333333,positive
thrift,2124,comment_1,after moving test to subdir to allow for distclean to run missing following in tutorials folder csharp/tutorial.sln d/async_client.d d/client.d d/server.d erl/client.erl erl/client.sh erl/json_client.erl erl/README erl/server.erl erl/server.sh gen-html/index.html gen-html/style.css go/server.crt go/server.key hs/HaskellClient.hs hs/HaskellServer.hs java/build.xml js/build.xml js/src/Httpd.java js/tutorial.html ocaml/CalcClient.ml ocaml/CalcServer.ml ocaml/_oasis ocaml/README perl/PerlClient.pl perl/PerlServer.pl php/PhpClient.php php/PhpServer.php php/runserver.py shared.thrift tutorial.thrift,architecture_debt,violation_of_modularity,"Thu, 15 Aug 2013 21:41:34 +0000","Fri, 16 Aug 2013 01:55:51 +0000","Fri, 16 Aug 2013 01:23:00 +0000",13286,after moving test to subdir to allow for distclean to run missing following in tutorials folder csharp/CsharpClient/CsharpClient.cs csharp/CsharpClient/CsharpClient.csproj csharp/CsharpClient/Properties/AssemblyInfo.cs csharp/CsharpServer/CsharpServer.cs csharp/CsharpServer/CsharpServer.csproj csharp/CsharpServer/Properties/AssemblyInfo.cs csharp/tutorial.sln d/async_client.d d/client.d d/server.d delphi/DelphiClient/DelphiClient.dpr delphi/DelphiClient/DelphiClient.dproj delphi/DelphiServer/DelphiServer.dpr delphi/DelphiServer/DelphiServer.dproj delphi/Tutorial.groupproj erl/client.erl erl/client.sh erl/json_client.erl erl/README erl/server.erl erl/server.sh gen-html/index.html gen-html/shared.html gen-html/style.css gen-html/tutorial.html go/server.crt go/server.key hs/HaskellClient.hs hs/HaskellServer.hs hs/ThriftTutorial.cabal java/build.xml java/src/CalculatorHandler.java java/src/JavaClient.java java/src/JavaServer.java js/build.xml js/src/Httpd.java js/tutorial.html ocaml/CalcClient.ml ocaml/CalcServer.ml ocaml/_oasis ocaml/README perl/PerlClient.pl perl/PerlServer.pl php/PhpClient.php php/PhpServer.php php/runserver.py py.twisted/PythonServer.tac shared.thrift tutorial.thrift,0.05517241379,0.0375,neutral
thrift,2141,comment_0,"Hi  and other cocoa users, from my feelings we should follow the approach used in THRIFT-2204 as it is more in line with the other languages. On the other side, two SSL implementations for cocoa seems one too much, but I like the idea of improving the tutorial code by including the SSL feature, so I'd keep that part, updated to the solution used in THRIFT-2204. I'd like to know your opinion on that matter. Any other cocoa people reading this are invited to chime in as well. I'm not a cocoa user yet, so I may overlook sth. important. Thanks, JensG",code_debt,low_quality_code,"Wed, 21 Aug 2013 12:50:19 +0000","Thu, 10 Oct 2019 23:00:18 +0000","Mon, 14 Jan 2019 15:03:26 +0000",170388787,"Hi drobakowski and other cocoa users, from my feelings we should follow the approach used in THRIFT-2204 as it is more in line with the other languages. On the other side, two SSL implementations for cocoa seems one too much, but I like the idea of improving the tutorial code by including the SSL feature, so I'd keep that part, updated to the solution used in THRIFT-2204. I'd like to know your opinion on that matter. Any other cocoa people reading this are invited to chime in as well. I'm not a cocoa user yet, so I may overlook sth. important. Thanks, JensG",0.3258571429,0.3258571429,neutral
thrift,2171,comment_0,"Hi Red, I totally agree and more tests are very welcome! We don't have any tests for the new JSON protocol for example :( I also think we should probably move thrift/test/nodejs to like in java. A lot of people get thrift for node through npm, which only gives you the lib folder... Any thoughts?",test_debt,lack_of_tests,"Fri, 6 Sep 2013 05:04:24 +0000","Sun, 6 Jul 2014 04:01:01 +0000","Thu, 3 Apr 2014 23:36:43 +0000",18124339,"Hi Red, I totally agree and more tests are very welcome! We don't have any tests for the new JSON protocol for example I also think we should probably move thrift/test/nodejs to thrift/lib/nodejs/test, like in java. A lot of people get thrift for node through npm, which only gives you the lib folder... Any thoughts?",0.025,0.1583333333,neutral
thrift,2210,comment_8,Unfortunately the change caused the Hive object not able to be serialized into JSON with Here is the struct: struct SkewedInfo { 1: list<string 2: list<list<string 3: map<list<string} The field is defined as a map with list as the key. The structure can be serialized and deserialized correctly with libthrift 0.9.1. This change regressed the logic. People do use TSimpleJSONProtocol to serialize Hive objects for audit logs or such. This is a blocker for thrift upgrading. Could we remove the check for the case when the key is a 'list'? Thanks!,design_debt,non-optimal_design,"Thu, 26 Sep 2013 06:37:02 +0000","Sun, 6 Mar 2016 17:25:17 +0000","Fri, 27 Sep 2013 14:13:50 +0000",113808,"Unfortunately the change caused the Hive object not able to be serialized into JSON with TSimpleJSONProtocol: https://github.com/apache/hive/blob/master/metastore/if/hive_metastore.thrift#L237 Here is the struct: struct SkewedInfo { 1: list<string> skewedColNames, // skewed column names 2: list<list<string>> skewedColValues, //skewed values 3: map<list<string>, string> skewedColValueLocationMaps, //skewed value to location mappings } The skewedColValueLocationMaps field is defined as a map with list as the key. The structure can be serialized and deserialized correctly with libthrift 0.9.1. This change regressed the logic. People do use TSimpleJSONProtocol to serialize Hive objects for audit logs or such. This is a blocker for thrift upgrading. Could we remove the check for the case when the key is a 'list'? Thanks!",0.1428571429,0.1642857143,negative
thrift,2246,comment_3,Some edge cases still produce warnings.,code_debt,low_quality_code,"Tue, 29 Oct 2013 10:12:32 +0000","Mon, 11 Nov 2013 20:41:33 +0000","Mon, 4 Nov 2013 23:52:38 +0000",567606,Some edge cases still produce warnings.,-0.1595,-0.1595,neutral
thrift,2255,comment_3,"Hello, I can give an update on this patch: This version does not work with exceptions. Additionally I think to remove the copyTo implementation. It was in the patch since the beginning, but up to now I haven't used the copy methods, so it does not seem to be usefull and should be removed.(I will update the patch)",code_debt,dead_code,"Thu, 7 Nov 2013 02:08:12 +0000","Thu, 10 Jul 2014 13:42:32 +0000","Thu, 10 Jul 2014 13:42:32 +0000",21209660,"Hello, I can give an update on this patch: https://issues.apache.org/jira/browse/THRIFT-1712 This version does not work with exceptions. Additionally I think to remove the copyTo implementation. It was in the patch since the beginning, but up to now I haven't used the copy methods, so it does not seem to be usefull and should be removed.(I will update the patch)",-0.15,-0.15,negative
thrift,2292,comment_0,Can you please provide a list of the missing dependencies or errors you are encountering,build_debt,under-declared_dependencies,"Fri, 20 Dec 2013 09:53:21 +0000","Mon, 23 Dec 2013 16:12:40 +0000","Mon, 23 Dec 2013 16:12:40 +0000",281959,Can you please provide a list of the missing dependencies or errors you are encountering,-0.2,-0.2,neutral
thrift,2292,comment_2,"I already implement cordova hook that do this job for me on after platform add, and all is brilliant but you have weak link in your build system. In exec.js file your exec implementation fail and stop build due to buffer overflow. Error is ""Error: stdout maxBuffer exceeded"". All that should be done is adding maxBuffer: 800*1024 option to exec call, also 800 can be replaced on any other big number. How can i contribute with this and other fixes to cordova? I saw your repository on github and even forked it. So if i send you pull requests with fixes do you include them in next releases?",build_debt,build_others,"Fri, 20 Dec 2013 09:53:21 +0000","Mon, 23 Dec 2013 16:12:40 +0000","Mon, 23 Dec 2013 16:12:40 +0000",281959,"I already implement cordova hook that do this job for me on after platform add, and all is brilliant but you have weak link in your build system. In exec.js file your exec implementation fail and stop build due to buffer overflow. Error is ""Error: stdout maxBuffer exceeded"". All that should be done is adding maxBuffer: 800*1024 option to exec call, also 800 can be replaced on any other big number. How can i contribute with this and other fixes to cordova? I saw your repository on github and even forked it. So if i send you pull requests with fixes do you include them in next releases?",-0.0549375,-0.0549375,neutral
thrift,2292,comment_1,"Right now i working on jumio plugin for cordova And jumio sdk require adding their android library project to android build. According to cordova plugin specs its not implemented. More info about referencing android library projects to your project you can get here To add a reference to a library project, navigate to the <sdk android update project \ --target <target_ID--path --library",requirement_debt,requirement_partially_implemented,"Fri, 20 Dec 2013 09:53:21 +0000","Mon, 23 Dec 2013 16:12:40 +0000","Mon, 23 Dec 2013 16:12:40 +0000",281959,"Right now i working on jumio plugin for cordova http://www.jumio.com/netverify/netverify-mobile/ And jumio sdk require adding their android library project to android build. According to cordova plugin specs its not implemented. More info about referencing android library projects to your project you can get here http://developer.android.com/tools/projects/projects-cmdline.html To add a reference to a library project, navigate to the <sdk>/tools/ directory and use this command: android update project \ --target <target_ID> \ --path path/to/your/project --library path/to/library_projectA",0.3545,0.3545,neutral
thrift,2293,comment_4,"Hi, Thanks community for finding the fix for this. While testing and reviewing the patch, we noticed the createSSLContext() allows for the condition where both the keystore and truststore is set: if && { null); } For the case above where both the keystore and truststore parameters are set, the truststore file input stream should still leak (depend on GC) since it was not implicitly closed before the reference ""fin"" is reused for the keystore. We got around this by allocating a different file input stream reference for the keystore and truststore and closed both in the finally block. Best Regards, T.",code_debt,low_quality_code,"Fri, 20 Dec 2013 12:46:55 +0000","Fri, 11 Apr 2014 15:19:01 +0000","Thu, 26 Dec 2013 14:39:54 +0000",525179,"Hi, Thanks community for finding the fix for this. While testing and reviewing the patch, we noticed the createSSLContext() allows for the condition where both the keystore and truststore is set: if (params.isKeyStoreSet && params.isTrustStoreSet) { ctx.init(kmf.getKeyManagers(), tmf.getTrustManagers(), null); } For the case above where both the keystore and truststore parameters are set, the truststore file input stream should still leak (depend on GC) since it was not implicitly closed before the reference ""fin"" is reused for the keystore. We got around this by allocating a different file input stream reference for the keystore and truststore and closed both in the finally block. Best Regards, T.",0.278875,0.1239444444,neutral
thrift,2329,comment_0,"Had this on my todo list, will get it taken care of, thanks for the ticket",requirement_debt,requirement_partially_implemented,"Tue, 21 Jan 2014 22:28:01 +0000","Thu, 10 Jul 2014 13:42:32 +0000","Thu, 10 Jul 2014 13:42:32 +0000",14656471,"Had this on my todo list, will get it taken care of, thanks for the ticket",0.4,0.4,positive
thrift,2351,comment_1,"Patch for above issues. I tried to get integration tests to verify this however it seems there are no tests that exercise PHP on the service end and given the trivial nature of this patch I wanted to to submit it without committing several more hours to understand and implement more complete coverage of PHP services in the integration test harness. I was interested to see that only Java is currently tested with the compact protocol which seems an odd decision - certainly the documentation and many ""comparisons"" of thirft vs XXX on the web make a big thing about the compact protocol and give it the appearance of a first class citizen and yet it seems no one ever tried to test it for cross-language RPC? Anyway, hopefully you can see from this patch that these are obvious mistakes and typos and should be included even without the extra effort of making integration test runner able to demonstrate the bug. Thanks. Let me know if I need to submit this differently.",documentation_debt,low_quality_documentation,"Thu, 6 Feb 2014 18:00:17 +0000","Mon, 29 Jun 2015 21:32:53 +0000","Thu, 10 Jul 2014 13:42:25 +0000",13290128,"Patch for above issues. I tried to get integration tests to verify this however it seems there are no tests that exercise PHP on the service end and given the trivial nature of this patch I wanted to to submit it without committing several more hours to understand and implement more complete coverage of PHP services in the integration test harness. I was interested to see that only Java is currently tested with the compact protocol which seems an odd decision - certainly the documentation and many ""comparisons"" of thirft vs XXX on the web make a big thing about the compact protocol and give it the appearance of a first class citizen and yet it seems no one ever tried to test it for cross-language RPC? Anyway, hopefully you can see from this patch that these are obvious mistakes and typos and should be included even without the extra effort of making integration test runner able to demonstrate the bug. Thanks. Let me know if I need to submit this differently.",0.04554444444,0.04554444444,neutral
thrift,2351,comment_4,"no test case and *make cross* integration, we can close this if you like.",test_debt,lack_of_tests,"Thu, 6 Feb 2014 18:00:17 +0000","Mon, 29 Jun 2015 21:32:53 +0000","Thu, 10 Jul 2014 13:42:25 +0000",13290128,"no test case and make cross integration, we can close this if you like.",0.4,0.4,neutral
thrift,2375,comment_3,Hi Jens Thanks so much! Here is one more testcase where {{<a>}} tags are not processed correctly,code_debt,low_quality_code,"Sun, 23 Feb 2014 02:53:28 +0000","Tue, 1 Apr 2014 20:01:32 +0000","Tue, 11 Mar 2014 20:33:57 +0000",1446029,Hi Jens Thanks so much! Here is one more testcase where <a> tags are not processed correctly,-0.05,-0.05,negative
thrift,2405,comment_0,"This patch repairs the Node.js Multiplex server implementation. The full node.js make check now passes. As far as I can tell from walking the code history, the client side Multiplex code never worked for multiple servers. The node.js code base dumps several pages of jsHint warnings right now. The three files attached here are now jsHint clean, but there's still more to do. It would be great if we could require new JavaScript code to jsHint clean and include working tests prior to commit.",code_debt,low_quality_code,"Sun, 16 Mar 2014 08:53:57 +0000","Sun, 6 Jul 2014 04:01:01 +0000","Tue, 22 Apr 2014 13:44:51 +0000",3214254,"This patch repairs the Node.js Multiplex server implementation. The full node.js make check now passes. As far as I can tell from walking the code history, the client side Multiplex code never worked for multiple servers. The node.js code base dumps several pages of jsHint warnings right now. The three files attached here are now jsHint clean, but there's still more to do. It would be great if we could require new JavaScript code to jsHint clean and include working tests prior to commit.",0.07772222222,0.07772222222,neutral
thrift,2431,comment_1,"Agree, this is a real issue; I'm not convinced adding timing based checks here is good for the Travis CI build system either. They should probably be removed. I'm sure the intention was to find poorly performing code paths, however that should be done with callgrind, not with a unit test.",design_debt,non-optimal_design,"Sun, 30 Mar 2014 05:34:11 +0000","Wed, 6 Jan 2016 02:06:07 +0000","Sun, 29 Nov 2015 16:28:18 +0000",52656847,"Agree, this is a real issue; I'm not convinced adding timing based checks here is good for the Travis CI build system either. They should probably be removed. I'm sure the intention was to find poorly performing code paths, however that should be done with callgrind, not with a unit test.",-0.2794444444,-0.2794444444,negative
thrift,2435,comment_1,"Proposed The idea is, to reference the enum types at these places always via full namespace to prevent ambiguities.",code_debt,low_quality_code,"Mon, 31 Mar 2014 08:43:58 +0000","Sat, 31 May 2014 21:09:58 +0000","Wed, 2 Apr 2014 21:22:58 +0000",218340,"Proposed patch. The idea is, to reference the enum types at these places always via full namespace to prevent ambiguities.",-0.2,-0.1,neutral
thrift,2449,comment_1,"Hey Jens, I also think this is a good distinction to have manifest at the compiler level. Haven't tested the code but the patch looks good to me. +1 -Randy",test_debt,lack_of_tests,"Sat, 5 Apr 2014 09:48:18 +0000","Sun, 13 Apr 2014 22:37:07 +0000","Sun, 13 Apr 2014 20:04:00 +0000",728142,"Hey Jens, I also think this is a good distinction to have manifest at the compiler level. Haven't tested the code but the patch looks good to me. +1 -Randy",0.5025,0.5025,positive
thrift,2511,comment_0,This patch adds the compact protocol to the Node.js lib. It includes various ThriftTest tests in This patch also repairs a map of maps bug in the node.js test driver and repairs all jshint warnings in There are however still many (valid) jshint warnings in thrift compiler generated node.js code. Will create a new issue for that.,code_debt,low_quality_code,"Sun, 4 May 2014 08:16:17 +0000","Sun, 6 Jul 2014 04:00:56 +0000","Sun, 11 May 2014 07:27:33 +0000",601876,This patch adds the compact protocol to the Node.js lib. It includes various ThriftTest tests in thirft/lib/nodejs/test/testAll.sh. This patch also repairs a map of maps bug in the node.js test driver and repairs all jshint warnings in thrift/lib/node.js/lib. There are however still many (valid) jshint warnings in thrift compiler generated node.js code. Will create a new issue for that.,-0.05105555556,-0.07595,neutral
thrift,2540,comment_0,"There is a simple workaround for this: It should be clear that this workaround is not ideal, and the configure script should be fixed.",design_debt,non-optimal_design,"Fri, 23 May 2014 10:39:14 +0000","Tue, 21 Jul 2015 02:21:13 +0000","Mon, 23 Mar 2015 17:42:16 +0000",26290982,"There is a simple workaround for this: It should be clear that this workaround is not ideal, and the configure script should be fixed.",0.0,0.0,negative
thrift,2590,comment_2,Patch that adds the three specified files to the project file. Please verify if it's working as i haven't installed the necessary dependencies to compile the c++ library so i couldn't really test it. But it should work.,test_debt,lack_of_tests,"Thu, 26 Jun 2014 15:17:36 +0000","Sun, 6 Jul 2014 04:00:57 +0000","Fri, 4 Jul 2014 20:18:56 +0000",709280,Patch that adds the three specified files to the project file. Please verify if it's working as i haven't installed the necessary dependencies to compile the c++ library so i couldn't really test it. But it should work.,0.3444444444,0.3444444444,neutral
thrift,2605,comment_0,patch attached fixes both TSocket and TServerSocket also - removed mixed line endings in TSocket,code_debt,low_quality_code,"Mon, 7 Jul 2014 14:06:20 +0000","Thu, 10 Jul 2014 13:36:09 +0000","Tue, 8 Jul 2014 05:33:09 +0000",55609,patch attached fixes both TSocket and TServerSocket also - removed mixed line endings in TSocket,0.0,0.0,neutral
thrift,2607,comment_0,just removed unused field,code_debt,dead_code,"Mon, 7 Jul 2014 14:22:17 +0000","Thu, 10 Jul 2014 13:36:08 +0000","Mon, 7 Jul 2014 19:56:05 +0000",20028,just removed unused field,0.0,0.0,neutral
thrift,2609,comment_0,"field used - this field could be removed, but by using it code is more consistent",code_debt,dead_code,"Mon, 7 Jul 2014 14:32:58 +0000","Thu, 10 Jul 2014 13:36:10 +0000","Mon, 7 Jul 2014 20:03:38 +0000",19840,"field used - this field could be removed, but by using it code is more consistent",0.0,0.0,neutral
thrift,2666,comment_0,Replaced {{PYTHONHASHSEED}} by a literal. Any better solution is welcome.,design_debt,non-optimal_design,"Thu, 14 Aug 2014 20:14:46 +0000","Wed, 5 Nov 2014 04:48:58 +0000","Mon, 1 Sep 2014 21:08:54 +0000",1558448,Replaced PYTHONHASHSEED by a literal. Any better solution is welcome.,0.2416666667,0.2416666667,neutral
thrift,2781,comment_1,"The idlgen feature is not strictly required for Thrift. So as a workaround, the patch simply removes it from the build run.",design_debt,non-optimal_design,"Fri, 10 Oct 2014 17:22:03 +0000","Wed, 5 Nov 2014 04:48:31 +0000","Fri, 10 Oct 2014 17:33:47 +0000",704,"The idlgen feature is not strictly required for Thrift. So as a workaround, the patch simply removes it from the build run.",0.0,0.0,neutral
thrift,2791,comment_5,"This is not the appropriate way to add buffering to the server. When you create your server, you can do something like transport, protocolFactory) and it will buffer the sockets it creates. After this patch, my code as above is creating a double layer of buffers, and that is not a good change. Can we please revert this patch and encourage creating buffered server sockets the correct way?",design_debt,non-optimal_design,"Fri, 24 Oct 2014 16:50:39 +0000","Mon, 10 Nov 2014 23:14:30 +0000","Wed, 29 Oct 2014 17:58:42 +0000",436083,"This is not the appropriate way to add buffering to the server. When you create your server, you can do something like thrift.NewTSimpleServer4(processor, transport, thrift.NewTBufferedTransportFactory(1024), protocolFactory) and it will buffer the sockets it creates. After this patch, my code as above is creating a double layer of buffers, and that is not a good change. Can we please revert this patch and encourage creating buffered server sockets the correct way?",-0.12875,-0.08583333333,negative
thrift,2833,comment_4,"Hey , our current java package/deploy is not the nicest and is using a mix of ant and maven. I have a ticket open and would like to switch everything over to using maven for the next 0.9.3 release and take care of all these types of issues in the java* packages of Thrift",design_debt,non-optimal_design,"Mon, 17 Nov 2014 13:56:36 +0000","Mon, 26 Jan 2015 01:56:37 +0000","Tue, 18 Nov 2014 18:35:48 +0000",103152,"Hey Zlika, our current java package/deploy is not the nicest and is using a mix of ant and maven. I have a ticket open and would like to switch everything over to using maven for the next 0.9.3 release and take care of all these types of issues in the java* packages of Thrift",0.2,0.2,negative
thrift,2851,comment_1,"Ok I see the point now. I stumbled upon it again, because in THRIFT-2502 the Peek function for iostream_transport got commented out. There seem to be no support from GO to provide such a function to most of the transport. IMHO Only barely used memory_buffer could get a real implementation. So the question is if we should carry along the Peek() function even though there will be no real implementation for quiet some time. Currently the implementation doesn't seem to follow a strict line. E.g. the transport interface doesn't carry a Peek method and not all transport implements the Peek function. I vote for remove, as we gain nothing, and appreciate the differences of having different language platforms with different abilities.",code_debt,low_quality_code,"Mon, 24 Nov 2014 18:49:14 +0000","Mon, 8 Dec 2014 20:35:00 +0000","Tue, 25 Nov 2014 20:48:59 +0000",93585,"Ok I see the point now. I stumbled upon it again, because in THRIFT-2502 the Peek function for iostream_transport got commented out. There seem to be no support from GO to provide such a function to most of the transport. IMHO Only barely used memory_buffer could get a real implementation. So the question is if we should carry along the Peek() function even though there will be no real implementation for quiet some time. Currently the implementation doesn't seem to follow a strict line. E.g. the transport interface doesn't carry a Peek method and not all transport implements the Peek function. I vote for remove, as we gain nothing, and appreciate the differences of having different language platforms with different abilities.",0.03275,0.03275,negative
thrift,2851,comment_2,"After sleeping a night about it, I think you are right. In its current form Peek() is not only next to useless, but more important misleading to the less careful observer. So unless someone comes up with a better implementation, we should throw it out. Committed.",code_debt,low_quality_code,"Mon, 24 Nov 2014 18:49:14 +0000","Mon, 8 Dec 2014 20:35:00 +0000","Tue, 25 Nov 2014 20:48:59 +0000",93585,"After sleeping a night about it, I think you are right. In its current form Peek() is not only next to useless, but more important misleading to the less careful observer. So unless someone comes up with a better implementation, we should throw it out. Committed.",0.359875,0.359875,negative
thrift,2851,comment_0,"It may not be the best way how it is implemented, but {{Peek()}} is a standard functionality for all Thrift transports. Throughout the library the core concepts and the implementations should be as consistent as possible. Even though {{Peek()}} is not listed (note that the list is marked as not exhaustive), IMHO making it better = @all: Other opinions welcome, though.",design_debt,non-optimal_design,"Mon, 24 Nov 2014 18:49:14 +0000","Mon, 8 Dec 2014 20:35:00 +0000","Tue, 25 Nov 2014 20:48:59 +0000",93585,"If there are useless, we can remove them right? It may not be the best way how it is implemented, but Peek() is a standard functionality for all Thrift transports. Throughout the library the core concepts and the implementations should be as consistent as possible. Even though Peek() is not listed here (note that the list is marked as not exhaustive), IMHO making it better => yes, removing => no. @all: Other opinions welcome, though.",0.375,0.294,negative
thrift,2853,comment_1,Thanks for applying my patch so quickly! I forgot to remove the comments that doesn't apply anymore. This PR removes them. I try to be more careful on future patches.,documentation_debt,outdated_documentation,"Mon, 24 Nov 2014 22:11:49 +0000","Tue, 25 Nov 2014 00:18:22 +0000","Mon, 24 Nov 2014 23:40:16 +0000",5307,Thanks for applying my patch so quickly! I forgot to remove the comments that doesn't apply anymore. This PR removes them. I try to be more careful on future patches.,0.15,0.15,neutral
thrift,2932,comment_1,"Both of these libs predate my involvement in Thrift but when first exposed to them a couple years ago they were primordial and supporting only very limited functionality. The browser lib only did JSON/XHR/HTTP and Node only did Binary/Socket so they could not talk to each other (browser was/is tested against Java). No npm or bower support at that time either. The libs are now out on npm and bower (which is a mess due to the whole repo download thing). Compact and JSON protocols were added to Node. Many things that didn't work at all were fixed and lots of tests were added. Other than the tests (which were needed just to get things working more broadly) all of this work has been feature oriented and incremental. People tend to provide patches to fix or add things they need, not clean up stuff that is a mess (partly because it might break existing code, for example changing the call back interface in the browser to add error objects). So we have some baggage... That said, you are not alone. Many would like to see the kind of improvements mentioned here and in other tickets. There's still time to make a big push before Thrift v1.0. Patches welcome!",design_debt,non-optimal_design,"Wed, 7 Jan 2015 05:51:21 +0000","Tue, 21 Jul 2015 02:21:10 +0000","Sat, 14 Feb 2015 23:02:36 +0000",3345075,"Both of these libs predate my involvement in Thrift but when first exposed to them a couple years ago they were primordial and supporting only very limited functionality. The browser lib only did JSON/XHR/HTTP and Node only did Binary/Socket so they could not talk to each other (browser was/is tested against Java). No npm or bower support at that time either. The libs are now out on npm and bower (which is a mess due to the whole repo download thing). Compact and JSON protocols were added to Node. Many things that didn't work at all were fixed and lots of tests were added. Other than the tests (which were needed just to get things working more broadly) all of this work has been feature oriented and incremental. People tend to provide patches to fix or add things they need, not clean up stuff that is a mess (partly because it might break existing code, for example changing the call back interface in the browser to add error objects). So we have some baggage... That said, you are not alone. Many would like to see the kind of improvements mentioned here and in other tickets. There's still time to make a big push before Thrift v1.0. Patches welcome!",-0.01326923077,-0.01326923077,neutral
thrift,2932,comment_3,"Hey Andrew, Thanks for the consolidated patch. Several nice improvements here. I am concerned about the new sequence id modification though. It propagates the sequencing data into the transport. This may seem reasonable in isolation but the transport should not be involved at this level of abstraction. In Apache Thrift, Protocols are responsible for the physical layout of bits (on the wire, on disk or in memory). This includes where, how and if sequence numbers are packaged. Transports are responsible for transmission of the data only. This may involve packetizing, framing, encrypting or what have you, but the key is that the Protocol and the Transport are isolated. The Transport receives/returns an opaque block of bits from/to the Protocol. The current node implementation is not clean in this regards and ultimately needs some structural refactoring. For example the multiplexing read implementation is a hack (paying no attention to the service name in the payload and rather wiring everything to seqid lookups). I think we should try to migrate things in the direction of the generalized Apache Thrift model. This is non-trivial due to the pure async approach of JavaScript (a late model option in the reference CPP and Java implementations). The heart of the node problem here is that Apache Thrift is a layered system. Lower layers provide services to upper layers and services do what you tell them, they dont call you with demands. The node implementation wires callbacks from the bottom up (with several call chains back down from proto to trans in the process). These conflicting approaches are merged with unsatisfactory results in my opinion. To respect the Apache Thrift layered approach the data event needs to be handled by the client (or by a much more focused connection helper) and this piece of code needs to then call down the layered stack like all other languages (client- In short, getting the node client side call return in order is a fair size chunk of work and should fall into a separate issue.",design_debt,non-optimal_design,"Wed, 7 Jan 2015 05:51:21 +0000","Tue, 21 Jul 2015 02:21:10 +0000","Sat, 14 Feb 2015 23:02:36 +0000",3345075,"Hey Andrew, Thanks for the consolidated patch. Several nice improvements here. I am concerned about the new sequence id modification though. It propagates the sequencing data into the transport. This may seem reasonable in isolation but the transport should not be involved at this level of abstraction. In Apache Thrift, Protocols are responsible for the physical layout of bits (on the wire, on disk or in memory). This includes where, how and if sequence numbers are packaged. Transports are responsible for transmission of the data only. This may involve packetizing, framing, encrypting or what have you, but the key is that the Protocol and the Transport are isolated. The Transport receives/returns an opaque block of bits from/to the Protocol. The current node implementation is not clean in this regards and ultimately needs some structural refactoring. For example the multiplexing read implementation is a hack (paying no attention to the service name in the payload and rather wiring everything to seqid lookups). I think we should try to migrate things in the direction of the generalized Apache Thrift model. This is non-trivial due to the pure async approach of JavaScript (a late model option in the reference CPP and Java implementations). The heart of the node problem here is that Apache Thrift is a layered system. Lower layers provide services to upper layers and services do what you tell them, they dont call you with demands. The node implementation wires callbacks from the bottom up (with several call chains back down from proto to trans in the process). These conflicting approaches are merged with unsatisfactory results in my opinion. To respect the Apache Thrift layered approach the data event needs to be handled by the client (or by a much more focused connection helper) and this piece of code needs to then call down the layered stack like all other languages (client->proto->trans) for reads or writes. In short, getting the node client side call return in order is a fair size chunk of work and should fall into a separate issue. Specific comments (line nos post patch): ---------------------------------------------------------- lib/nodejs/lib/thrift/http_connection.js LINE 157: This is an exotic case where the client does not implement a receive handler for the server message. An extreme edge case that by definition can have no callback, so emit error is probably the only reasonable out. Also keep in mind that prior to deserializing at the protocol level we have no idea what the transmitted seqid is. Without a seqid we cannot know the callback to invoke. lib/nodejs/lib/thrift/http_connection.js LINE 184: Here we are communicating normal application behavior using exceptions, which always smells a bit (this is not your code but it is making things murky here). If a real exception arises here it is appropriately scoped to the client rather than the call. It will be something like the connection failed. Call context exceptions should be transmitted by the protocol, received successfully around line 145 and presented to the callback in the err position. Anything else should be handled at a higher level of abstraction. If I am writing a client I dont want to have to handle call failure in every callback, that logic should be provided in a single error event handler across all calls. lib/nodejs/lib/thrift/thrift.js LINE 148: This appears to be an in house utility, also ""self.called"" never appears to be set anywhere. If you are fixing a specific problem in this code or if I am missing the point let me know and maybe give me an example case to look over. In the meantime Im going to patch commit the hunks that are clear improvements here (nextTick additions and the like). You can close this ticket and open new specific tickets for individual issues not handled here or we can carry on here, up to you. I will attach the partial patch committed. Best, Randy",0.06996491228,0.079075,neutral
thrift,2937,comment_1,I'm ok with this. However we should set it also to the same level(256MB) as we did for TNonblockingServer with THRIFT-1337,design_debt,non-optimal_design,"Fri, 9 Jan 2015 09:00:05 +0000","Tue, 21 Jul 2015 02:21:16 +0000","Sun, 15 Feb 2015 18:28:27 +0000",3230902,I'm ok with this. However we should set it also to the same level(256MB) as we did for TNonblockingServer with THRIFT-1337 https://github.com/apache/thrift/blob/master/lib/cpp/src/thrift/server/TNonblockingServer.h#L126,0.375,0.375,neutral
thrift,2969,comment_0,"These patches together represent a major refactoring of the nodejs library testing code, eliminating most of the duplication. The testAll.sh script now tests every possible implementation. There is room for parallelizing these tests for speed, but that can be the subject of another patch. I'm trying to move as fast as possible to improve the nodejs code. The sooner this and my other patches can be reviewed the faster I can contribute more patches before we hit 1.0.0.",code_debt,duplicated_code,"Sat, 31 Jan 2015 01:30:54 +0000","Tue, 21 Jul 2015 02:21:07 +0000","Mon, 16 Feb 2015 08:54:10 +0000",1408996,"These patches together represent a major refactoring of the nodejs library testing code, eliminating most of the duplication. The testAll.sh script now tests every possible implementation. There is room for parallelizing these tests for speed, but that can be the subject of another patch. I'm trying to move as fast as possible to improve the nodejs code. The sooner this and my other patches can be reviewed the faster I can contribute more patches before we hit 1.0.0.",-0.008333333333,-0.008333333333,positive
thrift,2969,comment_5,"Hey Andrew, Great patch. Big improvement. I had to remove a spurious require xtend from server.js but otherwise as submitted. -Randy",code_debt,low_quality_code,"Sat, 31 Jan 2015 01:30:54 +0000","Tue, 21 Jul 2015 02:21:07 +0000","Mon, 16 Feb 2015 08:54:10 +0000",1408996,"Hey Andrew, Great patch. Big improvement. I had to remove a spurious require xtend from server.js but otherwise as submitted. -Randy",0.23225,0.23225,positive
thrift,3027,comment_1,"Hi , the patch introduces a new dependency to boost into the Thrift compiler. Can we avoid that? Have a look into there are some similar functions that may serve as model. Next, the {{to_upper_copy()}} function expects a locale, the default is {{std::locale()}} which is ... Not sure if that is what we want. The generated code should not depend on a particular system locale.",design_debt,non-optimal_design,"Fri, 6 Mar 2015 14:23:55 +0000","Fri, 22 May 2015 17:24:08 +0000","Sat, 14 Mar 2015 14:39:43 +0000",692148,"Hi magrath, the patch introduces a new dependency to boost into the Thrift compiler. Can we avoid that? Have a look into t_c_glib_generator.cc, there are some similar functions that may serve as model. Next, the to_upper_copy() function expects a locale, the default is std::locale() which is ... Default constructor. Constructs a copy of the global C++ locale, which is the locale most recently used as the argument to std::locale::global or a copy of std::locale::classic if no call to std::locale::global has been made. Not sure if that is what we want. The generated code should not depend on a particular system locale.",-0.07,-0.0875,neutral
thrift,3040,comment_1,"Committed, thanks for the patch! Note, our Bower solution is a mess (cloning the entire thrift repo) and may change in the future. Also the src dir thrift.js may end up broken into multiple files, with thrift.js only being available in lib/js/dist after a grunt build. In the mean time this patch is a needed fix.",code_debt,complex_code,"Sun, 15 Mar 2015 11:27:22 +0000","Tue, 21 Jul 2015 02:21:18 +0000","Sun, 15 Mar 2015 15:41:09 +0000",15227,"Committed, thanks for the patch! Note, our Bower solution is a mess (cloning the entire thrift repo) and may change in the future. Also the src dir thrift.js may end up broken into multiple files, with thrift.js only being available in lib/js/dist after a grunt build. In the mean time this patch is a needed fix.",0.025,0.025,negative
thrift,3047,comment_1,I didn't include this in THRIFT-3041 because it made the generated sources exceedingly difficult to compare.,design_debt,non-optimal_design,"Thu, 19 Mar 2015 05:03:56 +0000","Thu, 30 Apr 2015 18:57:11 +0000","Thu, 16 Apr 2015 20:15:46 +0000",2473910,I didn't include this in THRIFT-3041 because it made the generated sources exceedingly difficult to compare.,-0.2,-0.2,negative
thrift,3191,comment_0,"Linking a few things together. Given the lack of test infrastructure for perl, I will be fixing these at the same time. I am updating ""make cross"" for THRIFT-3053 to support perl client and server, ssl or no ssl. Getting that working required me to solve THRIFT-3189 and THRIFT-3191 along the way.",test_debt,lack_of_tests,"Tue, 16 Jun 2015 20:20:02 +0000","Tue, 25 Aug 2015 04:44:52 +0000","Thu, 30 Jul 2015 13:10:22 +0000",3775820,"Linking a few things together. Given the lack of test infrastructure for perl, I will be fixing these at the same time. I am updating ""make cross"" for THRIFT-3053 to support perl client and server, ssl or no ssl. Getting that working required me to solve THRIFT-3189 and THRIFT-3191 along the way.",0.1,0.1,neutral
thrift,3241,comment_1,"It is by design, and is considered a production issue. 1. Run with X memory. 2. Max out the load, and observe memory use. 3. Use less memory the next run if there is significant unused memory, more if they are OOM'ing. 4. If you cannot raise memory, lower the concurrency per server. Steps 1-3 are completely automated in Google production systems and elsewhere. So in Thrift what's important is exposing the concurrency limits in all pool/async implementations. In Go LimitListener is useful.",design_debt,non-optimal_design,"Mon, 13 Jul 2015 09:44:58 +0000","Fri, 31 Jul 2015 21:41:16 +0000","Sun, 26 Jul 2015 00:31:05 +0000",1089967,"It is by design, and is considered a production issue. 1. Run with X memory. 2. Max out the load, and observe memory use. 3. Use less memory the next run if there is significant unused memory, more if they are OOM'ing. 4. If you cannot raise memory, lower the concurrency per server. Steps 1-3 are completely automated in Google production systems and elsewhere. So in Thrift what's important is exposing the concurrency limits in all pool/async implementations. In Go LimitListener is useful.",0.05833333333,0.05833333333,neutral
thrift,3274,comment_1,"Better, but not perfect. The tutorial seems to suffer from the same problems:",documentation_debt,low_quality_documentation,"Tue, 28 Jul 2015 20:52:27 +0000","Sun, 29 Nov 2015 15:52:49 +0000","Tue, 3 Nov 2015 22:41:47 +0000",8473760,"Better, but not perfect. The tutorial seems to suffer from the same problems:",-0.2,-0.2,negative
thrift,3276,comment_1,"I haven't, but neither TBase64Utils.java nor TJSONProtocol.java have changed in any relevant way since 0.9.1. I suspect this was just never tested against a client that sends padded base64 strings.",test_debt,lack_of_tests,"Wed, 29 Jul 2015 18:11:53 +0000","Wed, 6 Jan 2016 02:06:15 +0000","Sat, 10 Oct 2015 23:08:36 +0000",6325003,"I haven't, but neither TBase64Utils.java nor TJSONProtocol.java have changed in any relevant way since 0.9.1. I suspect this was just never tested against a client that sends padded base64 strings.",-0.095375,-0.095375,negative
thrift,3320,comment_4,"Hi , Thanks for your comments. At present, thrift doesn't not offer users a pure and complete asynchronous interface suite. Developers cannot write PDUs to a transport concurrently. Assuming that thrift has thread-safe transports and a set of asynchronous Interfaces in future releases, how to deserialize *_results from transports if are leveraged? In my proposal, we can get service names after ""TMessage""s are parsed. With service names and procedure names, specific instances of *_results can be selected to read data from transport. If we just store the IDs of requests, a mapping between id and *_result shall be maintained. And this is not elegant.",design_debt,non-optimal_design,"Wed, 9 Sep 2015 02:46:17 +0000","Sun, 10 Feb 2019 12:57:02 +0000","Sun, 10 Feb 2019 12:57:02 +0000",108036645,"Hi jking3, Thanks for your comments. At present, thrift doesn't not offer users a pure and complete asynchronous interface suite. Developers cannot write PDUs to a transport concurrently. Assuming that thrift has thread-safe transports and a set of asynchronous Interfaces in future releases, how to deserialize *_results from transports if ""TMultiplexedProtocol""s are leveraged? In my proposal, we can get service names after ""TMessage""s are parsed. With service names and procedure names, specific instances of *_results can be selected to read data from transport. If we just store the IDs of requests, a mapping between id and *_result shall be maintained. And this is not elegant.",-0.08483333333,-0.08483333333,negative
thrift,3413,comment_0,"The attached files and are enriched test cases, as there are some more issues.",test_debt,low_coverage,"Mon, 9 Nov 2015 23:36:08 +0000","Fri, 18 Mar 2016 17:54:29 +0000","Fri, 18 Mar 2016 17:54:29 +0000",11211501,"The attached files base.thrift and extended.thrift are enriched test cases, as there are some more issues.",0.0,0.0,neutral
thrift,3415,comment_1,I'll do some scans with clang's iwyu and clean a little bit around in THeader* related includes,code_debt,low_quality_code,"Wed, 11 Nov 2015 16:04:45 +0000","Wed, 6 Jan 2016 02:06:10 +0000","Thu, 12 Nov 2015 15:39:03 +0000",84858,I'll do some scans with clang's iwyu (include-what-you-use) and clean a little bit around in THeader* related includes,0.4,0.4,neutral
thrift,3419,comment_5,"The release cycle for thrift seems to be quite long. Given that this bug renders the plugin unusable, it may make sense to release this soon rather that waiting until the next thrift release. Its easy enough to build one's own released version of the plugin, but having to do so adds an extra point of friction to getting started with a tool that's already short on documentation.",documentation_debt,low_quality_documentation,"Thu, 12 Nov 2015 05:07:21 +0000","Sat, 20 Feb 2021 15:28:06 +0000","Thu, 17 Jan 2019 19:54:34 +0000",100450033,"The release cycle for thrift seems to be quite long. Given that this bug renders the plugin unusable, it may make sense to release this soon rather that waiting until the next thrift release. Its easy enough to build one's own released version of the plugin, but having to do so adds an extra point of friction to getting started with a tool that's already short on documentation.",0.1486666667,0.1486666667,negative
thrift,3447,comment_2,"I've looked a bit more into this and written a very stripped down test client and server using your excerpt as a starting point on the server. My client opens the transport but doesn't actually make any RPC calls to the Server. Looking at Parallel Stacks after stopping the server thread I can see that the client threads will not notice the stop request until either the underlying System.Net.Socket (In this particular case) receives at least one byte, or times out (which I think is probably a really long time in TCP). Just for reference - The server's Serve() thread does complete, so it should definitely not accept any new client connections. However, the client threads will stay blocked until the above scenario. Unfortunately as Thrift is targeted at .NET 3.5 the use of CancellationToken isn't available until .NET 4 which is a pity as that's quite a nice way to handle such things... It'll have to be resolved another way. I'll have a look at this further tomorrow to see if I can suggest an elegant solution. I'm most concerned about is whether there's a neat way to do this so that it doesn't cause issues with Transports which are not based on Sockets. Given that there are at least some similarities with the C++ case (I've not read it in full detail), it may be prudent to implement a similar solution.",design_debt,non-optimal_design,"Wed, 25 Nov 2015 12:27:12 +0000","Thu, 3 Jan 2019 03:31:15 +0000","Tue, 27 Feb 2018 17:55:11 +0000",71299679,"I've looked a bit more into this and written a very stripped down test client and server using your excerpt as a starting point on the server. My client opens the transport but doesn't actually make any RPC calls to the Server. Looking at Parallel Stacks after stopping the server thread I can see that the client threads will not notice the stop request until either the underlying System.Net.Socket (In this particular case) receives at least one byte, or times out (which I think is probably a really long time in TCP). Just for reference - The server's Serve() thread does complete, so it should definitely not accept any new client connections. However, the client threads will stay blocked until the above scenario. Unfortunately as Thrift is targeted at .NET 3.5 the use of CancellationToken isn't available until .NET 4 which is a pity as that's quite a nice way to handle such things... It'll have to be resolved another way. I'll have a look at this further tomorrow to see if I can suggest an elegant solution. I'm most concerned about is whether there's a neat way to do this so that it doesn't cause issues with Transports which are not based on Sockets. Given that there are at least some similarities with the C++ case (I've not read it in full detail), it may be prudent to implement a similar solution.",-0.04874358974,-0.04874358974,neutral
thrift,3447,comment_3,"This boils down to the use of Blocking Sockets in Thrift. It's the same problem described by many other people outside the thrift world, that you can't interrupt a socket whilst it's blocked. The unanimous answer for C# in general is to use non-blocking sockets. A work-around to the behaviour might be to get clients to regularly call a void function (such as defining a poll() function in the .thrift service definition). However this is wasteful of bandwidth and I only mention is as a possible way to achieve the desired behaviour prior to the existence of a code patch. I think this will require some significant coding effort. If I can find some time I'll look into adapting Thrift's C# lib to use async sockets - I'd welcome a 2nd opinion on this.",design_debt,non-optimal_design,"Wed, 25 Nov 2015 12:27:12 +0000","Thu, 3 Jan 2019 03:31:15 +0000","Tue, 27 Feb 2018 17:55:11 +0000",71299679,"This boils down to the use of Blocking Sockets in Thrift. It's the same problem described by many other people outside the thrift world, that you can't interrupt a socket whilst it's blocked. The unanimous answer for C# in general is to use non-blocking sockets. A work-around to the behaviour might be to get clients to regularly call a void function (such as defining a poll() function in the .thrift service definition). However this is wasteful of bandwidth and I only mention is as a possible way to achieve the desired behaviour prior to the existence of a code patch. I think this will require some significant coding effort. If I can find some time I'll look into adapting Thrift's C# lib to use async sockets - I'd welcome a 2nd opinion on this.",0.2213333333,0.2213333333,neutral
thrift,3572,comment_0,"I have a fix, though I think there must be a simpler one. In any case, it does amend the build; I'll make a pull request after testing with THRIFT-3573 to make sure nothing new pops up.",code_debt,complex_code,"Wed, 20 Jan 2016 20:42:02 +0000","Sat, 20 Feb 2021 15:27:01 +0000","Sat, 23 Jan 2016 16:28:45 +0000",244003,"I have a fix, though I think there must be a simpler one. In any case, it does amend the build; I'll make a pull request after testing with THRIFT-3573 to make sure nothing new pops up.",0.25,0.25,neutral
thrift,3605,comment_2,"A few comments: - A few generators did not document all switches, some did not dcoument any switches at all. I fixed what I found. - The Python generator stood out form the crowd by being the least maintainable one. I tried hard to no break anything and to mimic the existing behaviour to the best of my knowledge, but it was a hard task. If, despite of my efforts, I still managed to overlooked some obscure edge case combination, please bear with me.",code_debt,complex_code,"Mon, 8 Feb 2016 21:53:44 +0000","Fri, 19 Feb 2016 02:17:59 +0000","Sun, 14 Feb 2016 10:11:28 +0000",476264,"A few comments: A few generators did not document all switches, some did not dcoument any switches at all. I fixed what I found. The Python generator stood out form the crowd by being the least maintainable one. I tried hard to no break anything and to mimic the existing behaviour to the best of my knowledge, but it was a hard task. If, despite of my efforts, I still managed to overlooked some obscure edge case combination, please bear with me.",-0.1355625,-0.10845,negative
thrift,3617,comment_1,"Does not have much to do with the ticket itself, but is there any specific reason why we should not sort that list alphabetically? Makes it much easier to compare things (like forgotten targets). $0,02 JensG",code_debt,low_quality_code,"Fri, 12 Feb 2016 15:35:40 +0000","Fri, 19 Feb 2016 02:17:39 +0000","Sat, 13 Feb 2016 14:26:49 +0000",82269,"Does not have much to do with the ticket itself, but is there any specific reason why we should not sort that list alphabetically? Makes it much easier to compare things (like forgotten targets). $0,02 JensG",-0.06666666667,-0.06666666667,neutral
thrift,3619,comment_3,"This workaround will break gtest on OSX with clang libc++. Using C\+\+11 should have already implied but manually setting this macro to {{0}} will just make gtest skip checking C\+\+11 and include {{tr1/tuple}} rather than {{tuple}}, which seems like a bug in gtest, as I see it. See {{tr1/tuple}} works for libstdc++ even on C\+\+11, but not for libc++. I think thrift should detect C\+\+11 and switch to the non-tr1-prefixed headers when available, just like what gtest does, and not depend on the odd behavior of",code_debt,low_quality_code,"Fri, 12 Feb 2016 18:54:01 +0000","Mon, 22 Feb 2016 13:17:21 +0000","Thu, 18 Feb 2016 22:04:34 +0000",529833,"This workaround will break gtest on OSX with clang libc++. Using C++11 should have already implied GTEST_USE_OWN_TR1_TUPLE=0, but manually setting this macro to 0 will just make gtest skip checking C++11 and include tr1/tuple rather than tuple, which seems like a bug in gtest, as I see it. See this. tr1/tuple works for libstdc++ even on C++11, but not for libc++. I think thrift should detect C++11 and switch to the non-tr1-prefixed headers when available, just like what gtest does, and not depend on the odd behavior of GTEST_USE_OWN_TR1_TUPLE.",0.375,0.3,neutral
thrift,3619,comment_4," yes, see what I had to do for Parquet: It would be nice if Thrift would not pass on this burden to thirdparty users, but in the meantime we are stuck with the released 0.9.2 and 0.9.3 versions where this workaround is necessary.",design_debt,non-optimal_design,"Fri, 12 Feb 2016 18:54:01 +0000","Mon, 22 Feb 2016 13:17:21 +0000","Thu, 18 Feb 2016 22:04:34 +0000",529833,"richardtsai  yes, see what I had to do for Parquet: https://github.com/apache/parquet-cpp/blob/master/CMakeLists.txt#L115 It would be nice if Thrift would not pass on this burden to thirdparty users, but in the meantime we are stuck with the released 0.9.2 and 0.9.3 versions where this workaround is necessary.",0.085,0.085,neutral
thrift,3619,comment_1,"Ah, I misread Mark's comment, apologies. Let me verify in the Parquet build environment and I will report back to confirm (and add this to the developer documentation).",documentation_debt,low_quality_documentation,"Fri, 12 Feb 2016 18:54:01 +0000","Mon, 22 Feb 2016 13:17:21 +0000","Thu, 18 Feb 2016 22:04:34 +0000",529833,"Ah, I misread Mark's comment, apologies. Let me verify in the Parquet build environment and I will report back to confirm (and add this to the developer documentation).",-0.1,-0.1,neutral
thrift,3864,comment_0,"If large block allocation or garbage collection is not efficient in golang, that sounds like a language issue. One is not supposed to have to think about heap or stack in go, from what I understand (I am by no means an expert). The proposed solution sounds like it would create a cyclic dependency.",architecture_debt,violation_of_modularity,"Mon, 27 Jun 2016 14:23:22 +0000","Thu, 3 Jan 2019 14:33:44 +0000","Thu, 3 Jan 2019 14:33:44 +0000",79488622,"If large block allocation or garbage collection is not efficient in golang, that sounds like a language issue. One is not supposed to have to think about heap or stack in go, from what I understand (I am by no means an expert). The proposed solution sounds like it would create a cyclic dependency.",-0.2291666667,-0.2291666667,negative
thrift,3864,comment_1,"Let me put it this way: From what I know the GC is Java is by no means the 100% perfect solution either. So if one knows about an yet unfixed issue in the underlying ecosystem, having at least a workaround at hand sounds not so bad to me. Whether it is Java or Go or anything else. So from my point of view iff we can have a * working solution, * that is more efficent in terms of throughput * and does not introduce other problems then (and only then) we should have a look at it. Now the big question: Is this the case?",code_debt,slow_algorithm,"Mon, 27 Jun 2016 14:23:22 +0000","Thu, 3 Jan 2019 14:33:44 +0000","Thu, 3 Jan 2019 14:33:44 +0000",79488622,"Let me put it this way: From what I know the GC is Java is by no means the 100% perfect solution either. So if one knows about an yet unfixed issue in the underlying ecosystem, having at least a workaround at hand sounds not so bad to me. Whether it is Java or Go or anything else. So from my point of view iff we can have a working solution, that is more efficent in terms of throughput and does not introduce other problems then (and only then) we should have a look at it. Now the big question: Is this the case?",0.03,0.03,negative
thrift,3864,comment_3,"There is no clean way to implement this without a fairly ugly BC break. Go's garbage collector has also improved dramatically since this issue was opened, so I'm going to close this. If anyone still has concerns or ideas, please reopen.",design_debt,non-optimal_design,"Mon, 27 Jun 2016 14:23:22 +0000","Thu, 3 Jan 2019 14:33:44 +0000","Thu, 3 Jan 2019 14:33:44 +0000",79488622,"There is no clean way to implement this without a fairly ugly BC break. Go's garbage collector has also improved dramatically since this issue was opened, so I'm going to close this. If anyone still has concerns or ideas, please reopen.",0.2579444444,0.2579444444,negative
thrift,3957,comment_4,"Okay, I fixed up the pull request that's outstanding. Sadly, the documentation on TSocket.h isn't very descriptive about setting the timeout and the implications of such. I've seen looping code in TSocket that revolves around TIMED_OUT however my guess is that it existed in the past to attempt to give the socket thread(s) a chance to unblock from a poll, select, or recv call and check to see if the server has been stopped. That's no longer needed since 0.9.3 because of the interruptable socket implementation, so this change works for me. The code that was in TConnectedClient was pulled from the three different server implementations and I believe all of them used to ignore TIMED_OUT so this could cause a behavioral difference in future versions that folks may not expect, however it is ""more correct"".",documentation_debt,low_quality_documentation,"Thu, 3 Nov 2016 14:26:38 +0000","Mon, 12 Dec 2016 18:02:35 +0000","Thu, 10 Nov 2016 21:03:25 +0000",628607,"Okay, I fixed up the pull request that's outstanding. Sadly, the documentation on TSocket.h isn't very descriptive about setting the timeout and the implications of such. I've seen looping code in TSocket that revolves around TIMED_OUT however my guess is that it existed in the past to attempt to give the socket thread(s) a chance to unblock from a poll, select, or recv call and check to see if the server has been stopped. That's no longer needed since 0.9.3 because of the interruptable socket implementation, so this change works for me. The code that was in TConnectedClient was pulled from the three different server implementations and I believe all of them used to ignore TIMED_OUT so this could cause a behavioral difference in future versions that folks may not expect, however it is ""more correct"".",0.2940833333,0.2940833333,neutral
thrift,4048,comment_0,"As support for the older cocoa compiler and library have been removed (see THRIFT-4719), all of the issues in Jira related to that code have also been removed. For legacy cocoa support you can use version 0.12.0 - everyone is expected to move to swift if they want to use the next release of Thrift.",architecture_debt,using_obsolete_technology,"Sun, 29 Jan 2017 14:57:45 +0000","Thu, 10 Oct 2019 23:00:08 +0000","Mon, 14 Jan 2019 15:03:35 +0000",61776350,"As support for the older cocoa compiler and library have been removed (see THRIFT-4719), all of the issues in Jira related to that code have also been removed. For legacy cocoa support you can use version 0.12.0 - everyone is expected to move to swift if they want to use the next release of Thrift.",0.3666666667,0.3666666667,neutral
thrift,4164,comment_2,"PR #1235 exposed the fact that before that PR, we were quietly stopping openssl from using the locking callbacks we were providing. This would lead to unsafe multi-thread behavior during shutdown. I looked at guaranteeing this in core: I noodled through it and the solutions were all a bit ugly, involving the factory to keep a smart pointer of each socket it made, and a callback from TSSLSocket::close() to the factory to take it out of a set of sockets it had made. Then in ~TSSLSocketFactory we would need to guarantee the set of sockets left in the factory were the last instance (nobody else was holding onto them as well) so we could guarantee that when the set of sockets is reset, all the sockets the factory ever made were released.Then the factory could clean up openssl safely. OR, we clearly document the requirement that a factory must outlive any sockets it makes. This is what I'm going to do, so will revise the PR to fix our own TestClient.cpp which was in violation of the requirement that a TSSLSocketFactory must outlive any TSSLSocket it creates when the TSSLSocketFactory is managing openssl initialization and destruction, which was causing the build failures.",requirement_debt,non-functional_requirements_not_fully_satisfied,"Mon, 3 Apr 2017 14:24:40 +0000","Thu, 14 Dec 2017 13:55:39 +0000","Tue, 4 Apr 2017 13:37:13 +0000",83553,"PR #1235 exposed the fact that before that PR, we were quietly stopping openssl from using the locking callbacks we were providing. This would lead to unsafe multi-thread behavior during shutdown. I looked at guaranteeing this in core: I noodled through it and the solutions were all a bit ugly, involving the factory to keep a smart pointer of each socket it made, and a callback from TSSLSocket::close() to the factory to take it out of a set of sockets it had made. Then in ~TSSLSocketFactory we would need to guarantee the set of sockets left in the factory were the last instance (nobody else was holding onto them as well) so we could guarantee that when the set of sockets is reset, all the sockets the factory ever made were released.Then the factory could clean up openssl safely. OR, we clearly document the requirement that a factory must outlive any sockets it makes. This is what I'm going to do, so will revise the PR to fix our own TestClient.cpp which was in violation of the requirement that a TSSLSocketFactory must outlive any TSSLSocket it creates when the TSSLSocketFactory is managing openssl initialization and destruction, which was causing the build failures.",-0.01329166667,-0.01329166667,negative
thrift,4308,comment_2,"Switching to ld.gold exposed other issues (some good, some bad) and I'm not sure I can use it on everything yet.",design_debt,non-optimal_design,"Mon, 4 Sep 2017 17:57:27 +0000","Thu, 27 Dec 2018 15:24:55 +0000","Tue, 30 Jan 2018 13:00:19 +0000",12769372,"Switching to ld.gold exposed other issues (some good, some bad) and I'm not sure I can use it on everything yet.",0.047,0.047,negative
thrift,4362,comment_1,"Do you think I can submit this patch as-is or do you have any suggestions for improvement? Edit: As mentioned, with my patch the size may be checked multiple times. So I think it's a good idea to remove the other calls to the size-check method.",design_debt,non-optimal_design,"Tue, 10 Oct 2017 18:05:00 +0000","Thu, 14 Dec 2017 13:55:02 +0000","Wed, 25 Oct 2017 12:42:05 +0000",1276625,"Do you think I can submit this patch as-is or do you have any suggestions for improvement? Edit: As mentioned, with my patch the size may be checked multiple times. So I think it's a good idea to remove the other calls to the size-check method.",0.4393333333,0.4393333333,neutral
thrift,4437,comment_1,"This issue would have been caught by the `ThriftWS` test suite, which is not enabled for some reason. I will add a commit to my PR that enables these tests.",test_debt,lack_of_tests,"Wed, 27 Dec 2017 11:08:19 +0000","Thu, 27 Dec 2018 15:25:16 +0000","Thu, 28 Dec 2017 13:03:35 +0000",93316,"This issue would have been caught by the `ThriftWS` test suite, which is not enabled for some reason. I will add a commit to my PR that enables these tests.",0.1,0.1,neutral
thrift,4468,comment_0,"I don't see this patch as part of the Thrift library. The entire Console construct is basically only a helper that was introduced as part of the initial development. Since the model was the C# library, the original author introduced that class, very likely to reduce dependencies. The implementation is rather raw (read: shitty), and - in the case of the real console it is not safe against concurrent accesses. If asked, I would rather throw it out instead of polishing it and adding features. We are dealing with RPC here, not with providing cool console output stuff. Long story short: I'm against it. -1",code_debt,slow_algorithm,"Mon, 22 Jan 2018 14:29:43 +0000","Thu, 27 Dec 2018 15:24:53 +0000","Thu, 25 Jan 2018 23:16:49 +0000",290826,"I don't see this patch as part of the Thrift library. The entire Console construct is basically only a helper that was introduced as part of the initial development. Since the model was the C# library, the original author introduced that class, very likely to reduce dependencies. The implementation is rather raw (read: shitty), and - in the case of the real console it is not safe against concurrent accesses. If asked, I would rather throw it out instead of polishing it and adding features. We are dealing with RPC here, not with providing cool console output stuff. Long story short: I'm against it. -1",-0.02552083333,-0.02552083333,negative
thrift,4468,comment_1,"OK. With this approach, this class is really better to remove. Its name is misleading about thread safety",requirement_debt,non-functional_requirements_not_fully_satisfied,"Mon, 22 Jan 2018 14:29:43 +0000","Thu, 27 Dec 2018 15:24:53 +0000","Thu, 25 Jan 2018 23:16:49 +0000",290826,"OK.With this approach, this class is really better to remove. Its name is misleading about thread safety",0.325,0.325,negative
thrift,4546,comment_3,"Part of the reason that the thrift.apache.org site is so woefully under-maintained is that it uses a content manage system from the last century. I have to recommend that we move all our development related content to markdown files within the GitHub repository so that we have better control over it. We should be able to teach GitHub and/or the CI build systems that if the only changes are to a documentation directly, not to do a build.",architecture_debt,using_obsolete_technology,"Fri, 6 Apr 2018 23:04:12 +0000","Fri, 28 Dec 2018 13:06:40 +0000","Fri, 28 Dec 2018 13:06:34 +0000",22946542,"Part of the reason that the thrift.apache.org site is so woefully under-maintained is that it uses a content manage system from the last century. I have to recommend that we move all our development related content to markdown files within the GitHub repository so that we have better control over it. We should be able to teach GitHub and/or the CI build systems that if the only changes are to a documentation directly, not to do a build.",0.2851333333,0.2851333333,negative
thrift,4616,comment_0,AIX is not an environment we can currently test/verify in our Continuous Integration environment. You will need to make changes like the one your already found in order to use it. I apologize that we cannot assist you further but we have no access to an AIX environment. Perhaps someone on the thrift user mailing list could be of more assistance? It sounds like we're missing headers in some places to cover what's needed. I would prefer to see changes to headers rather than an pre-include.,build_debt,build_others,"Thu, 9 Aug 2018 15:19:42 +0000","Thu, 9 Aug 2018 22:18:51 +0000","Thu, 9 Aug 2018 22:18:06 +0000",25104,AIX is not an environment we can currently test/verify in our Continuous Integration environment. You will need to make changes like the one your already found in order to use it. I apologize that we cannot assist you further but we have no access to an AIX environment. Perhaps someone on the thrift user mailing list could be of more assistance? It sounds like we're missing headers in some places to cover what's needed. I would prefer to see changes to headers rather than an pre-include.,-0.07291666667,-0.07291666667,negative
thrift,4857,comment_3,"All right, I just created a [pull Let me know if I missed anything. The contribution instructions weren't clear whether the language indication (e.g. {{java}}) in the pull request should be uppercase or lowercase. I saw both in historical commits. As the official contribution instructions showed e.g. {{perl}}, I guess that you want to follow the token form used in the {{thrift --gen}} CLI, so I went with {{java}} rather than {{Java}} in the commit message. I have my own opinions about approaches to generating hash codes (doing it manually is tedious and error-prone), and I could quibble about the approach to checking for class equivalence. For example, the equality check compares exact classes, which the author apparently thought would be more efficient, but this will break if anyone ever subclasses {{TField}}. Either someone should make {{TField}} a {{final}} class, or use a normal {{instanceof}} in comparison. But these issues are ancillary to the core bug here, so I tried to make my change as surgical as possible, especially as this is my first contribution. Let me know what you think!",design_debt,non-optimal_design,"Fri, 3 May 2019 16:47:14 +0000","Wed, 16 Oct 2019 22:26:37 +0000","Mon, 13 May 2019 20:54:17 +0000",878823,"All right, I just created a pull request. Let me know if I missed anything. The contribution instructions weren't clear whether the language indication (e.g. java) in the pull request should be uppercase or lowercase. I saw both in historical commits. As the official contribution instructions showed e.g. perl, I guess that you want to follow the token form used in the thrift --gen CLI, so I went with java rather than Java in the commit message. I have my own opinions about approaches to generating hash codes (doing it manually is tedious and error-prone), and I could quibble about the approach to checking for class equivalence. For example, the equality check compares exact classes, which the author apparently thought would be more efficient, but this will break if anyone ever subclasses TField. Either someone should make TField a final class, or use a normal instanceof in comparison. But these issues are ancillary to the core bug here, so I tried to make my change as surgical as possible, especially as this is my first contribution. Let me know what you think!",0.1394074074,0.1429666667,neutral
thrift,4862,comment_0,"NB. Slight compilcation comes from the rather unfortunate (but documented) fact, that [Delphi generates RTTI info only for some specific types of Hence, in some cases the values can be printed as names, but in others we still have to print the numeric value.",design_debt,non-optimal_design,"Thu, 9 May 2019 21:58:50 +0000","Wed, 20 Nov 2019 02:04:34 +0000","Fri, 10 May 2019 22:01:44 +0000",86574,"NB. Slight compilcation comes from the rather unfortunate (but documented) fact, thatDelphi generates RTTI info only for some specific types of enums. Hence, in some cases the values can be printed as names, but in others we still have to print the numeric value.",-0.05475,-0.0365,negative
